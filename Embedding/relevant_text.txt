Toggle the table of contents Earth observation From Wikipedia, the free encyclopedia Information about the Earth environment, remote or in situ Earth observation (EO) is the gathering of information about the physical, chemical, and biological systems of the planet Earth . [1] It can be performed via remote-sensing technologies ( Earth observation satellites ) or through direct-contact sensors in ground-based or airborne platforms (such as weather stations and weather balloons , for example). [2] [3] According to the Group on Earth Observations (GEO), the concept encompasses both " space-based or remotely-sensed data, as well as ground-based or in situ data ". [4] Earth observation is used to monitor and assess the status of and changes in natural and built environments . [1] Terminology[ edit ] In Europe, Earth observation has often been used to refer to satellite-based remote sensing, [1] but the term is also used to refer to any form of observations of the Earth system, including in situ and airborne observations, for example. The GEO, which has over 100 member countries and over 100 participating organizations, uses EO in this broader sense. [4] In the US, the term remote sensing was used since the 1960s [5] to refer to satellite-based remote sensing. Remote sensing has also been used more broadly for observations using any form of remote sensing technology, including airborne sensors and even ground-based sensors such as cameras. [5] Perhaps the least ambiguous term to use for satellite-based sensors is satellite remote sensing (SRS), an acronym which is gradually starting to appear in the literature. [5] [6]
Toggle the table of contents Remote sensing This is the latest accepted revision , reviewed on 8 April 2024. Acquisition of information at a significant distance from the subject Not to be confused with remote viewing . Synthetic aperture radar image of Death Valley colored using polarimetry Remote sensing is the acquisition of information about an object or phenomenon without making physical contact with the object, in contrast to in situ or on-site observation . The term is applied especially to acquiring information about Earth and other planets . Remote sensing is used in numerous fields, including geophysics , geography , land surveying and most Earth science disciplines (e.g. exploration geophysics , hydrology , ecology , meteorology , oceanography , glaciology , geology ). It also has military, intelligence, commercial, economic, planning, and humanitarian applications, among others. In current usage, the term remote sensing generally refers to the use of satellite - or aircraft-based sensor technologies to detect and classify objects on Earth. It includes the surface and the atmosphere and oceans , based on propagated signals (e.g. electromagnetic radiation ). It may be split into "active" remote sensing (when a signal is emitted by a satellite or aircraft to the object and its reflection is detected by the sensor) and "passive" remote sensing (when the reflection of sunlight is detected by the sensor). [1] [2] [3] [4] Overview[ edit ] This video is about how Landsat was used to identify areas of conservation in the Democratic Republic of the Congo , and how it was used to help map an area called MLW in the north. Remote sensing can be divided into two types of methods: Passive remote sensing and Active remote sensing. Passive sensors gather radiation that is emitted or reflected by the object or surrounding areas. Reflected sunlight is the most common source of radiation measured by passive sensors. Examples of passive remote sensors include film photography , infrared , charge-coupled devices , and radiometers . Active collection, on the other hand, emits energy in order to scan objects and areas whereupon a sensor then detects and measures the radiation that is reflected or backscattered from the target. RADAR and LiDAR are examples of active remote sensing where the time delay between emission and return is measured, establishing the location, speed and direction of an object. Illustration of remote sensing Remote sensing makes it possible to collect data of dangerous or inaccessible areas. Remote sensing applications include monitoring deforestation in areas such as the Amazon Basin , glacial features in Arctic and Antarctic regions, and depth sounding of coastal and ocean depths. Military collection during the Cold War made use of stand-off collection of data about dangerous border areas. Remote sensing also replaces costly and slow data collection on the ground, ensuring in the process that areas or objects are not disturbed. Orbital platforms collect and transmit data from different parts of the electromagnetic spectrum , which in conjunction with larger scale aerial or ground-based sensing and analysis, provides researchers with enough information to monitor trends such as El Niño and other natural long and short term phenomena. Other uses include different areas of the earth sciences such as natural resource management , agricultural fields such as land usage and conservation, [5] [6] greenhouse gas monitoring , [7] oil spill detection and monitoring, [8] and national security and overhead, ground-based and stand-off collection on border areas. [9] Types of data acquisition techniques[ edit ] The basis for multispectral collection and analysis is that of examined areas or objects that reflect or emit radiation that stand out from surrounding areas. For a summary of major remote sensing satellite systems see the overview table. Applications of remote sensing[ edit ] Radar image of Aswan Dam, Egypt taken by Umbra Conventional radar is mostly associated with aerial traffic control, early warning, and certain large-scale meteorological data. Doppler radar is used by local law enforcements' monitoring of speed limits and in enhanced meteorological collection such as wind speed and direction within weather systems in addition to precipitation location and intensity. Other types of active collection includes plasmas in the ionosphere . Interferometric synthetic aperture radar is used to produce precise digital elevation models of large scale terrain (See RADARSAT , TerraSAR-X , Magellan ). Laser and radar altimeters on satellites have provided a wide range of data. By measuring the bulges of water caused by gravity, they map features on the seafloor to a resolution of a mile or so. By measuring the height and wavelength of ocean waves, the altimeters measure wind speeds and direction, and surface ocean currents and directions. Ultrasound (acoustic) and radar tide gauges measure sea level, tides and wave direction in coastal and offshore tide gauges. Light detection and ranging (LIDAR) is well known in examples of weapon ranging, laser illuminated homing of projectiles. LIDAR is used to detect and measure the concentration of various chemicals in the atmosphere, while airborne LIDAR can be used to measure the heights of objects and features on the ground more accurately than with radar technology. Vegetation remote sensing is a principal application of LIDAR. [10] Radiometers and photometers are the most common instrument in use, collecting reflected and emitted radiation in a wide range of frequencies. The most common are visible and infrared sensors, followed by microwave, gamma-ray, and rarely, ultraviolet. They may also be used to detect the emission spectra of various chemicals, providing data on chemical concentrations in the atmosphere. Examples of remote sensing equipment deployed byor interfaced with oceanographic research vessels . [11] Radiometers are also used at night, because artificial light emissions are a key signature of human activity. [12] Applications include remote sensing of population, GDP, and damage to infrastructure from war or disasters. Radiometers and radar onboard of satellites can be used to monitor volcanic eruptions [13] [14] Spectropolarimetric Imaging has been reported to be useful for target tracking purposes by researchers at the U.S. Army Research Laboratory . They determined that manmade items possess polarimetric signatures that are not found in natural objects. These conclusions were drawn from the imaging of military trucks, like the Humvee , and trailers with their acousto-optic tunable filter dual hyperspectral and spectropolarimetric VNIR Spectropolarimetric Imager. [15] [16] Stereographic pairs of aerial photographs have often been used to make topographic maps by imagery and terrain analysts in trafficability and highway departments for potential routes, in addition to modelling terrestrial habitat features. [17] [18] [19] Simultaneous multi-spectral platforms such as Landsat have been in use since the 1970s. These thematic mappers take images in multiple wavelengths of electromagnetic radiation (multi-spectral) and are usually found on Earth observation satellites , including (for example) the Landsat program or the IKONOS satellite. Maps of land cover and land use from thematic mapping can be used to prospect for minerals, detect or monitor land usage, detect invasive vegetation, deforestation, and examine the health of indigenous plants and crops ( satellite crop monitoring ), including entire farming regions or forests. [20] Prominent scientists using remote sensing for this purpose include Janet Franklin and Ruth DeFries . Landsat images are used by regulatory agencies such as KYDOW to indicate water quality parameters including Secchi depth, chlorophyll density, and total phosphorus content. Weather satellites are used in meteorology and climatology. Hyperspectral imaging produces an image where each pixel has full spectral information with imaging narrow spectral bands over a contiguous spectral range. Hyperspectral imagers are used in various applications including mineralogy, biology, defence, and environmental measurements. Within the scope of the combat against desertification , remote sensing allows researchers to follow up and monitor risk areas in the long term, to determine desertification factors, to support decision-makers in defining relevant measures of environmental management, and to assess their impacts. [21] Remotely sensed multi- and hyperspectral images can be used for assessing biodiversity at different scales. Since the spectral properties of different plants species are unique, it is possible to get information about properties that relates to biodiversity such as habitat heterogeneity, spectral diversity and plant functional trait. [22] [23] [24] Remote sensing has been used to detect rare plants to aid in conservation efforts. Prediction, detection, and the ability to record biophysical conditions were possible from medium to very high resolutions. [25] Further information: Satellite geodesy Geodetic remote sensing can be gravimetric or geometric. Overhead gravity data collection was first used in aerial submarine detection. This data revealed minute perturbations in the Earth's gravitational field that may be used to determine changes in the mass distribution of the Earth, which in turn may be used for geophysical studies, as in GRACE . Geometric remote sensing includes position and deformation imaging using InSAR , LIDAR, etc. [27] Acoustic and near-acoustic[ edit ] Sonar : passive sonar, listening for the sound made by another object (a vessel, a whale etc.); active sonar, emitting pulses of sounds and listening for echoes, used for detecting, ranging and measurements of underwater objects and terrain. Seismograms taken at different locations can locate and measure earthquakes (after they occur) by comparing the relative intensity and precise timings. Ultrasound : Ultrasound sensors, that emit high-frequency pulses and listening for echoes, used for detecting water waves and water level, as in tide gauges or for towing tanks. To coordinate a series of large-scale observations, most sensing systems depend on the following: platform location and the orientation of the sensor. High-end instruments now often use positional information from satellite navigation systems . The rotation and orientation are often provided within a degree or two with electronic compasses. Compasses can measure not just azimuth (i. e. degrees to magnetic north), but also altitude (degrees above the horizon), since the magnetic field curves into the Earth at different angles at different latitudes. More exact orientations require gyroscopic-aided orientation , periodically realigned by different methods including navigation from stars or known benchmarks. Spectral resolution The wavelength of the different frequency bands recorded – usually, this is related to the number of frequency bands recorded by the platform. Current Landsat collection is that of seven bands, including several in the infrared spectrum, ranging from a spectral resolution of 0.7 to 2.1 μm. The Hyperion sensor on Earth Observing-1 resolves 220 bands from 0.4 to 2.5 μm, with a spectral resolution of 0.10 to 0.11 μm per band. Radiometric resolution The number of different intensities of radiation the sensor is able to distinguish. Typically, this ranges from 8 to 14 bits, corresponding to 256 levels of the gray scale and up to 16,384 intensities or "shades" of colour, in each band. It also depends on the instrument noise . Temporal resolution The frequency of flyovers by the satellite or plane, and is only relevant in time-series studies or those requiring an averaged or mosaic image as in deforesting monitoring. This was first used by the intelligence community where repeated coverage revealed changes in infrastructure, the deployment of units or the modification/introduction of equipment. Cloud cover over a given area or object makes it necessary to repeat the collection of said location. Data processing[ edit ] In order to create sensor-based maps, most remote sensing systems expect to extrapolate sensor data in relation to a reference point including distances between known points on the ground. This depends on the type of sensor used. For example, in conventional photographs, distances are accurate in the center of the image, with the distortion of measurements increasing the farther you get from the center. Another factor is that of the platen against which the film is pressed can cause severe errors when photographs are used to measure ground distances. The step in which this problem is resolved is called georeferencing and involves computer-aided matching of points in the image (typically 30 or more points per image) which is extrapolated with the use of an established benchmark, "warping" the image to produce accurate spatial data. As of the early 1990s, most satellite images are sold fully georeferenced. In addition, images may need to be radiometrically and atmospherically corrected. Radiometric correction Allows avoidance of radiometric errors and distortions. The illumination of objects on the Earth's surface is uneven because of different properties of the relief. This factor is taken into account in the method of radiometric distortion correction. [28] Radiometric correction gives a scale to the pixel values, e. g. the monochromatic scale of 0 to 255 will be converted to actual radiance values. Topographic correction (also called terrain correction) In rugged mountains, as a result of terrain, the effective illumination of pixels varies considerably. In a remote sensing image, the pixel on the shady slope receives weak illumination and has a low radiance value, in contrast, the pixel on the sunny slope receives strong illumination and has a high radiance value. For the same object, the pixel radiance value on the shady slope will be different from that on the sunny slope. Additionally, different objects may have similar radiance values. These ambiguities seriously affected remote sensing image information extraction accuracy in mountainous areas. It became the main obstacle to the further application of remote sensing images. The purpose of topographic correction is to eliminate this effect, recovering the true reflectivity or radiance of objects in horizontal conditions. It is the premise of quantitative remote sensing application. Atmospheric correction Elimination of atmospheric haze by rescaling each frequency band so that its minimum value (usually realised in water bodies) corresponds to a pixel value of 0. The digitizing of data also makes it possible to manipulate the data by changing gray-scale values. Interpretation is the critical process of making sense of the data. The first application was that of aerial photographic collection which used the following process; spatial measurement through the use of a light table in both conventional single or stereographic coverage, added skills such as the use of photogrammetry, the use of photomosaics, repeat coverage, Making use of objects' known dimensions in order to detect modifications. Image Analysis is the recently developed automated computer-aided application that is in increasing use. Object-Based Image Analysis (OBIA) is a sub-discipline of GIScience devoted to partitioning remote sensing (RS) imagery into meaningful image-objects, and assessing their characteristics through spatial, spectral and temporal scale. Old data from remote sensing is often valuable because it may provide the only long-term data for a large extent of geography. At the same time, the data is often complex to interpret, and bulky to store. Modern systems tend to store the data digitally, often with lossless compression . The difficulty with this approach is that the data is fragile, the format may be archaic, and the data may be easy to falsify. One of the best systems for archiving data series is as computer-generated machine-readable ultrafiche , usually in typefonts such as OCR-B , or as digitized half-tone images. Ultrafiches survive well in standard libraries, with lifetimes of several centuries. They can be created, copied, filed and retrieved by automated systems. They are about as compact as archival magnetic media, and yet can be read by human beings with minimal, standardized equipment. Generally speaking, remote sensing works on the principle of the inverse problem : while the object or phenomenon of interest (the state) may not be directly measured, there exists some other variable that can be detected and measured (the observation) which may be related to the object of interest through a calculation. The common analogy given to describe this is trying to determine the type of animal from its footprints. For example, while it is impossible to directly measure temperatures in the upper atmosphere, it is possible to measure the spectral emissions from a known chemical species (such as carbon dioxide) in that region. The frequency of the emissions may then be related via thermodynamics to the temperature in that region. Data processing levels[ edit ] To facilitate the discussion of data processing in practice, several processing "levels" were first defined in 1986 by NASA as part of its Earth Observing System [29] and steadily adopted since then, both internally at NASA (e. g., [30] ) and elsewhere (e. g., [31] ); these definitions are: Level Description 0 Reconstructed, unprocessed instrument and payload data at full resolution, with any and all communications artifacts (e. g., synchronization frames, communications headers, duplicate data) removed. 1a Reconstructed, unprocessed instrument data at full resolution, time-referenced, and annotated with ancillary information, including radiometric and geometric calibration coefficients and georeferencing parameters (e. g., platform ephemeris) computed and appended but not applied to the Level 0 data (or if applied, in a manner that level 0 is fully recoverable from level 1a data). 1b Level 1a data that have been processed to sensor units (e. g., radar backscatter cross section, brightness temperature, etc.); not all instruments have Level 1b data; level 0 data is not recoverable from level 1b data. 2 Derived geophysical variables (e. g., ocean wave height, soil moisture, ice concentration) at the same resolution and location as Level 1 source data. 3 Variables mapped on uniform spacetime grid scales, usually with some completeness and consistency (e. g., missing points interpolated, complete regions mosaicked together from multiple orbits, etc.). 4 Model output or results from analyses of lower level data (i. e., variables that were not measured by the instruments but instead are derived from these measurements). A Level 1 data record is the most fundamental (i. e., highest reversible level) data record that has significant scientific utility, and is the foundation upon which all subsequent data sets are produced. Level 2 is the first level that is directly usable for most scientific applications; its value is much greater than the lower levels. Level 2 data sets tend to be less voluminous than Level 1 data because they have been reduced temporally, spatially, or spectrally. Level 3 data sets are generally smaller than lower level data sets and thus can be dealt with without incurring a great deal of data handling overhead. These data tend to be generally more useful for many applications. The regular spatial and temporal organization of Level 3 datasets makes it feasible to readily combine data from different sources. While these processing levels are particularly suitable for typical satellite data processing pipelines, other data level vocabularies have been defined and may be appropriate for more heterogeneous workflows. Applications[ edit ] Satellite images provide very useful information to produce statistics on topics closely related to the territory, such as agriculture, forestry or land cover in general. The first large project to apply Landsata 1 images for statistics was LACIE (Large Area Crop Inventory Experiment), run by NASA, NOAA and the USDA in 1974–77. [32] [33] Many other application projects on crop area estimation have followed, including the Italian AGRIT project and the MARS project of the Joint Research Centre (JRC) of the European Commission. [34] Forest area and deforestation estimation have also been a frequent target of remote sensing projects [35] [36] , the same as land cover and land use [37] Groud truth or reference data to train and validate image classification require a field survey if we are targetting annual crops or individual forest species, but may be substituted by photointerpretation if we look at wider classes that can be reliably identified on aerial photos or satellite images. It is relevant to highlight that probabilistic sampling is not critical for the selection of training pixels for image classification, but it is necessary for accuracy assessment of the classified images and area estimation. [38] [39] [40] Additional care is recommended to ensure that training and validation datasets are not spatially correlated. [41] We suppose now that we have classified images or a land cover map produced by visual photo-interpretation, with a legend of mapped classes that suits our purpose, taking again the example of wheat. The straightforward approach is counting the number of pixels classified as wheat and multiplying by the area of each pixel. Many authors have noticed that estimator is that it is generally biased because commission and omission errors in a confusion matrix do not compensate each other [42] [43] [44] The main strength of classified satellite images or other indicators computed on satellite images is providing cheap information on the whole target area or most of it. This information usually has a good correlation with the target variable (ground truth) that is usually expensive to observe in an unbiased and accurate way. Therefore it can be observed on a probabilistic sample selected on an area sampling frame . Traditional survey methodology provides different methods to combine accurate information on a sample with less accurate, but exhaustive, data for a covariable or proxy that is cheaper to collect.  For agricultural statistics, field surveys are usually required, while photo-interpretation may better for land cover classes that can be reliably identified on aerial photographs or high resolution satellite images. Additional uncertainty can appear because of imperfect reference data (ground truth or similar). [45] [46]
Toggle the table of contents Earth observation satellite From Wikipedia, the free encyclopedia Satellite specifically designed to observe Earth from orbit Six Earth observation satellites comprising the A-train satellite constellation as of 2014. e An Earth observation satellite or Earth remote sensing satellite is a satellite used or designed for Earth observation (EO) from orbit , including spy satellites and similar ones intended for non-military uses such as environmental monitoring , meteorology , cartography and others. The most common type are Earth imaging satellites, that take satellite images , analogous to aerial photographs ; some EO satellites may perform remote sensing without forming pictures, such as in GNSS radio occultation . The first occurrence of satellite remote sensing can be dated to the launch of the first artificial satellite, Sputnik 1 , by the Soviet Union on October 4, 1957. [1] Sputnik 1 sent back radio signals, which scientists used to study the ionosphere . [2] The United States Army Ballistic Missile Agency launched the first American satellite, Explorer 1 , for NASA's Jet Propulsion Laboratory on January 31, 1958. The information sent back from its radiation detector led to the discovery of the Earth's Van Allen radiation belts . [3] The TIROS-1 spacecraft, launched on April 1, 1960, as part of NASA's Television Infrared Observation Satellite (TIROS) program, sent back the first television footage of weather patterns to be taken from space. [1] In 2008, more than 150 Earth observation satellites were in orbit, recording data with both passive and active sensors and acquiring more than 10 terabits of data daily. [1] By 2021, that total had grown to over 950, with the largest number of satellites operated by US-based company Planet Labs . [4] Most Earth observation satellites carry instruments that should be operated at a relatively low altitude. Most orbit at altitudes above 500 to 600 kilometers (310 to 370 mi). Lower orbits have significant air-drag , which makes frequent orbit reboost maneuvers necessary. The Earth observation satellites ERS-1, ERS-2 and Envisat of European Space Agency as well as the MetOp spacecraft of EUMETSAT are all operated at altitudes of about 800 km (500 mi). The Proba-1 , Proba-2 and SMOS spacecraft of European Space Agency are observing the Earth from an altitude of about 700 km (430 mi). The Earth observation satellites of UAE, DubaiSat-1 & DubaiSat-2 are also placed in Low Earth Orbits (LEO) orbits and providing satellite imagery of various parts of the Earth. [5] [6] To get global coverage with a low orbit, a polar orbit is used. A low orbit will have an orbital period of roughly 100 minutes and the Earth will rotate around its polar axis about 25° between successive orbits. The ground track moves towards the west 25° each orbit, allowing a different section of the globe to be scanned with each orbit. Most are in Sun-synchronous orbits . A geostationary orbit , at 36,000 km (22,000 mi), allows a satellite to hover over a constant spot on the earth since the orbital period at this altitude is 24 hours. This allows uninterrupted coverage of more than 1/3 of the Earth per satellite, so three satellites, spaced 120° apart, can cover the whole Earth. This type of orbit is mainly used for meteorological satellites . See also: Remote sensing § History Herman Potočnik explored the idea of using orbiting spacecraft for detailed peaceful and military observation of the ground in his 1928 book, The Problem of Space Travel. He described how the special conditions of space could be useful for scientific experiments. The book described geostationary satellites (first put forward by Konstantin Tsiolkovsky ) and discussed communication between them and the ground using radio, but fell short of the idea of using satellites for mass broadcasting and as telecommunications relays. [7]
Toggle the table of contents Landsat program From Wikipedia, the free encyclopedia American network of Earth-observing satellites for international research purposes Landsat 7 , launched in 1999, is the 7th of 9 satellites in the Landsat program. A false-color satellite image of Kolkata , India, from Landsat 7 in 2004, showing rivers, vegetated areas, and developed areas A land cover map of the big island of Hawaii using 1999–2001 data from Landsat 7 , showing black lava flows from Mauna Loa , grayish dormant Mauna Kea , a plume of smoke from active Kilauea , dark green tropical forests, and light green agricultural areas. The Landsat program is the longest-running enterprise for acquisition of satellite imagery of Earth . It is a joint NASA / USGS program. On 23 July 1972, the Earth Resources Technology Satellite was launched. This was eventually renamed to Landsat 1 in 1975. [1] The most recent, Landsat 9 , was launched on 27 September 2021. The instruments on the Landsat satellites have acquired millions of images. The images, archived in the United States and at Landsat receiving stations around the world, are a unique resource for global change research and applications in agriculture , cartography , geology , forestry , regional planning , surveillance and education , and can be viewed through the U.S. Geological Survey (USGS) "EarthExplorer" website. Landsat 7 data has eight spectral bands with spatial resolutions ranging from 15 to 60 m (49 to 197 ft); the temporal resolution is 16 days. [2] Landsat images are usually divided into scenes for easy downloading. Each Landsat scene is about 115 miles long and 115 miles wide (or 100 nautical miles long and 100 nautical miles wide, or 185 kilometers long and 185 kilometers wide). Virginia Norwood , "The Mother of Landsat", designed the multispectral scanner. Interview with Jim Irons – Landsat 8 Project Scientist – NASA Goddard Space Flight Center In 1965, William T. Pecora , the then director of the United States Geological Survey , proposed the idea of a remote sensing satellite program to gather facts about the natural resources of our planet. Pecora stated that the program was "conceived in 1966 largely as a direct result of the demonstrated utility of the Mercury and Gemini orbital photography to Earth resource studies." While weather satellites had been monitoring Earth's atmosphere since 1960 and were largely considered useful, there was no appreciation of terrain data from space until the mid-1960s. So, when Landsat 1 was proposed, it met with intense opposition from the Bureau of Budget and those who argued high-altitude aircraft would be the fiscally responsible choice for Earth remote sensing. Concurrently, the Department of Defense feared that a civilian program such as Landsat would compromise the secrecy of their reconnaissance missions. Additionally, there were geopolitical concerns about photographing foreign countries without permission. In 1965, NASA began methodical investigations of Earth remote sensing using instruments mounted on planes. In 1966, the USGS convinced the Secretary of the Interior , Stewart Udall , to announce that the Department of the Interior (DOI) was going to proceed with its own Earth-observing satellite program. This savvy political stunt coerced NASA to expedite the building of Landsat. But budgetary constraints and sensor disagreements between application agencies (notably the Department of Agriculture and DOI) again stymied the satellite construction process. Finally, by 1970 NASA had a green light to build a satellite. Remarkably, within only two years, Landsat 1 was launched, heralding a new age of remote sensing of land from space. [3] The Hughes Aircraft Company from Santa Barbara Research Center initiated, designed, and fabricated the first three Multispectral Scanners (MSS) in 1969. The first MSS prototype, designed by Virginia Norwood , was completed within nine months, in the fall of 1970. It was tested by scanning Half Dome at Yosemite National Park . For this design work Norwood was called "The Mother of Landsat". [4] Working at NASA's Goddard Space Flight Center, Valerie L. Thomas managed the development of early Landsat image processing software systems and became the resident expert on the Computer Compatible Tapes, or CCTs, that were used to store early Landsat imagery. Thomas was one of the image processing specialists who facilitated the ambitious Large Area Crop Inventory Experiment, known as LACIE — a project that showed for the first time that global crop monitoring could be done with Landsat satellite imagery. [5] The program was initially called the Earth Resources Technology Satellites Program, which was used from 1966 to 1975. In 1975, the name was changed to Landsat. In 1979, President of the United States Jimmy Carter 's Presidential Directive 54 [6] [7] transferred Landsat operations from NASA to National Oceanic and Atmospheric Administration (NOAA), recommended development of a long term operational system with four additional satellites beyond Landsat 3, and recommended transition to private sector operation of Landsat. This occurred in 1985 when the Earth Observation Satellite Company (EOSAT), a partnership of Hughes Aircraft Company and RCA , was selected by NOAA to operate the Landsat system with a ten-year contract. EOSAT operated Landsat 4 and Landsat 5, had exclusive rights to market Landsat data, and was to build Landsats 6 and 7. In 1989, this transition had not been fully completed when NOAA's funding for the Landsat program was due to run out (NOAA had not requested any funding, and U.S. Congress had appropriated only six months of funding for the fiscal year) [8] and NOAA directed that Landsat 4 and Landsat 5 be shut down. [9] The head of the newly formed National Space Council , Vice President Dan Quayle , noted the situation and arranged emergency funding that allowed the program to continue with the data archives intact. [8] [9] [10] [11] Again in 1990 and 1991, Congress provided only half of the year's funding to NOAA, requesting that agencies that used Landsat data provide the funding for the other six months of the upcoming year. [8] In 1992, various efforts were made to procure funding for follow on Landsats and continued operations, but by the end of the year EOSAT ceased processing Landsat data. Landsat 6 was finally launched on 5 October 1993, but was lost in a launch failure. Processing of Landsat 4 and 5 data was resumed by EOSAT in 1994. NASA finally launched Landsat 7 on 15 April 1999. The value of the Landsat program was recognized by Congress in October 1992 when it passed the Land Remote Sensing Policy Act (Public Law 102-555) authorizing the procurement of Landsat 7 and assuring the continued availability of Landsat digital data and images, at the lowest possible cost, to traditional and new users of the data. 6 January 1978 5 years, 6 months and 14 days Originally named Earth Resources Technology Satellite 1. Landsat 1 carried two vital instruments: a camera built by the Radio Corporation of America (RCA) known as the Return Beam Vidicon (RBV); and the Multi spectral Scanner (MSS) built by the Hughes Aircraft Company . 25 February 1982 7 years, 1 month and 3 days Nearly identical copy of Landsat 1. Payload consisting of a Return Beam Vidicon (RBV) and a Multi spectral Scanner (MSS). The specifications of these instruments were identical to Landsat 1. 31 March 1983 5 years and 26 days Nearly identical copy of Landsat 1 and Landsat 2. Payload consisting of a Return Beam Vidicon (RBV) as well as a Multi spectral Scanner (MSS). Included with the MSS was a short-lived thermal band. MSS data was considered more scientifically applicable than the RBV which was rarely used for engineering evaluation purposes. 14 December 1993 11 years, 4 months and 28 days Landsat 4 carried an updated Multi Spectral Scanner (MSS) used on previous Landsat missions, as well as a Thematic Mapper. 5 June 2013 [12] 29 years, 3 months and 4 days Nearly identical copy of Landsat 4. Longest Earth-observing satellite mission in history. Designed and built at the same time as Landsat 4, this satellite carried the same payload consisting of a Multi Spectral Scanner (MSS) as well as a Thematic Mapper. 5 October 1993 0 days Failed to reach orbit. Landsat 6 was an upgraded version of its predecessors. Carrying the same Multi spectral Scanner (MSS) but also carrying an Enhanced Thematic Mapper, which added a 15m resolution panchromatic band. 6 April 2022 24 years, 11 months and 22 days Operating with scan line corrector disabled since May 2003. [13] The main component on Landsat 7 was the Enhanced Thematic Mapper Plus (ETM+). Still consisting of the 15m-resolution panchromatic band, but also includes a full aperture calibration. This allows for 5% absolute radiometric calibration . [14] active 11 years, 1 month and 26 days Originally named Landsat Data Continuity Mission from launch until 30 May 2013, when NASA operations were turned over to United States Geological Survey (USGS). [15] Landsat 8 has two sensors with its payload, the Operational Land Imager (OLI) and the Thermal InfraRed Sensor (TIRS). [16] 2 years, 6 months and 10 days Landsat 9 is a rebuild of its predecessor Landsat 8. [17] [18] Timeline Spatial and spectral resolution[ edit ] Landsat 1 through 5 carried the Landsat Multispectral Scanner (MSS). Landsat 4 and 5 carried both the MSS and Thematic Mapper (TM) instruments. Landsat 7 uses the Enhanced Thematic Mapper Plus (ETM+) scanner. Landsat 8 uses two instruments, the Operational Land Imager (OLI) for optical bands and the Thermal Infrared Sensor (TIRS) for thermal bands. The band designations, bandpasses, and pixel sizes for the Landsat instruments are: [19] Landsat 1–5 Multispectral Scanner (MSS) Landsat 1–3 MSS Band 6 – Near Infrared (NIR) Band 3 – NIR 0.8 – 1.1 60* * Original MSS pixel size was 79 x 57 meters; production systems now resample the data to 60 meters. Landsat 4–5 Thematic Mapper (TM) Bands Band 5 – Shortwave Infrared (SWIR) 1 1.55 – 1.75 * TM Band 6 was acquired at 120-meter resolution, but products are resampled to 30-meter pixels. Landsat 7 Enhanced Thematic Mapper Plus (ETM+) Bands * ETM+ Band 6 is acquired at 60-meter resolution, but products are resampled to 30-meter pixels. The spectral band placement for each sensor of Landsat Landsat 8 Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) [20] Bands Band 1 - Ultra Blue (coastal/aerosol) 0.435 – 0.451 11.50 – 12.51 100* (30) * TIRS bands are acquired at 100 meter resolution, but are resampled to 30 meter in delivered data product. An advantage of Landsat imagery, and remote sensing in general, is that it provides data at a synoptic global level that is impossible to replicate with in situ measurements. However, there are tradeoffs between the local detail of the measurements (radiometric resolution, number of spectral bands) and the spatial scale of the area being measured. Landsat imagery is coarse in spatial resolution compared to using other remote sensing methods, such as imagery from airplanes. Compared to other satellites, Landsat's spatial resolution is relatively high, yet revisit time is relatively less frequent. MultiSpectral Scanner (MSS)[ edit ] The Landsat program incorporated the Multispectral Scanner (MSS) from its first mission up to its fifth. The MSS, gave the United States an advantage in satellite imaging, facilitating the launch of Landsat ahead of the French SPOT satellite. The MSS was unique in its design. Rather than a static camera, it employed a moving mirror, capturing Earth's images in four distinct spectral bands. This capability allowed the MSS to record variations in sunlight reflected from the Earth. Notably, Landsat 3's MSS was further advanced, with an added capability to detect heat radiation. [21] One of the prominent features of the MSS was its consistent imaging. Each captured frame represented an area on the Earth's surface approximately 83 meters in length and 68 meters in width. Additionally, the system was designed to ensure a continuous image sweep across a swath equivalent to 185 km on the Earth's surface. The MSS's design also emphasized precision; by precisely timing the mirror's movements, it ensured that consecutive images did not overlap. [21] However, by the 1980s, the cost dynamics shifted. Accessing Landsat's imagery became substantially more expensive, making the French SPOT satellite's images a more cost-effective alternative for many users. The rise in Landsat's prices can be attributed to U.S. policy shifts, initiated under President Carter's leadership and finalized during President Reagan's administration. [7] [22] Uses of Landsat imagery[ edit ] One year after launch, Landsat 8 imagery had over one million file downloads by data users. Landsat data provides information that allows scientists to predict the distribution of species, as well as detecting both naturally occurring and human-generated changes over a greater scale than traditional data from field work. The different spectral bands used on satellites in the Landsat program provide many applications, ranging from ecology to geopolitical matters. Land cover determination is a common use of Landsat imagery around the world. [23] Landsat imagery provides one of the longest uninterrupted time series available from any single remote sensing program, spanning from 1972 to present. [24] Looking to the future, the successful launch of Landsat-9 in 2021 shows that this time series will be continued forward. [25] A false-color image of irrigated fields near Garden City, Kansas , taken by the Landsat 7 satellite. In 2015, the Landsat Advisory Group of the National Geospatial Advisory Committee reported that the top 16 applications of Landsat imagery produced savings of approximately 350 million to over 436 million dollars each year for federal and state governments, NGO's, and the private sector. That estimate did not include further savings from other uses beyond the top sixteen categories. [26] The top 16 categories for Landsat imagery use, listed in order of estimated annual savings for users, are: U.S. Department of Agriculture risk management U.S. Government mapping World agriculture supply and demand estimates Vineyard management and water conservation Flood mitigation mapping Waterfowl habitat mapping and monitoring Coastal change analysis National Geospatial-Intelligence Agency global shoreline mapping Wildfire risk assessment [26] Further uses of Landsat imagery include, but are not limited to: fisheries, forestry, shrinking inland water bodies, fire damage, glacier retreat, urban development, and discovery of new species. A few specific examples are explained below. Natural resources management[ edit ] Landsat image of the Aral Sea in 2013. Landsat images of burned land in Yellowstone National Park in 1989 and 2011. Landsat-5 false color images of the Columbia Glacier, Alaska in 1986 and 2011. Landsat false color image highlighting developed areas in pink in Vancouver , British Columbia, Canada. Fisheries[ edit ] In 1975, one potential application for the new satellite-generated imagery was to find high yield fishery areas. Through the Landsat Menhaden and Thread Investigation, some satellite data of the eastern portion of the Mississippi sound and another area off the coast of the Louisiana coast data was run through classification algorithms to rate the areas as high and low probability fishing zones, these algorithms yielded a classification that was proven with in situ measurements – to be over 80% accurate and found that water color, as seen from space, and turbidity significantly correlate with the distribution of menhaden – while surface temperature and salinity do not appear to be significant factors. Water color – measured with the multispectral scanners four spectral bands, was used to infer Chlorophyll , turbidity , and possibly fish distribution. [27] Forestry[ edit ] An ecological study used 16 ortho-rectified Landsat images to generate a land cover map of Mozambique 's mangrove forest. The main objective was to measure the mangrove cover and above ground biomass on this zone that until now could only be estimated, the cover was found with 93% accuracy to be 2909 square kilometers (27% lower than previous estimates). Additionally, the study helped confirm that geological setting has a greater influence on biomass distribution than latitude alone - the mangrove area is spread across 16° of latitude but it the biomass volume of it was affected more strongly by geographic conditions. [28] Climate change and environmental disasters[ edit ] Shrinking of the Aral Sea[ edit ] The shrinking of the Aral Sea has been described as "One of the planet's worst environmental disasters". Landsat imagery has been used as a record to quantify the amount of water loss and the changes to the shoreline. Satellite visual images have a greater impact on people than just words, and this shows the importance of Landsat imagery and satellite images in general. [29] Fires in Yellowstone National Park[ edit ] The Yellowstone fires of 1988 were the worst in the recorded history of the national park. They lasted from 14 June to 11 September 1988, when rain and snow helped halt the spread of the fires. The area affected by the fire was estimated to be 3,213 square kilometers – 36% of the park. Landsat imagery was used for the area estimation, and it also helped determine the reasons why the fire spread so quickly. Historic drought and a significant number of lightning strikes were some of the factors that created conditions for the massive fire, but anthropogenic actions amplified the disaster. On images generated previous to the fire, there is an evident difference between lands that display preservation practices and the lands that display clear cut activities for timber production. These two type of lands reacted differently to the stress of fires, and it is believed that that was an important factor on the behavior of the wildfire. Landsat imagery, and satellite imagery in general, have contributed to understanding fire science; fire danger, wildfire behavior and the effects of wildfire on certain areas. It has helped understanding of how different features and vegetation fuel fires, change temperature, and affect the spreading speed. [30] [31] Glacier retreat[ edit ] The serial nature of Landsat missions and the fact that is the longest-running satellite program gives it a unique perspective to generate information of Earth. Glacier retreat in a big scale can be traced back to previous Landsat missions, and this information can be used to generate climate change knowledge. The Columbia glacier retreat for example, can be observed in false-composite images since Landsat 4 in 1986. [32] Urban development[ edit ] Landsat imagery gives a time-lapse like series of images of development. Human development specifically, can be measured by the size a city grows over time. Further than just population estimates and energy consumption, Landsat imagery gives an insight of the type of urban development, and study aspects of social and political change through visible change. In Beijing for example, a series of ring roads started to develop in 1980s following the economic reform of 1970, and the change in development rate and construction rate was accelerated in these time periods. [32] Ecology[ edit ] Discovery of new species[ edit ] In 2005, Landsat imagery assisted in the discovery of new species. Conservation scientist Julian Bayliss wanted to find areas that could potentially become conservation forests using Landsat generated satellite images. Bayliss saw a patch in Mozambique that until then had no detailed information. On a reconnaissance trip, he found great diversity of wildlife as well as three new species of butterflies and a new snake species. Following his discovery, he continued to study this forest and was able to map and determine the forest extent. [33] Recent and future Landsat satellites[ edit ] Landsat 8/9 and Landsat Next spectral band comparison Landsat 8 launched on 11 February 2013. It was launched on an Atlas V 401 from Vandenberg Air Force Base by the Launch Services Program . It will continue to obtain valuable data and imagery to be used in agriculture, education, business, science, and government. The new satellite was assembled in Arizona by Orbital Sciences Corporation . Landsat 9 launched on September 27, 2021. During FY2014 financial planning "appropriators chided NASA for unrealistic expectations that a Landsat 9 would cost US$1 billion, and capped spending at US$650 million" according to a report by the Congressional Research Service . United States Senate appropriators advised NASA to plan for a launch no later than 2020. [7] In April 2015, NASA and the USGS announced that work on Landsat 9 had commenced, with funding allocated for the satellite in the president's FY2016 budget, for a planned launch in 2023. [34] Funding for the development of a low-cost thermal infrared (TIR) free-flying satellite for launch in 2019 was also proposed, to ensure data continuity by flying in formation with Landsat 8. [34] In the future, there may also be more collaboration between Landsat satellites and other satellites with similar spatial and spectral resolution, such as the ESA 's Sentinel-2 constellation. [35] Landsat NeXt is planned to be launched in 2029. NeXt will measure 25 spectral bands; current Landsat's 8 and 9 can measure only 11. [36] Gallery[ edit ] Overview of the Thermal Infrared Sensor (TIRS), one of the instruments on Landsat 8. A timelapse of the Thermal Infrared Sensor (TIRS) instrument for Landsat 8 being cleaned, bagged, and packed to ship to Orbital Sciences Corp, where TIRS will be integrated with the spacecraft. Animation showing how different LDCM bands can be combined to obtain different information over the Florida Everglades . Screenshot capture from NASA TV showing the Atlas V during the launch of Landsat 8.
Toggle the table of contents Copernicus Programme From Wikipedia, the free encyclopedia Programme of the European Commission This article's use of external links may not follow Wikipedia's policies or guidelines. Please improve this article by removing excessive or inappropriate external links, and converting useful links where appropriate into footnote references . (December 2022) ( Cost €5,421 billion (2021-2027) Copernicus is the Earth observation component of the European Union Space Programme , managed by the European Commission and implemented in partnership with the EU Member States , the European Space Agency (ESA), the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT), the European Centre for Medium-Range Weather Forecasts (ECMWF), the Joint Research Centre (JRC), the European Environment Agency (EEA), the European Maritime Safety Agency (EMSA), Frontex , SatCen and Mercator Océan. [1] The programme aims at achieving a global, continuous, autonomous, high quality, wide range Earth observation capacity. Providing accurate, timely and easily accessible information to, among other things, improve the management of the environment, understand and mitigate the effects of climate change , and ensure civil security. Since 2021, Copernicus is a component of the EU Space Programme , which aims to bolster the EU Space policy in the fields of Earth Observation, Satellite Navigation, Connectivity, Space Research and Innovation and supports investments in critical infrastructure and disruptive technologies. Program definition[ edit ] The objective for Copernicus is to use vast amount of global data from satellites and from ground-based, airborne and seaborne measurement systems to produce timely and quality information, services and knowledge, and to provide autonomous and independent access to information in the domains of environment and security on a global level in order to help service providers, public authorities and other international organizations improve the quality of life for the citizens of Europe. In other words, it pulls together all the information obtained by the Copernicus environmental satellites , air and ground stations and sensors to provide a comprehensive picture of the "health" of Earth . [2] One of the benefits of the Copernicus programme is that the data and information produced in the framework of Copernicus are made available free-of-charge [3] to all its users and the public, thus allowing downstream services to be developed. The services offered by Copernicus cover six main interacting themes: atmosphere, marine, land, climate, emergency and security. [4] Copernicus builds upon three components: The space component (observation satellites and associated ground segment with missions observing land, atmospheric and oceanographic parameters). This comprises two types of satellite missions, ESA's six families of dedicated Sentinel (space missions) and missions from other space agencies, called Contributing Missions; [5] In-situ measurements (ground-based and airborne data-gathering networks providing information on oceans, continental surface and atmosphere); Services developed and managed by Copernicus and offered to its users and public in general. It was named after the scientist and observer Nicolaus Copernicus . Copernicus' theory of the heliocentric universe made a pioneering contribution to modern science. [6] Its costs during 1998 to 2020 are estimated at €6.7 billion with around €4.3 billion spent in the period 2014 to 2020 and shared between the EU (67%) and ESA (33%) with benefits of the data to the EU economy estimated at €30 billion through 2030. [7] ESA as a main partner has performed much of the design and oversees and co-funds the development of Sentinel missions 1, 2, 3, 4, 5 and 6 with each Sentinel mission consisting of at least 2 satellites and some, such as Sentinel 1, 2 and 3, consisting of 4 satellites. [8] They will also provide the instruments for Meteosat Third Generation and MetOp-SG weather satellites of EUMETSAT where ESA and EUMETSAT will also coordinate the delivery of data from upwards of 30 satellites that form the contributing satellite missions to Copernicus. [9] Italy and the Mediterranean, image captured by Copernicus Sentinel-3A on 28 September 2016. History[ edit ] The Copernicus programme was established by the Regulation (EU) No 377/2014 [3] in 2014, building on the previous EU's Earth monitoring initiative GMES (established by Regulation (EU) No 911/2010 [10] ). Over a few decades, European and national institutions have made substantial R&D efforts in the field of Earth observation. These efforts have resulted in tremendous achievements but the services and products developed during this period had limitations that were inherent to R&D activities (e.g. lack of service continuity on the long-term). The idea for a global and continuous European Earth observation system was developed under the name of Global Monitoring for Environment and Security (GMES) which was later re-branded into Copernicus after the EU became directly involved in financing and development. It follows and greatly expands on the work of the previous €2.3 billion European Envisat programme which operated from 2002 to 2012. [11] Copernicus moved from R&D to operational services following a phased approach. Pre-operational services (Fast Track Services and Pilot Services) were phased in between 2008 and 2010. Copernicus initial operations began in 2011. Copernicus became fully operational in 2014. [12] Chronology[ edit ] 19 May 1998: institutions involved in the development of space activities in Europe give birth to GMES through a declaration known as "The Baveno Manifesto". At that time, GMES stands for "Global Monitoring for Environmental Security". Year 1999: the name is changed to "Global Monitoring for Environment and Security" (GMES), thus illustrating that the management of the environment also has security implications. 2001: at the occasion of the Gothenburg Summit, the Heads of State and Government request that "the Community contribute to establishing by 2008 a European capacity for Global Monitoring for Environment and Security". October 2002: the nature and scope of the "Security" component of GMES are defined as addressing prevention of and response to crises related to natural and technological risk, humanitarian aid and international cooperation, monitoring of compliance with international treaties for conflict prevention, humanitarian and rescue tasks, peacekeeping tasks and surveillance of EU borders. February 2004: the Commission Communication "GMES: Establishing a GMES capacity by 2008" introduces an Action Plan aimed at establishing a working GMES capacity by 2008. In 2004, a Framework Agreement is also signed between EC and ESA, thus providing the basis for a space component of GMES. May 2005: the Commission Communication "GMES: From Concept to Reality" establishes priorities for the roll-out of GMES services in 2008, the initial focus being on land monitoring, marine monitoring and emergency response services, also known as Fast Track Services (FTS). Later services, also known as Pilot Services, are expected to address atmosphere monitoring, security and climate change. June 2006: the EC establishes the GMES Bureau, with the primary objective of ensuring the delivery of the priority services by 2008. Other objectives of the GMES Bureau are to address the issues of the GMES governance structure and the long-term financial sustainability of the system. May 2007: adoption of the European Space Policy Communication, recognising GMES as a major flagship of the Space Policy. September 2008: official launch of the three FTS services and two Pilot services in their pre-operational version at the occasion of the GMES Forum held in Lille , France . November 2008: the Commission Communication "GMES: We care for a Safer Planet" establishes a basis for further discussions on the financing, operational infrastructure and effective management of GMES. May 2009: the Commission Proposal for a Regulation on "the European Earth Observation Programme (GMES) and its initial operations (2011-2013)" proposes a legal basis for the GMES programme and EC funding of its initial operations. November 2010: the regulation on "the European Earth Observation Programme (GMES) and its initial operations (2011-2013)" entered into force. June 2011: the Commission presents its proposal for the next multiannual financial framework (MFF) corresponding to the period 2014-2020 (Communication "A Budget for Europe 2020"). In this document, the Commission proposes to foresee the funding of the GMES programme outside the multiannual financial framework after 2014. November 2011: The Commission Communication on the "European Earth monitoring programme (GMES) and its operations (from 2014 onwards)" presents the commission's proposals for the future funding, governance and operations of the GMES programme for the period 2014–2020. In particular, the Commission proposes to opt for the creation of a specific GMES fund, similar to the model chosen for the European Development Fund, with financial contributions from all Member States, based on their gross national income (GNI). April 2012: The Emergency Management Service – Mapping ("EMS-Mapping") is declared the first fully operational service within the GMES Initial Operations. [13] December 2012: the Commission announces the name change to Copernicus. October 2014: ESA and European Commission have established a budget for Copernicus Programme covering years 2014-2020 within Multiannual Financial Framework . Budget provided a total of €4.3 billion, including €3.15 billion for ESA to cover operations of the satellite network and construction of the remaining satellites. [14] [15] November 2020: launch of Sentinel-6 Michael Freilich to enable the provision of high-precision and timely observations of the topography of the global ocean January 2021: the regulation (EU) 2021/696 of the European Parliament and of the Council of 28 April 2021 establishing the Union Space Programme entered into force establishing a budget of €5,421 billion under the Multiannual Financial Framework (MFF) corresponding to the period 2021-2027. Earth Observation missions[ edit ] Sentinel missions[ edit ] ESA is currently developing seven missions under the Sentinel programme (Sentinel 1, 2, 3, 4, 5P, 5, 6). The Sentinel missions include radar and super-spectral imaging for land, ocean and atmospheric monitoring. Each Sentinel mission is based on a constellation of two satellites to fulfill and revisit the coverage requirements for each mission, providing robust datasets for all Copernicus services. The Sentinel missions have the following objectives: Sentinel-1 provides all-weather, day and night radar imaging for land and ocean services. [16] Sentinel-1A satellite was successfully launched on 3 April 2014, by a Soyuz , from the Centre Spatial Guyanais . [17] Sentinel-1B satellite was launched on 25 April 2016. Mission declared as ended 3 August 2022. Sentinel-2 provides high-resolution optical imaging for land services (e.g. imagery of vegetation, soil and water cover, inland waterways and coastal areas). [18] Sentinel-2 will also provide information for emergency services. Both satellites launched aboard Vega rockets from Centre Spatial Guyanais . Sentinel-2A , successfully launched on 23 June 2015. [19] Sentinel-2B , followed 7 March 2017. Sentinel-3 provides ocean and global land monitoring services. [20] Both satellites were launched by a Eurockot Rokot vehicle from the Plesetsk Cosmodrome in Russia . [21] [22] Sentinel-3A satellite was launched on 16 February 2016. Sentinel-3B satellite followed on 25 April 2018. Sentinel-4 will provide data for atmospheric composition monitoring as a payload upon a Meteosat Third Generation satellite. [23] It will be launched in 2024. [24] [25] Sentinel-5 Precursor , launched 13 October 2017. [26] The primary purpose of this mission is to reduce the data gap (especially SCIAMACHY atmospheric observations) between the loss of Envisat in 2012, and the launch of Sentinel-5 in 2021. [27] Sentinel-5 will also provide data for atmospheric composition monitoring. [28] It will be embarked on a EUMETSAT Polar System Second Generation ( EPS-SG ) spacecraft and launched in 2021. [25] Sentinel-6 is intended to provide continuity in high precision altimetry sea level measurements following the Jason-3 satellite. [29] Sentinel-6A , was launched in November 2020 by a SpaceX Falcon 9 vehicle from Vandenberg SLC-4E . [30] Sentinel-6B , is scheduled for launch in November 2025 by a SpaceX Falcon 9. [31] In preparation for the second-generation of Copernicus (Copernicus 2.0), six High Priority Candidate "expansion" missions are currently being studied by ESA to address EU Policy and gaps in Copernicus user needs, and to increase the current capabilities of the Copernicus Space Component: Sentinel-7 : Anthropogenic CO2 emissions monitoring (CO2M) [32] Sentinel-8 : High spatio-temporal resolution land surface temperature (LSTM) [33] Sentinel-9 : Copernicus Polar Ice and Snow Topography Altimeter (CRISTAL) [32] Sentinel-10 : Copernicus Hyperspectral Imaging Mission for the Environment (CHIME) [32] Sentinel-11 : Copernicus Imaging Microwave Radiometer (CIMR) [32] Sentinel-12 : Radar Observing System for Europe – L-band SAR (ROSE-L), scheduled for launch no earlier than 2028 [32] [34] Contributing missions[ edit ] Before the Sentinel missions provide data to Copernicus, numerous existing or planned space missions provide or will provide data useful to the provision of Copernicus services. These missions are often referred to as "Copernicus Contributing Missions (CCMs)": ERS : the European Remote Sensing Satellite ERS-1 (1991–2000) was ESA's first Earth observation satellite. ERS-2 (1995–2011) provided data related to ocean surface temperature, winds at sea and atmospheric ozone. Envisat (2002–2012): launched in 2002, ESA's Envisat was the largest civilian Earth Observation spacecraft ever built. It carried sophisticated optical and radar instruments among which the Advanced Synthetic Aperture Radar (ASAR) and the Medium Resolution Imaging Spectrometer (MERIS). Envisat provided continuous observation and monitoring of the Earth's land, atmosphere, oceans and ice caps. After losing contact with the satellite on 8 April 2012, ESA formally announced the end of Envisat's mission on 9 May 2012. [35] Earth Explorers : ESA's Earth Explorers are smaller research missions dedicated to specific aspects of our Earth environment. Earth Explorer missions focus on research of the atmosphere, biosphere, hydrosphere, cryosphere and the Earth's interior with the overall emphasis on learning more about the interactions between these components and the impact that human activity is having on natural Earth processes. The following two of the nine missions selected for implementation currently (as of 2020) contribute to Copernicus: SMOS (Soil Moisture and Ocean Salinity), launched on 2 November 2009. CryoSat-2 (the measurement of the thickness of floating ice), launched on 8 April 2010. MSG : the Meteosat Second Generation is a joint project between ESA and EUMETSAT. MetOp : MetOp is Europe's first polar-orbiting satellite dedicated to operational meteorology. MetOp is a series of three satellites launched sequentially over 12 years from October 2006 to November 2018. The series provides data for both operational meteorology and climate studies. French SPOT : SPOT (Satellite Pour l'Observation de la Terre) consists of a series of earth observation satellites providing high-resolution images of the Earth. SPOT-4 and SPOT-5 include sensors called VEGETATION able to monitor continental ecosystems. German TerraSAR-X : TerraSAR-X is an Earth observation satellite providing high quality topographic information. TerraSAR-X data has a wide range of applications (e.g. land use / land cover mapping, topographic mapping, forest monitoring, emergency response monitoring, and environmental monitoring ). Italian COSMO-SkyMed : the COnstellation of small Satellites for the Mediterranean basin Observation is an Earth observation satellite system that consists of (in the 1st generation) four satellites equipped with Synthetic-aperture radar (SAR) sensors. Applications include seismic hazard analysis, environmental disaster monitoring and agricultural mapping. As of 2020, a second-generation of COSMO-SkyMed satellites (called Cosmo-Skymed 2nd generation) is under development. UK and international DMC : the Disaster Monitoring Constellation (DMC) is a constellation of remote-sensing satellites. There have been eight satellites in the DMC-program; 3 are currently (as of 2020) active. The constellation provides emergency Earth imaging for disaster relief under the International Charter for Space and Major Disasters. French-American OSTM/Jason-2 (2008-2019): the OSTM/JASON-2 satellite provided precise measurements of ocean surface topography, surface wind speed, and wave height; as this type of measurement is a crucial requirement for the Copernicus Marine Services, the European Commission has included this type of mission in its latest communication on the future Copernicus Space Component as Sentinel-6. French Pléiades : the Pléiades constellation consists of two satellites providing very high-resolution images of the Earth. Planet Labs , a commercial satellite imagery provider whose goal is to image the entirety of the planet daily to monitor changes and pinpoint trends. OroraTech , a Germany-based commercial earth observation provider focussed on wildfire situational awareness, is delivering its FOREST-2 thermal-infrared data (MWIR, 2x LWIR). [36] Data provided by non-European satellite missions (e.g. Landsat , GOSAT , Radarsat-2 ) can also be used by Copernicus. DigitalGlobe , an American commercial vendor of space imagery and geospatial content, provides imagery from satellites with a true maximum resolution of up to 25 cm. The DigitalGlobe tasking constellation currently includes GeoEye-1 , WorldView-1 , WorldView-2 and WorldView-3 . Archive data is also available from Ikonos and QuickBird . LANDSAT program (8 satellites, 3 active). GOSAT program (2 satellites, 2 active). In-Situ Coordination[ edit ] GMES In-Situ Coordination (GISC) was a FP7 funded initiative, lasted for three years (January 2010 – December 2012) and was coordinated by the European Environment Agency (EEA). Since 2014 EEA has been responsible for Copernicus In-Situ coordination under the Contribution Agreement between the EU (represented by the European Commission) and the EEA, signed 1 December 2014. In situ data are all data from sources other than Earth observation satellites. Consequently, all ground-based, air-borne, and ship/buoy-based observations and measurements that are needed to implement and operate the Copernicus services are part of the in-situ component. In-situ data are indispensable; they are assimilated into forecasting models, provide calibration and validation of space-based information, and contribute to analysis or filling gaps not available from space sources. GISC was undertaken with reference to other initiatives, such as INSPIRE (Infrastructure for Spatial Information in the European Community) and SEIS (Shared Environmental Information System) as well as existing coordination and data exchange networks. The coordinated access to data retains the capacity to link directly data providers and the service providers because it is based on the principles of SEIS and INSPIRE. The implementation of INSPIRE is embedded in the synergies and meta-data standards that were used in GISC. Data and information aims to be managed as close as possible to its source in order to achieve a distributed system, by involving countries and existing capacities that maintain and operate the required observation infrastructure. Services component[ edit ] Copernicus services are dedicated to the monitoring and forecasting of the Earth's subsystems. They contribute directly to the monitoring of climate change. Copernicus services also address emergency management (e.g. in case of natural disaster, technological accidents or humanitarian crises) and security-related issues (e.g. maritime surveillance, border control). Copernicus services address six main thematic areas: Emergency Management Service (see video available on the Copernicus.eu website: Copernicus Emergency Management Service ). The service was declared operational on 1 April 2012. Land Monitoring (see video available on the Copernicus.eu website: Copernicus Land Monitoring Service ). The service was declared operational on 1 April 2012. Marine Environment Monitoring (see video available on the Copernicus.eu website: Copernicus Marine Environment Monitoring Service ). The service was declared operational on 1 May 2015. Atmosphere Monitoring (see video available on the Copernicus.eu website: Copernicus Atmosphere Monitoring Service ). The service was declared operational in July 2015. Climate Change (see video available on the Copernicus.eu website: Copernicus Climate Change Monitoring Service ) The development of the pre-operational version of the services has been realised by a series of projects launched by the European Commission and partly funded through the EU's 7th Framework Programme (FP7). These projects were geoland2 (land), MyOcean (marine), SAFER (emergency response), MACC and its successor MACC II (atmosphere) and G-MOSAIC (security). Most of these projects also contributed to the monitoring of Climate Change. geoland2 started on 1 September 2008. The project covered a wide range of domains such as land use, land cover change, soil sealing , water quality and availability, spatial planning, forest management , carbon storage and global food security . MyOcean started on 1 January 2009. It covered themes such as maritime security, oil spill prevention , marine resource management, climate change , seasonal forecast, coastal activities, ice survey and water pollution . SAFER started on 1 January 2009. The project addressed three main domains: civil protection, humanitarian aid and Security crises management. MACC started on 1 June 2009. The project continued and refined the products developed in the projects GEMS and PROMOTE . A second phase (MACC II) lasted until July 2014 allowing the now operational Copernicus atmospheric monitoring service (CAMS, see above). GMOSAIC started on 1 January 2009. Together with the LIMES project Wayback Machine (co-funded by the European Commission under FP6), GMOSAIC specifically dealt with the Security domain of Copernicus addressing topics such as Support to Intelligence and Early Warning and Support to Crisis Management Operations. Interaction[ edit ] "The information provided by the Copernicus services can be used by end-users for a wide range of applications in a variety of areas. These include urban area management, sustainable development and nature protection, regional and local planning, agriculture, forestry and fisheries, health, civil protection, infrastructure, transport and mobility, as well as tourism". [4] Copernicus is the European Union 's contribution to the Global Earth Observation System of Systems (GEOSS) thus delivering geospatial information globally. Some Copernicus services make use of OpenStreetMap data in their maps production. [37] Other relevant initiatives[ edit ] Other initiatives will also facilitate the development and functioning of Copernicus services: INSPIRE : this initiative aims at building a European spatial data infrastructure beyond national boundaries. Urban Atlas: Compiled from thousands of satellite photographs, the Urban Atlas provides detailed and cost-effective digital mapping, ensuring that city planners have the most up-to-date and accurate data available on land use and land cover. The Urban Atlas will enable urban planners to better assess risks and opportunities, ranging from the threat of flooding and the impact of climate change, to identifying new infrastructure and public transport needs. All cities in the EU will be covered by the Urban Atlas by 2011. SEIS : The Shared Environmental Information System (SEIS) is a collaborative initiative of the European Commission and the European Environment Agency (EEA) to establish together with the Member States an integrated and shared EU-wide environmental information system. Heterogeneous Missions Accessibility , the European Space Agency initiative for interoperability of Earth observation satellite payload data ground segments. Copernicus is one of three related initiatives that are the subject of the GIGAS ( GEOSS , INSPIRE and GMES an Action in Support) harmonization and analysis project [38] under the auspices of the EU 7th Framework Programme . [39] Third country participation[ edit ] In addition to the 27 Member States of the European Union, the Copernicus programme allows for the participation at various scope for third country participation. This participation is conducted through agreements with the European Union. One has to distinguish those countries that contribute to the budget and those that agree on exchanging data with the program. Many international partner countries get special access to Sentinel data in exchange for sharing in-situ data from their country. These states are: 2014–2020 budget contributing countries
Toggle the table of contents Moderate Resolution Imaging Spectroradiometer "MODIS" redirects here. For the singular, see Modi . For other uses, see Modis . Ash plumes on Kamchatka Peninsula , eastern Russia. Hurricane Katrina near Florida peninsula. California wildfires. Solar irradiance spectrum and MODIS bands. External view of the MODIS unit. Exploded view of the MODIS subsystems. This detailed, photo-like view of Earth is based largely on observations from MODIS. The Moderate Resolution Imaging Spectroradiometer (MODIS) is a satellite-based sensor used for earth and climate measurements. There are two MODIS sensors in Earth orbit : one on board the Terra ( EOS AM) satellite, launched by NASA in 1999; and one on board the Aqua (EOS PM) satellite, launched in 2002. MODIS has now been replaced by the VIIRS ,[ citation needed ] which first launched in 2011 aboard the Suomi NPP satellite. The MODIS instruments were built by Santa Barbara Remote Sensing. [1] They capture data in 36 spectral bands ranging in wavelength from 0.4 μm to 14.4 μm and at varying spatial resolutions (2 bands at 250 m, 5 bands at 500 m and 29 bands at 1 km). Together the instruments image the entire Earth every 1 to 2 days. They are designed to provide measurements in large-scale global dynamics including changes in Earth's cloud cover , radiation budget and processes occurring in the oceans, on land, and in the lower atmosphere . Support and calibration is provided by the MODIS characterization support team (MCST). [2] Applications[ edit ] This section needs expansion. You can help by adding to it . (September 2014) With its high temporal resolution although low spatial resolution, MODIS data are useful to track changes in the landscape over time. Examples of such applications are the monitoring of vegetation health by means of time-series analyses with vegetation indices, [3] long term land cover changes (e.g. to monitor deforestation rates), [4] [5] [6] [7] global snow cover trends, [8] [9] water inundation from pluvial, riverine, or sea level rise flooding in coastal areas, [10] change of water levels of major lakes such as the Aral Sea , [11] [12] and the detection and mapping of wildland fires in the United States. [13] The United States Forest Service 's Remote Sensing Applications Center analyzes MODIS imagery on a continuous basis to provide information for the management and suppression of wildfires. [14] Specifications Orbit 705 km, 10:30 a.m. descending node (Terra) or 1:30 p.m. ascending node (Aqua), Sun-synchronous, near-polar, circular Scan rate
Toggle the table of contents Weather satellite From Wikipedia, the free encyclopedia Type of satellite designed to record the state of the Earth's atmosphere Not to be confused with Atmospheric satellite . GOES-16, a United States weather satellite of the meteorological-satellite service A weather satellite or meteorological satellite is a type of Earth observation satellite that is primarily used to monitor the weather and climate of the Earth.  Satellites can be polar orbiting (covering the entire Earth asynchronously), or geostationary (hovering over the same spot on the equator ). [1] While primarily used to detect the development and movement of storm systems and other cloud patterns, meteorological satellites can also detect other phenomena such as city lights, fires, effects of pollution, auroras , sand and dust storms , snow cover, ice mapping, boundaries of ocean currents , and energy flows. Other types of environmental information are collected using weather satellites. Weather satellite images helped in monitoring the volcanic ash cloud from Mount St. Helens and activity from other volcanoes such as Mount Etna . [2] Smoke from fires in the western United States such as Colorado and Utah have also been monitored. El Niño and its effects on weather are monitored daily from satellite images.  The Antarctic ozone hole is mapped from weather satellite data.  Collectively, weather satellites flown by the U.S., Europe, India, China, Russia, and Japan provide nearly continuous observations for a global weather watch. Further information: First images of Earth from space The first television image of Earth from space from the TIROS-1 weather satellite in 1960 A mosaic of photographs of the United States from the ESSA-9 weather satellite, taken on June 26, 1969 As early as 1946, the idea of cameras in orbit to observe the weather was being developed.  This was due to sparse data observation coverage and the expense of using cloud cameras on rockets.  By 1958, the early prototypes for TIROS and Vanguard (developed by the Army Signal Corps) were created. [3] The first weather satellite, Vanguard 2 , was launched on February 17, 1959. [4] It was designed to measure cloud cover and resistance, but a poor axis of rotation and its elliptical orbit kept it from collecting a notable amount of useful data.  The Explorer VI and VII satellites also contained weather-related experiments. [3] The first weather satellite to be considered a success was TIROS-1 , launched by NASA on April 1, 1960. [5] TIROS operated for 78 days and proved to be much more successful than Vanguard 2.  TIROS paved the way for the Nimbus program , whose technology and findings are the heritage of most of the Earth-observing satellites NASA and NOAA have launched since then.  Beginning with the Nimbus 3 satellite in 1969, temperature information through the tropospheric column began to be retrieved by satellites from the eastern Atlantic and most of the Pacific Ocean, which led to significant improvements to weather forecasts . [6] The ESSA and NOAA polar orbiting satellites followed suit from the late 1960s onward.  Geostationary satellites followed, beginning with the ATS and SMS series in the late 1960s and early 1970s, then continuing with the GOES series from the 1970s onward.  Polar orbiting satellites such as QuikScat and TRMM began to relay wind information near the ocean's surface starting in the late 1970s, with microwave imagery which resembled radar displays, which significantly improved the diagnoses of tropical cyclone strength, intensification, and location during the 2000s and 2010s. The DSCOVR satellite, owned by NOAA, was launched in 2015 and became the first deep space satellite that can observe and predict space weather. It can detect potentially dangerous weather such as solar wind and geomagnetic storms . This is what has given humanity the capability to make accurate and preemptive space weather forecasts since the late 2010s. [7] In Europe, the first Meteosat geostationary operational meteorological satellite, Meteosat-1, was launched in 1977 on a Delta launch vehicle. The satellite was a spin-stabilised cylindrical design, 2.1m in diameter and 3.2m tall, rotating at approx. 100 rpm and carrying the Meteosat Visible and Infrared Imager (MVIRI) instrument. Successive Meteosat first generation satellites were launched, on European Ariane-4 launchers from Kourou in French Guyana, up to and including Meteosat-7 which acquired data from 1997 until 2017, operated initially by the European Space Agency and later, from 1995, by the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT). The Meteosat Second Generation (MSG) satellites - also spin stabilised although physically larger and twice the mass of the first generation - were developed by ESA with European industry and in cooperation with EUMETSAT who then operate the satellites from their headquarters in Darmstadt, Germany with this same approach followed for all subsequent European meteorological satellites. Meteosat-8, the first MSG satellite, was launched in 2002 on an Ariane-5 launcher, carrying the Spinning Enhanced Visible and Infrared Imager (SEVIRI) and Geostationary Earth Radiation Budget (GERB) instruments, along with payloads to support the COSPAS-SARSAT Search and Rescue (SAR) and ARGOS Data Collection Platform (DCP) missions. SEVIRI provided an increased number of spectral channels over MVIRI and imaged the full-Earth disc at double the rate. Meteosat-9 was launched to complement Meteosat-8 in 2005, with the second pair consisting of Meteosat-10 and Meteosat-11 launched in 2012 and 2015, respectively. The Meteosat Third Generation (MTG) programme launched its first satellite in 2022, and featured a number of changes over its predecessors in support of its mission to gather data for weather forecasting and climate monitoring. The MTG satellites are three-axis stabilised rather than spin stabilised, giving greater flexibility in satellite and instrument design. The MTG system features separate Imager and Sounder satellite models that share the same satellite bus, with a baseline of three satellites - two Imagers and one Sounder - forming the operational configuration. The imager satellites carry the Flexible Combined Imager (FCI), succeeding MVIRI and SEVIRI to give even greater resolution and spectral coverage, scanning the full Earth disc every ten minutes, as well as a new Lightning Imager (LI) payload. The sounder satellites carry the Infrared Sounder (IRS) and Ultra-violet Visible Near-infrared (UVN) instruments. UVN is part of the European Commission 's Copernicus programme and fulfils the Sentinel-4 mission to monitor air quality, trace gases and aerosols over Europe hourly at high spatial resolution. Two MTG satellites - one Imager and one Sounder - will operate in close proximity from the 0-deg geostationary location over western Africa to observe the eastern Atlantic Ocean, Europe, Africa and the Middle East, while a second imager satellite will operate from 9.5-deg East to perform a Rapid Scanning mission over Europe. MTG continues Meteosat support to the ARGOS and Search and Rescue missions. MTG-I1 launched in one of the last Ariane-5 launches, with the subsequent satellites planned to launch in Ariane-6 when it enters service. In 2006, the first European low-Earth orbit operational meteorological satellite, Metop -A was launched into a Sun-synchronous orbit at 817 km altitude by a Soyuz launcher from Baikonur, Kazakhstan. This operational satellite - which forms the space segment of the Eumetsat Polar System (EPS) - built on the heritage from ESA's ERS and Envisat experimental missions, and was followed at six-year intervals by Metop-B and Metop-C - the latter launched from French Guyana in a "Europeanised" Soyuz. Each carry thirteen different passive and active instruments ranging in design from imagers and sounders to a scatterometer and a radio-occultation instrument. The satellite service module is based on the SPOT-5 bus, while the payload suite is a combination of new and heritage instruments from both Europe and the US under the Initial Joint Polar System agreement between EUMETSAT and NOAA. A second generation of Metop satellites (Metop-SG) is in advanced development with launch of the first satellite foreseen in 2025. As with MTG, Metop-SG will launch on Ariane-6 and comprise two satellite models to be operated in pairs in replacement of the single first generation satellites to continue the EPS mission. Observation[ edit ] These meteorological-satellite service , however, see more than clouds and cloud systems Observation is typically made via different 'channels' of the electromagnetic spectrum , in particular, the visible and infrared portions. Some of these channels include: [8] [9] Visible and Near Infrared: 0.6–1.6 μm – for recording cloud cover during the day Infrared: 3.9–7.3 μm (water vapor), 8.7–13.4 μm (thermal imaging) Visible spectrum[ edit ] Visible-light images from weather satellites during local daylight hours are easy to interpret even by the average person, clouds, cloud systems such as fronts and tropical storms, lakes, forests, mountains, snow ice, fires, and pollution such as smoke, smog, dust and haze are readily apparent.  Even wind can be determined by cloud patterns, alignments and movement from successive photos. [10] Infrared spectrum[ edit ] The thermal or infrared images recorded by sensors called scanning radiometers enable a trained analyst to determine cloud heights and types, to calculate land and surface water temperatures, and to locate ocean surface features. Infrared satellite imagery can be used effectively for tropical cyclones with a visible eye pattern, using the Dvorak technique , where the difference between the temperature of the warm eye and the surrounding cold cloud tops can be used to determine its intensity (colder cloud tops generally indicate a more intense storm). [11] Infrared pictures depict ocean eddies or vortices and map currents such as the Gulf Stream which are valuable to the shipping industry. Fishermen and farmers are interested in knowing land and water temperatures to protect their crops against frost or increase their catch from the sea. Even El Niño phenomena can be spotted. Using color-digitized techniques, the gray shaded thermal images can be converted to color for easier identification of desired information. The geostationary Himawari 8 satellite's first true-colour composite PNG image The geostationary GOES-17 satellite's Level 1B Calibrated Radiances - True Colour Composite PNG image Each meteorological satellite is designed to use one of two different classes of orbit: geostationary and polar orbiting . Geostationary[ edit ] "geostationary meteorological satellite" redirects here. For the Japanese satellites called "Geostationary Meteorological Satellite", see Himawari (satellite) . Geostationary weather satellites orbit the Earth above the equator at altitudes of 35,880 km (22,300 miles).  Because of this orbit , they remain stationary with respect to the rotating Earth and thus can record or transmit images of the entire hemisphere below continuously with their visible-light and infrared sensors. The news media use the geostationary photos in their daily weather presentation as single images or made into movie loops. These are also available on the city forecast pages of www.noaa.gov (example Dallas, TX). [12] Several geostationary meteorological spacecraft are in operation. The United States' GOES series has three in operation: GOES-15 , GOES-16 and GOES-17 . GOES-16 and-17 remain stationary over the Atlantic and Pacific Oceans, respectively. [13] GOES-15 was retired in early July 2019. [14] The satellite GOES 13 that was previously owned by the National Oceanic and Atmospheric Association (NOAA) was transferred to the U.S. Space Force in 2019 and renamed the EWS-G1; becoming the first geostationary weather satellite to be owned and operated by the U.S. Department of Defense. [15] Russia 's new-generation weather satellite Elektro-L No.1 operates at 76°E over the Indian Ocean. The Japanese have the MTSAT -2 located over the mid Pacific at 145°E and the Himawari 8 at 140°E. The Europeans have four in operation, Meteosat -8 (3.5°W) and Meteosat-9 (0°) over the Atlantic Ocean and have Meteosat-6 (63°E) and Meteosat-7 (57.5°E) over the Indian Ocean. China currently has four Fengyun (风云) geostationary satellites (FY-2E at 86.5°E, FY-2F at 123.5°E, FY-2G at 105°E and FY-4A at 104.5 °E) operated. [16] India also operates geostationary satellites called INSAT which carry instruments for meteorological purposes. Polar orbiting[ edit ] Computer-controlled motorized parabolic dish antenna for tracking LEO weather satellites. Polar orbiting weather satellites circle the Earth at a typical altitude of 850 km (530 miles) in a north to south (or vice versa) path, passing over the poles in their continuous flight.  Polar orbiting weather satellites are in sun-synchronous orbits , which means they are able to observe any place on Earth and will view every location twice each day with the same general lighting conditions due to the near-constant local solar time . Polar orbiting weather satellites offer a much better resolution than their geostationary counterparts due their closeness to the Earth. The United States has the NOAA series of polar orbiting meteorological satellites, presently NOAA-15, NOAA-18 and NOAA-19 ( POES ) and NOAA-20 ( JPSS ). Europe has the Metop -A, Metop -B and Metop -C satellites operated by EUMETSAT . Russia has the Meteor and RESURS series of satellites. China has FY -3A, 3B and 3C. India has polar orbiting satellites as well. DMSP[ edit ] Turnstile antenna for reception of 137 MHz LEO weather satellite transmissions The United States Department of Defense 's Meteorological Satellite ( DMSP ) can "see" the best of all weather vehicles with its ability to detect objects almost as 'small' as a huge oil tanker .  In addition, of all the weather satellites in orbit, only DMSP can "see" at night in the visual.  Some of the most spectacular photos have been recorded by the night visual sensor; city lights, volcanoes , fires, lightning, meteors , oil field burn-offs, as well as the Aurora Borealis and Aurora Australis have been captured by this 720 kilometres (450 mi) high space vehicle's low moonlight sensor. At the same time, energy use and city growth can be monitored since both major and even minor cities, as well as highway lights, are conspicuous.  This informs astronomers of light pollution . The New York City Blackout of 1977 was captured by one of the night orbiter DMSP space vehicles. In addition to monitoring city lights, these photos are a life saving asset in the detection and monitoring of fires.  Not only do the satellites see the fires visually day and night, but the thermal and infrared scanners on board these weather satellites detect potential fire sources below the surface of the Earth where smoldering occurs.  Once the fire is detected, the same weather satellites provide vital information about wind that could fan or spread the fires.  These same cloud photos from space tell the firefighter when it will rain. Some of the most dramatic photos showed the 600 Kuwaiti oil fires that the fleeing Army of Iraq started on February 23, 1991.  The night photos showed huge flashes, far outstripping the glow of large populated areas.  The fires consumed huge quantities of oil; the last was doused on November 6, 1991. Uses[ edit ] Infrared image of storms over the central United States from the GOES-17 satellite Snowfield monitoring, especially in the Sierra Nevada , can be helpful to the hydrologist keeping track of available snowpack for runoff vital to the watersheds of the western United States.  This information is gleaned from existing satellites of all agencies of the U.S. government (in addition to local, on-the-ground measurements).  Ice floes, packs, and bergs can also be located and tracked from weather spacecraft. Even pollution whether it is nature-made or human-made can be pinpointed.  The visual and infrared photos show effects of pollution from their respective areas over the entire earth.  Aircraft and rocket pollution, as well as condensation trails , can also be spotted.  The ocean current and low level wind information gleaned from the space photos can help predict oceanic oil spill coverage and movement. Almost every summer, sand and dust from the Sahara Desert in Africa drifts across the equatorial regions of the Atlantic Ocean.  GOES-EAST photos enable meteorologists to observe, track and forecast this sand cloud.  In addition to reducing visibilities and causing respiratory problems, sand clouds suppress hurricane formation by modifying the solar radiation balance of the tropics. Other dust storms in Asia and mainland China are common and easy to spot and monitor, with recent examples of dust moving across the Pacific Ocean and reaching North America. In remote areas of the world with few local observers, fires could rage out of control for days or even weeks and consume huge areas before authorities are alerted.  Weather satellites can be a valuable asset in such situations. Nighttime photos also show the burn-off in gas and oil fields.  Atmospheric temperature and moisture profiles have been taken by weather satellites since 1969. [17]
Toggle the table of contents Imaging radar Application of radar which is used to create two-dimensional images Not to be confused with Radar display . A SAR radar image acquired by the SIR-C/X-SAR radar on board the Space Shuttle Endeavour shows the Teide volcano. The city of Santa Cruz de Tenerife is visible as the purple and white area on the lower right edge of the island. Lava flows at the summit crater appear in shades of green and brown, while vegetation zones appear as areas of purple, green and yellow on the volcano's flanks. Imaging radar is an application of radar which is used to create two-dimensional images , typically of landscapes. Imaging radar provides its light to illuminate an area on the ground and take a picture at radio wavelengths. It uses an antenna and digital computer storage to record its images. In a radar image, one can see only the energy that was reflected back towards the radar antenna. The radar moves along a flight path and the area illuminated by the radar, or footprint, is moved along the surface in a swath, building the image as it does so. [1] Digital radar images are composed of many dots.  Each pixel in the radar image represents the radar backscatter for that area on the ground ( terrain return ): brighter areas represent high backscatter, darker areas represents low backscatter. [1] The traditional application of radar is to display the position and motion of typically highly reflective objects (such as aircraft or ships ) by sending out a radiowave signal, and then detecting the direction and delay of the reflected signal. Imaging radar on the other hand attempts to form an image of one object (e.g. a landscape) by furthermore registering the intensity of the reflected signal to determine the amount of scattering . The registered electromagnetic scattering is then mapped onto a two-dimensional plane, with points with a higher reflectivity getting assigned usually a brighter color, thus creating an image. Several techniques have evolved to do this. Generally they take advantage of the Doppler effect caused by the rotation or other motion of the object and by the changing view of the object brought about by the relative motion between the object and the back-scatter that is perceived by the radar of the object (typically, a plane) flying over the earth. Through recent improvements of the techniques, radar imaging is getting more accurate. Imaging radar has been used to map the Earth, other planets, asteroids, other celestial objects and to categorize targets for military systems. Description[ edit ] An imaging radar is a kind of radar equipment which can be used for imaging. A typical radar technology includes emitting radio waves, receiving their reflection, and using this information to generate data. For an imaging radar, the returning waves are used to create an image. When the radio waves reflect off objects, this will make some changes in the radio waves and can provide data about the objects, including how far the waves traveled and what kind of objects they encountered. Using the acquired data, a computer can create a 3-D or 2-D image of the target. [2] Imaging radar has several advantages. [3] It can operate in the presence of obstacles that obscure the target, and can penetrate ground (sand), water, or walls. [4] [5] Applications[ edit ] Applications include: surface topography & coastal change; land use monitoring, agricultural monitoring, ice patrol, environmental monitoring ;weather radar- storm monitoring, wind shear warning;medical microwave tomography; [5] through wall radar imaging; [6] 3-D measurements, [7] etc. Through wall radar imaging[ edit ] Wall parameter estimation uses Ultra Wide-Band radar systems.  The handle M-sequence UWB radar with horn and circular antennas was used for data gathering and supporting the scanning method. [6] 3-D measurements[ edit ] 3-D measurements are supplied by amplitude-modulated laser radars—Erim sensor and Perceptron sensor.  In terms of speed and reliability for median-range operations, 3-D measurements have superior performance. [7] Techniques and methods[ edit ] Current radar imaging techniques rely mainly on synthetic aperture radar (SAR) and inverse synthetic aperture radar (ISAR) imaging.  Emerging technology utilizes monopulse radar 3-D imaging. Real aperture radar[ edit ] Real aperture radar (RAR) is a form of radar that transmits a narrow angle beam of pulse radio wave in the range direction at right angles to the flight direction and receives the backscattering from the targets which will be transformed to a radar image from the received signals. Usually the reflected pulse will be arranged in the order of return time from the targets, which corresponds to the range direction scanning. The resolution in the range direction depends on the pulse width.  The resolution in the azimuth direction is identical to the multiplication of beam width and the distance to a target. [8] AVTIS radar[ edit ] The AVTIS radar is a 94 GHz real aperture 3D imaging radar.  It uses Frequency-Modulated Continuous-Wave modulation and employs a mechanically scanned monostatic with sub-metre range resolution. [9] Main article: Lidar Laser radar is a remote sensing technology that measures distance by illuminating a target with a laser and analyzing the reflected light. [10] Laser radar is used for multi-dimensional imaging and information gathering.  In all information gathering modes, lasers that transmit in the eye-safe region are required as well as sensitive receivers at these wavelengths. [11] 3-D imaging requires the capacity to measure the range to the first scatter within every pixel. Hence, an array of range counters is needed.  A monolithic approach to an array of range counters is being developed.  This technology must be coupled with highly sensitive detectors of eye-safe wavelengths. [11] To measure Doppler information requires a different type of detection scheme than is used for spatial imaging. The returned laser energy must be mixed with a local oscillator in a heterodyne system to allow extraction of the Doppler shift. [11] Synthetic aperture radar (SAR)[ edit ] Main article: Synthetic aperture radar Synthetic-aperture radar (SAR) is a form of radar which moves a real aperture or antenna through a series of positions along the objects to provide distinctive long-term coherent-signal variations. This can be used to obtain higher resolution. SARs produce a two-dimensional (2-D) image. One dimension in the image is called range and is a measure of the "line-of-sight" distance from the radar to the object. Range is determined by measuring the time from transmission of a pulse to receiving the echo from a target. Also, range resolution is determined by the transmitted pulse width. The other dimension is called azimuth and is perpendicular to range. The ability of SAR to produce relatively fine azimuth resolution makes it different from other radars. To obtain fine azimuth resolution, a physically large antenna is needed to focus the transmitted and received energy into a sharp beam. The sharpness of the beam defines the azimuth resolution. An airborne radar could collect data while flying this distance and process the data as if it came from a physically long antenna. The distance the aircraft flies in synthesizing the antenna is known as the synthetic aperture. A narrow synthetic beam width results from the relatively long synthetic aperture, which gets finer resolution than a smaller physical antenna. [12] Inverse aperture radar (ISAR)[ edit ] Main article: Inverse synthetic aperture radar Inverse synthetic aperture radar (ISAR) is another kind of SAR system which can produce high-resolution on two- and three-dimensional images. An ISAR system consists of a stationary radar antenna and a target scene that is undergoing some motion. ISAR is theoretically equivalent to SAR in that high-azimuth resolution is achieved via relative motion between the sensor and object, yet the ISAR moving target scene is usually made up of non cooperative objects. Algorithms with more complex schemes for motion error correction are needed for ISAR imaging than those needed in SAR. ISAR technology uses the movement of the target rather than the emitter to make the synthetic aperture. ISAR radars are commonly used on vessels or aircraft and can provide a radar image of sufficient quality for target recognition. The ISAR image is often adequate to discriminate between various missiles, military aircraft, and civilian aircraft. [13] Disadvantages of ISAR[ edit ] The ISAR imaging cannot obtain the real azimuth of the target There sometimes exists a reverse image. For example, the image formed of a boat when it rolls forwards and backwards in the ocean.[ clarification needed ] The ISAR image is the 2-D projection image of the target on the Range-Doppler plane which is perpendicular to the rotating axis. When the Range-Doppler plane and the coordinate plane are different, the ISAR image can not reflect the real shape of the target. Thus, the ISAR imaging can not obtain the real shape information of the target in most situations. [13] Rolling is side to side.  Pitching is forward and backwards, yawing is turning left or right. Monopulse radar 3-D imaging technique[ edit ] Main article: Monopulse radar Monopulse radar 3-D imaging technique uses 1-D range image and monopulse angle measurement to get the real coordinates of each scatterer. Using this technique, the image doesn't vary with the change of the target's movement.  Monopulse radar 3-D imaging utilizes the ISAR techniques to separate scatterers in the Doppler domain and perform monopulse angle measurement. Monopulse radar 3-D imaging can obtain the 3 views of 3-D objects by using any two of the three parameters obtained from the azimuth difference beam, elevation difference beam and range measurement, which means the views of front, top and side can be azimuth-elevation, azimuth-range and elevation-range, respectively. Monopulse imaging generally adapts to near-range targets, and the image obtained by monopulse radar 3-D imaging is the physical image which is consistent with the real size of the object. [14] This article may have been created or edited in return for undisclosed payments, a violation of Wikipedia's terms of use . It may require cleanup to comply with Wikipedia's content policies , particularly neutral point of view . (May 2021) 4D imaging radar[ edit ] 4D imaging radar leverages a Multiple Input Multiple Output (MiMo) antenna array for high-resolution detection, mapping and tracking of multiple static and dynamic targets simultaneously. It combines 3D imaging with Doppler analysis to create the additional dimension – velocity. [15] A 60GHz 4D imaging radar sensor from Vayyar Imaging. A 4D imaging radar system measures the time of flight from each transmitting (Tx) antenna to a target and back to each receiving (Rx) antenna, processing data from the numerous ellipsoids formed. The point at which the ellipsoids intersect – known as a hot spot - reveals the exact position of a target at any given moment. Its versatility and reliability make 4D imaging radar ideal for smart home, automotive, retail, security, healthcare and many other environments. The technology is valued for combining all the benefits of camera, LIDAR, thermal imaging and ultrasonic technologies, with additional benefits: Resolution: the large MiMo antenna array enables accurate detection and tracking of multiple static and dynamic targets simultaneously. Cost efficiency: 4D imaging radar costs around the same as a 2D radar sensor, but with immense added value: richer data, higher accuracy and more functionality, while offering an optimal price-performance balance. Robustness and privacy: There are no optics involved, so this technology is robust in all lighting and weather conditions. 4D imaging radar does not require line of sight with targets, enabling its operation in darkness, smoke, steam, glare and inclement weather. It also ensures privacy [ dubious – discuss ] and discreet surveillance by design, an increasingly important concern across all industries.
Data collection[ edit ] In a typical SAR application, a single radar antenna is attached to an aircraft or spacecraft such that a substantial component of the antenna's radiated beam has a wave-propagation direction perpendicular to the flight-path direction. The beam is allowed to be broad in the vertical direction so it will illuminate the terrain from nearly beneath the aircraft out toward the horizon. Image resolution and bandwidth[ edit ] Resolution in the range dimension of the image is accomplished by creating pulses which define very short time intervals, either by emitting short pulses consisting of a carrier frequency and the necessary sidebands, all within a certain bandwidth, or by using longer " chirp pulses " in which frequency varies (often linearly) with time within that bandwidth. The differing times at which echoes return allow points at different distances to be distinguished. Image resolution of SAR in its range coordinate (expressed in image pixels per distance unit) is mainly proportional to the radio bandwidth of whatever type of pulse is used. In the cross-range coordinate, the similar resolution is mainly proportional to the bandwidth of the Doppler shift of the signal returns within the beamwidth. Since Doppler frequency depends on the angle of the scattering point's direction from the broadside direction, the Doppler bandwidth available within the beamwidth is the same at all ranges. Hence the theoretical spatial resolution limits in both image dimensions remain constant with variation of range. However, in practice, both the errors that accumulate with data-collection time and the particular techniques used in post-processing further limit cross-range resolution at long ranges. Image resolution and beamwidth[ edit ] SAR antenna of the SAOCOM satellites. The total signal is that from a beamwidth-sized patch of the ground. To produce a beam that is narrow in the cross-range direction[ clarification needed ], diffraction effects require that the antenna be wide in that dimension. Therefore, the distinguishing, from each other, of co-range points simply by strengths of returns that persist for as long as they are within the beam width is difficult with aircraft-carryable antennas, because their beams can have linear widths only about two orders of magnitude (hundreds of times) smaller than the range. (Spacecraft-carryable ones can do 10 or more times better.) However, if both the amplitude and the phase of returns are recorded, then the portion of that multi-target return that was scattered radially from any smaller scene element can be extracted by phase-vector correlation of the total return with the form of the return expected from each such element. The process can be thought of as combining the series of spatially distributed observations as if all had been made simultaneously with an antenna as long as the beamwidth and focused on that particular point. The "synthetic aperture" simulated at maximum system range by this process not only is longer than the real antenna, but, in practical applications, it is much longer than the radar aircraft, and tremendously longer than the radar spacecraft. Although some references to SARs have characterized them as "radar telescopes", their actual optical analogy is the microscope, the detail in their images being smaller than the length of the synthetic aperture. In radar-engineering terms, while the target area is in the " far field " of the illuminating antenna, it is in the "near field" of the simulated one. Careful design and operation can accomplish resolution of items smaller than a millionth of the range, for example, 30 cm at 300 km, or about one foot at nearly 200 miles (320 km). Pulse transmission and reception[ edit ] The conversion of return delay time to geometric range can be very accurate because of the natural constancy of the speed and direction of propagation of electromagnetic waves. However, for an aircraft flying through the never-uniform and never-quiescent atmosphere, the relating of pulse transmission and reception times to successive geometric positions of the antenna must be accompanied by constant adjusting of the return phases to account for sensed irregularities in the flight path. SAR's in spacecraft avoid that atmosphere problem, but still must make corrections for known antenna movements due to rotations of the spacecraft, even those that are reactions to movements of onboard machinery. Locating a SAR in a crewed space vehicle may require that the humans carefully remain motionless relative to the vehicle during data collection periods. Returns from scatterers within the range extent of any image are spread over a matching time interval. The inter-pulse period must be long enough to allow farthest-range returns from any pulse to finish arriving before the nearest-range ones from the next pulse begin to appear, so that those do not overlap each other in time. On the other hand, the interpulse rate must be fast enough to provide sufficient samples for the desired across-range (or across-beam) resolution. When the radar is to be carried by a high-speed vehicle and is to image a large area at fine resolution, those conditions may clash, leading to what has been called SAR's ambiguity problem. The same considerations apply to "conventional" radars also, but this problem occurs significantly only when resolution is so fine as to be available only through SAR processes. Since the basis of the problem is the information-carrying capacity of the single signal-input channel provided by one antenna, the only solution is to use additional channels fed by additional antennas. The system then becomes a hybrid of a SAR and a phased array, sometimes being called a Vernier array. Data processing[ edit ] Combining the series of observations requires significant computational resources, usually using Fourier transform techniques. The high digital computing speed now available allows such processing to be done in near-real time on board a SAR aircraft. (There is necessarily a minimum time delay until all parts of the signal have been received.) The result is a map of radar reflectivity, including both amplitude and phase. Amplitude data[ edit ] The amplitude information, when shown in a map-like display, gives information about ground cover in much the same way that a black-and-white photo does. Variations in processing may also be done in either vehicle-borne stations or ground stations for various purposes, so as to accentuate certain image features for detailed target-area analysis. Phase data[ edit ] Although the phase information in an image is generally not made available to a human observer of an image display device, it can be preserved numerically, and sometimes allows certain additional features of targets to be recognized. Coherence speckle[ edit ] Unfortunately, the phase differences between adjacent image picture elements ("pixels") also produce random interference effects called "coherence speckle ", which is a sort of graininess with dimensions on the order of the resolution, causing the concept of resolution to take on a subtly different meaning. This effect is the same as is apparent both visually and photographically in laser-illuminated optical scenes. The scale of that random speckle structure is governed by the size of the synthetic aperture in wavelengths, and cannot be finer than the system's resolution. Speckle structure can be subdued at the expense of resolution. Optical holography[ edit ] Before rapid digital computers were available, the data processing was done using an optical holography technique. The analog radar data were recorded as a holographic interference pattern on photographic film at a scale permitting the film to preserve the signal bandwidths (for example, 1:1,000,000 for a radar using a 0.6-meter wavelength). Then light using, for example, 0.6-micrometer waves (as from a helium–neon laser ) passing through the hologram could project a terrain image at a scale recordable on another film at reasonable processor focal distances of around a meter. This worked because both SAR and phased arrays are fundamentally similar to optical holography, but using microwaves instead of light waves. The "optical data-processors" developed for this radar purpose [46] [47] [48] were the first effective analog optical computer systems, and were, in fact, devised before the holographic technique was fully adapted to optical imaging. Because of the different sources of range and across-range signal structures in the radar signals, optical data-processors for SAR included not only both spherical and cylindrical lenses, but sometimes conical ones. Image appearance[ edit ] The following considerations apply also to real-aperture terrain-imaging radars, but are more consequential when resolution in range is matched to a cross-beam resolution that is available only from a SAR. 25cm resolution SAR image of downtown Cleveland, Ohio by Umbra Range, cross-range, and angles[ edit ] The two dimensions of a radar image are range and cross-range. Radar images of limited patches of terrain can resemble oblique photographs, but not ones taken from the location of the radar. This is because the range coordinate in a radar image is perpendicular to the vertical-angle coordinate of an oblique photo. The apparent entrance-pupil position (or camera center ) for viewing such an image is therefore not as if at the radar, but as if at a point from which the viewer's line of sight is perpendicular to the slant-range direction connecting radar and target, with slant-range increasing from top to bottom of the image. Because slant ranges to level terrain vary in vertical angle, each elevation of such terrain appears as a curved surface, specifically a hyperbolic cosine one. Verticals at various ranges are perpendiculars to those curves. The viewer's apparent looking directions are parallel to the curve's "hypcos" axis. Items directly beneath the radar appear as if optically viewed horizontally (i.e., from the side) and those at far ranges as if optically viewed from directly above. These curvatures are not evident unless large extents of near-range terrain, including steep slant ranges, are being viewed. Visibility[ edit ] When viewed as specified above, fine-resolution radar images of small areas can appear most nearly like familiar optical ones, for two reasons. The first reason is easily understood by imagining a flagpole in the scene. The slant-range to its upper end is less than that to its base. Therefore, the pole can appear correctly top-end up only when viewed in the above orientation. Secondly, the radar illumination then being downward, shadows are seen in their most-familiar "overhead-lighting" direction. The image of the pole's top will overlay that of some terrain point which is on the same slant range arc but at a shorter horizontal range ("ground-range"). Images of scene surfaces which faced both the illumination and the apparent eyepoint will have geometries that resemble those of an optical scene viewed from that eyepoint. However, slopes facing the radar will be foreshortened and ones facing away from it will be lengthened from their horizontal (map) dimensions. The former will therefore be brightened and the latter dimmed. Returns from slopes steeper than perpendicular to slant range will be overlaid on those of lower-elevation terrain at a nearer ground-range, both being visible but intermingled. This is especially the case for vertical surfaces like the walls of buildings. Another viewing inconvenience that arises when a surface is steeper than perpendicular to the slant range is that it is then illuminated on one face but "viewed" from the reverse face. Then one "sees", for example, the radar-facing wall of a building as if from the inside, while the building's interior and the rear wall (that nearest to, hence expected to be optically visible to, the viewer) have vanished, since they lack illumination, being in the shadow of the front wall and the roof. Some return from the roof may overlay that from the front wall, and both of those may overlay return from terrain in front of the building. The visible building shadow will include those of all illuminated items. Long shadows may exhibit blurred edges due to the illuminating antenna's movement during the "time exposure" needed to create the image. Mirroring artefacts and shadows[ edit ] Surfaces that we usually consider rough will, if that roughness consists of relief less than the radar wavelength, behave as smooth mirrors, showing, beyond such a surface, additional images of items in front of it. Those mirror images will appear within the shadow of the mirroring surface, sometimes filling the entire shadow, thus preventing recognition of the shadow. The direction of overlay of any scene point is not directly toward the radar, but toward that point of the SAR's current path direction that is nearest to the target point. If the SAR is "squinting" forward or aft away from the exactly broadside direction, then the illumination direction, and hence the shadow direction, will not be opposite to the overlay direction, but slanted to right or left from it. An image will appear with the correct projection geometry when viewed so that the overlay direction is vertical, the SAR's flight-path is above the image, and range increases somewhat downward. Objects in motion[ edit ] Objects in motion within a SAR scene alter the Doppler frequencies of the returns. Such objects therefore appear in the image at locations offset in the across-range direction by amounts proportional to the range-direction component of their velocity. Road vehicles may be depicted off the roadway and therefore not recognized as road traffic items. Trains appearing away from their tracks are more easily properly recognized by their length parallel to known trackage as well as by the absence of an equal length of railbed signature and of some adjacent terrain, both having been shadowed by the train. While images of moving vessels can be offset from the line of the earlier parts of their wakes, the more recent parts of the wake, which still partake of some of the vessel's motion, appear as curves connecting the vessel image to the relatively quiescent far-aft wake. In such identifiable cases, speed and direction of the moving items can be determined from the amounts of their offsets. The along-track component of a target's motion causes some defocus. Random motions such as that of wind-driven tree foliage, vehicles driven over rough terrain, or humans or other animals walking or running generally render those items not focusable, resulting in blurring or even effective invisibility. These considerations, along with the speckle structure due to coherence, take some getting used to in order to correctly interpret SAR images. To assist in that, large collections of significant target signatures have been accumulated by performing many test flights over known terrains and cultural objects. This section is an excerpt from History of synthetic-aperture radar .[ edit ] The history of synthetic-aperture radar begins in 1951, with the invention of the technology by mathematician Carl A. Wiley , and its development in the following decade. Initially developed for military use, the technology has since been applied in the field of planetary science . Relationship to phased arrays[ edit ] Further information: Phased array A technique closely related to SAR uses an array (referred to as a " phased array ") of real antenna elements spatially distributed over either one or two dimensions perpendicular to the radar-range dimension. These physical arrays are truly synthetic ones, indeed being created by synthesis of a collection of subsidiary physical antennas. Their operation need not involve motion relative to targets. All elements of these arrays receive simultaneously in real time, and the signals passing through them can be individually subjected to controlled shifts of the phases of those signals. One result can be to respond most strongly to radiation received from a specific small scene area, focusing on that area to determine its contribution to the total signal received. The coherently detected set of signals received over the entire array aperture can be replicated in several data-processing channels and processed differently in each. The set of responses thus traced to different small scene areas can be displayed together as an image of the scene. In comparison, a SAR's (commonly) single physical antenna element gathers signals at different positions at different times. When the radar is carried by an aircraft or an orbiting vehicle, those positions are functions of a single variable, distance along the vehicle's path, which is a single mathematical dimension (not necessarily the same as a linear geometric dimension). The signals are stored, thus becoming functions, no longer of time, but of recording locations along that dimension. When the stored signals are read out later and combined with specific phase shifts, the result is the same as if the recorded data had been gathered by an equally long and shaped phased array. What is thus synthesized is a set of signals equivalent to what could have been received simultaneously by such an actual large-aperture (in one dimension) phased array. The SAR simulates (rather than synthesizes) that long one-dimensional phased array. Although the term in the title of this article has thus been incorrectly derived, it is now firmly established by half a century of usage. While operation of a phased array is readily understood as a completely geometric technique, the fact that a synthetic aperture system gathers its data as it (or its target) moves at some speed means that phases which varied with the distance traveled originally varied with time, hence constituted temporal frequencies. Temporal frequencies being the variables commonly used by radar engineers, their analyses of SAR systems are usually (and very productively) couched in such terms. In particular, the variation of phase during flight over the length of the synthetic aperture is seen as a sequence of Doppler shifts of the received frequency from that of the transmitted frequency. Once the received data have been recorded and thus have become timeless, the SAR data-processing situation is also understandable as a special type of phased array, treatable as a completely geometric process. The core of both the SAR and the phased array techniques is that the distances that radar waves travel to and back from each scene element consist of some integer number of wavelengths plus some fraction of a "final" wavelength. Those fractions cause differences between the phases of the re-radiation received at various SAR or array positions. Coherent detection is needed to capture the signal phase information in addition to the signal amplitude information. That type of detection requires finding the differences between the phases of the received signals and the simultaneous phase of a well-preserved sample of the transmitted illumination.
From Wikipedia, the free encyclopedia US weather satellite series "GOES" redirects here. For other uses, see GOES (disambiguation) . Geostationary Operational Environmental Satellite GOES-8, a decommissioned weather satellite. Manufacturer Geostationary Extended Observations The launch of GOES-N, which was renamed GOES-13 after attaining orbit The Geostationary Operational Environmental Satellite (GOES), operated by the United States' National Oceanic and Atmospheric Administration (NOAA)'s National Environmental Satellite, Data, and Information Service division, supports weather forecasting , severe storm tracking, and meteorology research. Spacecraft and ground-based elements of the system work together to provide a continuous stream of environmental data. The National Weather Service (NWS) and the Meteorological Service of Canada use the GOES system for their North American weather monitoring and forecasting operations, and scientific researchers use the data to better understand land, atmosphere, ocean, and climate dynamics. The GOES system uses geosynchronous equatorial satellites that, since the launch of SMS-1 in 1974, have been a basic element of U.S. weather monitoring and forecasting. The procurement, design, and manufacture of GOES satellites is overseen by NASA . NOAA is the official provider of both GOES terrestrial data and GOES space weather data. Data can also be accessed using the SPEDAS software. History[ edit ] The first GOES satellite, GOES-1, was launched in October 1975. Two more followed, launching almost two minutes short of a year apart, on 16 June 1977 and 1978 respectively. Prior to the GOES satellites two Synchronous Meteorological Satellites (SMS) satellites had been launched; SMS-1 in May 1974, and SMS-2 in February 1975. The SMS-derived satellites were spin-stabilized spacecraft, which provided imagery through a Visible and Infrared Spin Scan Radiometer , or VISSR.  The first three GOES satellites used a Philco-Ford bus developed for the earlier Synchronous Meteorological Satellites (SMS) generation. [1] Following the three SMS GOES spacecraft, five satellites were procured from Hughes , which became the first generation GOES satellites. Four of these reached orbit, with GOES-G being lost in a launch failure. [2] First generation GOES satellite The next five GOES satellites were constructed by Space Systems/Loral , under contract to NASA. [3] The imager and sounder instruments were produced by ITT Aerospace/Communication Division. GOES-8 and -9 were designed to operate for three years, while -10, -11 and -12 have expected lifespans of five years. GOES-11 and -12 were launched carrying enough fuel for ten years of operation, in the event that they survived beyond their expected lifespan. A contract to develop four third-generation GOES satellites was awarded to Hughes Corporation , with the satellites scheduled for launch on Delta III rockets between 2002 and 2010. [4] After a merger with Hughes, Boeing took over the development contracts, with launches transferred to the Delta IV , following the Delta III's retirement. The contract for the fourth satellite, GOES-Q, was later cancelled [5] . The first third-generation satellite, GOES-13, was launched in May 2006, originally serving as an on-orbit backup. [6] However, in April 2010, GOES-12 was moved to South America coverage and GOES-13 was moved to the GOES-East role. [7] Third generation satellites have an expected lifespan of seven years, but will carry excess fuel to allow them to operate for longer if possible, as with the last two-second generation satellites. 60° W Vacant The fourth-generation satellites, the GOES-R series, [9] were built by Lockheed Martin using the A2100 satellite bus . The GOES-R series is a four-satellite program (GOES-R, -S, -T and -U) intended to extend the availability of the operational GOES satellite system through 2036. [10] GOES-R launched on 19 November 2016. [9] It was renamed GOES-16 upon reaching orbit. Second of the series GOES-S, was launched on 1 March 2018. It was renamed GOES-17 upon reaching orbit. Operationally available[ edit ] Four GOES satellites are available for operational use. GOES-14 is in storage at 105° W. The launch of this satellite, which was designated GOES-O before orbiting, was delayed several times. [11] [12] It was launched successfully on 27 June 2009 from Space Launch Complex 37, piggybacking on a Delta IV rocket. [13] It underwent Post-Launch Testing until December 2009 and then was placed in on-orbit storage. [14] This satellite is a part of the GOES-N Series. GOES-14 has been and will be activated should another GOES satellite suffer a problem or be decommissioned. [15] It was temporarily designated GOES-East because of technical difficulties with GOES-13 and moved towards the GOES-East location. After resolution of those problems, GOES-14 was returned to storage. [16] GOES-15 , which was designated GOES-P before orbiting, was launched successfully on 4 March 2010. [17] [18] From 2011 to 2018, it occupied the GOES-West position at 135°W over the Pacific Ocean. [19] It moved eastward to 128° W beginning on 29 October 2018 in order to make room for GOES-17 , which took over the GOES-West position on 10 December 2018. [20] GOES-15 operated in tandem with GOES-17 for some time, but was retired in early 2020 and moved to a parking orbit. [21] [22] GOES-15 was temporarily returned to operational status in August 2020 to fill a gap in the sensor capabilities of GOES-17 due to a hardware issue. GOES-16 occupies the GOES-East position at 75° W. This satellite, which was designated GOES-R before orbiting, was launched by an Atlas V rocket from Space Launch Complex 41 at Cape Canaveral Air Force Station in Florida on 19 November 2016. [23] It underwent Post-Launch Testing through early 2017 before replacing GOES-13 as GOES-East. GOES-17 occupies the GOES-West position at 137.2° W. The satellite, designated as GOES-S before orbiting, was launched by an Atlas V rocket from Space Launch Complex 41 on 1 March 2018. [24] Following post-launch testing and troubleshooting of a problem in its imager, the satellite was declared operational in February 2019. Inactive or repurposed[ edit ] Several GOES satellites are still in orbit but are either inactive or have been re-purposed. Although GOES-3 ceased to be used for weather operations in 1989, it spent over 20 years as a critical part of communications between the U.S. and Amundsen–Scott South Pole Station before being decommissioned in 2016. [25] Geostationary satellites expend fuel to keep themselves stationary over the equator, and thus cannot normally ordinarily be seen from the poles. When that fuel is depleted, solar and lunar perturbations increase the satellite's inclination so that its ground track begins to describe an analemma (a figure-8 in the north-south direction). This usually ends the satellite's primary mission. However, when the inclination is high enough the satellite may begin to rise above the polar horizons at the extremes of the figure-8, as was the case for GOES-3. A nine-meter dish was constructed at the station, and communication with the satellite could be obtained for about five hours per day. Data rates were around 2.048 megabytes/second (bi-directional) under optimum conditions. GOES-8 , which was designated GOES-I before orbiting, was the GOES-East satellite when it was in operation. It is in a parking orbit and is drifting westerly at a rate of about 4° daily. [26] It was decommissioned on 1 April 2003 and deactivated on 5 May 2004 after the failure of its propulsion system. [27] GOES-10 , which was designated GOES-K before orbiting, was decommissioned on 2 December 2009 and was boosted to a graveyard orbit . It no longer had the fuel for required maneuvers to keep it on station. [28] GOES-11 , which was designated GOES-L before orbiting, had a partial failure on 6 December 2011. It was decommissioned on 16 December 2011 and boosted into a graveyard orbit. GOES-12 , which was designated GOES-M before orbiting, was decommissioned on 16 August 2013 and boosted into a graveyard orbit. [29] GOES-13 , which was designated GOES-N before orbiting, was decommissioned on 3 January 2018 and boosted into storage orbit. It was transferred to the U.S. Space Force and positioned at 61.5ºE under the new name EWS-G1. Following three years of monitoring the Indian Ocean, EWS-G1 was retired on 31 October 2023 when EWS-G2 (formerly GOES-15) took over. GOES-15 , which was designated GOES-P before orbiting, was launched successfully on 4 March 2010. [30] [18] From 2011 to 2018, it occupied the GOES-West position at 135°W over the Pacific Ocean. [31] It moved eastward to 128° W beginning on 29 October 2018 in order to make room for GOES-17 , which took over the GOES-West position on 10 December 2018. [20] GOES-15 operated in tandem with GOES-17 for some time, but was retired in early 2020 and moved to a parking orbit. [32] [33] GOES-15 was temporarily returned to operational status in August 2020 to fill a gap in the sensor capabilities of GOES-17 due to a hardware issue. Like GOES-13, GOES-15 was then transferred to the U.S. Space Force and renamed EWS-G2 to monitor the Indian Ocean until approximately 2030. Coverage map of GOES-11 and GOES-12 when active (2007). GOES-12 visible light image. Purpose[ edit ] GOES data relay pattern. Designed to operate in geostationary orbit 35,790 kilometres (22,240 mi) above the Earth, the GOES spacecraft continuously view the continental United States , the Pacific and Atlantic Oceans, Central America , South America , and southern Canada. The three-axis, body-stabilized design enables the sensors to "stare" at the Earth and thus more frequently image clouds, monitor the Earth's surface temperature and water vapour fields, and sound the atmosphere for its vertical thermal and vapor structures. The evolution of atmospheric phenomena can be followed, ensuring real-time coverage of meteorological events such as severe local storms and tropical cyclones . The importance of this capability was proven during hurricanes Hugo (1989) and Andrew (1992). The GOES spacecraft also enhance operational services and improve support for atmospheric science research, numerical weather prediction models, and environmental sensor design and development. Satellite data is broadcast on the L-band , and received at the NOAA Command and Data Acquisition ground station at Wallops Island, Virginia [34] from which it is disseminated to users. Additionally, anyone may receive data directly from the satellites by utilizing a small dish , and processing the data with special software. [35] The GOES satellites are controlled from the Satellite Operations Control Center in Suitland, Maryland. During significant weather or other events, the normal schedules can be altered to provide the coverage requested by the NWS and other agencies. Space Weather -- March 2012. [36] GOES-12 and above also have provided a platform for the Solar X-Ray Imager (SXI) and space environment monitoring (SEM) instruments. The SXI provides high-cadence monitoring of large scale solar structures to support the Space Environment Services Center's (SESC) mission. The SXI unit on GOES-13, however, was damaged by a solar flare in 2006. The SESC, as the nation's "space weather" service, receives, monitors, and interprets a wide variety of solar-terrestrial data. It also issues reports, alerts, and forecasts for special events such as solar flares or geomagnetic storms. This information is important to the operation of military and civilian radio wave and satellite communication and navigation systems. The information also is important to electric power networks, the missions of geophysical explorers, Space Station astronauts, high-altitude aviators, and scientific researchers. The SEM measures the effect of the Sun on the near-Earth solar-terrestrial electromagnetic environment, providing real-time data to the SESC. Payload[ edit ] The main mission of a GOES satellite is carried out by the primary payload instruments, which are the Imager and the Sounder. The Imager is a multichannel instrument that senses infrared radiant energy and visible reflected solar energy from the Earth's surface and atmosphere. The Sounder provides data for vertical atmospheric temperature and moisture profiles, surface and cloud top temperature, and ozone distribution. GOES also offers the Data Collection System, a ground-based meteorological platform satellite data collection and relay service. [37] Other instruments on board the spacecraft are the SEM set, which consists of a magnetometer , an X-ray sensor, a high energy proton and alpha particle detector, and an energetic particles sensor. The GOES-N series (GOES-13 through GOES-15) spacecraft also have a sun-pointed extreme ultraviolet sensor. Invertible GOES logo designed for Space Systems/Loral by Scott Kim In addition, the GOES satellites carry a search and rescue repeater that collects data from Emergency Position-Indicating Radio Beacons and Emergency Locator Transmitter beacons, which are used during search-and-rescue operations by the U.S. Air Force Rescue Coordination Center . The proposed instrument package for the GOES-R series initially included the following: [38] [39] Advanced Baseline Imager (ABI) Hyperspectral Environmental Suite (HES) Space Environment In-Situ Suite (SEISS), which includes two Magnetospheric Particle Sensors (MPS-HI and MPS-LO), an Energetic Heavy Ion Sensor, and a Solar and Galactic Proton Sensor Solar Imaging Suite, which includes the Solar Ultraviolet Imager (SUVI), the Solar X-Ray Sensor (XRS), and the Extreme Ultraviolet Sensor (EUVS)
Toggle the table of contents Global Positioning System From Wikipedia, the free encyclopedia American satellite-based radio navigation service This article is about the American global navigation satellite system. For similar systems, see Satellite navigation . "GPS" redirects here. For GPS devices, see Satellite navigation device . For other uses, see GPS (disambiguation) . Global Positioning System (GPS) 30–500 cm (0.98–16 ft) Constellation size February 22, 1978; 46 years ago (1978-02-22) Total launches $12 billion [1] (initial constellation)$1.84 billion per year (2023) [1] (operating cost) Website Artist's impression of GPS Block IIR satellite in Earth orbit Civilian GPS receivers (" GPS navigation device ") in a marine application e The Global Positioning System (GPS), originally Navstar GPS, [2] is a satellite-based radio navigation system owned by the United States government and operated by the United States Space Force . [3] It is one of the global navigation satellite systems (GNSS) that provide geolocation and time information to a GPS receiver anywhere on or near the Earth where there is an unobstructed line of sight to four or more GPS satellites. [4] It does not require the user to transmit any data, and operates independently of any telephonic or Internet reception, though these technologies can enhance the usefulness of the GPS positioning information. It provides critical positioning capabilities to military, civil, and commercial users around the world. Although the United States government created, controls and maintains the GPS system, it is freely accessible to anyone with a GPS receiver. [5] Overview[ edit ] The GPS project was started by the U.S. Department of Defense in 1973. The first prototype spacecraft was launched in 1978 and the full constellation of 24 satellites became operational in 1993. Originally limited to use by the United States military, civilian use was allowed from the 1980s following an executive order from President Ronald Reagan after the Korean Air Lines Flight 007 disaster. [6] Advances in technology and new demands on the existing system have now led to efforts to modernize the GPS and implement the next generation of GPS Block IIIA satellites and Next Generation Operational Control System (OCX) [7] which was authorized by the U.S. Congress in 2000. From the early 1990s, GPS positional accuracy was degraded by the United States government using a technology called Selective Availability , which could selectively degrade or deny access to the system at any time, [8] as happened to the Indian military in 1999 during the Kargil War . As a result, several non-US entities—including Russia , China , India , Japan , and the European Union —have developed or are developing their own global or regional satellite navigation systems. Selective Availability was discontinued on May 1, 2000, in accordance with a bill signed into law by President Bill Clinton . [9] When Selective Availabilty was discontinued, GPS was accurate to about 5 meters (16 ft). GPS receivers that use the L5 band have much higher accuracy of 30 centimeters (12 in), while those for high-end applications such as engineering and land surveying are accurate to within 2 cm (3⁄4 in) and can even provide sub-millimeter accuracy with long-term measurements. [9] [10] [11] Consumer devices such as smartphones can be accurate to 4.9 m (16 ft) or better when used with assistive services like Wi-Fi positioning . [12] As of July 2023 [update] , 18 GPS satellites broadcast L5 signals, which are considered pre-operational prior to being broadcast by a full complement of 24 satellites in 2027. [13] Air Force film introducing the Navstar Global Positioning System, circa 1977 GPS constellation system animation The GPS project was launched in the United States in 1973 to overcome the limitations of previous navigation systems, [14] combining ideas from several predecessors, including classified engineering design studies from the 1960s. The U.S. Department of Defense developed the system, which originally used 24 satellites, for use by the United States military, and became fully operational in 1995. Civilian use was allowed from the 1980s. Roger L. Easton of the Naval Research Laboratory , Ivan A. Getting of The Aerospace Corporation , and Bradford Parkinson of the Applied Physics Laboratory are credited with inventing it. [15] The work of Gladys West on the creation of the mathematical geodetic Earth model is credited as instrumental in the development of computational techniques for detecting satellite positions with the precision needed for GPS. [16] [17] The design of GPS is based partly on similar ground-based radio-navigation systems, such as LORAN and the Decca Navigator , developed in the early 1940s. In 1955, Friedwardt Winterberg proposed a test of general relativity —detecting time slowing in a strong gravitational field using accurate atomic clocks placed in orbit inside artificial satellites. Special and general relativity predicted that the clocks on GPS satellites, as observed by those on Earth, run 38 microseconds faster per day than those on the Earth. The design of GPS corrects for this difference; because without doing so, GPS calculated positions would accumulate errors of up to 10 kilometers per day (6 mi/d). [18] Predecessors[ edit ] When the Soviet Union launched its first artificial satellite ( Sputnik 1 ) in 1957, two American physicists, William Guier and George Weiffenbach, at Johns Hopkins University 's Applied Physics Laboratory (APL) decided to monitor its radio transmissions. [19] Within hours they realized that, because of the Doppler effect , they could pinpoint where the satellite was along its orbit. The Director of the APL gave them access to their UNIVAC to do the heavy calculations required. The Naval Research Laboratory ’s managers for the Timation program and, later, the GPS program: Roger L. Easton (left) and Al Bartholemew . Early the next year, Frank McClure, the deputy director of the APL, asked Guier and Weiffenbach to investigate the inverse problem: pinpointing the user's location, given the satellite's. (At the time, the Navy was developing the submarine-launched Polaris missile, which required them to know the submarine's location.) This led them and APL to develop the TRANSIT system. [20] In 1959, ARPA (renamed DARPA in 1972) also played a role in TRANSIT. [21] [22] [23] TRANSIT was first successfully tested in 1960. [24] It used a constellation of five satellites and could provide a navigational fix approximately once per hour. In 1967, the U.S. Navy developed the Timation satellite, which proved the feasibility of placing accurate clocks in space, a technology required for GPS. In the 1970s, the ground-based OMEGA navigation system, based on phase comparison of signal transmission from pairs of stations, [25] became the first worldwide radio navigation system. Limitations of these systems drove the need for a more universal navigation solution with greater accuracy. Although there were wide needs for accurate navigation in military and civilian sectors, almost none of those was seen as justification for the billions of dollars it would cost in research, development, deployment, and operation of a constellation of navigation satellites. During the Cold War arms race , the nuclear threat to the existence of the United States was the one need that did justify this cost in the view of the United States Congress. This deterrent effect is why GPS was funded. It is also the reason for the ultra-secrecy at that time. The nuclear triad consisted of the United States Navy's submarine-launched ballistic missiles (SLBMs) along with United States Air Force (USAF) strategic bombers and intercontinental ballistic missiles (ICBMs). Considered vital to the nuclear deterrence posture, accurate determination of the SLBM launch position was a force multiplier . Precise navigation would enable United States ballistic missile submarines to get an accurate fix of their positions before they launched their SLBMs. [26] The USAF, with two thirds of the nuclear triad, also had requirements for a more accurate and reliable navigation system. The U.S. Navy and U.S. Air Force were developing their own technologies in parallel to solve what was essentially the same problem. To increase the survivability of ICBMs, there was a proposal to use mobile launch platforms (comparable to the Soviet SS-24 and SS-25 ) and so the need to fix the launch position had similarity to the SLBM situation. In 1960, the Air Force proposed a radio-navigation system called MOSAIC (MObile System for Accurate ICBM Control) that was essentially a 3-D LORAN. A follow-on study, Project 57, was performed in 1963 and it was "in this study that the GPS concept was born". That same year, the concept was pursued as Project 621B, which had "many of the attributes that you now see in GPS" [27] and promised increased accuracy for Air Force bombers as well as ICBMs. Navigation Technology Satellite – II (Timation IV): NTS-II, the first satellite completely designed and built by NRL under GPS Joint Program funding. Launched June 23, 1977. Updates from the Navy TRANSIT system were too slow for the high speeds of Air Force operation. The Naval Research Laboratory (NRL) continued making advances with their Timation (Time Navigation) satellites, first launched in 1967, second launched in 1969, with the third in 1974 carrying the first atomic clock into orbit and the fourth launched in 1977. [28] Another important predecessor to GPS came from a different branch of the United States military. In 1964, the United States Army orbited its first Sequential Collation of Range ( SECOR ) satellite used for geodetic surveying. [29] The SECOR system included three ground-based transmitters at known locations that would send signals to the satellite transponder in orbit. A fourth ground-based station, at an undetermined position, could then use those signals to fix its location precisely. The last SECOR satellite was launched in 1969. [30] Development[ edit ] With these parallel developments in the 1960s, it was realized that a superior system could be developed by synthesizing the best technologies from 621B, Transit, Timation, and SECOR in a multi-service program. Satellite orbital position errors, induced by variations in the gravity field and radar refraction among others, had to be resolved. A team led by Harold L Jury of Pan Am Aerospace Division in Florida from 1970 to 1973, used real-time data assimilation and recursive estimation to do so, reducing systematic and residual errors to a manageable level to permit accurate navigation. [31] During Labor Day weekend in 1973, a meeting of about twelve military officers at the Pentagon discussed the creation of a Defense Navigation Satellite System (DNSS). It was at this meeting that the real synthesis that became GPS was created. Later that year, the DNSS program was named Navstar. [32] Navstar is often erroneously considered an acronym for "NAVigation System using Timing And Ranging" but was never considered as such by the GPS Joint Program Office (TRW may have once advocated for a different navigational system that used that acronym). [33] With the individual satellites being associated with the name Navstar (as with the predecessors Transit and Timation), a more fully encompassing name was used to identify the constellation of Navstar satellites, Navstar-GPS. [34] Ten " Block I " prototype satellites were launched between 1978 and 1985 (an additional unit was destroyed in a launch failure). [35] The effect of the ionosphere on radio transmission was investigated in a geophysics laboratory of Air Force Cambridge Research Laboratory , renamed to Air Force Geophysical Research Lab (AFGRL) in 1974. AFGRL developed the Klobuchar model for computing ionospheric corrections to GPS location. [36] Of note is work done by Australian space scientist Elizabeth Essex-Cohen at AFGRL in 1974. She was concerned with the curving of the paths of radio waves ( atmospheric refraction ) traversing the ionosphere from NavSTAR satellites. [37] After Korean Air Lines Flight 007 , a Boeing 747 carrying 269 people, was shot down by a Soviet interceptor aircraft after straying in prohibited airspace because of navigational errors, [38] in the vicinity of Sakhalin and Moneron Islands , President Ronald Reagan issued a directive making GPS freely available for civilian use, once it was sufficiently developed, as a common good. [39] The first Block II satellite was launched on February 14, 1989, [40] and the 24th satellite was launched in 1994. The GPS program cost at this point, not including the cost of the user equipment but including the costs of the satellite launches, has been estimated at US$5 billion (equivalent to $10 billion in 2023). [41] Initially, the highest-quality signal was reserved for military use, and the signal available for civilian use was intentionally degraded, in a policy known as Selective Availability . This changed on May 1, 2000, with President Bill Clinton signing a policy directive to turn off Selective Availability to provide the same accuracy to civilians that was afforded to the military. The directive was proposed by the U.S. Secretary of Defense, William Perry , in view of the widespread growth of differential GPS services by private industry to improve civilian accuracy. Moreover, the U.S. military was developing technologies to deny GPS service to potential adversaries on a regional basis. [42] Selective Availability was removed from the GPS architecture beginning with GPS-III. Since its deployment, the U.S. has implemented several improvements to the GPS service, including new signals for civil use and increased accuracy and integrity for all users, all the while maintaining compatibility with existing GPS equipment. Modernization of the satellite system has been an ongoing initiative by the U.S. Department of Defense through a series of satellite acquisitions to meet the growing needs of the military, civilians, and the commercial market. As of early 2015, high-quality Standard Positioning Service (SPS) GPS receivers provided horizontal accuracy of better than 3.5 meters (11 ft), [9] although many factors such as receiver and antenna quality and atmospheric issues can affect this accuracy. GPS is owned and operated by the United States government as a national resource. The Department of Defense is the steward of GPS. The Interagency GPS Executive Board (IGEB) oversaw GPS policy matters from 1996 to 2004. After that, the National Space-Based Positioning, Navigation and Timing Executive Committee was established by presidential directive in 2004 to advise and coordinate federal departments and agencies on matters concerning the GPS and related systems. [43] The executive committee is chaired jointly by the Deputy Secretaries of Defense and Transportation. Its membership includes equivalent-level officials from the Departments of State, Commerce, and Homeland Security, the Joint Chiefs of Staff and NASA . Components of the executive office of the president participate as observers to the executive committee, and the FCC chairman participates as a liaison. The U.S. Department of Defense is required by law to "maintain a Standard Positioning Service (as defined in the federal radio navigation plan and the standard positioning service signal specification) that will be available on a continuous, worldwide basis" and "develop measures to prevent hostile use of GPS and its augmentations without unduly disrupting or degrading civilian uses". Timeline and modernization[ edit ] This section is in list format but may read better as prose . You can help by converting this section , if appropriate. Editing help is available. (July 2023) Summary of satellites [44] [45] [46] Block Currently in orbit and healthy Success 32 (Last update: September 3, 2023) USA-203 from Block IIR-M is unhealthy [47] For a more complete list, see List of GPS satellites In 1972, the USAF Central Inertial Guidance Test Facility (Holloman AFB) conducted developmental flight tests of four prototype GPS receivers in a Y configuration over White Sands Missile Range , using ground-based pseudo-satellites. [48] In 1978, the first experimental Block-I GPS satellite was launched. [35] In 1983, after Soviet interceptor aircraft shot down the civilian airliner KAL 007 that strayed into prohibited airspace because of navigational errors, killing all 269 people on board, U.S. President Ronald Reagan announced that GPS would be made available for civilian uses once it was completed, [49] [50] although it had been publicly known as early as 1979, that the CA code (Coarse/Acquisition code) would be available to civilian users. [51] [52] By 1985, ten more experimental Block-I satellites had been launched to validate the concept. Beginning in 1988, command and control of these satellites was moved from Onizuka AFS , California to the 2nd Satellite Control Squadron (2SCS) located at Falcon Air Force Station in Colorado Springs, Colorado . [53] [54] On February 14, 1989, the first modern Block-II satellite was launched. The Gulf War from 1990 to 1991 was the first conflict in which the military widely used GPS. [55] In 1991, a project to create a miniature GPS receiver successfully ended, replacing the previous 16 kg (35 lb) military receivers with a 1.25 kg (2.8 lb) handheld receiver. [22] In 1991, TomTom , a Dutch sat-nav manufacturer was founded. In 1992, the 2nd Space Wing , which originally managed the system, was inactivated and replaced by the 50th Space Wing . Emblem of the 50th Space Wing By December 1993, GPS achieved initial operational capability (IOC), with a full constellation (24 satellites) available and providing the Standard Positioning Service (SPS). [56] Full Operational Capability (FOC) was declared by Air Force Space Command (AFSPC) in April 1995, signifying full availability of the military's secure Precise Positioning Service (PPS). [56] In 1996, recognizing the importance of GPS to civilian users as well as military users, U.S. President Bill Clinton issued a policy directive [57] declaring GPS a dual-use system and establishing an Interagency GPS Executive Board to manage it as a national asset. In 1998, United States Vice President Al Gore announced plans to upgrade GPS with two new civilian signals for enhanced user accuracy and reliability, particularly with respect to aviation safety, and in 2000 the United States Congress authorized the effort, referring to it as GPS III . On May 2, 2000 "Selective Availability" was discontinued as a result of the 1996 executive order, allowing civilian users to receive a non-degraded signal globally. In 2004, the United States government signed an agreement with the European Community establishing cooperation related to GPS and Europe's Galileo system . In 2004, United States President George W. Bush updated the national policy and replaced the executive board with the National Executive Committee for Space-Based Positioning, Navigation, and Timing. [58] November 2004, Qualcomm announced successful tests of assisted GPS for mobile phones . [59] In 2005, the first modernized GPS satellite was launched and began transmitting a second civilian signal (L2C) for enhanced user performance. [60] On September 14, 2007, the aging mainframe-based Ground Segment Control System was transferred to the new Architecture Evolution Plan. [61] On May 19, 2009, the United States Government Accountability Office issued a report warning that some GPS satellites could fail as soon as 2010. [62] On May 21, 2009, the Air Force Space Command allayed fears of GPS failure, saying: "There's only a small risk we will not continue to exceed our performance standard." [63] On January 11, 2010, an update of ground control systems caused a software incompatibility with 8,000 to 10,000 military receivers manufactured by a division of Trimble Navigation Limited of Sunnyvale, Calif.[ clarification needed ] [64] On February 25, 2010, [65] the U.S. Air Force awarded the contract to Raytheon Company to develop the GPS Next Generation Operational Control System (OCX) to improve accuracy and availability of GPS navigation signals, and serve as a critical part of GPS modernization. Awards[ edit ] AFSPC Vice Commander Lt. Gen. DT Thompson presents Gladys West with an award as she is inducted into the Air Force Space and Missile Pioneers Hall of Fame. On February 10, 1993, the National Aeronautic Association selected the GPS Team as winners of the 1992 Robert J. Collier Trophy , the US's most prestigious aviation award. This team combines researchers from the Naval Research Laboratory, the USAF, the Aerospace Corporation , Rockwell International Corporation, and IBM Federal Systems Company. The citation honors them "for the most significant development for safe and efficient navigation and surveillance of air and spacecraft since the introduction of radio navigation 50 years ago". Two GPS developers received the National Academy of Engineering Charles Stark Draper Prize for 2003: Ivan Getting , emeritus president of The Aerospace Corporation and an engineer at MIT , established the basis for GPS, improving on the World War II land-based radio system called LORAN (Long-range Radio Aid to Navigation). Bradford Parkinson , professor of aeronautics and astronautics at Stanford University , conceived the present satellite-based system in the early 1960s and developed it in conjunction with the U.S. Air Force. Parkinson served twenty-one years in the Air Force, from 1957 to 1978, and retired with the rank of colonel. GPS developer Roger L. Easton received the National Medal of Technology on February 13, 2006. [66] Francis X. Kane (Col. USAF, ret.) was inducted into the U.S. Air Force Space and Missile Pioneers Hall of Fame at Lackland A.F.B., San Antonio, Texas, March 2, 2010, for his role in space technology development and the engineering design concept of GPS conducted as part of Project 621B. In 1998, GPS technology was inducted into the Space Foundation Space Technology Hall of Fame . [67] On October 4, 2011, the International Astronautical Federation (IAF) awarded the Global Positioning System (GPS) its 60th Anniversary Award, nominated by IAF member, the American Institute for Aeronautics and Astronautics (AIAA). The IAF Honors and Awards Committee recognized the uniqueness of the GPS program and the exemplary role it has played in building international collaboration for the benefit of humanity. [68] On December 6, 2018, Gladys West was inducted into the Air Force Space and Missile Pioneers Hall of Fame in recognition of her work on an extremely accurate geodetic Earth model, which was ultimately used to determine the orbit of the GPS constellation. [69] On February 12, 2019, four founding members of the project were awarded the Queen Elizabeth Prize for Engineering with the chair of the awarding board stating: "Engineering is the foundation of civilisation; there is no other foundation; it makes things happen. And that's exactly what today's Laureates have done –  they've made things happen. They've re-written, in a major way, the infrastructure of our world." [70] Principles[ edit ] This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (March 2015) ( Learn how and when to remove this template message ) The GPS satellites carry very stable atomic clocks that are synchronized with one another and with the reference atomic clocks at the ground control stations; any drift of the clocks aboard the satellites from the reference time maintained on the ground stations is corrected regularly. [71] Since the speed of radio waves ( speed of light ) [72] is constant and independent of the satellite speed, the time delay between when the satellite transmits a signal and the ground station receives it is proportional to the distance from the satellite to the ground station. With the distance information collected from multiple ground stations, the location coordinates of any satellite at any time can be calculated with great precision. Each GPS satellite carries an accurate record of its own position and time, and broadcasts that data continuously. Based on data received from multiple GPS satellites , an end user's GPS receiver can calculate its own four-dimensional position in spacetime ; However, at a minimum, four satellites must be in view of the receiver for it to compute four unknown quantities (three position coordinates and the deviation of its own clock from satellite time). [73] More detailed description[ edit ] Each GPS satellite continually broadcasts a signal ( carrier wave with modulation ) that includes: A pseudorandom code (sequence of ones and zeros) that is known to the receiver. By time-aligning a receiver-generated version and the receiver-measured version of the code, the time of arrival (TOA) of a defined point in the code sequence, called an epoch, can be found in the receiver clock time scale A message that includes the time of transmission (TOT) of the code epoch (in GPS time scale) and the satellite position at that time Conceptually, the receiver measures the TOAs (according to its own clock) of four satellite signals. From the TOAs and the TOTs, the receiver forms four time of flight (TOF) values, which are (given the speed of light) approximately equivalent to receiver-satellite ranges plus time difference between the receiver and GPS satellites multiplied by speed of light, which are called pseudo-ranges. The receiver then computes its three-dimensional position and clock deviation from the four TOFs. In practice the receiver position (in three dimensional Cartesian coordinates with origin at the Earth's center) and the offset of the receiver clock relative to the GPS time are computed simultaneously, using the navigation equations to process the TOFs. The receiver's Earth-centered solution location is usually converted to latitude , longitude and height relative to an ellipsoidal Earth model. The height may then be further converted to height relative to the geoid , which is essentially mean sea level. These coordinates may be displayed, such as on a moving map display , or recorded or used by some other system, such as a vehicle guidance system. Further information: § Geometric interpretation Although usually not formed explicitly in the receiver processing, the conceptual time differences of arrival (TDOAs) define the measurement geometry. Each TDOA corresponds to a hyperboloid of revolution (see Multilateration ). The line connecting the two satellites involved (and its extensions) forms the axis of the hyperboloid. The receiver is located at the point where three hyperboloids intersect. [74] [75] It is sometimes incorrectly said that the user location is at the intersection of three spheres. While simpler to visualize, this is the case only if the receiver has a clock synchronized with the satellite clocks (i.e., the receiver measures true ranges to the satellites rather than range differences). There are marked performance benefits to the user carrying a clock synchronized with the satellites. Foremost is that only three satellites are needed to compute a position solution. If it were an essential part of the GPS concept that all users needed to carry a synchronized clock, a smaller number of satellites could be deployed, but the cost and complexity of the user equipment would increase. Receiver in continuous operation[ edit ] The description above is representative of a receiver start-up situation. Most receivers have a track algorithm , sometimes called a tracker, that combines sets of satellite measurements collected at different times—in effect, taking advantage of the fact that successive receiver positions are usually close to each other. After a set of measurements are processed, the tracker predicts the receiver location corresponding to the next set of satellite measurements. When the new measurements are collected, the receiver uses a weighting scheme to combine the new measurements with the tracker prediction. In general, a tracker can (a) improve receiver position and time accuracy, (b) reject bad measurements, and (c) estimate receiver speed and direction. The disadvantage of a tracker is that changes in speed or direction can be computed only with a delay, and that derived direction becomes inaccurate when the distance traveled between two position measurements drops below or near the random error of position measurement. GPS units can use measurements of the Doppler shift of the signals received to compute velocity accurately. [76] More advanced navigation systems use additional sensors like a compass or an inertial navigation system to complement GPS. Non-navigation applications[ edit ] For a list of applications, see § Applications . GPS requires four or more satellites to be visible for accurate navigation. The solution of the navigation equations gives the position of the receiver along with the difference between the time kept by the receiver's on-board clock and the true time-of-day, thereby eliminating the need for a more precise and possibly impractical receiver based clock. Applications for GPS such as time transfer , traffic signal timing, and synchronization of cell phone base stations , make use of this cheap and highly accurate timing. Some GPS applications use this time for display, or, other than for the basic position calculations, do not use it at all. Although four satellites are required for normal operation, fewer apply in special cases. If one variable is already known, a receiver can determine its position using only three satellites. For example, a ship on the open ocean usually has a known elevation close to 0m , and the elevation of an aircraft may be known. [a] Some GPS receivers may use additional clues or assumptions such as reusing the last known altitude, dead reckoning , inertial navigation , or including information from the vehicle computer, to give a (possibly degraded) position when fewer than four satellites are visible. [77] [78] [79] Structure[ edit ] This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (March 2015) ( Learn how and when to remove this template message ) The current GPS consists of three major segments. These are the space segment, a control segment, and a user segment. [52] The U.S. Space Force develops, maintains, and operates the space and control segments. GPS satellites broadcast signals from space, and each GPS receiver uses these signals to calculate its three-dimensional location (latitude, longitude, and altitude) and the current time. [80] See also: GPS satellite blocks and List of GPS satellites GPS II underwent a four-month series of qualification tests in the AEDC Mark I Space Chamber to determine whether the satellite could withstand extreme heat and cold in space, 1985. A visual example of a 24-satellite GPS constellation in motion with the Earth rotating. Notice how the number of satellites in view from a given point on the Earth's surface changes with time. The point in this example is in Golden, Colorado, USA ( 39°44′49″N 105°12′39″W﻿ / ﻿39.7469°N 105.2108°W﻿ / 39.7469; -105.2108 ). The space segment (SS) is composed of 24 to 32 satellites, or Space Vehicles (SV), in medium Earth orbit , and also includes the payload adapters to the boosters required to launch them into orbit. The GPS design originally called for 24 SVs, eight each in three approximately circular orbits , [81] but this was modified to six orbital planes with four satellites each. [82] The six orbit planes have approximately 55° inclination (tilt relative to the Earth's equator ) and are separated by 60° right ascension of the ascending node (angle along the equator from a reference point to the orbit's intersection). [83] The orbital period is one-half of a sidereal day , i.e., 11 hours and 58 minutes, so that the satellites pass over the same locations [84] or almost the same locations [85] every day. The orbits are arranged so that at least six satellites are always within line of sight from everywhere on the Earth's surface (see animation at right). [86] The result of this objective is that the four satellites are not evenly spaced (90°) apart within each orbit. In general terms, the angular difference between satellites in each orbit is 30°, 105°, 120°, and 105° apart, which sum to 360°. [87] Orbiting at an altitude of approximately 20,200 km (12,600 mi); orbital radius of approximately 26,600 km (16,500 mi), [88] each SV makes two complete orbits each sidereal day , repeating the same ground track each day. [89] This was very helpful during development because even with only four satellites, correct alignment means all four are visible from one spot for a few hours each day. For military operations, the ground track repeat can be used to ensure good coverage in combat zones. As of February 2019 [update] , [90] there are 31 satellites in the GPS constellation , 27 of which are in use at a given time with the rest allocated as stand-bys. A 32nd was launched in 2018, but as of July 2019 is still in evaluation. More decommissioned satellites are in orbit and available as spares. The additional satellites improve the precision of GPS receiver calculations by providing redundant measurements. With the increased number of satellites, the constellation was changed to a nonuniform arrangement. Such an arrangement was shown to improve accuracy but also improves reliability and availability of the system, relative to a uniform system, when multiple satellites fail. [91] With the expanded constellation, nine satellites are usually visible at any time from any point on the Earth with a clear horizon, ensuring considerable redundancy over the minimum four satellites needed for a position. Ground monitor station used from 1984 to 2007, on display at the Air Force Space and Missile Museum The control segment (CS) is composed of: a master control station (MCS), an alternative master control station, four dedicated ground antennas, and six dedicated monitor stations. The MCS can also access Satellite Control Network (SCN) ground antennas (for additional command and control capability) and NGA ( National Geospatial-Intelligence Agency ) monitor stations. The flight paths of the satellites are tracked by dedicated U.S. Space Force monitoring stations in Hawaii, Kwajalein Atoll , Ascension Island , Diego Garcia , Colorado Springs, Colorado and Cape Canaveral , along with shared NGA monitor stations operated in England, Argentina, Ecuador, Bahrain, Australia and Washington DC. [92] The tracking information is sent to the MCS at Schriever Space Force Base 25 km (16 mi) ESE of Colorado Springs, which is operated by the 2nd Space Operations Squadron (2 SOPS) of the U.S. Space Force. Then 2 SOPS contacts each GPS satellite regularly with a navigational update using dedicated or shared (AFSCN) ground antennas (GPS dedicated ground antennas are located at Kwajalein , Ascension Island , Diego Garcia , and Cape Canaveral ). These updates synchronize the atomic clocks on board the satellites to within a few nanoseconds of each other, and adjust the ephemeris of each satellite's internal orbital model. The updates are created by a Kalman filter that uses inputs from the ground monitoring stations, space weather information, and various other inputs. [93] When a satellite's orbit is being adjusted, the satellite is marked unhealthy, so receivers do not use it. After the maneuver, engineers track the new orbit from the ground, upload the new ephemeris, and mark the satellite healthy again. The operation control segment (OCS) currently serves as the control segment of record. It provides the operational capability that supports GPS users and keeps the GPS operational and performing within specification. OCS successfully replaced the legacy 1970s-era mainframe computer at Schriever Air Force Base in September 2007. After installation, the system helped enable upgrades and provide a foundation for a new security architecture that supported U.S. armed forces. OCS will continue to be the ground control system of record until the new segment, Next Generation GPS Operation Control System [7] (OCX), is fully developed and functional. The US Department of Defense has claimed that the new capabilities provided by OCX will be the cornerstone for revolutionizing GPS's mission capabilities, enabling U.S. Space Force to greatly enhance GPS operational services to U.S. combat forces, civil partners and myriad domestic and international users. [94] [95] The GPS OCX program also will reduce cost, schedule and technical risk. It is designed to provide 50% [96] sustainment cost savings through efficient software architecture and Performance-Based Logistics. In addition, GPS OCX is expected to cost millions less than the cost to upgrade OCS while providing four times the capability. The GPS OCX program represents a critical part of GPS modernization and provides significant information assurance improvements over the current GPS OCS program. OCX will have the ability to control and manage GPS legacy satellites as well as the next generation of GPS III satellites, while enabling the full array of military signals. Built on a flexible architecture that can rapidly adapt to the changing needs of today's and future GPS users allowing immediate access to GPS data and constellation status through secure, accurate and reliable information. Provides the warfighter with more secure, actionable and predictive information to enhance situational awareness. Enables new modernized signals (L1C, L2C, and L5) and has M-code capability, which the legacy system is unable to do. Provides significant information assurance improvements over the current program including detecting and preventing cyber attacks, while isolating, containing and operating during such attacks. Supports higher volume near real-time command and control capabilities and abilities. On September 14, 2011, [97] the U.S. Air Force announced the completion of GPS OCX Preliminary Design Review and confirmed that the OCX program is ready for the next phase of development. The GPS OCX program missed major milestones and pushed its launch into 2021, 5 years past the original deadline. According to the Government Accounting Office in 2019, the 2021 deadline looked shaky. [98] The project remained delayed in 2023, and was (as of June 2023) 73% over its original estimated budget. [99] [100] In late 2023, Frank Calvelli, the assistant secretary of the Air Force for space acquisitions and integration, stated that the project was estimated to go live some time during the summer of 2024. [101] Further information: GPS navigation device GPS receivers come in a variety of formats, from devices integrated into cars, phones, and watches, to dedicated devices such as these. The first portable GPS survey unit, a Leica WM 101, displayed at the Irish National Science Museum at Maynooth The user segment (US) is composed of hundreds of thousands of U.S. and allied military users of the secure GPS Precise Positioning Service, and tens of millions of civil, commercial and scientific users of the Standard Positioning Service. In general, GPS receivers are composed of an antenna, tuned to the frequencies transmitted by the satellites, receiver-processors, and a highly stable clock (often a crystal oscillator ). They may also include a display for providing location and speed information to the user. GPS receivers may include an input for differential corrections, using the RTCM SC-104 format. This is typically in the form of an RS-232 port at 4,800 bit/s speed. Data is actually sent at a much lower rate, which limits the accuracy of the signal sent using RTCM.[ citation needed ] Receivers with internal DGPS receivers can outperform those using external RTCM data.[ citation needed ] As of 2006 [update] , even low-cost units commonly include Wide Area Augmentation System (WAAS) receivers. A typical GPS receiver with integrated antenna Many GPS receivers can relay position data to a PC or other device using the NMEA 0183 protocol. Although this protocol is officially defined by the National Marine Electronics Association (NMEA), [102] references to this protocol have been compiled from public records, allowing open source tools like gpsd to read the protocol without violating intellectual property laws.[ clarification needed ] Other proprietary protocols exist as well, such as the SiRF and MTK protocols. Receivers can interface with other devices using methods including a serial connection, USB , or Bluetooth . Applications[ edit ] This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (March 2015) (
) An image of global sea surface temperatures acquired from the NOAA/ AVHRR satellite The Advanced Very-High-Resolution Radiometer (AVHRR) instrument is a space-borne sensor that measures the reflectance of the Earth in five spectral bands that are relatively wide by today's standards. AVHRR instruments are or have been carried by the National Oceanic and Atmospheric Administration (NOAA) family of polar orbiting platforms ( POES ) and European MetOp satellites. The instrument scans several channels; two are centered on the red (0.6 micrometres) and near- infrared (0.9 micrometres) regions, a third one is located around 3.5 micrometres, and another two the thermal radiation emitted by the planet, around 11 and 12 micrometres. [1] The first AVHRR instrument was a four-channel radiometer . The last version, AVHRR/3, first carried on NOAA-15 launched in May 1998, acquires data in six channels. The AVHRR has been succeeded by the Visible Infrared Imaging Radiometer Suite , carried on the Joint Polar Satellite System spacecraft. Operation[ edit ] NOAA has at least two polar-orbiting meteorological satellites in orbit at all times, with one satellite crossing the equator in the early morning and early evening and the other crossing the equator in the afternoon and late evening. The primary sensor on board both satellites is the AVHRR instrument. Morning-satellite data are most commonly used for land studies, while data from both satellites are used for atmosphere and ocean studies. Together they provide twice-daily global coverage, and ensure that data for any region of the earth are no more than six hours old. The swath width, the width of the area on the Earth's surface that the satellite can "see", is approximately 2,500 kilometers (~1,540 mi). The satellites orbit between 833 or 870 kilometers (+/− 19 kilometers, 516–541 miles) above the surface of the Earth. [2] The highest ground resolution that can be obtained from the current AVHRR instruments is 1.1-kilometer (0.68 mi) per pixel at the nadir . AVHRR data have been collected continuously since 1981. [2] The primary purpose of these instruments is to monitor clouds and to measure the thermal emission of the Earth. These sensors have proven useful for a number of other applications, however, including the surveillance of land surfaces, ocean state, aerosols, etc. AVHRR data are particularly relevant to study climate change and environmental degradation because of the comparatively long records of data already accumulated (over 20 years). The main difficulty associated with these investigations is to properly deal with the many limitations of these instruments, especially in the early period (sensor calibration, orbital drift, limited spectral and directional sampling, etc.). The AVHRR instrument also flies on the MetOp series of satellites. The three planned MetOp satellites are part of the EUMETSAT Polar System (EPS) run by EUMETSAT . Calibration and validation[ edit ] Remote sensing applications of the AVHRR sensor are based on validation (matchup) techniques of co-located ground observations and satellite observations. Alternatively, radiative transfer calculations are performed. There are specialized codes which allow simulation of the AVHRR observable brightness temperatures and radiances in near infrared and infrared channels. [3] [4] Pre-launch calibration of visible channels (Ch. 1 and 2)[ edit ] Prior to launch, the visible channels (Ch. 1 and 2) of AVHRR sensors are calibrated by the instrument manufacturer, ITT, Aerospace/Communications Division, and are traceable to NIST standards.  The calibration relationship between electronic digital count response (C) of the sensor and the albedo (A) of the calibration target are linearly regressed: [2] A = S * C + I where S and I are the slope and intercept (respectively) of the calibration regression [NOAA KLM].  However, the highly accurate prelaunch calibration will degrade during launch and transit to orbit as well as during the operational life of the instrument [Molling et al., 2010]. Halthore et al. [2008] note that sensor degradation is mainly caused by thermal cycling, outgassing in the filters, damage from higher energy radiation (such as ultraviolet (UV)), and condensation of outgassed gases onto sensitive surfaces. One major design fault of AVHRR instruments is that they lack the capability to perform accurate, onboard calibrations once on orbit [NOAA KLM].  Thus, post-launch on-orbit calibration activities (known as vicarious calibration methods) must be performed to update and ensure the accuracy of retrieved radiances and the subsequent products derived from these values [Xiong et al., 2010].  Numerous studies have been performed to update the calibration coefficients and provide more accurate retrievals versus using the pre-launch calibration. On-orbit individual/few sensor absolute calibration[ edit ] Rao and Chen[ edit ] Rao and Chen [1995] use the Libyan Desert as a radiometrically stable calibration target to derive relative annual degradation rates for Channels 1 and 2 for AVHRR sensors on board the NOAA -7, -9, and -11 satellites.  Additionally, with an aircraft field campaign over the White Sands desert site in New Mexico, USA [See Smith et al., 1988], an absolute calibration for NOAA-9 was transferred from a well calibrated spectrometer on board a U-2 aircraft flying at an altitude of ~18 km in a congruent path with the NOAA-9 satellite above.  After being corrected for the relative degradation, the absolute calibration of NOAA-9 is then passed onto NOAA −7 and −11 via a linear relationship using Libyan Desert observations that are restricted to similar viewing geometries as well as dates in the same calendar month [Rao and Chen, 1995], and any sensor degradation is corrected for by adjusting the slope (as a function of days after launch) between the albedo and digital count signal recorded [Rao and Chen, 1999]. Loeb[ edit ] In another similar method using surface targets, Loeb [1997] uses spatiotemporal uniform ice surfaces in Greenland and Antarctica to produce second-order polynomial reflectance calibration curves as a function of solar zenith angle; calibrated NOAA-9 near-nadir reflectances are used to generate the curves that can then derive the calibrations for other AHVRRs in orbit (e.g. NOAA-11, -12, and -14). It was found that the ratio of calibration coefficients derived by Loeb [1997] and Rao and Chen [1995] are independent of solar zenith angle, thus implying that the NOAA-9-derived calibration curves provide an accurate relation between the solar zenith angle and observed reflectance over Greenland and Antarctica. Iwabuchi[ edit ] Iwabuchi [2003] employed a method to calibrate NOAA-11 and -14 that uses clear-sky ocean and stratus cloud reflectance observations in a region of the NW Pacific Ocean and radiative transfer calculations of a theoretical molecular atmosphere to calibrate AVHRR Ch. 1.  Using a month of clear-sky observations over the ocean, an initial minimum guess to the calibration slope is made.  An iterative method is then used to achieve the optimal slope values for Ch. 1 with slope corrections adjusting for uncertainties in ocean reflectance, water vapor, ozone, and noise.  Ch. 2 is then subsequently calibrated under the condition that the stratus cloud optical thickness in both channels must be the same (spectrally uniform in the visible) if their calibrations are correct [Iwabuchi, 2003]. Vermote and Saleous[ edit ] A more contemporary calibration method for AVHRR uses the on-orbit calibration capabilities of the VIS/IR channels of MODIS .  Vermote and Saleous [2006] present a methodology that uses MODIS to characterize the BRDF of an invariant desert site.  Due to differences in the spectral bands used for the instruments' channels, spectral translation equations were derived to accurately transfer the calibration accounting for these differences.  Finally, the ratio of AVHRR observed to that modeled from the MODIS observation is used to determine the sensor degradation and adjust the calibration accordingly. Others[ edit ] Methods for extending the calibration and record continuity also make use of similar calibration activities [Heidinger et al., 2010]. Long-term calibration and record continuity[ edit ] In the discussion thus far, methods have been posed that can calibrate individual or are limited to a few AVHRR sensors.  However, one major challenge from a climate point of view is the need for record continuity spanning 30+ years of three generations of AVHRR instruments as well as more contemporary sensors such as MODIS and VIIRS .  Several artifacts may exist in the nominal AVHRR calibration, and even in updated calibrations, that cause a discontinuity in the long-term radiance record constructed from multiple satellites [Cao et al., 2008]. International Satellite Cloud Climatology Project (ISCCP) method[ edit ] Brest and Rossow [1992], and the updated methodology [Brest et al., 1997], put forth a robust method for calibration monitoring of individual sensors and normalization of all sensors to a common standard.  The International Satellite Cloud Climatology Project (ISCCP) method begins with the detection of clouds and corrections for ozone, Rayleigh scatter, and seasonal variations in irradiance to produce surface reflectances.  Monthly histograms of surface reflectance are then produced for various surface types, and various histogram limits are then applied as a filter to the original sensor observations and ultimately aggregated to produce a global, cloud free surface reflectance. After filtering, the global maps are segregated into monthly mean SURFACE, two bi-weekly SURFACE, and a mean TOTAL reflectance maps.  The monthly mean SURFACE reflectance maps are used to detect long-term trends in calibration.  The bi-weekly SURFACE maps are compared to each other and are used to detect short-term changes in calibration. Finally, the TOTAL maps are used to detect and assess bias in the processing methodology.  The target histograms are also examined, as changes in mode reflectances and in population are likely the result of changes in calibration. Long-term record continuity[ edit ] Long-term record continuity is achieved by the normalization between two sensors.  First, observations from the operational time period overlap of two sensors are processed.  Next, the two global SURFACE maps are compared via a scatter plot.  Additionally, observations are corrected for changes in solar zenith angle caused by orbital drift.  Ultimately, a line is fit to determine the overall long-term drift in calibration, and, after a sensor is corrected for drift, normalization is performed on observations that occur during the same operational period [Brest et al., 1997]. Calibration using the moderate-resolution imaging spectroradiometer[ edit ] Another recent method for the absolute calibration of the AHVRR record makes use of the contemporary MODIS sensor onboard NASA's TERRA and AQUA satellites.  The MODIS instrument has high calibration accuracy and can track its own radiometric changes due to the inclusion of an onboard calibration system for the VIS/NIR spectral region [MCST].  The following method utilizes the high accuracy of MODIS to absolutely calibrate AVHRRs via simultaneous nadir overpasses (SNOs) of both MODIS/AVHRR and AVHRR/AVHRR satellite pairs as well as MODIS-characterized surface reflectances for a Libyan Desert target and Dome-C in Antarctica [Heidinger et al., 2010].  Ultimately, each individual calibration event available (MODIS/AVHRR SNO, Dome C, Libyan Desert, or AVHRR/AVHRR SNO) is used to provide a calibration slope time series for a given AVHRR sensor.  Heidinger et al. [2010] use a second-order polynomial from a least-squares fit to determine the time series. The first step involves using a radiative transfer model that will convert observed MODIS scenes into those that a perfectly calibrated AVHRR would see.  For MODIS/AVHRR SNO occurrences, it was determined that the ratio of AVHRR to MODIS radiances in both Ch1 and Ch2 are modeled well by a second-order polynomial of the radio of MODIS reflectances in channels 17 and 18.  Channels 17 and 18 are located in a spectral region (0.94mm) sensitive to atmospheric water vapor, a quantity that affects the accurate calibration of AVHRR Ch. 2.  Using the Ch17 to Ch 18 ratio, an accurate guess at the total precipitable water (TPW) is obtained to further increase the accuracy of MODIS to AVHRR SNO calibrations.  The Libyan Desert and Dome-C calibration sites are used when MODIS/AVHRR SNOs do not occur.  Here, the AVHRR to MODIS ratio of reflectances is modeled as a third-order polynomial using the natural logarithm of TWP from the NCEP reanalysis.  Using these two methods, monthly calibration slopes are generated with a linear fit forced through the origin of the adjusted MODIS reflectances versus AVHRR counts. To extend the MODIS reference back for AVHRRs prior to the MODIS era (pre-2000), Heidinger et al. [2010] use the stable Earth targets of Dome C in Antarctica and the Libyan Desert.  MODIS mean nadir reflectances over the target are determined and are plotted versus the solar zenith angle.  The counts for AVHRR observations at a given solar zenith angle and corresponding MODIS reflectance, corrected for TWP, are then used to determine what AVHRR value would be provided it had the MODIS calibration.  The calibration slope is now calculated. Calibration using direct AVHRR/AVHRR SNOs[ edit ] One final method used by Heidinger et al. [2010] for extending the MODIS calibration back to AVHRRs that operated outside of the MODIS era is through direct AVHRR/AVHRR SNOs.  Here, the counts from AVHRRs are plotted and a regression forced through the origin calculated.  This regression is used to transfer the accurate calibration of one AVHRRs reflectances to the counts of an un-calibrated AVHRR and produce appropriate calibration slopes.  These AVHRR/AVHRR SNOs do not provide an absolute calibration point themselves; rather they act as anchors for the relative calibration between AVHRRs that can be used to transfer the ultimate MODIS calibration. Next-generation system[ edit ] Operational experience with the MODIS [5] sensor onboard NASA's Terra and Aqua led to the development of AVHRR's follow-on, VIIRS . [6] VIIRS is currently operating on board the Suomi NPP and NOAA-20 satellites. [7] Launch and service dates[ edit ] Satellite name
) ) {\displaystyle L_{T}(\lambda )=L_{r}(\lambda )+L_{a}(\lambda )+L_{ra}(\lambda )+TL_{g}(\lambda )+t(L_{f}(\lambda )+L_{W}(\lambda ))} Where LT(λ) is total radiance at the top of the atmosphere, Lr(λ) is Rayleigh scattering by air molecules, La(λ) is scattering by aerosols in the absence of air, Lra(λ) is interactions between air molecules and aerosols, TLg(λ) is reflections from glint,  t(Lf(λ) is reflections from foam, and LW(λ)) is reflections from the subsurface of the water, or the water-leaving radiance. [2] Others may divide radiance into some slightly different components, [8] though in each case the reflectance parameters must be resolved in order to estimate water-leaving radiance and thus chlorophyll concentrations. Data products[ edit ] Though SeaWiFS was designed primarily to monitor ocean chlorophyll a concentrations from space, it also collected many other parameters that are freely available to the public for research and educational purposes. These parameters aside from chlorophyll a include reflectance, the diffuse attenuation coefficient, particulate organic carbon (POC) concentration, particulate inorganic carbon (PIC) concentration, colored dissolved organic matter (CDOM) index, photosynthetically active radiation (PAR), and normalized fluorescence line height (NFLH). In addition, despite being designed to measure ocean chlorophyll, SeaWiFS also estimates Normalized Difference Vegetation Index (NDVI), which is a measure of photosynthesis on land. Data access[ edit ] A false color SeaWiFS image shows a high concentration of phytoplankton chlorophyll in the Brazil Current Confluence region east of Argentina. Warm colors indicate high chlorophyll levels, and cooler colors indicate lower chlorophyll. SeaWiFS data are freely accessible from a variety of websites, most of which are government run. The primary location for SeaWiFS data is NASA's OceanColor website [1] , which maintains the time series of the entire SeaWiFS mission.  The website allows users to browse individual SeaWiFS images based on time and area selections. The website also allows for browsing of different temporal and spatial scales with spatial scales ranging from 4 km to 9 km for mapped data. Data are provided at numerous temporal scales including daily, multiple days (e.g., 3, 8), monthly, and seasonal images, all the way up to composites of the entire mission. Data are also available via ftp and bulk download. Data can be browsed and retrieved in a variety of formats and levels of processing, with four general levels from unprocessed to modeled output. [9] Level 0 is unprocessed data that is not usually provided to users. Level 1 data are reconstructed but either unprocessed or minimally processed. Level 2 data contain derived geophysical variables, though are not on a uniform space/time grid. Level 3 data contain derived geophysical variables binned or mapped to a uniform grid.  Lastly, Level 4 data contain modeled or derived variables such as ocean primary productivity . Scientists who aim to create calculations of chlorophyll or other parameters that differ from those provided on the OceanColor website would likely use Level 1 or 2 data. This might be done, for example, to calculate parameters for a specific region of the globe, whereas the standard SeaWiFS data products are designed for global accuracy with necessary tradeoffs for specific regions. Scientists who are more interested in relating the standard SeaWiFS outputs to other processes will commonly use Level 3 data, particularly if they do not have the capacity, training, or interest in working with Level 1 or 2 data. Level 4 data may be used for similar research if interested in a modeled product. Software[ edit ] NASA offers free software designed specifically to work with SeaWiFS data through the ocean color website. This software, entitled SeaDAS (SeaWiFS Data Analysis System), is built for visualization and processing of satellite data and can work with Level 1, 2, and 3 data. Though it was originally designed for SeaWiFS data, its capabilities have since been expanded to work with many other satellite data sources.  Other software or programming languages can also be used to read in and work with SeaWiFS data, such as Matlab , IDL , or Python . Applications[ edit ] Biological pump, air-sea cycling and sequestering of CO2 Estimating the amount of global or regional chlorophyll, and therefore phytoplankton, has large implications for climate change and fisheries production. Phytoplankton play a huge role in the uptake of the world's carbon dioxide, a primary contributor to climate change . A percentage of these phytoplankton sink to ocean floor, effectively taking carbon dioxide out of the atmosphere and sequestering it in the deep ocean for at least a thousand years. Therefore, the degree of primary production from the ocean could play a large role in slowing climate change. Or, if primary production slows, climate change could be accelerated. Some have proposed fertilizing the ocean with iron in order to promote phytoplankton blooms and remove carbon dioxide from the atmosphere. Whether these experiments are undertaken or not, estimating chlorophyll concentrations in the world's oceans and their role in the ocean's biological pump could play a key role in our ability to foresee and adapt to climate change. Phytoplankton is a key component in the base of the oceanic food chain and oceanographers have hypothesized a link between oceanic chlorophyll and fisheries production for some time. [10] The degree to which phytoplankton relates to marine fish production depends on the number of trophic links in the food chain, and how efficient each link is. Estimates of the number of trophic links  and trophic efficiencies from phytoplankton to commercial fisheries have been widely debated, though they have been little substantiated. [11] More recent research suggests that positive relationships between chlorophyll a and fisheries production can be modeled [12] and can be very highly correlated when examined on the proper scale. For example, Ware and Thomson (2005) found an r2 of 0.87 between resident fish yield (metric tons km-2) and mean annual chlorophyll a concentrations (mg m-3). [13] Others have found the Pacific's Transition Zone Chlorophyll Front (chlorophyll density of 0.2 mg m-3) to be defining feature in loggerhead turtle distribution. [14] References[ edit ] Cracknell, A. P.; Newcombe, S. K.; Black, A. F.; Kirby, N. E. (2001). "The ABDMAP (Algal Bloom Detection, Monitoring and Prediction) Concerted Action". International Journal of Remote Sensing. 22 (2–3): 205–247. Bibcode : 2001IJRS...22..205C . doi : 10.1080/014311601449916 . S2CID 140603142 . NASA, Goddard Space Flight Center (February 14, 2011). "Ocean Color Browse" . Retrieved February 14, 2011. Hooker, S.B.; McClain, C.R. (1 April 2000). "The calibration and validation of SeaWiFS data" . Progress in Oceanography. 45 (3–4): 427–465. Bibcode : 2000PrOce..45..427H . doi : 10.1016/S0079-6611(00)00012-4 . O'Reilly, John E.; Maritorena, Stéphane; Mitchell, B. Greg; Siegel, David A.; Carder, Kendall L.; Garver, Sara A.; Kahru, Mati; McClain, Charles (1 January 1998). "Ocean color chlorophyll algorithms for SeaWiFS" . Journal of Geophysical Research. 103 (C11): 24937–24953. Bibcode : 1998JGR...10324937O . doi : 10.1029/98JC02160 .
25 June 2016, 02:58:27 UTC Revolution no. 87867 Terra (EOS AM-1) is a multi-national scientific research satellite operated by NASA in a Sun-synchronous orbit around the Earth .  It takes simultaneous measurements of Earth's atmosphere, land, and water to understand how Earth is changing and to identify the consequences for life on Earth. [1] It is the flagship of the Earth Observing System (EOS) and the first satellite of the system which was followed by Aqua (launched in 2002) and Aura (launched in 2004). Terra was launched in 1999. The name "Terra" comes from the Latin word for Earth. A naming contest was held by NASA among U.S. high school students. The winning essay was submitted by Sasha Jones of Brentwood, Missouri . The identifier "AM-1" refers to its orbit, passing over the equator in the morning. Launch[ edit ] The satellite was launched from Vandenberg Air Force Base on December 18th, 1999, aboard an Atlas IIAS vehicle and began collecting data on February 24th, 2000. It was placed into a near-polar, sun-synchronous orbit at an altitude of 705 km (438 mi), with a 10:30am descending node. Mission[ edit ] Fireball over the Bering Sea viewed from space by the Terra satellite(December 18, 2018) Terra carries a payload of five remote sensors designed to monitor the state of Earth's environment and ongoing changes in its climate system: [2] ASTER (Advanced Spaceborne Thermal Emission and Reflection Radiometer) [3] ASTER creates high-resolution images of clouds, ice, water and the land surface using 3 different sensor subsystems. They are the Shortwave Infrared (SWIR); Thermal Infrared (TIR); and Visible and Near Infrared (VNIR). They cover 14 multi-spectral bands from visible to the thermal infrared. The SWIR stopped working in 2008. ASTER was provided by Japan's Ministry of Economy, Trade, and Industry. [4] CERES (Clouds and the Earth's Radiant Energy System) MISR (Multi-angle Imaging SpectroRadiometer) MODIS (Moderate-resolution Imaging Spectroradiometer) [5] MOPITT (Measurements of Pollution in the Troposphere) [6] Data from the satellite helps scientists better understand the spread of pollution around the globe. Studies have used instruments on Terra to examine trends in global carbon monoxide and aerosol pollution. [7] The data collected by Terra will ultimately become a new, 15-year global data set. After launch, operators observed that high energy protons like those found over the South Atlantic Anomaly or the poles could induce single-event upsets that would cause the Motor Drive Assembly (MDA) Built-In Test Equipment (BITE) to turn off the MDA. These false shut-downs occur 12–14 times a month and eventually the operations team automated the recovery to reduce the impact of these shut-downs. [8] Starting in 2007, increased thermal resistance in the SWIR cryocooler of the ASTER instrument caused the temperature to gradually increase. By 2008, despite frequent attempts to recycle the cryocooler the data began to significantly degrade and on January 12, 2009, ASTER managers declared the SWIR no longer functional due to anomalously high SWIR detector temperatures. Data gathered after April 2008 was declared not usable. [9] On October 13, 2009, Terra suffered a single battery cell failure anomaly and a battery heater control anomaly likely the result of a Micrometeoroid or Orbital Debris (MMOD) strike. [8] On February 27, 2020, the Terra flight operations team conducted Terra's last inclination adjust maneuver due to the satellite's limited remaining fuel. With no more such maneuvers, Terra's mean local time has begun to drift starting in April 2021 and with it, its data quality. In October 2022, Terra is expected to reach and exceed a 10:15 AM MLT crossing which is expected to lead operators to initiate a constellation exit to a lower orbit altitude (694 km). Decommissioning would occur around 2025-26 followed by an un-controlled reentry several years later. [10] Malicious cyber activities[ edit ] In June and October 2008 the spacecraft was targeted by hackers who gained unauthorized access to its command and control systems, but did not issue any commands. [11] Gallery of images by Terra[ edit ] The first image taken by Terra. The effects of the European winter storms of 2009–2010 on Great Britain, seen from Terra. The Deepwater Horizon oil spill oil slick as seen from space by NASA 's Terra satellite on May 1, 2010. The Deepwater Horizon oil slick just off the Louisiana coast on April 30, 2010, visible from space. Hurricane Karl , the most destructive hurricane of the 2010 Atlantic Hurricane Season , approaches Mexico on September 16. Satellite image of Sweden in March 2002. Satellite image of the Ash distribution over Australia from the 2022 Hunga Tonga–Hunga Ha'apai eruption . Animation of Terra's orbit around the Earth. Earth is not shown.
02 June 2016, 10:25:37 UTC Revolution no. 74897 Logotype of the mission. Aqua (EOS PM-1) is a NASA scientific research satellite in orbit around the Earth , studying the precipitation , evaporation , and cycling of water . It is the second major component of the Earth Observing System (EOS) preceded by Terra (launched 1999) and followed by Aura (launched 2004). The name "Aqua" comes from the Latin word for water. The satellite was launched from Vandenberg Air Force Base on May 4, 2002, aboard a Delta II rocket . Aqua operated in a Sun-synchronous orbit as the third in the satellite formation called the " A Train " with several other satellites ( OCO-2 , the Japanese GCOM W1 , PARASOL , CALIPSO , Cloud Sat , and Aura ) for most of its first 20 years; but in January 2022 Aqua left the A-Train (as Cloud Sat, CALIPSO and PARASOL had already done) when, due to its fuel limitations, it transitioned to a free-drift mode, wherein its equatorial crossing time is slowly drifting to later times, from its tightly controlled orbit. [1] Mission[ edit ] Aqua is one of NASA's missions for Earth science operating in the A-Train constellation. It has demonstrated a very high level of precision in making the primary long-term measurements of the mission. These highly calibrated climate quality measurements of radiance, reflectance, and backscatter have been used to cross-calibrate past and present sensors launched by NASA, as well as a variety of sensors launched from other agencies and the international community. Thousands of scientists and operational users from around the world have made use of the Aqua data to address NASA's 6 interdisciplinary Earth science focus areas: Atmospheric Composition, Weather, Carbon Cycle and Ecosystems, Water and Energy Cycle, Climate Variability and Change, and Earth Surface and Interior. Aqua has experienced some minor, non-mission ending anomalies. Because of a 2007 anomaly with the Solid State Recorder (SSR) it can only hold two orbits worth of data. A series of solar array and array regulator electronics anomalies starting in 2010 has led to the loss of 23 strings of solar cells out of a total of 132 strings. [2] A 2005 short circuit within a battery cell led to a partial loss of cell capacity. In 2009, a solar panel thermistor failed and an error in the Solar Array offset was detected. The offset issue has been corrected periodically since then. On September 8, 2007, the Dual Thruster Module (DTM-2) Heater experienced an anomaly. [3] On August 16, 2020, The Formatter Multiplexer Unit (FMU) experienced an anomaly, corrupting some data in the SSR and stopping all data streams until it was recovered on September 2, 2020. [4] The current end of mission plan is to let Aqua's orbit decay naturally at least until June 2024 and continue data collection into 2026 or even 2027, dependent on such items as budget, fuel, hardware, power, and end-of mission requirements. [2] Aqua's life could be extended with a possible re-fueling mission. A worst-case scenario would result in a re-entry by 2046. [5] Instruments[ edit ] Aqua carries six instruments for studies of water on the Earth's surface and in the atmosphere , of which four are still operating: Advanced Microwave Scanning Radiometer-EOS (AMSR-E) — measures cloud properties, sea surface temperature , near-surface wind speed , radiative energy flux, surface water, ice and snow . Furnished by the National Space Development Agency of Japan . The AMSR-E instrument had over 480 pounds of spinning mass, and the lubricant in the bearing assembly gradually deteriorated over the course of the mission. By 2007, there was a noticeable increase in motor current. This led to the development of new contingency procedures in case of high current or torque, which were put in place in 2011. In October 2011, the instrument began to cause yaw vibrations in the spacecraft that exceeded torque limits and on October 4, 2011, was automatically slowed to 4 rpm from the normal 40 rpm and then, because it could not maintain 4 rpm, was slowed to a stop. [6] A recovery procedure was developed and tested through 2012, culminating with a December 4, 2012 successful acceleration to 2.0767 rpm to allow for cross-calibration with the AMSR-2 instrument launched in 2012 aboard the GCOM-W1 satellite. [7] On December 4, 2015, AMSR-E was slowed to a stop and then on March 3, 2016, it was turned off. [8] Moderate Resolution Imaging Spectroradiometer (MODIS) — also measures cloud properties and radiative energy flux, also aerosol properties; land cover and land use change , fires and volcanoes .  An identical MODIS instrument is also aboard Terra . As of 2017, some degradation of channels in the MODIS short wave visible bands has been observed by the Ocean Biology Processing Group (OBPG). [9] Advanced Microwave Sounding Unit (AMSU-A) — measures atmospheric temperature and humidity . Since launch, AMSU has lost 5 of 15 channels. [10] [11] Power to the AMSU-A2 microwave instrument was lost at 19:47 UT September 24, 2016. This caused the loss of Channels 1 and 2. All recovery attempts were unsuccessful and no further recovery attempts are planned. [3] Atmospheric Infrared Sounder (AIRS) — measures atmospheric temperature and humidity, land and sea surface temperatures. Cooler A telemetry became frozen on March 24, 2014, but this had no impact on science gathering. [12] On September 25, 2016, Cooler-A experienced a shut down anomaly. Anomaly recovery occurred two days later and also cleared a condition that had disabled Cooler-A telemetry since the 2014 Cooler-A anomaly. [11] Humidity Sounder for Brazil (HSB) — VHF band equipment measuring atmospheric humidity. Furnished by Instituto Nacional de Pesquisas Espaciais of Brazil. The HSB instrument has been in survival mode, and thus non-operational, since February 5, 2003, when the scan mirror motor failed. [12] Clouds and the Earth's Radiant Energy System (CERES) — Flying Modules 3 and 4, measure broadband radiative energy flux. CERES' shortwave channel on module 4 failed on March 30, 2005, but its two other channels remain operational. [12] The Aqua spacecraft has a mass of about 2,850 kilograms (6,280 lb), plus propellant of about 230 kilograms (510 lb) at launch. Stowed for launch, the satellite fit in a volume of 2.68 m x 2.49 m x 6.49 m. Deployed, Aqua is 4.81 m x 16.70 m x 8.04 m. Aqua instruments
Toggle the table of contents Earth Observing System From Wikipedia, the free encyclopedia NASA program involving satellites Observe Earth to improve understanding of climate, weather, land and atmosphere Status Uncrewed vehicle(s) All The Earth Observing System (EOS) is a program of NASA comprising a series of artificial satellite missions and scientific instruments in Earth orbit designed for long-term global observations of the land surface, biosphere , atmosphere , and oceans . Since the early 1970s, NASA has been developing its Earth Observing System, launching a series of Landsat satellites in the decade. Some of the first included passive microwave imaging in 1972 through the Nimbus 5 satellite. [1] Following the launch of various satellite missions, the conception of the program began in the late 1980s and expanded rapidly through the 1990s. [2] Since the inception of the program, it has continued to develop, including; land, sea, radiation and atmosphere. [1] Collected in a system known as EOSDIS , NASA uses this data in order to study the progression and changes in the biosphere of Earth. The main focus of this data collection surrounds climatic science. The program is the centrepiece of NASA's Earth Science Enterprise . History and development[ edit ] TIROS-1 Satellite displayed at National Air and Space Museum in Washington Prior to the development of the current Earth Observing System (EOS), the foundations for this program were laid in the early 1960s and 1970s. TIROS-1 , the very first full-scale, low Earth orbit weather satellite . [3] The primary objective of TIROS-1 was to explore television infrared observation as a method of monitoring and studying the surface of Earth. Critical to the development of the satellites currently in use, TIROS-1 was a program that allowed NASA to use experimental instruments and data collection methods to study meteorology worldwide. Crucially, this new information gathered by TIROS-1 would allow meteorologists and scientists to observe large-scale weather events. In doing so, they would be able to answer questions such as  "should we evacuate the coast because of the hurricane?". [3] Following TIROS, the experimental Applications Technology Satellite (ATS) program was developed. The main objective of these satellites were weather predictions and the study of the environment of space. Significantly, this program focused on launching satellites to orbit geosynchronously and evaluate the effectiveness of this orbit pattern in observing the Earth. [1] ATS-3 , the longest-lasting mission, saw a life span of over 20 years. It was the first satellite to capture colour images from space and acted significantly as a medium of communications. [1] After the success of TIROS-1 and ATS-3, NASA in conjunction with United States Geological Survey (USGS), progressed forward in Earth observation through a series of Landsat satellites launched throughout the 1970s and 1980s. The Nimbus 5 satellite launched in 1972 used passive microwave imaging; a highly successful method to observe changes in sea ice cover. [1] Observation was furthered by succeeding missions such as Nimbus 7 , fitted with a coastal zone colour scanner (CZCS) for detailing colour changes in the Earth's oceans, and a Total Ozone Mapping Spectrometer (TOMS) to measure solar irradiance and the reflected radiance from the Earth's atmosphere. [1] The early satellites of these programs have paved the way for much of the EOS program today. The TIROS satellites were extremely important in the testing and development of not only the Earth observing instruments such as spectrometers , but much was also learnt from the various sensors used in order to maintain these satellites in orbit for sustainable periods of time. Sensors such as horizons sensors were tested on these early satellites and have been adapted to produce more advanced methods of observation and operating configurations. [1] Operation and technology - Logistics[ edit ] According to NASA's Earth Observing System mission page, there are over 30 missions that remain active. [4] As an evolving program, the EOS can collect a variety of data through various instruments that have been developed. Below outlines various sensors on different EOS missions and the data they collect. Mission / Satellites Landsat 5-8 Operational Land Imager (OLI) [5] Developed by Ball Aerospace & Technologies Corporation, the OLI is a crucial aspect of modern LandSat vehicles. Using 7000 sensors per band (Spectrum band), the OLI on NASA's most recent LandSat (LANDSAT 8) Satellite, will image/view the entire earth every 16 days. Enhanced Thematic Mapper + (ETM+) [6] [7] Used in conjunction with OLI, the ETM + images the Earth in 30m Pixels. To ensure quality, each scan has a correction due to Scan-Line correcting. CloudSat Cloud Profiling Radar (CPR) [8] Operates at 96 GHz. Crucially, the CPR is used to detail cloud-sized particles. These can be in the form of snow, cloud ice, water and light rains. Lidar [9] Similarly to Radar, Lidar measures by the time a light (Laser) source takes to return to the sensor. CALIPSO, fitted with Lidar Level 2, mainly focused with measuring condensable vapours such as water and nitric acid.  Collects Polar Stratific cloud data. AURA Microwave Limb Sounder (MLS) [10] Used to measure microwave emissions (Thermal) that naturally occurs. The name Limb refers to the "edge" of Earth's atmosphere. This data collected includes atmospheric gas profiles and atmospheric temperature and pressure. Tropospheric Emission Spectrometer (TES) [11] TES is an infrared sensor aboard AURA used to investigate the troposphere of Earth's Atmosphere. Crucially, it helps scientists understand the impact of Carbon dioxide in the atmosphere and the OZONE layer and its changes. AQUA Advanced Microwave Scanning Radiometer  (AMSR-E) [12] AMSR-E, a critical instrument used to measure physical properties occurring on Earth. Rain precipitation, various sea and land temperatures, snow and ice cover, and water vapour from the ocean are just some properties that are measured using microwave scanning radiometer. Detecting microwave emissions, the data is evaluated to determine various characteristics about each geophysical property. Moderate Resolution Imaging Spectroradiometer (MODIS) [13] Measuring in 36 different spectral bands, the MODIS system is critical on AQUA. Used to increase understanding of global properties and dynamics, MODIS helps Scientists to predict changes on Land, water and lower atmosphere. Data collection and uses[ edit ] Since the inception of the program, the aim overall has remained the same: "monitor and understand key components of the climate system and their interactions through long-term global observations." [4] Through the use of various programs such as LandSat and the A-Train programs, scientists are gaining a greater understanding of Earth and its changes. Currently, the data collected by the satellites in EOS is digitised and collated by the Earth Observing System Data and Information System. Scientists then use this data to predict weather events, and more recently to predict the effects of climate change for treaties such as Paris Climate agreements, with data mainly being collected by EOS and then analysed. Intergovernmental agencies and partnerships[ edit ] In a broader sense of Earth observing and all missions that impact EOS, there have been a variety of intergovernmental partnerships and international partnerships that have helped fund, research and develop the complex array of satellites and spacecraft that make the Earth Observing System successful in its role. In total, intergovernmental partnerships account for almost 37% of all missions while 27% of the missions also involve international partnerships with other countries and international companies. As of 2022, there have been nine LandSat satellites with LandSat 7, 8, and 9 orbiting the Earth. The LandSat program has involved many organisations since its inception, particularly the United States Geological Survey (USGS). Other intergovernmental agencies that have been a part of the Earth Observing program include the Environmental Science Services Administration (ESSA), US Department of Defence (USDOD), United States Department of Energy (USDOE) and the US National Oceanic and Atmospheric Administration (NOAA). These intergovernmental agencies cooperating allow for greater funding for the program along with collaboration of government resources from various agencies. Often these partnerships begin with another governmental agency wanting a specific instrument as a part of a payload included on a mission. [14] Similarly, international partnerships with countries have either resulted from a specific payload (instrument) accompanying an existing mission that NASA has developed or NASA collaborating and requiring the use of facilities of another Space agency such as the European Space Agency. A partnership like this was observed in 2000 when the ERS-1 satellite was launched from the Guiana Space Centre; a spaceport in French Guiana, South America. International agencies that have assisted or collaborated with NASA include CONAE (Argentinian Space Agency), CNES (French Space Agency), DLR (German Aerospace Centre), the state space federation Roscosmos of the Russian Federation, and JAXA (Japanese Space Agency; previously NASDA). [2] Over the program's life, there have also been various corporate and organisational partnerships with companies both based in America and internationally. In 2002, the SeaWIFS missions saw a collaboration with GEOeye, an American satellite imaging company. Similarly, organisations such as the International Council for Science (ICSU), International standards Organisation (IOS), World Data System (WDS) and the committee on Earth Observing Satellites (CEOS) have been involved in the planning, data collection, and data analysis of missions. As mentioned, funding, instrumental additions and over assistance in coordination and data analysis are all benefits of these partnerships. [15] Mission list with launch dates[ edit ] NASA Earth Science Division Operating Missions as of 2 February 2015 This animation shows the orbits of NASA's 2011 fleet of Earth remote sensing observatories Active mission
Joint space mission between NASA and JAXA Tropical Rainfall Measuring Mission Artist conception of the TRMM satellite Mission type 27 November 1997, 21:27 UTC Rocket 6 June 2015, 06:54 UTC [2] Orbital parameters Global Precipitation Measurement (GPM) → The Tropical Rainfall Measuring Mission (TRMM) was a joint space mission between NASA and JAXA designed to monitor and study tropical rainfall . The term refers to both the mission itself and the satellite that the mission used to collect data. TRMM was part of NASA's Mission to Planet Earth , a long-term, coordinated research effort to study the Earth as a global system. The satellite was launched on 27 November 1997 from the Tanegashima Space Center in Tanegashima , Japan. TRMM operated for 17 years, including several mission extensions, before being decommissioned on 15 April 2015. TRMM re-entered Earth's atmosphere on 16 June 2015. Background[ edit ] Tropical precipitation is a difficult parameter to measure, due to large spatial and temporal variations. However, understanding tropical precipitation is important for weather and climate prediction, as this precipitation contains three-fourths of the energy that drives atmospheric wind circulation. [3] Prior to TRMM, the distribution of rainfall worldwide was known to only a 50% of certainty. [4] The concept for TRMM was first proposed in 1984. The science objectives, as first proposed, were: [3] To advance understanding of the global energy and water cycles by providing distributions of rainfall and latent heating over the global Tropics. To understand the mechanisms through which changes in tropical rainfall influence global circulation and to improve ability to model these processes in order to predict global circulations and rainfall variability at monthly and longer timescales. To provide rain and latent heating distributions to improve the initialization of models ranging from 24-hour forecasts to short-range climate variations. To help to understand, to diagnose, and to predict the onset and development of the El Niño , El Niño–Southern Oscillation , and the propagation of the 30- to 60-day oscillations in the Tropics . To help to understand the effect that rainfall has on the ocean thermohaline circulations and the structure of the upper ocean. To allow cross calibration between TRMM and other sensors with life expectancies beyond that of TRMM itself. To evaluate the diurnal variability of tropical rainfall globally. To evaluate a space-based system for rainfall measurements. Japan joined the initial study for the TRMM mission in 1986. [3] Development of the satellite became a joint project between the space agencies of the United States and Japan, with Japan providing the Precipitation Radar (PR) and H-II launch vehicle, and the United States providing the satellite bus and remaining instruments. [5] The project received formal support from the United States Congress in 1991, followed by spacecraft construction from 1993 through 1997. TRMM launched from Tanegashima Space Center on 27 November 1997. [3] Spacecraft[ edit ] The Tropical Rainfall Measuring Mission (TRMM), one of the spacecraft in the NASA Earth Probe series of research satellites, is a highly focused, limited-objective program aimed at measuring monthly and seasonal rainfall over the global tropics and subtropics. TRMM is a joint project between the United States and Japan to measure rainfall between 35.0° North and 35.0° South at 350 km altitude. [6] Mission extensions and de-orbit[ edit ] To extend TRMM's mission life beyond its primary mission, NASA boosted the spacecraft's orbit altitude to 402.5 km in 2001. [7] In 2005, NASA director Michael Griffin decided to extend the mission again by using the propellant originally intended for a controlled descent. This came after a 2002 NASA risk review put the probability of a human injury or death caused by TRMM's uncontrolled re-entry at 1-in-5,000, about twice the casualty risk deemed acceptable for re-entering NASA satellites;  and a subsequent recommendation from the National Research Council panel that the mission be extended despite the risk of an uncontrolled entry. [8] Battery issues began to limit the spacecraft in 2014 and the mission operations team had to make decisions about how to ration power. In March 2014, the VIRS instruments was turned off to extend the battery life. [7] In July 2014, with propellant on TRMM running low, NASA decided to cease station-keeping maneuvers and allow the spacecraft's orbit to slowly decay, while continuing to collect data. The remaining fuel, initially reserved to avoid collisions with other satellites or space debris, was depleted in early March 2015. [7] Re-entry was originally expected sometime between May 2016 and November 2017, but occurred sooner due to heightened solar activity. [9] The probe's primary sensor, the precipitation radar, was switched off for the final time on 1 April 2015 and the final scientific sensor, LIS, was turned off on 15 April 2015. [8] Re-entry occurred on 16 June 2015 at 06:54 UTC. [10] Instruments aboard the TRMM[ edit ] Precipitation Radar[ edit ] The Precipitation Radar (PR) was the first space-borne instrument designed to provide three-dimensional maps of storm structure. The measurements yielded information on the intensity and distribution of the rain, on the rain type, on the storm depth and on the height at which the snow melts into rain. The estimates of the heat released into the atmosphere at different heights based on these measurements can be used to improve models of the global atmospheric circulation. The PR operated at 13.8 GHz and measured the 3-D rainfall distribution over land and ocean surfaces. It defined a layer depth of perception and hence measured rainfall that actually reached the latent heat of atmosphere. It had a 4.3 km resolution at radii with 220 km swath. TRMM Microwave Imager[ edit ] The TRMM Microwave Imager (TMI) was a passive microwave sensor designed to provide quantitative rainfall information over a wide swath under the TRMM satellite. By carefully measuring the minute amounts of microwave energy emitted by the Earth and its atmosphere , TMI was able to quantify the water vapor , the cloud water, and the rainfall intensity in the atmosphere . It was a relatively small instrument that consumed little power. This, combined with the wide swath and the quantitative information regarding rainfall made TMI the "workhorse" of the rain-measuring package on Tropical Rainfall Measuring Mission. TMI is not a new instrument. It is based on the design of the highly successful Special Sensor Microwave/Imager (SSM/I) which has been flying continuously on Defense Meteorological Satellites since 1987. The TMI measures the intensity of radiation at five separate frequencies: 10.7, 19.4, 21.3, 37.0, 85.5 GHz. These frequencies are similar to those of the SSM/I, except that TMI has the additional 10.7 GHz channel designed to provide a more-linear response for the high rainfall rates common in tropical rainfall. The other main improvement that is expected from TMI is due to the improved ground resolution. This improvement, however, is not the result of any instrument improvements, but rather a function of the lower altitude of TRMM 402 kilometers compared to 860 kilometers of SSM/I). TMI has a 878-kilometer wide swath on the surface. The higher resolution of TMI on TRMM, as well as the additional 10.7 GHz frequency, makes TMI a better instrument than its predecessors. The additional information supplied by the Precipitation Radar further helps to improve algorithms. The improved rainfall products over a wide swath will serve both TRMM as well as the continuing measurements being made by the SSM/I and radiometers flying on the NASA's EOS-PM ( Aqua (satellite) ) and the Japanese ADEOS II satellites. Visible and Infrared Scanner[ edit ] The Visible and Infrared Scanner (VIRS) was one of the three instruments in the rain-measuring package and serves as a very indirect indicator of rainfall. VIRS, as its name implies, sensed radiation coming up from the Earth in five spectral regions, ranging from visible to infrared , or 0.63 to 12 mm . VIRS was included in the primary instrument package for two reasons. First was its ability to delineate rainfall. The second, and even more important reason, was to serve as a transfer standard to other measurements that are made routinely using Polar Operational Environmental Satellites (POES) and Geostationary Operational Environmental Satellite (GOES) satellites. The intensity of the radiation in the various spectral regions (or bands) can be used to determine the brightness (visible and near infrared) or temperature (infrared) of the source. Clouds and the Earth's Radiant Energy Sensor[ edit ] Main article: Clouds and the Earth's Radiant Energy System Clouds and the Earth's Radiant Energy System (CERES) measured the energy at the top of the atmosphere , as well as estimates energy levels within the atmosphere and at the Earth's surface. The CERES instrument was based on the successful Earth Radiation Budget Experiment (ERBS) which used three satellites to provide global energy budget measurements from 1984 to 1993. [11] Using information from very high resolution cloud imaging instruments on the same spacecraft, CERES determines cloud properties, including cloud-amount, altitude , thickness, and the size of the cloud particles. These measurements are important to understanding the Earth's total climate system and improving climate prediction models. It only operated during January–August 1998, and in March 2000, so the available data record is quite brief (although later CERES instruments were flown on other missions such as the Earth Observing System (EOS) AM (Terra) and PM (Aqua) satellites.) Lightning Imaging Sensor[ edit ] The Lightning Imaging Sensor (LIS) was a small, highly sophisticated instrument that detects and locates lightning over the tropical region of the globe. The lightning detector was a compact combination of optical and electronic elements including a staring imager capable of locating and detecting lightning within individual storms. The imager's field of view allowed the sensor to observe a point on the Earth or a cloud for 80 seconds, a sufficient time to estimate the flashing rate, which told researchers whether a storm was growing or decaying.
From Wikipedia, the free encyclopedia Joint mission between JAXA and NASA Global Precipitation Measurement Artist's concept of the GPM Core Observatory Mission type Planned: 3 years Elapsed: 10 years, 1 month, 11 days Spacecraft properties February 27, 2014, 18:37 (2014-02-27UTC18:37) UTC Rocket Epoch 26 March 2017 Global Precipitation Measurement (GPM) is a joint mission between JAXA and NASA as well as other international space agencies to make frequent (every 2–3 hours) observations of Earth's precipitation . It is part of NASA's Earth Systematic Missions program and works with a satellite constellation to provide full global coverage. The project provides global precipitation maps to assist researchers in improving the forecasting of extreme events, studying global climate, and adding to current capabilities for using such satellite data to benefit society. [1] GPM builds on the notable successes of the Tropical Rainfall Measuring Mission (TRMM), which was also a joint NASA-JAXA activity. The project is managed by NASA's Goddard Space Flight Center , and consists of a GPM Core Observatory satellite assisted by a constellation of spacecraft from other agencies and missions. [2] The Core Observatory satellite measures the two and three dimensional structure of Earth's precipitation patterns and provides a new calibration standard for the rest of the satellite constellation. The GPM Core Observatory was assembled and tested at Goddard Space Flight Center, and launched from Tanegashima Space Center , Japan, on a Mitsubishi Heavy Industries H-IIA rocket. The launch occurred on February 28, 2014, at 3:37 am JST on the first attempt. [3] Agencies in the United States, Japan, India and France (together with Eumetsat ) operate the remaining satellites in the constellation for agency-specific goals, but also cooperatively provide data for GPM. [2] GPM has five broad science objectives: [4] advance precipitation measurement from space improve knowledge of precipitation systems, water-cycle variability and freshwater availability improve climate modeling and prediction improve weather forecasting and climate reanalysis improve hydrological modeling and prediction Main instruments[ edit ] Visualization of GPM collecting data on March 17th, 2014 over the last major snow storm of winter 2013–2014 to hit the U.S. east coast. The GPM Core Observatory in the electromagnetic testing chamber at NASA Goddard Space Flight Center in March 2013. The silver disc and drum (center) is the GPM Microwave Imager, and the large block on the base is the Dual-frequency Precipitation Radar. Dual-Frequency Precipitation Radar (DPR)[ edit ] The DPR is a spaceborne radar, providing three-dimensional maps of storm structure across its swath, including the intensity of rainfall and snowfall at the surface. The DPR has two frequencies, allowing researchers to estimate the sizes of precipitation particles and detect a wider range of precipitation rates. The Ku-band radar, similar to the PR on TRMM, covers a 245 km (152 mile) swath. Nested inside that, the Ka-band radar covers a 120 km (74.5 mile) swath. [5] Data from the DPR is sent to the ground via a single-access link with TDRSS relay satellites. [6] GPM Microwave Imager (GMI)[ edit ] The GMI is a passive sensor that observes the microwave energy emitted by the Earth and atmosphere at 13 different frequency/polarization channels. These data allow quantitative maps of precipitation across a swath that is 885 km (550 miles) wide. This instrument continues the legacy of TRMM microwave observations, while adding four additional channels, better resolution, and more reliable calibration. [5] Data from the GMI is continuously sent to the ground via a multiple-access link with TDRSS relay satellites. [6] Precipitation data sets[ edit ] GPM produces and distributes a wide variety of precipitation data products. Processing takes place at the Precipitation Processing System (PPS) at NASA Goddard Space Flight Center , as well as at the JAXA facility in Japan. Data is provided at multiple "levels" of processing, from raw satellite measurements to best-estimate global precipitation maps using combinations of all the constellation observations and other meteorological data. All data from the mission is made freely available to the public on NASA websites. [7] Precipitation data is made available in a variety of formats, spatial and temporal resolutions, and processing levels which are accessible on the Precipitation Measurement Missions "Data Access" webpage. [8] Several data visualization and analysis tools have been made available to provide easy access for the science and applications communities, which include the in-browser Earth science data analysis tool Giovanni, [9] a web API, [10] and a 3D near-realtime global precipitation viewer. [11] Full-Scale Harness Mockup Model of the Core GPM Spacecraft being used for harness assembly inside the Acoustic Chamber at GSFC . Social media and outreach[ edit ] This animation shows GPM collecting some of its very first data on March 10th over a Pacific storm east of Japan. In addition to maintaining social media accounts [12] [13] [14] and the GPM Road to Launch Blog , JAXA and NASA developed several outreach activities specific to this mission prior to launch that the public could participate in. After launch a series of featured articles [15] and videos [16] were produced to highlight various scientific goals and discoveries of the mission, and an "Extreme Weather" blog is maintained to provide timely updates about the latest extreme precipitation events and natural disasters occurring around the world. A Precipitation Education website [17] is also maintained to provide teachers and students with lesson plans, animations, and other resources to teach about the water cycle , Earth science, and the GPM mission.
Microwave Radiometer (JMR) Jason-1 has five 5 instruments: Poseidon 2 – Nadir pointing Radar altimeter using C band and Ku band for measuring height above sea surface. Jason Microwave Radiometer (JMR) –  measures water vapor along altimeter path to correct for pulse delay DORIS ( Doppler Orbitography  and Radiopositioning Integrated by Satellite ) for orbit determination to within 10 cm or less and ionospheric correction data for Poseidon 2. BlackJack Global Positioning System receiver provides precise orbit ephemeris data Laser retroreflector array works with ground stations to track the satellite and calibrate and verify altimeter measurements. The Jason-1 satellite, its altimeter instrument and a position-tracking antenna were built in France. The radiometer, Global Positioning System receiver and laser retroreflector array were built in the United States. Use of information[ edit ] TOPEX/Poseidon and Jason-1 have led to major advances in the science of physical oceanography and in climate studies. [9] Their 15-year data record of ocean surface topography has provided the first opportunity to observe and understand the global change of ocean circulation and sea level. The results have improved the understanding of the role of the ocean in climate change and improved weather and climate predictions. Data from these missions are used to improve ocean models, forecast hurricane intensity, and identify and track large ocean/atmosphere phenomena such as El Niño and La Niña . The data are also used every day in applications as diverse as routing ships, improving the safety and efficiency of offshore industry operations, managing fisheries, and tracking marine mammals. [10] Their 15-year data record of ocean surface topography has provided the first opportunity to observe and understand the global change of ocean circulation and sea level. The results have improved the understanding of the role of the ocean in climate change and improved weather and climate predictions. Data from these missions are used to improve ocean models, forecast hurricane intensity, and identify and track large ocean/atmosphere phenomena such as El Niño and La Niña. The data are also used every day in applications as diverse as routing ships, improving the safety and efficiency of offshore industry operations, managing fisheries, and tracking marine mammals. TOPEX/Poseidon and Jason-1 have made major contributions [11] to the understanding of: Ocean variability[ edit ] Although the 1993–2005 Topex/Poseidon satellite (on the left) measured an average annual Global Mean Sea Level rise of 3.1 mm/year, Jason-1 is measuring only 2.3 mm/year GMSL rise, and the Envisat satellite (2002–2012) is measuring just 0.5 mm/year GMSL rise. In this graph, the vertical scale represents globally averaged mean sea level. Seasonal variations in sea level have been removed to show the underlying trend. (Image credit: University of Colorado) The missions revealed the surprising variability of the ocean, how much it changes from season to season, year to year, decade to decade and on even longer time scales. They ended the traditional notion of a quasi-steady, large-scale pattern of global ocean circulation by proving that the ocean is changing rapidly on all scales, from huge features such as El Niño and La Niña, which can cover the entire equatorial Pacific, to tiny eddies swirling off the large Gulf Stream in the Atlantic. Sea level change[ edit ] Further information: Sea level change Measurements by Jason-1 indicate that mean sea level has been rising at an average rate of 2.28 mm (0.09 inch) per year since 2001. This is somewhat less than the rate measured by the earlier TOPEX/Poseidon mission, but over four times the rate measured by the later Envisat mission. Mean sea level measurements from Jason-1 are continuously graphed at the Centre National d'Études Spatiales web site, on the Aviso page . A composite sea level graph, using data from several satellites, is also available on that site . The data record from these altimetry missions has given scientists important insights into how global sea level is affected by natural climate variability, as well as by human activities. Planetary Waves[ edit ] TOPEX/Poseidon and Jason-1 made clear the importance of planetary-scale waves, such as Rossby and Kelvin waves. No one had realized how widespread these waves are. Thousands of kilometers wide, these waves are driven by wind under the influence of Earth's rotation and are important mechanisms for transmitting climate signals across the large ocean basins. At high latitudes, they travel twice as fast as scientists believed previously, showing the ocean responds much more quickly to climate changes than was known before these missions. Further information: Tides The precise measurements of TOPEX/Poseidon's and Jason-1 have brought knowledge of ocean tides to an unprecedented level. The change of water level due to tidal motion in the deep ocean is known everywhere on the globe to within 2.5 centimeters (1 inch). This new knowledge has revised notions about how tides dissipate. Instead of losing all their energy over shallow seas near the coasts, as previously believed, about one third of tidal energy is actually lost to the deep ocean. There, the energy is consumed by mixing water of different properties, a fundamental mechanism in the physics governing the general circulation of the ocean. Ocean models[ edit ] TOPEX/Poseidon and Jason-1 observations provided the first global data for improving the performance of the numerical ocean models that are a key component of climate prediction models. TOPEX/Poseidon and Jason-1 data are available at the University of Colorado Center for Astrodynamics Research, [12] NASA's Physical Oceanography Distributed Active Archive Center, [13] and the French data archive center AVISO. [14] Benefits to society[ edit ] Altimetry data have a wide variety of uses from basic scientific research on climate to ship routing. Applications include: Climate Research : altimetry data are incorporated into computer models to understand and predict changes in the distribution of heat in the ocean, a key element of climate. El Niño and La Niña Forecasting: understanding the pattern and effects of climate cycles such as El Niño helps predict and mitigate the disastrous effects of floods and drought. Hurricane Forecasting: altimeter data and satellite ocean wind data are incorporated into atmospheric models for hurricane season forecasting and individual storm severity. Ship Routing: maps of ocean currents , eddies, and vector winds are used in commercial shipping and recreational yachting to optimize routes. Offshore Industries: cable-laying vessels and offshore oil operations require accurate knowledge of ocean circulation patterns to minimize impacts from strong currents. Marine Mammal Research: sperm whales, fur seals, and other marine mammals can be tracked, and therefore studied, around ocean eddies where nutrients and plankton are abundant. Fisheries Management: satellite data identify ocean eddies which bring an increase in organisms that comprise the marine food web, attracting fish and fishermen. Coral Reef Research: remotely sensed data are used to monitor and assess coral reef ecosystems, which are sensitive to changes in ocean temperature. Marine Debris Tracking: the amount of floating and partially submerged material, including nets, timber and ship debris, is increasing with human population. Altimetry can help locate these hazardous materials.
Period 112.00 minutes OSTM/Jason-2, or Ocean Surface Topography Mission/Jason-2 satellite, [1] was an international Earth observation satellite altimeter joint mission for sea surface height measurements between NASA and CNES . It was the third satellite in a series started in 1992 by the NASA/CNES TOPEX/Poseidon mission [2] and continued by the NASA/CNES Jason-1 mission launched in 2001. [3] History[ edit ] Like its two predecessors, OSTM/Jason-2 used high-precision ocean altimetry to measure the distance between the satellite and the ocean surface to within a few centimeters. These very accurate observations of variations in sea surface height — also known as ocean topography — provide information about global sea level , the speed and direction of ocean currents , and heat stored in the ocean. Jason-2 was built by Thales Alenia Space using a Proteus platform, under a contract from CNES, as well as the main Jason-2 instrument, the Poseidon-3 altimeter (successor to the Poseidon and Poseidon 2 altimeter on-board TOPEX/Poseidon and Jason-1 ). Scientists consider the 15-plus-year climate data record that this mission extended to be critical to understanding how ocean circulation is linked to global climate change . Team Vandenberg successfully launches a Delta II rocket from Space Launch Complex-2 at 12:46 a.m. Friday. The rocket carried the OSTM/Jason-2 Satellite into an 830-mile near-circular orbit. OSTM/Jason-2 was launched on 20 June 2008, at 07:46 UTC , from Space Launch Complex 2W at Vandenberg Air Force Base in California , by a Delta II 7320 rocket. [4] The spacecraft separated from the rocket 55 minutes later. [5] Jason-2 after separation from its launch vehicle It was placed in a 1,336 km (830 mi) circular, non-Sun-synchronous orbit at an inclination of 66.0° to Earth's equator , allowing it to monitor 95% of Earth's ice-free ocean every 10 days. Jason-1 was moved to the opposite side of Earth from Jason-2 and now flies over the same region of the ocean that Jason-2 flew over five days earlier. [6] Jason-1's ground tracks fall midway between those of Jason-2, which are about 315 km (196 mi) apart at the equator. This interleaved tandem mission provided twice the number of measurements of the ocean's surface, bringing smaller features such as ocean eddies into view. The tandem mission also helped pave the way for a future ocean altimeter mission that would collect much more detailed data with its single instrument than the two Jason satellites did together. With OSTM/Jason-2, ocean altimetry made the transition from research into operational mode. Responsibility for collecting these measurements moved from the space agencies to the world's weather and climate forecasting agencies, which use them for short-range, seasonal, and long-range weather and climate forecasting. [7] Science objectives[ edit ] Extend the time series of ocean surface topography measurements beyond TOPEX/Poseidon and Jason-1 to accomplish two decades of observations Provide a minimum of three years of global ocean surface topography measurement Determine the variability of ocean circulation at decadal time scales from combined data record of TOPEX/Poseidon and Jason-1 Improve the measure of the time-averaged ocean circulation Improve the measure of global sea-level change Improve open ocean tide models Ocean altimetry[ edit ] "Spaceborne radar altimeters have proven to be superb tools for mapping ocean-surface topography, the hills and valleys of the sea surface. These instruments send a microwave pulse to the ocean's surface and time how long it takes to return. A microwave radiometer corrects any delay that may be caused by water vapor in the atmosphere . Other corrections are also required to account for the influence of electrons in the ionosphere and the dry air mass of the atmosphere. Combining these data with the precise location of the spacecraft makes it possible to determine sea-surface height to within a few centimetres (about one inch). The strength and shape of the returning signal also provides information on wind speed and the height of ocean waves. These data are used in ocean models to calculate the speed and direction of ocean currents and the amount and location of heat stored in the ocean, which, in turn, reveals global climate variations ". [8] Atomic clock synchronization[ edit ] Another payload aboard Jason-2 is the T2L2 (Time Transfer by Laser Link) instrument. T2L2 is used to synchronize atomic clocks at ground stations, and to calibrate the on-board clock of the Jason-2 DORIS instrument. On 6 November 2008, CNES reported the T2L2 instrument was working well. [9] Jason 2 just before launch OSTM/Jason-2 was a joint effort by four organizations. [10] The mission participants were: National Aeronautics and Space Administration ( NASA ) France's Centre national d'études spatiales ( CNES ) European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT) CNES provided the spacecraft, NASA and CNES jointly provided the payload instruments, and NASA's Launch Services Program at the Kennedy Space Center was responsible for the launch management and countdown operations. After completing the on-orbit commissioning of the spacecraft, CNES handed over operation and control of the spacecraft to NOAA in October 2008. [11] CNES processed, distributed, and archived the research-quality data products that became available in 2009. EUMETSAT processed and distributed operational data received by its ground station to users in Europe and archived that data. NOAA processed and distributed operational data received by its ground stations to non-European users and archived that data along with the CNES data products. NOAA and EUMETSAT both generated near-real-time data products and distributed them to users. NASA evaluated the performance of the following instruments: the Advanced Microwave Radiometer (AMR), the Global Positioning System payload, and the Laser Retroreflector Assembly (LRA). NASA and CNES also validated scientific data products together. NASA's Jet Propulsion Laboratory in Pasadena, California , managed the mission for NASA's Science Mission Directorate in Washington, D.C. Prior similar missions[ edit ] OSTM/Jason-2's predecessor TOPEX/Poseidon caught the largest El Niño in a century seen in this image from 1 December 1997. The two previous altimetry missions, TOPEX/Poseidon and Jason-1 , led to major advances in the science of physical oceanography and in climate studies. [12] Their 15-year data record of ocean surface topography provided the first opportunity to observe and understand the global change of ocean circulation and sea level. Their results improved scientific understanding of the role of the ocean in climate change and improved weather and climate predictions. Data from these missions were used to improve ocean models, forecast hurricane intensity, and identify and track large ocean/atmosphere phenomena such as El Niño and La Niña . The data was also used in daily applications as diverse as routing ships, improving the safety and efficiency of offshore industry operations, managing fisheries and tracking marine mammals. Some of the areas in which TOPEX/Poseidon and Jason-1 have made major contributions, [13] and to which OSTM/Jason-2 continued to add, are: Ocean variability The missions revealed the surprising variability of the ocean, how much it changes from season to season, year to year, decade to decade and on even longer time scales. They ended the traditional notion of a quasi-steady, large-scale pattern of global ocean circulation by proving that the ocean is changing rapidly on all scales, from huge features such as El Nino and La Nina, which can cover the entire equatorial Pacific, to tiny eddies swirling off the large Gulf Stream in the Atlantic Ocean . Sea level change Measurements by TOPEX/Poseidon and Jason-1 show that mean sea level has been rising by about 3 mm (0.12 inches) a year since 1993. This is about twice the estimates from tide gauges for the previous century, indicating a possible recent acceleration in the rate of sea level rise. The data record from these altimetry missions has given scientists important insights into how global sea level is affected by natural climate variability, as well as by human activities. Planetary waves TOPEX/Poseidon and Jason-1 made clear the importance of planetary-scale waves , such as Rossby and Kelvin waves . Thousands of kilometres wide, these waves are driven by wind under the influence of Earth's rotation and are important mechanisms for transmitting climate signals across the large ocean basins. At high latitudes, they travel twice as fast as scientists believed previously, showing the ocean responds much more quickly to climate changes than was known before these missions. Ocean tides The precise measurements of TOPEX/Poseidon's and Jason-1 have brought knowledge of ocean tides to an unprecedented level. The change of water level due to tidal motion in the deep ocean is known everywhere on the globe to within 2.5 centimetres (one inch). This new knowledge has revised notions about how tides dissipate. Instead of losing all their energy over shallow seas near the coasts, as previously believed, about one third of tidal energy is actually lost to the deep ocean . There, the energy is consumed by mixing water of different properties , a fundamental mechanism in the physics governing the general circulation of the ocean. Ocean models TOPEX/Poseidon and Jason-1 observations provided the first global data for improving the performance of the numerical ocean models that are a key component of climate prediction models. Data use and benefits[ edit ] Validated data products in support of improved weather, climate and ocean forecasts were distributed to the public within a few hours of observation. Beginning in 2009, other data products for climate research were made available a few days to a few weeks after observations were taken by the satellite. Altimetry data have a wide variety of uses from basic scientific research on climate to ship routing. Applications include: Climate research : altimetry data are incorporated into computer models to understand and predict changes in the distribution of heat in ocean, a key element of climate. El Niño and La Niña forecasting: understanding the pattern and effects of climate cycles such as El Niño helps predict and mitigate the disastrous effects of floods and drought. Tropical cyclone forecasting: altimeter data and satellite ocean wind data are incorporated into atmospheric models for hurricane season forecasting and individual storm severity. Ship routing : maps of currents, eddies, and vector winds are used in commercial shipping and recreational yachting to optimize routes. Offshore industries : cable-laying vessels and offshore oil operations require accurate knowledge of ocean circulation patterns, to minimize impacts from strong currents. Marine mammal research: sperm whales , fur seals , and other marine mammals can be tracked, and therefore studied, around ocean eddies where nutrients and plankton are abundant. Fisheries management : satellite data identify ocean eddies which bring an increase in organisms that comprise the marine food web , attracting fish and fishermen. Coral reef research: remotely sensed data are used to monitor and assess coral reef ecosystems, which are sensitive to changes in ocean temperature. Marine debris tracking: altimetry can help locate hazardous materials such as floating and partially submerged fishing nets , timber, and ship debris. End of mission[ edit ] The OSTM/Jason-2 mission concluded on 1 October 2019, after NASA and its mission partners made the decision to decommission the spacecraft upon discovering significant recent deterioration of the spacecraft's power systems. [14] The decommissioning of the satellite took some days; the final decommissioning activities on the satellite ended 9 October 2019, with the satellite rendered fully inactive. [15] Because Jason-2 is orbiting at an altitude of over 1,300 km (810 mi), NASA estimates that it will remain in orbit for at least 500 to 1,000 years after decommissioning. [15] Future[ edit ] The fourth spacecraft to be part of the Ocean Surface Topography Mission is Jason-3 . Like its predecessors, the primary instrument aboard Jason-3 is a radar altimeter . Additional instruments include: [16] DORIS (Doppler Orbitography and Radiopositioning Integrated by Satellite) A Laser Retroreflector Array (LRA) A Global Positioning System (GPS) receiver Jason-3 launched from Vandenberg Air Force Base on board a SpaceX Falcon 9 v1.1 launch vehicle in 2016. [17] The satellite was shipped to Vandenberg Air Force Base on 18 June 2015, [18] and after delays due to a June 2015 Falcon 9 launch failure, the mission was launched 17 January 2016 at 18:42:18 UTC. [19] [20] The technologies and data-sets pioneered by Jason-1, OSTM/Jason-2, and Jason-3, will be continued through the Sentinel-6 /Jason-CS satellites, planned for launch in 2020 and 2025. [14]
(Redirected from Cryosat ) ESA programme monitoring polar ice using satellites CryoSat is an ESA programme to monitor variations in the extent and thickness of polar ice through use of a satellite in low Earth orbit . The information provided about the behaviour of coastal glaciers that drain thinning ice sheets will be key to better predictions of future sea level rise . The CryoSat-1 spacecraft was lost in a launch failure in 2005, [1] however the programme was resumed with the successful launch of a replacement, CryoSat-2 , launched on 8 April 2010. [2] [3] [4] Description[ edit ] CryoSat's primary instrument is SIRAL ( SAR / Interferometric Radar Altimeter ). SIRAL operates in one of three modes, depending on where (above the Earth's surface) CryoSat was flying. Over the oceans and ice sheet interiors, CryoSat operates like a traditional radar altimeter. Over sea ice, coherently transmitted echoes are combined ( synthetic aperture processing) to reduce the surface footprint so that CryoSat could map smaller ice floes. CryoSat's most advanced mode is used around the ice sheet margins and over mountain glaciers. Here, the altimeter performs synthetic aperture processing and uses a second antenna as an interferometer to determine the across-track angle to the earliest radar return. This provides the exact surface location being measured when the surface is sloping. The original CryoSat was proposed in 1998 by Duncan Wingham of University College London . The satellite's planned three-year mission was to survey natural and human driven changes in the cryosphere on Earth. It was designed to provide much more accurate data on the rate of change of the surface elevation of the polar ice sheets and sea ice thickness. It was the first ESA Earth Sciences satellite selected through open, scientific competition. [5] It was destroyed on launch October 8, 2005. The existing satellite is therefore, formally speaking, CryoSat-2, but the mission is still known simply as CryoSat. [6] Although largely the same as the original satellite a number of key improvements were included in CryoSat-2. The most significant was the decision to provide a fully duplicated payload to enable the mission to continue if a fault caused the loss of the SIRAL radar, but there were many other changes "under the hood". Some of these were caused by obsolescence in the original design, some improved reliability and others made the satellite easier to operate. Despite all the changes the mission remains the same and the performance, in terms of measurement capability and accuracy, remains the same. As of 14 January 2010 [update] , the launch was scheduled for February 25, 2010 with a Dnepr rocket from the Baikonur Cosmodrome , [7] but this was delayed. The CryoSat 2 launched on April 8, 2010 at 13:57 UTC. [8] [9] For positioning purposes, CryoSat included a DORIS receiver, a laser retroreflector and three star trackers . The ERS-1 and ERS-2 satellites were precursors that tested the techniques used by CryoSat. Main articles: CryoSat-1 and CryoSat-2 CryoSat-1 was launched from the Plesetsk Cosmodrome in Russia on October 8, 2005, using a Rockot launcher. (Rockot is a modified SS-19 rocket which was originally an ICBM designed to deliver nuclear weapons, but which Russia is now eliminating in accordance with the START treaties.) According to Mr. Yuri Bakhvalov, First Deputy Director General of the Khrunichev Space Centre , when the automatic command to switch off the second stage engine did not take effect, [10] [11] the second stage continued to operate until it ran out of fuel and as a consequence the planned separation of the third (Breeze-KM) stage of the rocket which carried the CryoSat satellite did not take place, and would thus have remained attached to the second stage. The upper rocket stages, together with the satellite, probably crashed in the Lincoln Sea . Analysis of the error revealed that it was caused by faults in the programming of the rocket, which had not been detected in simulations. [12] After the launch failure of CryoSat, ESA immediately started to plan a replacement CryoSat mission. This included securing the industrial team which had built the original, ordering parts which have a long delivery time and establishing a funding scheme within existing budgets. Due to the importance of the scientific goals of this satellite, there was enormous support for this, and the initial phases for CryoSat-2 were approved when ESA's Earth Observation Programme Board agreed to build a copy of the spacecraft on February 23, 2006. [13] Life-size model of CryoSat
From Wikipedia, the free encyclopedia Suomi NPP Suomi National Polar-orbiting PartnershipNPOESS Preparatory Project (NPP) Mission type 5 years (planned)12 years, 5 months, 11 days (elapsed) Spacecraft properties 2,128 kg (4,691 lb) [1] Dry mass 1.3 m x 1.3 m x 4.2 m Power 28 October 2011,09:48:01.828 UTC [2] Rocket Clouds and the Earth's Radiant Energy System CrIS Ozone Mapping and Profiler Suite ATMS Visible Infrared Imaging Radiometer Suite Insignia for the NPOESS Preparatory Project NOAA-20 → The Suomi National Polar-orbiting Partnership (Suomi NPP), previously known as the National Polar-orbiting Operational Environmental Satellite System Preparatory Project (NPP) and NPP-Bridge, is a weather satellite operated by the United States National Oceanic and Atmospheric Administration (NOAA). It was launched in 2011 and is currently in operation. Suomi NPP was originally intended as a pathfinder for the National Polar-orbiting Operational Environmental Satellite System (NPOESS) program, which was to have replaced NOAA's Polar Operational Environmental Satellites (POES) and the U.S. Air Force 's Defense Meteorological Satellite Program (DMSP). Suomi NPP was launched in 2011 after the cancellation of NPOESS to serve as a stop-gap between the POES satellites and the Joint Polar Satellite System (JPSS) which will replace them. Its instruments provide climate measurements that continue prior observations by NASA 's Earth Observing System (EOS). Name[ edit ] The satellite is named after Verner E. Suomi , a Finnish-American meteorologist at the University of Wisconsin–Madison . The name was announced on 24 January 2012, three months after the satellite's launch. [4] [5] History[ edit ] NPOESS Preparatory Project (NPP) is intended to bridge the gap between old Earth Observing System (EOS) and new systems (JPSS) by flying new instruments, on a new satellite bus, using a new ground data network. [7] Originally planned for launch five years earlier as a joint NASA / NOAA / DoD project, NPP was to be a pathfinder mission for the larger National Polar-orbiting Operational Environmental Satellite System (NPOESS) until DoD participation in the larger project was dissolved. The project continued as a civilian weather forecasting replacement for the NOAA Polar Operational Environmental Satellites (POES) series, and ensured continuity of climate measurements begun by the Earth Observing System (EOS) of NASA. [8] Launch[ edit ] The spacecraft was launched on 28 October 2011 at  from Vandenberg Space Force Base via a Delta II in the 7920-10 configuration (Extra Extended Long Tank with RS-27A engine first stage, 9 GEM-40 solid rocket motors, type 2 second stage with Aerojet AJ10 engine, no third stage and a 10-foot fairing). [9] [10] Additionally, the rocket deployed five CubeSats as a part of NASA ELaNa III manifest . Spacecraft[ edit ] Suomi NPP in the cleanroom before launch The Suomi NPP spacecraft has been built and integrated by BATC (Ball Aerospace and Technologies Corporation) of Boulder, Colorado (NASA/GSFC contract award in May 2002). The platform design is a variation of BCP 2000 (Ball Commercial Platform) bus of BATC of ICESat and CloudSat heritage. The spacecraft consists of an aluminum honeycomb structure. The ADCS (Attitude Determination and Control Subsystem) provides 3-axis stabilization using 4 reaction wheels for fine attitude control, 3 torquer bars for momentum unloading, thrusters for coarse attitude control (such as during large-angle slews for orbital maintenance), 2 star trackers for fine attitude determination, 3 gyroscopes for attitude and attitude rate determination between star tracker updates, 2 Earth sensors for safe-mode attitude control, and coarse Sun sensors for initial attitude acquisition, all monitored and controlled by the spacecraft controls a computer. ADCS provides real-time attitude knowledge of 10 arcsec (1 sigma) at the spacecraft navigation reference base, real-time spacecraft position knowledge of 25 m (1 sigma), and attitude control of 36 arcsec (1 sigma). The EPS (Electrical Power Subsystem) uses Gallium arsenide (GaAs) solar cells to generate an average power of about 2 kW (EOL). The solar array rotates once per orbit to maintain a nominally normal orientation to the Sun. In addition, a single-wing solar array is mounted on the anti-solar side of the spacecraft; its function is to preclude thermal input into the sensitive cryo radiators of the Visible Infrared Imaging Radiometer Suite (VIIRS) and Cross-track Infrared Sounder (CrIS) instruments. A regulated 28 ±6 VDC power bus distributes energy to all spacecraft subsystems and instruments. A nickel–hydrogen battery (NiH) system provides power for eclipse phase operations. The spacecraft has an on-orbit design lifetime of 5 years (available consumables for 7 years). The spacecraft dry mass is about 1400 kg. NPP is designed to support controlled reentry at the end of its mission life (via propulsive maneuvers to lower the orbit perigee to approximately 50 km and target any surviving debris for open ocean entry). NPP is expected to have sufficient debris that survives reentry so as to require controlled reentry to place the debris in a pre-determined location in the ocean. Instruments[ edit ] The Suomi NPP is the first in a new generation of satellites intended to replace the Earth Observing System (EOS) satellites, which were launched from 1997 to 2009. The satellite orbits the Earth about 14 times each day. Its five imaging systems include: Visible Infrared Imaging Radiometer Suite (VIIRS)[ edit ] The Visible Infrared Imaging Radiometer Suite (VIIRS) is the largest instrument aboard of Suomi-NPP (National Polar-Orbiting Operational Environmental Satellite System ( NPOESS ) Preparatory Project). It collects radiometric imagery in visible and infrared wavelengths of the land, atmosphere, ice, and ocean. It will survey broad swaths of the land, oceans, and air, enabling scientists to monitor everything from phytoplankton and other organisms in the sea, vegetation and forest cover, and the amount of sea ice at the poles. Data from VIIRS, collected from 22 channels across the electromagnetic spectrum , will also be used to observe active fires, ocean color, sea surface temperature , and other surface features. [11] Ozone Mapping and Profiler Suite (OMPS)[ edit ] The Ozone Mapping and Profiler Suite (OMPS) measures the ozone layer in our upper atmosphere tracking the status of global ozone distributions, including the ozone hole . It also monitors ozone levels in the troposphere . OMPS extends out 40-year long record ozone layer measurements while also providing improved vertical resolution compared to previous operational instruments. Closer to the ground, OMPS's measurements of harmful ozone improve air quality monitoring and when combined with cloud predictions; help to create the Ultraviolet index . OMPS has two sensors, both new designs, composed of three advanced hyperspectral-imaging spectrometers. [12] Clouds and the Earth's Radiant Energy System (CERES)[ edit ] The Clouds and the Earth's Radiant Energy System (CERES) will be used to study the Earth's radiation budget . Monitoring the amount of energy emitted and reflected by the planet, it measures both solar energy reflected by the Earth and heat emitted by our planet. This solar and thermal energy are key parts of the Earth's radiation budget. CERES instrument continues a multi-year record of the amount of energy entering and exiting from the top of the atmosphere of Earth . It will provide scientists with needed long-term, stable data sets to make accurate projections of global climate change. [13] Cross-track Infrared Sounder (CrIS)[ edit ] The Cross-track Infrared Sounder (CrIS) has 1305 spectral channels and will produce high-resolution, three-dimensional temperature, pressure, and moisture profiles. It measures continuous channels in the infrared region and has the ability to measure temperature profiles with improved accuracy over its predecessors. These profiles will be used to enhance weather forecasting models and will facilitate both short- and long-term weather forecasting. Over longer timescales, they will help improve understanding of climate phenomena. [14] Advanced Technology Microwave Sounder (ATMS)[ edit ] The Advanced Technology Microwave Sounder (ATMS), works in conjunction with the Cross-track Infrared Sounder (CrIS) to make detailed vertical profiles of atmospheric pressure, heat, and moisture. ATMS, a cross-track scanner with 22 channels, provides sounding observations needed to retrieve profiles of atmospheric temperature and moisture for civilian operational weather forecasting as well as continuity of these measurements for climate monitoring purposes. CrIS will operate at infrared wavelengths, while ATMS will operate at much shorter, microwave, wavelengths. [15] Mission[ edit ] Blue Marble 2012, created from Suomi NPP composite imagery The VIIRS sensor on board the spacecraft acquired its first measurements of Earth on 21 November 2011. [16] NASA also released a high resolution blue marble image of the Earth showing most of North America , which was created by NASA oceanographer Norman Kuring using data obtained on 4 January 2012 by the Visible Infrared Imager Radiometer Suite (VIIRS), one of five imaging systems aboard the satellite. That date was chosen because it was a fairly sunny day in most of North America. [6] As of 22 November 2020 [update] , beyond the initial 5-year mission, the spacecraft continues to operate. [17] Earth, created from Suomi NPP composite imagery. Earth at night, created from Suomi NPP composite imagery. Delta II rocket carrying NPP. NPP launch video First image acquired by the VIIRS sensor. This composite image of southern Africa and the surrounding oceans was captured by six orbits of the satellite. Iowa power outage
Toggle the table of contents Vegetation From Wikipedia, the free encyclopedia Assemblage of plant species For the medical term, see Vegetation (pathology) . These maps show a scale, or index of greenness, based on several factors: the number and type of plants, leafiness, and plant health. Where foliage is dense and plants are growing quickly, the index is high, represented in dark green. Regions with sparse vegetation and a low vegetation index are shown in tan. Based on measurements from the Moderate Resolution Imaging Spectroradiometer (MODIS) on NASA's Terra satellite. Areas where there is no data are gray. [1] Vegetation is an assemblage of plant species and the ground cover they provide. [2] It is a general term, without specific reference to particular taxa , life forms, structure, spatial extent, or any other specific botanical or geographic characteristics. It is broader than the term flora which refers to species composition . Perhaps the closest synonym is plant community , but vegetation can, and often does, refer to a wider range of spatial scales than that term does, including scales as large as the global. Primeval redwood forests , coastal mangrove stands, sphagnum bogs , desert soil crusts , roadside weed patches, wheat fields, cultivated gardens and lawns; all are encompassed by the term vegetation. The vegetation type is defined by characteristic dominant species, or a common aspect of the assemblage, such as an elevation range or environmental commonality. [3] The contemporary use of vegetation approximates that of ecologist Frederic Clements' term earth cover, an expression still used by the Bureau of Land Management . History of definition[ edit ] The distinction between vegetation (the general appearance of a community) and flora (the taxonomic composition of a community) was first made by Jules Thurmann (1849). Prior to this, the two terms (vegetation and flora) were used indiscriminately, [4] [5] and still are in some contexts. Augustin de Candolle (1820) also made a similar distinction but he used the terms "station" ( habitat type) and "habitation" ( botanical region ). [6] [7] Later, the concept of vegetation would influence the usage of the term biome with the inclusion of the animal element. [8] Other concepts similar to vegetation are " physiognomy of vegetation" ( Humboldt , 1805, 1807) and "formation" ( Grisebach , 1838, derived from "Vegetationsform", Martius , 1824). [5] [9] [10] [11] [12] Departing from Linnean taxonomy , Humboldt established a new science, dividing plant geography between taxonomists who studied plants as taxa and geographers who studied plants as vegetation. [13] The physiognomic approach in the study of vegetation is common among biogeographers working on vegetation on a world scale, or when there is a lack of taxonomic knowledge of someplace (e.g., in the tropics, where biodiversity is commonly high). [14] The concept of " vegetation type " is more ambiguous. The definition of a specific vegetation type may include not only physiognomy but also floristic and habitat aspects. [15] [16] Furthermore, the phytosociological approach in the study of vegetation relies upon a fundamental unit, the plant association , which is defined upon flora. [17] An influential, clear and simple classification scheme for types of vegetation was produced by Wagner & von Sydow (1888). [18] [19] Other important works with a physiognomic approach includes Grisebach (1872), Warming (1895, 1909), Schimper (1898), Tansley and Chipp (1926), Rübel (1930), Burtt Davy (1938), Beard (1944, 1955), André Aubréville (1956, 1957), Trochain (1955, 1957), Küchler (1967), Ellenberg and Mueller-Dombois (1967) (see vegetation classification ). Main article: Vegetation classification There are many approaches for the classification of vegetation (physiognomy, flora, ecology, etc.). [20] [21] [22] [23] Much of the work on vegetation classification comes from European and North American ecologists, and they have fundamentally different approaches. In North America, vegetation types are based on a combination of the following criteria: climate pattern, plant habit , phenology and/or growth form, and dominant species. In the current US standard (adopted by the Federal Geographic Data Committee (FGDC), and originally developed by UNESCO and The Nature Conservancy ), the classification is hierarchical and incorporates the non-floristic criteria into the upper (most general) five levels and limited floristic criteria only into the lower (most specific) two levels. In Europe, classification often relies much more heavily, sometimes entirely, on floristic (species) composition alone, without explicit reference to climate, phenology or growth forms. It often emphasizes indicator or diagnostic species which may distinguish one classification from another. In the FGDC standard, the hierarchy levels, from most general to most specific, are: system, class, subclass, group, formation, alliance, and association. The lowest level, or association, is thus the most precisely defined, and incorporates the names of the dominant one to three (usually two) species of a type. An example of a vegetation type defined at the level of class might be "Forest, canopy cover > 60%"; at the level of a formation as "Winter-rain, broad-leaved, evergreen, sclerophyllous, closed-canopy forest"; at the level of alliance as "Arbutus menziesii forest"; and at the level of association as "Arbutus menziesii-Lithocarpus dense flora forest", referring to Pacific madrone-tanoak forests which occur in California and Oregon, US. In practice, the levels of the alliance and/or an association are the most often used, particularly in vegetation mapping, just as the Latin binomial is most often used in discussing particular species in taxonomy and in general communication. Dynamics[ edit ] Like all the biological systems, plant communities are temporally and spatially dynamic; they change at all possible scales. Dynamism in vegetation is defined primarily as changes in species composition and/or vegetation structure. Vegetation types at the time of Last Glacial Maximum Temporally, a large number of processes or events can cause change, but for sake of simplicity, they can be categorized roughly as either abrupt or gradual. Abrupt changes are generally referred to as disturbances ; these include things like wildfires , high winds , landslides , floods , avalanches and the like. Their causes are usually external ( exogenous ) to the community—they are natural processes occurring (mostly) independently of the natural processes of the community (such as germination, growth, death, etc.). Such events can change vegetation structure and composition very quickly and for long time periods, and they can do so over large areas. Very few ecosystems are without some type of disturbance as a regular and recurring part of the long term system dynamic. Fire and wind disturbances are particularly common throughout many vegetation types worldwide. Fire is particularly potent because of its ability to destroy not only living plants, but also the seeds, spores, and living meristems representing the potential next generation, and because of fire's impact on fauna populations, soil characteristics and other ecosystem elements and processes (for further discussion of this topic see fire ecology ). Temporal change at a slower pace is ubiquitous; it comprises the field of ecological succession . Succession is the relatively gradual change in structure and taxonomic composition that arises as the vegetation itself modifies various environmental variables over time, including light, water and nutrient levels. These modifications change the suite of species most adapted to grow, survive and reproduce in an area, causing floristic changes. These floristic changes contribute to structural changes that are inherent in plant growth even in the absence of species changes (especially where plants have a large maximum size, i.e. trees), causing slow and broadly predictable changes in the vegetation. Succession can be interrupted at any time by disturbance, setting the system either back to a previous state, or off on another trajectory altogether. Because of this, successional processes may or may not lead to some static, final state . Moreover, accurately predicting the characteristics of such a state, even if it does arise, is not always possible. In short, vegetative communities are subject to many variables that together set limits on the predictability of future conditions. Spatial dynamics[ edit ] As a general rule, the larger an area under consideration, the more likely the vegetation will be heterogeneous across it. Two main factors are at work. First, the temporal dynamics of disturbance and succession are increasingly unlikely to be in synchrony across any area as the size of that area increases. That is, different areas will be at different developmental stages due to different local histories, particularly their times since last major disturbance. This fact interacts with inherent environmental variability (e.g. in soils, climate, topography, etc.), which is also a function of area. Environmental variability constrains the suite of species that can occupy a given area, and the two factors together interact to create a mosaic of vegetation conditions across the landscape. Only in agricultural or horticultural systems does vegetation ever approach perfect uniformity. In natural systems, there is always heterogeneity, although its scale and intensity will vary widely..
From Wikipedia, the free encyclopedia Commercial Earth-imaging satellite system operated by the French space agency CNES Spot-5 Satellite Athens as seen by the SPOT 5 satellite in 2002 SPOT ( French : Satellite Pour l’Observation de la Terre, [1] lit. "Satellite for observation of Earth") is a commercial high-resolution optical Earth imaging satellite system operating from space. It is run by Spot Image , based in Toulouse , France. It was initiated by the CNES (Centre national d'études spatiales – the French space agency) in the 1970s and was developed in association with the SSTC (Belgian scientific, technical and cultural services) and the Swedish National Space Board (SNSB). It has been designed to improve the knowledge and management of the Earth by exploring the Earth's resources, detecting and  forecasting phenomena involving climatology and oceanography, and monitoring human activities and natural phenomena. The SPOT system includes a series of satellites and ground control resources for satellite control and programming, image production, and distribution. Earlier satellites were launched using the European Space Agency 's Ariane 2, 3, and 4 rockets, while SPOT 6 and SPOT 7 were launched by the Indian PSLV . [2] [3] List of satellites[ edit ] SPOT Image is marketing the high-resolution images, which SPOT can take from every corner of the Earth. SPOT 1 launched February 22, 1986 with 10 panchromatic and 20 meter multispectral picture resolution capability. Withdrawn December 31, 1990. SPOT 2 launched January 22, 1990 and deorbited in July 2009. SPOT 3 launched September 26, 1993. Stopped functioning November 14, 1997. SPOT 4 launched March 24, 1998. Stopped functioning July, 2013. SPOT 5 launched May 4, 2002 with 2.5 m, 5 m and 10 m capability. Stopped functioning March 31, 2015. SPOT 6 launched September 9, 2012. SPOT 7 (Azersky) launched on June 30, 2014. Stopped functioning March 17, 2023. Orbit[ edit ] The SPOT orbit is polar , circular , Sun-synchronous , and phased . The inclination of the orbital plane combined with the rotation of the Earth around the polar axis allows the satellite to fly over any point on Earth within 26 days. The orbit has an altitude of 832 kilometers (517 mi), an inclination of 98.7°, and completing 14 + 5/26 revolutions per day. Generations[ edit ] SPOT 1, 2, and 3[ edit ] Since 1986 the SPOT family of satellites has been orbiting the Earth and has already taken more than 10 million high quality images. SPOT 1 was launched with the last Ariane 1 rocket on February 22, 1986. Two days later, the 1,800 kg (4,000 lb) SPOT 1 transmitted its first image with a spatial resolution of 10 or 20 meters (33 or 66 ft). SPOT 2 joined SPOT 1 in orbit on January 22, 1990, on the Ariane 4 maiden flight, and SPOT 3 followed on September 26, 1993, also on an Ariane 4. The satellite loads were identical, each including two identical HRV (High Resolution Visible) imaging instruments that were able to operate in two modes, either simultaneously or individually. The two spectral modes are panchromatic and multispectral. The panchromatic band has a resolution of 10 meters (33 ft), and the three multispectral bands (G, R, NIR) have resolutions of 20 meters (66 ft)s. They have a scene size of 3,600 km2 (1,400 sq mi) and a revisit interval of one to four days, depending on the latitude. Because the orbit of SPOT 1 was lowered in 2003, it will gradually lose altitude and break up naturally in the atmosphere. Deorbiting of SPOT 2, in accordance with Inter-Agency Space Debris Coordination Committee (IADC), commenced in mid-July 2009 for a period of two weeks, with a final burn on 29 July 2009. SPOT 3 is no longer functioning, due to problems with its stabilization system. SPOT 4[ edit ] SPOT 4 launched March 24, 1998 and stopped functioning July, 2013. In 2013, CNES lowered the altitude of SPOT 4 by 2.5 km (1.6 mi) to put it on a phased orbit with a five-day repeat cycle. On this orbit, SPOT4 was programmed to acquire a time-lapse series of images over 42 sites with a five days revisit period from February to end of May 2013. The data set it produced is aimed at helping future users of the Sentinel-2 mission to learn working with time-lapse series. The time-lapse series provided by SPOT4 (Take5) have the same repetitiveness as those that will be delivered by the Sentinel-2 satellites, starting in 2015 and 2016. SPOT 5[ edit ] SPOT 5 was launched on May 4, 2002, and has the goal to ensure continuity of services for customers and to improve the quality of data and images by anticipating changes in market requirements. SPOT 5 has two high resolution geometrical (HRG) instruments that were deduced from the HRVIR of SPOT 4. They offer a higher resolution of 2.5 to 5 meters in panchromatic mode and 10 meters in multispectral mode (20 metre on short wave infrared 1.58 – 1.75 µm). [4] SPOT 5 also features an HRS imaging instrument operating in panchromatic mode. HRS points forward and backward of the satellite. Thus, it is able to take stereopair images almost simultaneously to map relief. SPOT 6 and SPOT 7[ edit ] SPOT 6 was launched by India's Polar Satellite Launch Vehicle on flight C21 [2] at 04:23 UTC on 9 September 2012, while SPOT 7 was launched on PSLV flight C23 [3] at 04:42 UTC on 30 June 2014. They form a constellation of Earth-imaging satellites designed to provide continuity of high-resolution, wide-swath data up to 2024. EADS Astrium decided to build this constellation in 2009 on the basis of a perceived government need for this kind of data. Spot Image , a subsidiary of Astrium, funded the satellites alone and owned the system (satellites and ground segments) at time of launch. In December 2014, SPOT 7 was sold to Azerbaijan 's space agency Azercosmos, who renamed it Azersky . [5] The  architecture is similar to that of the Pleiades satellites , with a centrally mounted optical instrument, a three-axis star tracker , a fiber-optic gyro (FOG) and four control moment gyros (CMGs). SPOT 6 and SPOT 7 are phased in the same orbit as Pléiades 1A and Pléiades 1B at an altitude of 694 km (431 mi), forming a constellation of 2-by-2 satellites - 90° apart from one another. [6] Image product resolution:
Toggle the table of contents Altimeter From Wikipedia, the free encyclopedia Not to be confused with attitude indicator . Instrument used to determine the height of an object above a certain point Diagram showing the face of the "three-pointer" sensitive aircraft altimeter displaying an altitude of 10,180 ft (3,100 m). Reference pressure of about 29.92 inHg (1013 hPa) is showing in the Kollsman window An altimeter or an altitude meter is an instrument used to measure the altitude of an object above a fixed level. [1] The measurement of altitude is called altimetry, which is related to the term bathymetry , the measurement of depth under water. This section is an excerpt from Pressure altimeter .[ edit ] Digital barometric pressure sensor for altitude measurement in consumer electronic applications Altitude can be determined based on the measurement of atmospheric pressure . The greater the altitude, the lower the pressure. When a barometer is supplied with a nonlinear calibration so as to indicate altitude, the instrument is a type of altimeter called a pressure altimeter or barometric altimeter. A pressure altimeter is the altimeter found in most aircraft , and skydivers use wrist-mounted versions for similar purposes. Hikers and mountain climbers use wrist-mounted or hand-held altimeters, in addition to other navigational tools such as a map, magnetic compass, or GPS receiver. Sonic altimeter[ edit ] In 1931, the US Army Air Corps and General Electric tested a sonic altimeter for aircraft, which was considered more reliable and accurate than one that relied on air pressure when heavy fog or rain was present. The new altimeter used a series of high-pitched sounds like those made by a bat to measure the distance from the aircraft to the surface, which on return to the aircraft was converted to feet shown on a gauge inside the aircraft cockpit. [2] Main article: Radar altimeter The altimeter on this Piper PA-28 is seen on the top row of instruments, second from right A radar altimeter measures altitude more directly, using the time taken for a radio signal to reflect from the surface back to the aircraft. Alternatively, Frequency Modulated Continuous-wave radar can be used. The greater the frequency shift the further the distance travelled. This method can achieve much better accuracy than the pulsed radar for the same outlay and radar altimeters that use frequency modulation are industry standard. The radar altimeter is used to measure height above ground level during landing in commercial and military aircraft. Radar altimeters are also a component of terrain avoidance warning systems, warning the pilot if the aircraft is flying too low, or if there is rising terrain ahead. Radar altimeter technology is also used in terrain-following radar allowing combat aircraft to fly at very low height above the terrain. After extensive research and experimentation, it has been shown that "phase radio-altimeters" are most suitable for ground effect vehicles , as compared to laser, isotropic or ultrasonic altimeters. [3] Laser altimeter[ edit ] Lidar technology is used to help navigate the helicopter Ingenuity on its record-setting flights over the terrain of Mars by means of a downward-facing Lidar altimeter. [4] Global Positioning System[ edit ] Global Positioning System (GPS) receivers can also determine altitude by trilateration with four or more satellites . In aircraft, altitude determined using autonomous GPS is not reliable enough to supersede the pressure altimeter without using some method of augmentation . [5] In hiking and climbing, it is common to find that the altitude measured by GPS is off by as much as 400 feet (122 metres) depending on satellite orientation. [6]
Toggle the table of contents Radiometer From Wikipedia, the free encyclopedia Device for measuring the radiant flux (power) of electromagnetic radiation For the specific radiometer that this term often refers to, see Crookes radiometer . An example of a Crookes radiometer . The vanes rotate when exposed to light, with faster rotation for more intense light, providing a quantitative measurement of electromagnetic radiation intensity. A radiometer or roentgenometer is a device for measuring the radiant flux (power) of electromagnetic radiation . Generally, a radiometer is an infrared radiation detector or an ultraviolet detector. [1] Microwave radiometers operate in the microwave wavelengths. While the term radiometer can refer to any device that measures electromagnetic radiation (e.g. light), the term is often used to refer specifically to a Crookes radiometer ("light-mill"), a device invented in 1873 in which a rotor (having vanes which are dark on one side, and light on the other) in a partial vacuum spins when exposed to light.  A common misbelief (one originally held even by Crookes) is that the momentum of the absorbed light on the black faces makes the radiometer operate.  If this were true, however, the radiometer would spin away from the non-black faces, since the photons bouncing off those faces impart more momentum than the photons absorbed on the black faces.  Photons do exert radiation pressure on the faces, but those forces are dwarfed by other effects. The currently accepted explanation depends on having just the right degree of vacuum, and relates to the transfer of heat rather than the direct effect of photons. [2] [3] A Nichols radiometer demonstrates photon pressure.  It is much more sensitive than the Crookes radiometer and it operates in a complete vacuum, whereas operation of the Crookes radiometer requires an imperfect vacuum. The MEMS radiometer can operate on the principles of Nichols or Crookes and can operate over a wide spectrum of wavelength and particle energy levels. [4]
Toggle the table of contents Thematic Mapper Printable version From Wikipedia, the free encyclopedia A Thematic Mapper (TM) is one of the Earth observing sensors introduced in the Landsat program . The first was placed aboard Landsat 4 (decommissioned in 2001), and another was operational aboard Landsat 5 up to 2012. [1] [2] TM sensors feature seven bands of image data (three in visible wavelengths, four in infrared ) most of which have 30 meter spatial resolution. TM is a whisk broom scanner which takes multi-spectral images across its ground track . It does not directly produce a thematic map . TM Focal Plane The upper photo on the right is a 50 times magnification of the combined photomasks used to fabricate the Hughes H4040, the linear silicon photodiode array used in the Thematic Mapper to image the visible bands. Each of the 16 photodiodes is 100 microns square and their separation is 100 microns. There are two rows because it is scanned perpendicular to the lines of diodes and they produce a complete line with no separation. The alignment marks and their layer names can be seen at each end. Each layer is a different color. The pink layer is a second layer of aluminium acting as an aperture. The openings had to be 100u square exactly. The exact dimensions were required in order to achieve a 30 meter resolution on the ground. A set of four of these were fabricated and the fabrication process documented to NASA requirements and verified the dimensions as part of my employment at Hughes Aircraft Company's Industrial Products Division in Carlsbad California in 1978. The challenge was to customize each of these for one of the narrow visible bands that were required. To do that the thickness of the silicon nitride antireflective layer had to meet a precise target, for example, for one band the target was 120 nm while the next band required 130 nm. In addition all of the 16 photodiodes in the array had to have the same thickness. At the time all that was available to manufacture this film was an atmospheric deposition system that basically burned silane (SiH4) in the presence of ammonia in a horizontal tube heated to about 850 °C. But that process yielded a smoky film that varied significantly over the silicon wafer and the diode array. So with the help of a workmate who had invented low pressure (LPCVD) polysilicon deposition at Motorola a few years earlier a low pressure silicon nitride system was built using silane and ammonia to produce the precisely tunable and uniform thicknesses needed at each of the bands. Without that innovation there would have been no TM. The boron diffused photodiodes had to have very low dark current and high photosensitivity in order to meet the imager specifications. The large crosses visible on the pattern were used when the different arrays were aligned together at final assembly in the Thematic Mapper. The final assembly or the TM Focal Plane without filers is shown in the second photo to the right. [2] They flew in Landsat 4 and for 20 years maintained operation. Also shown on the right is a letter of acknowledgement from Hughes Aircraft Company's Industrial Products Division (IPD) on the invention of the LPCVDs silicon nitride system that made the Thematic Mapper diode arrays possible. Additionally a photo shows the Hughes (IPD) Newsletter from August 1978 highlighting the second level of aluminium that formed the light shield and set the aperture locations. It was a necessary requirement to prevent this shield from shorting out the lower layer aluminium leads to the photodiodes through defects and pinholes given that this layer was a large sheet over everything. Added ref 3. Hughes IPD Letter re award for LPCVD nitride invention and TM . Hughes Aircraft Industrial Products Division Newsletter The Thematic Mapper has become a useful tool in the study of albedo and its relationship to global warming and climate change . The TM on the Landsat 5 has proven useful in determining the amount of ice loss on glaciers due to melting.
Toggle the table of contents Multispectral imaging From Wikipedia, the free encyclopedia Capturing image data across multiple electromagnetic spectrum ranges For broader coverage of this topic, see Spectral imaging . "Multispectral analysis" redirects here. Not to be confused with spectral analysis . Video by SDO simultaneously showing sections of the Sun at various wavelengths Multispectral image of part of the Mississippi River obtained by combining three images acquired at different nominal wavelengths (800nm/infrared, 645nm/red, and 525nm/green) by Apollo 9 in 1969 Multispectral image of Bek crater and its ray system on the surface of Mercury , acquired by MESSENGER , combining images at wavelengths of 996, 748, 433 nm.  The bright yellow patches in other parts of the image are hollows . Multispectral imaging captures image data within specific wavelength ranges across the electromagnetic spectrum . The wavelengths may be separated by filters or detected with the use of instruments that are sensitive to particular wavelengths, including light from frequencies beyond the visible light range , i.e. infrared and ultra-violet . It can allow extraction of additional information the human eye fails to capture with its visible receptors for red, green and blue . It was originally developed for military target identification and reconnaissance. Early space-based imaging platforms incorporated multispectral imaging technology [1] to map details of the Earth related to coastal boundaries, vegetation, and landforms. [2] Multispectral imaging has also found use in document and painting analysis. [3] [4] Multispectral imaging measures light in a small number (typically 3 to 15) of spectral bands . Hyperspectral imaging is a special case of spectral imaging where often hundreds of contiguous spectral bands are available. [5] Spectral band usage[ edit ] Further information: False-color For different purposes, different combinations of spectral bands can be used. They are usually represented with red, green, and blue channels. Mapping of bands to colors depends on the purpose of the image and the personal preferences of the analysts. Thermal infrared is often omitted from consideration due to poor spatial resolution, except for special purposes. True-color uses only red, green, and blue channels, mapped to their respective colors. As a plain color photograph, it is good for analyzing man-made objects, and is easy to understand for beginner analysts. Green-red-infrared, where the blue channel is replaced with near infrared, is used for vegetation, which is highly reflective in near IR; it then shows as blue. This combination is often used to detect vegetation and camouflage. Blue-NIR-MIR, where the blue channel uses visible blue, green uses NIR (so vegetation stays green), and MIR is shown as red. Such images allow the water depth, vegetation coverage, soil moisture content, and the presence of fires to be seen, all in a single image. Many other combinations are in use. NIR is often shown as red, causing vegetation-covered areas to appear red. Typical spectral bands[ edit ] The wavelengths are approximate; exact values depend on the particular instruments (e.g. characteristics of satellite's sensors for Earth observation, characteristics of illumination and sensors for document analysis): Blue, 450–515/520 nm, is used for atmosphere and deep water imaging, and can reach depths up to 150 feet (50 m) in clear water. Green, 515/520–590/600 nm, is used for imaging vegetation and deep water structures, up to 90 feet (30 m) in clear water. Red, 600/630–680/690 nm, is used for imaging man-made objects, in water up to 30 feet (9 m) deep, soil, and vegetation. Near infrared (NIR), 750–900 nm, is used primarily for imaging vegetation. Mid-infrared (MIR), 1550–1750 nm, is used for imaging vegetation, soil moisture content, and some forest fires . Far-infrared (FIR), 2080–2350 nm, is used for imaging soil, moisture, geological features, silicates, clays, and fires. Thermal infrared , 10,400–12,500 nm, uses emitted instead of reflected radiation to image geological structures, thermal differences in water currents, fires, and for night studies. Radar and related technologies are useful for mapping terrain and for detecting various objects. Classification[ edit ] Unlike other aerial photographic and satellite image interpretation work, these multispectral images do not make it easy to identify directly the feature type by visual inspection. Hence the remote sensing data has to be classified first, followed by processing by various data enhancement techniques so as to help the user to understand the features that are present in the image. Such classification is a complex task which involves rigorous validation of the training samples depending on the classification algorithm used. The techniques can be grouped mainly into two types. Supervised classification techniques Unsupervised classification techniques Supervised classification makes use of training samples. Training samples are areas on the ground for which there is ground truth , that is, what is there is known. The spectral signatures of the training areas are used to search for similar signatures in the remaining pixels of the image, and we will classify accordingly.  This use of training samples for classification is called supervised classification.  Expert knowledge is very important in this method since the selection of the training samples and a biased selection can badly affect the accuracy of classification.  Popular techniques include the maximum likelihood principle and convolutional neural network .  The Maximum likelihood principle calculates the probability of a pixel belonging to a class (i.e. feature) and allots the pixel to its most probable class. Newer convolutional neural network based methods [6] account for both spatial proximity and entire spectra to determine the most likely class. In case of unsupervised classification no prior knowledge is required for classifying the features of the image. The natural clustering or grouping of the pixel values, i.e. the gray levels of the pixels, are observed. Then a threshold is defined for adopting the number of classes in the image. The finer the threshold value, the more classes there will be. However, beyond a certain limit the same class will be represented in different classes in the sense that variation in the class is represented. After forming the clusters, ground truth validation is done to identify the class the image pixel belongs to. Thus in this unsupervised classification apriori information about the classes is not required.  One of the popular methods in unsupervised classification is k-means clustering . Data analysis software[ edit ] MicroMSI is endorsed by the NGA . Opticks is an open-source remote sensing application. Multispec is freeware multispectral analysis software. [7] Gerbil is open source multispectral visualization and analysis software. [8] See also: Hyperspectral imaging § Applications Military target tracking[ edit ] Multispectral imaging measures light emission and is often used in detecting or tracking military targets. In 2003, researchers at the United States Army Research Laboratory and the Federal Laboratory Collaborative Technology Alliance reported a dual band multispectral imaging focal plane array (FPA). This FPA allowed researchers to look at two infrared (IR) planes at the same time. [9] Because mid-wave infrared (MWIR) and long wave infrared (LWIR) technologies measure radiation inherent to the object and require no external light source, they also are referred to as thermal imaging methods. The brightness of the image produced by a thermal imager depends on the objects emissivity and temperature. [10] Every material has an infrared signature that aids in the identification of the object. [11] These signatures are less pronounced in hyperspectral systems (which image in many more bands than multispectral systems) and when exposed to wind and, more dramatically, to rain. [11] Sometimes the surface of the target may reflect infrared energy. This reflection may misconstrue the true reading of the objects’ inherent radiation. [12] Imaging systems that use MWIR technology function better with solar reflections on the target's surface and produce more definitive images of hot objects, such as engines, compared to LWIR technology. [13] However, LWIR operates better in hazy environments like smoke or fog because less scattering occurs in the longer wavelengths. [10] Researchers claim that dual-band technologies combine these advantages to provide more information from an image, particularly in the realm of target tracking. [9] For nighttime target detection, thermal imaging outperformed single-band multispectral imaging. Dual band MWIR and LWIR technology resulted in better visualization during the nighttime than MWIR alone. Citation Citation. The US Army reports that its dual band LWIR/MWIR FPA demonstrated better visualizing of tactical vehicles than MWIR alone after tracking them through both day and night.[ citation needed ] Land mine detection[ edit ] By analyzing the emissivity of ground surfaces, multispectral imaging can detect the presence of underground missiles. Surface and sub-surface soil possess different physical and chemical properties that appear in spectral analysis. [11] Disturbed soil has increased emissivity in the wavelength range of 8.5 to 9.5 micrometers while demonstrating no change in wavelengths greater than 10 micrometers. [9] The US Army Research Laboratory's dual MWIR/LWIR FPA used "red" and "blue" detectors to search for areas with enhanced emissivity. The red detector acts as a backdrop, verifying realms of undisturbed soil areas, as it is sensitive to the 10.4 micrometer wavelength. The blue detector is sensitive to wavelengths of 9.3 micrometers. If the intensity of the blue image changes when scanning, that region is likely disturbed. The scientists reported that fusing these two images increased detection capabilities. [9] Ballistic missile detection[ edit ] Intercepting an intercontinental ballistic missile (ICBM) in its boost phase requires imaging of the hard body as well as the rocket plumes. MWIR presents a strong signal from highly heated objects including rocket plumes, while LWIR produces emissions from the missile's body material. The US Army Research Laboratory reported that with their dual-band MWIR/LWIR technology, tracking of the Atlas 5 Evolved Expendable Launch Vehicles, similar in design to ICBMs, picked up both the missile body and plumage. [9] Space-based imaging[ edit ] Most radiometers for remote sensing (RS) acquire multispectral images.  Dividing the spectrum into many bands, multispectral is the opposite of panchromatic , which records only the total intensity of radiation falling on each pixel . [14] Usually, Earth observation satellites have three or more radiometers . Each acquires one digital image (in remote sensing, called a 'scene') in a small spectral band. The bands are grouped into wavelength regions based on the origin of the light and the interests of the researchers. Weather forecasting[ edit ] Modern weather satellites produce imagery in a variety of spectra. [15] Multispectral imaging combines two to five spectral imaging bands of relatively large bandwidth into a single optical system. A multispectral system usually provides a combination of visible (0.4 to 0.7 µm), near infrared (NIR; 0.7 to 1 µm), short-wave infrared (SWIR; 1 to 1.7 µm), mid-wave infrared (MWIR; 3.5 to 5 µm) or long-wave infrared (LWIR; 8 to 12 µm) bands into a single system. — Valerie C. Coffey [16] In the case of Landsat satellites, several different band designations have been used, with as many as 11 bands ( Landsat 8 ) comprising a multispectral image. [17] [18] [19] Spectral imaging with a higher radiometric resolution (involving hundreds or thousands of bands), finer spectral resolution (involving smaller bands), or wider spectral coverage may be called hyperspectral or ultraspectral. [19] Documents and artworks[ edit ] Multispectral imaging can be employed for investigation of paintings and other works of art. [3] The painting is irradiated by ultraviolet , visible and infrared rays and the reflected radiation is recorded in a camera sensitive in this region of the spectrum. The image can also be registered using the transmitted instead of reflected radiation. In special cases the painting can be irradiated by UV , VIS or IR rays and the fluorescence of pigments or varnishes can be registered. [20] Multispectral analysis has assisted in the interpretation of ancient papyri , such as those found at Herculaneum , by imaging the fragments in the infrared range (1000 nm). Often, the text on the documents appears to the naked eye as black ink on black paper. At 1000 nm, the difference in how paper and ink reflect infrared light makes the text clearly readable. It has also been used to image the Archimedes palimpsest by imaging the parchment leaves in bandwidths from 365–870 nm, and then using advanced digital image processing techniques to reveal the undertext with Archimedes' work. [21] Multispectral imaging has been used in a Mellon Foundation project at Yale University to compare inks in medieval English manuscripts. [4] Multispectral imaging has also been used to examine discolorations and stains on old books and manuscripts.  Comparing the "spectral fingerprint" of a stain to the characteristics of known chemical substances can make it possible to identify the stain. This technique has been used to examine medical and alchemical texts, seeking hints about the activities of early chemists and the possible chemical substances they may have used in their experiments. Like a cook spilling flour or vinegar on a cookbook, an early chemist might have left tangible evidence on the pages of the ingredients used to make medicines. [22]
Toggle the table of contents Hyperspectral imaging From Wikipedia, the free encyclopedia Multi-wavelength imaging method For broader coverage of this topic, see Spectral imaging . Two-dimensional projection of a hyperspectral cube Hyperspectral imaging collects and processes information from across the electromagnetic spectrum . [1] The goal of hyperspectral imaging is to obtain the spectrum for each pixel in the image of a scene, with the purpose of finding objects, identifying materials, or detecting processes. [2] [3] There are three general types of spectral imagers. There are push broom scanners and the related whisk broom scanners (spatial scanning), which read images over time, band sequential scanners (spectral scanning), which acquire images of an area at different wavelengths, and snapshot hyperspectral imagers , which uses a staring array to generate an image in an instant. Whereas the human eye sees color of visible light in mostly three bands (long wavelengths, perceived as red; medium wavelengths, perceived as green; and short wavelengths, perceived as blue), spectral imaging divides the spectrum into many more bands. This technique of dividing images into bands can be extended beyond the visible. In hyperspectral imaging, the recorded spectra have fine wavelength resolution and cover a wide range of wavelengths. Hyperspectral imaging measures continuous spectral bands, as opposed to multiband imaging which measures spaced spectral bands. [4] Engineers build hyperspectral sensors and processing systems for applications in astronomy, agriculture, molecular biology, biomedical imaging, geosciences, physics, and surveillance. Hyperspectral sensors look at objects using a vast portion of the electromagnetic spectrum. Certain objects leave unique 'fingerprints' in the electromagnetic spectrum. Known as spectral signatures, these 'fingerprints' enable identification of the materials that make up a scanned object. For example, a spectral signature for oil helps geologists find new oil fields . [5] Sensors[ edit ] Figuratively speaking, hyperspectral sensors collect information as a set of 'images'. Each image represents a narrow wavelength range of the electromagnetic spectrum, also known as a spectral band. These 'images' are combined to form a three-dimensional (x,y,λ) hyperspectral data cube for processing and analysis, where x and y represent two spatial dimensions of the scene, and λ represents the spectral dimension (comprising a range of wavelengths). [6] Technically speaking, there are four ways for sensors to sample the hyperspectral cube: Spatial scanning, spectral scanning, snapshot imaging, [5] [7] and spatio-spectral scanning. [8] Hyperspectral cubes are generated from airborne sensors like NASA's Airborne Visible/Infrared Imaging Spectrometer (AVIRIS), or from satellites like NASA's EO-1 with its hyperspectral instrument Hyperion. [9] [10] However, for many development and validation studies, handheld sensors are used. [11] The precision of these sensors is typically measured in spectral resolution, which is the width of each band of the spectrum that is captured. If the scanner detects a large number of fairly narrow frequency bands, it is possible to identify objects even if they are only captured in a handful of pixels. However, spatial resolution is a factor in addition to spectral resolution. If the pixels are too large, then multiple objects are captured in the same pixel and become difficult to identify. If the pixels are too small, then the intensity captured by each sensor cell is low, and the decreased signal-to-noise ratio reduces the reliability of measured features. The acquisition and processing of hyperspectral images is also referred to as imaging spectroscopy or, with reference to the hyperspectral cube, as 3D spectroscopy. Scanning techniques[ edit ] Photos illustrating individual sensor outputs for the four hyperspectral imaging techniques. From left to right: Slit spectrum; monochromatic spatial map; 'perspective projection' of hyperspectral cube; wavelength-coded spatial map. There are four basic techniques for acquiring the three-dimensional (x, y, λ) dataset of a hyperspectral cube. The choice of technique depends on the specific application, seeing that each technique has context-dependent advantages and disadvantages. Spatial scanning[ edit ] Acquisition techniques for hyperspectral imaging, visualized as sections of the hyperspectral datacube with its two spatial dimensions (x,y) and one spectral dimension (lambda). In spatial scanning, each two-dimensional (2-D) sensor output represents a full slit spectrum (x, λ). Hyperspectral imaging (HSI) devices for spatial scanning obtain slit spectra by projecting a strip of the scene onto a slit and dispersing the slit image with a prism or a grating. These systems have the drawback of having the image analyzed per lines (with a push broom scanner ) and also having some mechanical parts integrated into the optical train. With these line-scan cameras , the spatial dimension is collected through platform movement or scanning. This requires stabilized mounts or accurate pointing information to 'reconstruct' the image. Nonetheless, line-scan systems are particularly common in remote sensing , where it is sensible to use mobile platforms. Line-scan systems are also used to scan materials moving by on a conveyor belt. A special case of line scanning is point scanning (with a whisk broom scanner ), where a point-like aperture is used instead of a slit, and the sensor is essentially one-dimensional instead of 2-D. [7] [12] Spectral scanning[ edit ] In spectral scanning, each 2-D sensor output represents a monochromatic ('single-colored'), spatial (x, y) map of the scene. HSI devices for spectral scanning are typically based on optical band-pass filters (either tunable or fixed). The scene is spectrally scanned by exchanging one filter after another while the platform remains stationary. In such 'staring', wavelength scanning systems, spectral smearing can occur if there is movement within the scene, invalidating spectral correlation/detection. Nonetheless, there is the advantage of being able to pick and choose spectral bands, and having a direct representation of the two spatial dimensions of the scene. [6] [7] [12] If the imaging system is used on a moving platform, such as an airplane, acquired images at different wavelengths corresponds to different areas of the scene. The spatial features on each of the images may be used to realign the pixels. Main article: Snapshot hyperspectral imaging In non-scanning, a single 2-D sensor output contains all spatial (x, y) and spectral (λ) data. HSI devices for non-scanning yield the full datacube at once, without any scanning. Figuratively speaking, a single snapshot represents a perspective projection of the datacube, from which its three-dimensional structure can be reconstructed. [7] [13] The most prominent benefits of these snapshot hyperspectral imaging systems are the snapshot advantage (higher light throughput) and shorter acquisition time. A number of systems have been designed, including computed tomographic imaging spectrometry (CTIS), fiber-reformatting imaging spectrometry (FRIS), integral field spectroscopy with lenslet arrays (IFS-L), multi-aperture integral field spectrometer (Hyperpixel Array), integral field spectroscopy with image slicing mirrors (IFS-S), image-replicating imaging spectrometry (IRIS), filter stack spectral decomposition (FSSD), coded aperture snapshot spectral imaging (CASSI), image mapping spectrometry (IMS), and multispectral Sagnac interferometry (MSI). [14] However, computational effort and manufacturing costs are high. In an effort to reduce the computational demands and potentially the high cost of non-scanning hyperspectral instrumentation, prototype devices based on Multivariate Optical Computing have been demonstrated. These devices have been based on the Multivariate Optical Element [15] [16] spectral calculation engine or the Spatial Light Modulator [17] spectral calculation engine. In these platforms, chemical information is calculated in the optical domain prior to imaging such that the chemical image relies on conventional camera systems with no further computing. As a disadvantage of these systems, no spectral information is ever acquired, i.e. only the chemical information, such that post processing or reanalysis is not possible. Main article: Spatiospectral scanning In spatiospectral scanning, each 2-D sensor output represents a wavelength-coded ('rainbow-colored', λ = λ(y)), spatial (x, y) map of the scene. A prototype for this technique, introduced in 2014, consists of a camera at some non-zero distance behind a basic slit spectroscope (slit + dispersive element). [8] [18] Advanced spatiospectral scanning systems can be obtained by placing a dispersive element before a spatial scanning system. Scanning can be achieved by moving the whole system relative to the scene, by moving the camera alone, or by moving the slit alone. Spatiospectral scanning unites some advantages of spatial and spectral scanning, thereby alleviating some of their disadvantages. [8] Distinguishing hyperspectral from multispectral imaging[ edit ] Multispectral and hyperspectral differences Hyperspectral imaging is part of a class of techniques commonly referred to as spectral imaging or spectral analysis . The term “hyperspectral imaging” derives from the development of NASA's Airborne Imaging Spectrometer (AIS) and AVIRIS in the mid-1980s. Although NASA prefers the earlier term “imaging spectroscopy” over “hyperspectral imaging,” use of the latter term has become more prevalent in scientific and non-scientific language. In a peer reviewed letter, experts recommend using the terms “imaging spectroscopy” or “spectral imaging” and avoiding exaggerated prefixes such as “hyper-,” “super-” and "ultra-,” to prevent misnomers in discussion. [19] Hyperspectral imaging is related to multispectral imaging . The distinction between hyper- and multi-band is sometimes based incorrectly on an arbitrary "number of bands" or on the type of measurement. Hyperspectral imaging (HSI) uses continuous and contiguous ranges of wavelengths (e.g. 400 - 1100 nm in steps of 1 nm) whilst multiband imaging (MSI) uses a subset of targeted wavelengths at chosen locations (e.g. 400 - 1100 nm in steps of 20 nm). [20] Multiband imaging deals with several images at discrete and somewhat narrow bands. Being "discrete and somewhat narrow" is what distinguishes multispectral imaging in the visible wavelength from color photography . A multispectral sensor may have many bands covering the spectrum from the visible to the longwave infrared. Multispectral images do not produce the "spectrum" of an object. Landsat is an excellent example of multispectral imaging. Hyperspectral deals with imaging narrow spectral bands over a continuous spectral range, producing the spectra of all pixels in the scene. A sensor with only 20 bands can also be hyperspectral when it covers the range from 500 to 700 nm with 20 bands each 10 nm wide. (While a sensor with 20 discrete bands covering the visible, near, short wave, medium wave and long wave infrared would be considered multispectral.) Ultraspectral could be reserved for interferometer type imaging sensors with a very fine spectral resolution. These sensors often have (but not necessarily) a low spatial resolution of several pixels only, a restriction imposed by the high data rate. See also: Multispectral imaging § Applications Hyperspectral remote sensing is used in a wide array of applications. Although originally developed for mining and geology (the ability of hyperspectral imaging to identify various minerals makes it ideal for the mining and oil industries, where it can be used to look for ore and oil), [11] [21] it has now spread into fields as widespread as ecology and surveillance, as well as historical manuscript research, such as the imaging of the Archimedes Palimpsest . This technology is continually becoming more available to the public. Organizations such as NASA and the USGS have catalogues of various minerals and their spectral signatures, and have posted them online to make them readily available for researchers. On a smaller scale, NIR hyperspectral imaging can be used to rapidly monitor the application of pesticides to individual seeds for quality control of the optimum dose and homogeneous coverage. Agriculture[ edit ] Hyperspectral camera embedded on OnyxStar HYDRA-12 UAV from AltiGator Although the cost of acquiring hyperspectral images is typically high for specific crops and in specific climates, hyperspectral remote sensing use is increasing for monitoring the development and health of crops. In Australia , work is under way to use imaging spectrometers to detect grape variety and develop an early warning system for disease outbreaks. [22] Furthermore, work is under way to use hyperspectral data to detect the chemical composition of plants, [23] which can be used to detect the nutrient and water status of wheat in irrigated systems. [24] On a smaller scale, NIR hyperspectral imaging can be used to rapidly monitor the application of pesticides to individual seeds for quality control of the optimum dose and homogeneous coverage. [25] Another application in agriculture is the detection of animal proteins in compound feeds to avoid bovine spongiform encephalopathy (BSE) , also known as mad-cow disease. Different studies have been done to propose alternative tools to the reference method of detection, (classical microscopy ). One of the first alternatives is near infrared microscopy (NIR), which combines the advantages of microscopy and NIR. In 2004, the first study relating this problem with hyperspectral imaging was published. [26] Hyperspectral libraries that are representative of the diversity of ingredients usually present in the preparation of compound feeds were constructed. These libraries can be used together with chemometric tools to investigate the limit of detection, specificity and reproducibility of the NIR hyperspectral imaging method for the detection and quantification of animal ingredients in feed. HSI cameras can also be used to detect stress from heavy metals in plants and become an earlier and faster alternative to post-harvest wet chemical methods. [27] [28] Waste sorting and recycling[ edit ] Hyperspectral imaging can provide information about the chemical constituents of materials which makes it useful for waste sorting and recycling . [29] It has been applied to distinguish between substances with different fabrics and to identify natural, animal and synthetic fibers. [30] HSI cameras can be integrated with machine vision systems and, via simplifying platforms, allow end-customers to create new waste sorting applications and other sorting/identification applications. [31] A system of machine learning and hyperspectral camera can distinguish between 12 different types of plastics such as PET and PP for automated separation of waste of, as of 2020, highly unstandardized [32] [ additional citation(s) needed ] plastics products and packaging . [33] [34] Eye care[ edit ] Researchers at the Université de Montréal are working with Photon etc. and Optina Diagnostics [35] to test the use of hyperspectral photography in the diagnosis of retinopathy and macular edema before damage to the eye occurs.  The metabolic hyperspectral camera will detect a drop in oxygen consumption in the retina, which indicates potential disease. An ophthalmologist will then be able to treat the retina with injections to prevent any potential damage. [36] Food processing[ edit ] A line scan push-broom system was used to scan the cheeses and images were acquired using a Hg-Cd-Te array (386x288) equipped linescan camera with halogen light as a radiation source. In the food processing industry, hyperspectral imaging, combined with intelligent software, enables digital sorters (also called optical sorters ) to identify and remove defects and foreign material (FM) that are invisible to traditional camera and laser sorters. [37] [38] By improving the accuracy of defect and FM removal, the food processor’s objective is to enhance product quality and increase yields. Adopting hyperspectral imaging on digital sorters achieves non-destructive, 100 percent inspection in-line at full production volumes. The sorter’s software compares the hyperspectral images collected to user-defined accept/reject thresholds, and the ejection system automatically removes defects and foreign material. Hyperspectral image of "sugar end" potato strips shows invisible defects The recent commercial adoption of hyperspectral sensor-based food sorters is most advanced in the nut industry where installed systems maximize the removal of stones, shells and other foreign material (FM) and extraneous vegetable matter (EVM) from walnuts, pecans, almonds, pistachios, peanuts and other nuts. Here, improved product quality, low false reject rates and the ability to handle high incoming defect loads often justify the cost of the technology. Commercial adoption of hyperspectral sorters is also advancing at a fast pace in the potato processing industry where the technology promises to solve a number of outstanding product quality problems. Work is under way to use hyperspectral imaging to detect “sugar ends,” [39] “hollow heart” [40] and “common scab,” [41] conditions that plague potato processors. Mineralogy[ edit ] A set of stones is scanned with a Specim LWIR-C imager in the thermal infrared range from 7.7 μm to 12.4 μm. The quartz and feldspar spectra are clearly recognizable. [42] Geological samples, such as drill cores , can be rapidly mapped for nearly all minerals of commercial interest with hyperspectral imaging. Fusion of SWIR and LWIR spectral imaging is standard for the detection of minerals in the feldspar , silica , calcite , garnet , and olivine groups, as these minerals have their most distinctive and strongest spectral signature in the LWIR regions. [42] Hyperspectral remote sensing of minerals is well developed. Many minerals can be identified from airborne images, and their relation to the presence of valuable minerals, such as gold and diamonds, is well understood. Currently, progress is towards understanding the relationship between oil and gas leakages from pipelines and natural wells, and their effects on the vegetation and the spectral signatures. Recent work includes the PhD dissertations of Werff [43] and Noomen. [44] Surveillance[ edit ] Hyperspectral thermal infrared emission measurement, an outdoor scan in winter conditions, ambient temperature -15°C—relative radiance spectra from various targets in the image are shown with arrows. The infrared spectra of the different objects such as the watch glass have clearly distinctive characteristics. The contrast level indicates the temperature of the object. This image was produced with a Specim LWIR hyperspectral imager. [42] Hyperspectral surveillance is the implementation of hyperspectral scanning technology for surveillance purposes. Hyperspectral imaging is particularly useful in military surveillance because of countermeasures that military entities now take to avoid airborne surveillance. The idea that drives hyperspectral surveillance is that hyperspectral scanning draws information from such a large portion of the light spectrum that any given object should have a unique spectral signature in at least a few of the many bands that are scanned. Hyperspectral imaging has also shown potential to be used in facial recognition purposes. Facial recognition algorithms using hyperspectral imaging have been shown to perform better than algorithms using traditional imaging. [45] Traditionally, commercially available thermal infrared hyperspectral imaging systems have needed liquid nitrogen or helium cooling, which has made them impractical for most surveillance applications. In 2010, Specim introduced a thermal infrared hyperspectral camera that can be used for outdoor surveillance and UAV applications without an external light source such as the sun or the moon. [46] [47] Astronomy[ edit ] In astronomy, hyperspectral imaging is used to determine a spatially-resolved spectral image. Since a spectrum is an important diagnostic, having a spectrum for each pixel allows more science cases to be addressed. In astronomy, this technique is commonly referred to as integral field spectroscopy , and examples of this technique include FLAMES [48] and SINFONI [49] on the Very Large Telescope , but also the Advanced CCD Imaging Spectrometer on Chandra X-ray Observatory uses this technique. Remote chemical imaging of a simultaneous release of SF6 and NH3 at 1.5 km using the Telops Hyper-Cam imaging spectrometer [50] Main article: Chemical imaging Soldiers can be exposed to a wide variety of chemical hazards. These threats are mostly invisible but detectable by hyperspectral imaging technology. The Telops Hyper-Cam, introduced in 2005, has demonstrated this at distances up to 5 km. [51] Environment[ edit ] Top panel: Contour map of the time-averaged spectral radiance at 2078 cm−1 corresponding to a CO2 emission line. Bottom panel: Contour map of the spectral radiance at 2580 cm−1 corresponding to continuum emission from particulates in the plume. The translucent gray rectangle indicates the position of the stack. The horizontal line at row 12 between columns 64-128 indicate the pixels used to estimate the background spectrum. Measurements made with the Telops Hyper-Cam. [52] Most countries require continuous monitoring of emissions produced by coal and oil-fired power plants, municipal and hazardous waste incinerators, cement plants, as well as many other types of industrial sources. This monitoring is usually performed using extractive sampling systems coupled with infrared spectroscopy techniques. Some recent standoff measurements performed allowed the evaluation of the air quality but not many remote independent methods allow for low uncertainty measurements. Civil engineering[ edit ] Recent research indicates that hyperspectral imaging may be useful to detect the development of cracks in pavements [53] which are hard to detect from images taken with visible spectrum cameras. [53] Hyperspectral imaging has also been used to detect cancer, identify nerves and analyze bruises. [54] Advantages and disadvantages[ edit ] The primary advantage to hyperspectral imaging is that, because an entire spectrum is acquired at each point, the operator needs no prior knowledge of the sample, and postprocessing allows all available information from the dataset to be mined. Hyperspectral imaging can also take advantage of the spatial relationships among the different spectra in a neighbourhood, allowing more elaborate spectral-spatial models for a more accurate segmentation and classification of the image. [55] [56] The primary disadvantages are cost and complexity. Fast computers, sensitive detectors, and large data storage capacities are needed for analyzing hyperspectral data. Significant data storage capacity is necessary since uncompressed hyperspectral cubes are large, multidimensional datasets, potentially exceeding hundreds of megabytes . All of these factors greatly increase the cost of acquiring and processing hyperspectral data. Also, one of the hurdles researchers have had to face is finding ways to program hyperspectral satellites to sort through data on their own and transmit only the most important images, as both transmission and storage of that much data could prove difficult and costly. [9] As a relatively new analytical technique, the full potential of hyperspectral imaging has not yet been realized.
From Wikipedia, the free encyclopedia European organization dedicated to space exploration "ESA" redirects here. For other uses, see ESA (disambiguation) . European Space Agency Irish : Gníomhaireacht Spáis na hEorpa Italian : Agenzia Spaziale Europea Main control room of the European Space Operations Centre in Darmstadt , Germany Agency overview 30 May 1975; 48 years ago (1975-05-30) Type English, French and German (working languages) [1] [2] Administrator www.esa.int The European Space Agency (ESA) [a] is a 22-member intergovernmental body devoted to space exploration . [7] With its headquarters in Paris and a staff of around 2,200 people globally as of 2022, ESA was founded in 1975. Its 2024 annual budget was €7.8 billion. [8] [4] ESA's space flight programme includes human spaceflight (mainly through participation in the International Space Station program); the launch and operation of crewless exploration missions to other planets (such as Mars ) and the Moon; Earth observation, science and telecommunication; designing launch vehicles ; and maintaining a major spaceport , the Guiana Space Centre at Kourou ( French Guiana ), France. The main European launch vehicle Ariane 6 will be operated through Arianespace with ESA sharing in the costs of launching and further developing this launch vehicle. The agency is also working with NASA to manufacture the Orion spacecraft service module that flies on the Space Launch System . [9] [10] History[ edit ] This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. See also: European Space Research Organisation and European Launcher Development Organisation ESTEC buildings in Noordwijk , Netherlands. ESTEC was the main technical centre of ESRO and remains so for the successor organisation (ESA). After World War II , many European scientists left Western Europe in order to work with the United States. Although the 1950s boom made it possible for Western European countries to invest in research and specifically in space-related activities, Western European scientists realised solely national projects would not be able to compete with the two main superpowers. In 1958, only months after the Sputnik shock , Edoardo Amaldi (Italy) and Pierre Auger (France), two prominent members of the Western European scientific community, met to discuss the foundation of a common Western European space agency. The meeting was attended by scientific representatives from eight countries. The Western European nations decided to have two agencies: one concerned with developing a launch system, ELDO (European Launcher Development Organisation), and the other the precursor of the European Space Agency, ESRO (European Space Research Organisation). The latter was established on 20 March 1964 by an agreement signed on 14 June 1962. From 1968 to 1972, ESRO launched seven research satellites, but ELDO was not able to deliver a launch vehicle. Both agencies struggled with the underfunding and diverging interests of their participants. ESA in its current form was founded with the ESA Convention in 1975, when ESRO was merged with ELDO. ESA had ten founding member states: Belgium , Denmark , France , West Germany , Italy , the Netherlands , Spain , Sweden , Switzerland , and the United Kingdom . [11] These signed the ESA Convention in 1975 and deposited the instruments of ratification by 1980, when the convention came into force. During this interval the agency functioned in a de facto fashion. ESA launched its first major scientific mission in 1975, Cos-B , a space probe monitoring gamma-ray emissions in the universe, which was first worked on by ESRO. Mock-up of the Ariane 1 ESA collaborated with NASA on the International Ultraviolet Explorer (IUE), the world's first high-orbit telescope, which was launched in 1978 and operated successfully for 18 years. A number of successful Earth-orbit projects followed, and in 1986 ESA began Giotto , its first deep-space mission, to study the comets Halley and Grigg–Skjellerup . Hipparcos , a star-mapping mission, was launched in 1989 and in the 1990s SOHO , Ulysses and the Hubble Space Telescope were all jointly carried out with NASA. Later scientific missions in cooperation with NASA include the Cassini–Huygens space probe, to which ESA contributed by building the Titan landing module Huygens . As the successor of ELDO , ESA has also constructed rockets for scientific and commercial payloads. Ariane 1 , launched in 1979, carried mostly commercial payloads into orbit from 1984 onward. The next two versions of the Ariane rocket were intermediate stages in the development of a more advanced launch system, the Ariane 4 , which operated between 1988 and 2003 and established ESA as the world leader [12] in commercial space launches in the 1990s. Although the succeeding Ariane 5 experienced a failure on its first flight, it has since firmly established itself within the heavily competitive commercial space launch market with 112 successful launches until 2021. The successor launch vehicle, the Ariane 6 , is under development and had a successful long-firing engine test in November 2023. The ESA plans for the Ariane 6 to launch in June or July 2024. [13] [14] The beginning of the new millennium saw ESA become, along with agencies like NASA, JAXA , ISRO , the CSA and Roscosmos , one of the major participants in scientific space research . Although ESA had relied on co-operation with NASA in previous decades, especially the 1990s, changed circumstances (such as tough legal restrictions on information sharing by the United States military ) led to decisions to rely more on itself and on co-operation with Russia. A 2011 press issue thus stated: [15] Russia is ESA's first partner in its efforts to ensure long-term access to space. There is a framework agreement between ESA and the government of the Russian Federation on cooperation and partnership in the exploration and use of outer space for peaceful purposes, and cooperation is already underway in two different areas of launcher activity that will bring benefits to both partners. Notable ESA programmes include SMART-1 , [16] a probe testing cutting-edge space propulsion technology, the Mars Express and Venus Express missions, [17] [18] as well as the development of the Ariane 5 rocket and its role in the ISS partnership. ESA maintains its scientific and research projects mainly for astronomy-space missions such as Corot , launched on 27 December 2006, [19] a milestone in the search for exoplanets . On 21 January 2019, ArianeGroup and Arianespace announced a one-year contract with ESA to study and prepare for a mission to mine the Moon for lunar regolith . [20] In 2021 the ESA ministerial council agreed to the " Matosinhos manifesto" which set three priority areas (referred to as accelerators) "space for a green future, a rapid and resilient crisis response, and the protection of space assets", and two further high visibility projects (referred to as inspirators) an icy moon sample return mission; and human space exploration. [21] [22] In the same year the recruitment process began for the 2022 European Space Agency Astronaut Group . [23] 1 July 2023 saw the launch of the Euclid spacecraft , developed jointly with the Euclid Consortium, after 10 years of planning and building it is designed to better understand dark energy and dark matter by accurately measuring the accelerating expansion of the universe . [24] Facilities[ edit ] The agency's facilities date back to ESRO and are deliberately distributed among various countries and areas. The most important are the following centres: ESA headquarter is in Paris, France ESA science missions are based at ESTEC in Noordwijk , Netherlands; Earth Observation missions at ESA Centre for Earth Observation in Frascati , Italy; ESA Mission Control ( ESOC ) is in Darmstadt , Germany; the European Astronaut Centre (EAC) that trains astronauts for future missions is situated in Cologne , Germany; the European Space Security and Education Centre (ESEC), located in Redu, Belgium; the ESTRACK tracking and deep space communication network. Many other facilities are operated by national space agencies in close collaboration with ESA. Mission[ edit ] The treaty establishing the European Space Agency reads: [25] The purpose of the Agency shall be to provide for and to promote, for exclusively peaceful purposes, cooperation among European States in space research and technology and their space applications, with a view to their being used for scientific purposes and for operational space applications systems… ESA is responsible for setting a unified space and related industrial policy, recommending space objectives to the member states, and integrating national programs like satellite development, into the European program as much as possible. [25] Jean-Jacques Dordain – ESA's Director General (2003–2015) – outlined the European Space Agency's mission in a 2003 interview: [26] Today space activities have pursued the benefit of citizens, and citizens are asking for a better quality of life on Earth. They want greater security and economic wealth, but they also want to pursue their dreams, to increase their knowledge, and they want younger people to be attracted to the pursuit of science and technology. I think that space can do all of this: it can produce a higher quality of life, better security, more economic wealth, and also fulfill our citizens' dreams and thirst for knowledge, and attract the young generation. This is the reason space exploration is an integral part of overall space activities. It has always been so, and it will be even more important in the future. Activities and programmes[ edit ] ESA describes its work in two overlapping ways: For the general public, the various fields of work are described as "Activities". Budgets are organised as "Programmes". These are either mandatory or optional. According to the ESA website, the activities are: Observing the Earth Mandatory[ edit ] Every member country (known as 'Member States') must contribute to these programmes: [31] The European Space Agency Science Programme is a long-term programme of space science missions. Technology Development Element Programme [32] Science Core Technology Programme Optional[ edit ] Depending on their individual choices the countries can contribute to the following programmes, becoming 'Participating States', listed according to: [33] Launchers Employment[ edit ] As of 2023, ESA employs around 2200 people, and thousands of contractors. Initially, new employees are contracted for a expandable four-year term, which is until the organization's retirement age of 63. According to ESA's documents, the staff can receive myriad of perks, such as financial childcare support, retirement plans, and financial help when migrating. ESA also allows employees prevent any private documents or correspondences from disclosure to outside parties. Ars Technica 's 2023 report, which contained testimonies of 18 people, suggested that there is a widespread harassment between management and its employees, especially with its contractors. Since ESA is an international organization, unaffiliated with any single nation, any form of legal action is difficult to raise against the organization. [34] Member states, funding and budget[ edit ] Membership and contribution to ESA[ edit ] ESA member states ESA ECS states ESA Cooperation Agreement states By 2015, ESA was an intergovernmental organisation of 22 member states. [7] Member states participate to varying degrees in the mandatory (25% of total expenditures in 2008) and optional space programmes (75% of total expenditures in 2008). [35] The 2008 budget amounted to €3.0 billion whilst the 2009 budget amounted to €3.6 billion. [36] The total budget amounted to about €3.7 billion in 2010, €3.99 billion in 2011, €4.02 billion in 2012, €4.28 billion in 2013, €4.10 billion in 2014 and €4.33 billion in 2015. [37] English and French are the two official languages within ESA. Additionally, official documents are also provided in German and documents regarding the Spacelab are also provided in Italian. If found appropriate, the agency may conduct its correspondence in any language of a member state. The following table lists all the member states and adjunct members, their ESA convention ratification dates, and their contributions in 2022: [38] Member state, or source 7,150.0 100% ^ a b c These nations are considered initial signatories, but since they were members of neither ESRO nor ELDO (the precursor organisations to ESA) the Convention could only enter into force when the last of the other 10 founders ratified it. ^ a b c d e f g h i Acceded members became ESA member states upon signing an accession agreement. [41] [42] [43] [44] [45] [46] [47] ^ Canada is a Cooperating State of ESA. [48] [49] ^ Framework Agreement establishing the legal basis for cooperation between ESA and the European Union came into force in May 2004. Non-full member states[ edit ] Previously associated members were Austria, Norway and Finland, all of which later joined ESA as full members. As of November 8, 2023 there are five associate members: Slovenia, Latvia, Lithuania, Slovakia and Canada. The first four members have shown interest in full membership and may eventually apply within the next years. Slovenia[ edit ] Since 2016, Slovenia has been an associated member of the ESA. [53] In November 2023 Slovenia formally applied for full membership, and it is expected that the final decision will be made by the ESA Council in 2024. [55] Latvia[ edit ] Latvia became the second current associated member on 30 June 2020, when the Association Agreement was signed by ESA Director Jan Wörner and the Minister of Education and Science of Latvia , Ilga Šuplinska in Riga . The Saeima ratified it on 27 July. [50] Lithuania[ edit ] In May 2021, Lithuania became the third current associated member. [56] As a consequence its citizens became eligible to apply to the 2022 ESA Astronaut group , applications for which were scheduled to close one week later. The deadline was therefore extended by three weeks to allow Lithuanians a fair chance to apply. [57] Slovakia[ edit ] Slovakia's Associate membership came into effect on 13 October 2022, for an initial duration of seven years. The Association Agreement supersedes the European Cooperating State (ECS) Agreement, which entered into force upon Slovakia's subscription to the Plan for European Cooperating States Charter on 4 February 2016, a scheme introduced at ESA in 2001. The ECS Agreement was subsequently extended until 3 August 2022. [52] Canada[ edit ] Since 1 January 1979, Canada has had the special status of a Cooperating State within ESA. By virtue of this accord, the Canadian Space Agency takes part in ESA's deliberative bodies and decision-making and also in ESA's programmes and activities. Canadian firms can bid for and receive contracts to work on programmes. The accord has a provision ensuring a fair industrial return to Canada. [58] The most recent Cooperation Agreement was signed on 15 December 2010 with a term extending to 2020. [59] [60] For 2014, Canada's annual assessed contribution to the ESA general budget was €6,059,449 ( CAD$ 8,559,050). [61] For 2017, Canada has increased its annual contribution to €21,600,000 ( CAD$ 30,000,000). [62] Budget appropriation and allocation[ edit ] European Space Agency 2016 budget by domain out of a total budget is 5250M€. ESA is funded from annual contributions by individual states as well as from an annual contribution by the European Union (EU). [63] The budget of ESA was €5.250 billion in 2016. [64] Every 3–4 years, ESA member states agree on a budget plan for several years at an ESA member states conference. This plan can be amended in future years, however provides the major guideline for ESA for several years. [ citation needed ] The 2016 budget allocations for major areas of ESA activity are shown in the chart on the right. [64] Countries typically have their own space programmes that differ in how they operate organisationally and financially with ESA. For example, the French space agency CNES has a total budget of €2015 million, of which €755 million is paid as direct financial contribution to ESA. [65] Several space-related projects are joint projects between national space agencies and ESA (e.g. COROT ). Also, ESA is not the only European governmental space organisation (for example European Union Satellite Centre and the European Union Space Programme Agency ). See also: Enlargement of the European Space Agency After the decision of the ESA Council of 21/22 March 2001, the procedure for accession of the European states was detailed as described the document titled "The Plan for European Co-operating States (PECS)". [66] Nations that want to become a full member of ESA do so in 3 stages. First a Cooperation Agreement is signed between the country and ESA. In this stage, the country has very limited financial responsibilities. If a country wants to co-operate more fully with ESA, it signs a European Cooperating State (ECS) Agreement, albeit to be a candidate for said agreement, a country must be European. The ECS Agreement makes companies based in the country eligible for participation in ESA procurements. The country can also participate in all ESA programmes, except for the Basic Technology Research Programme. While the financial contribution of the country concerned increases, it is still much lower than that of a full member state. The agreement is normally followed by a Plan For European Cooperating State (or PECS Charter). This is a 5-year programme of basic research and development activities aimed at improving the nation's space industry capacity. At the end of the 5-year period, the country can either begin negotiations to become a full member state or an associated state or sign a new PECS Charter. [67] Many countries, most of which joined the EU in both 2004 and 2007, have started to co-operate with ESA on various levels: Applicant state 11 June 2014 [78] [79] 8 April 2015 [80] AEM During the Ministerial Meeting in December 2014, ESA ministers approved a resolution calling for discussions to begin with Israel, Australia and South Africa on future association agreements. The ministers noted that "concrete cooperation is at an advanced stage" with these nations and that "prospects for mutual benefits are existing". [93] A separate space exploration strategy resolution calls for further co-operation with the United States, Russia and China on " LEO exploration, including a continuation of ISS cooperation and the development of a robust plan for the coordinated use of space transportation vehicles and systems for exploration purposes, participation in robotic missions for the exploration of the Moon, the robotic exploration of Mars, leading to a broad Mars Sample Return mission in which Europe should be involved as a full partner, and human missions beyond LEO in the longer term." [93] In August 2019, ESA and the Australian Space Agency signed a joint statement of intent "to explore deeper cooperation and identify projects in a range of areas including deep space, communications, navigation, remote asset management, data analytics and mission support." [94] Details of the cooperation were laid out in a framework agreement signed by the two entities. On 17 November 2020, ESA signed a memorandum of understanding (MOU) with the South African National Space Agency (SANSA). SANSA CEO Dr. Valanathan Munsami tweeted: "Today saw another land mark event for SANSA with the signing of an MoU with ESA. This builds on initiatives that we have been discussing for a while already and which gives effect to these. Thanks Jan for your hand of friendship and making this possible." [95] Launch vehicles[ edit ] The ESA currently has only one operational launch vehicle, Vega , and another, Ariane 6 preparing for launch in 2024. [96] Rocket launches are carried out by Arianespace , which has 23 shareholders representing the industry that manufactures the Ariane 5 as well as CNES , at ESA's Guiana Space Centre . Because many communication satellites have equatorial orbits, launches from French Guiana are able to take larger payloads into space than from spaceports at higher latitudes . In addition, equatorial launches give spacecraft an extra 'push' of nearly 500 m/s due to the higher rotational velocity of the Earth at the equator compared to near the Earth's poles where rotational velocity approaches zero. Main article: Vega (rocket) Vega rocket Vega is ESA's carrier for small satellites. Developed by seven ESA members led by Italy , it is capable of carrying a payload with a mass of between 300 and 1500 kg to an altitude of 700 km, for low polar orbit . Its maiden launch from Kourou was on 13 February 2012. [97] Vega began full commercial exploitation in December 2015. [98] The rocket has three solid propulsion stages and a liquid propulsion upper stage (the AVUM ) for accurate orbital insertion and the ability to place multiple payloads into different orbits. [99] [100] A larger version of the Vega launcher, Vega-C had its first flight in July 2022. [101] The new evolution of the rocket incorporates a larger first stage booster, the P120C replacing the P80 , an upgraded Zefiro (rocket stage) second stage, and the AVUM+ upper stage. This new variant enables larger single payloads, dual payloads, return missions, and orbital transfer capabilities. [102] Ariane launch vehicle development funding[ edit ] Historically, the Ariane family rockets have been funded primarily "with money contributed by ESA governments seeking to participate in the program rather than through competitive industry bids. This [has meant that] governments commit multiyear funding to the development with the expectation of a roughly 90% return on investment in the form of industrial workshare." ESA is proposing changes to this scheme by moving to competitive bids for the development of the Ariane 6 . [103] Future rocket development[ edit ] Future projects include the Prometheus reusable engine technology demonstrator, Phoebus (an upgraded second stage for Ariane 6), and Themis (a reusable first stage). [104] [105] Human space flight[ edit ] Formation and development[ edit ] Ulf Merbold became the first ESA astronaut to fly into space. At the time ESA was formed, its main goals did not encompass human space flight; rather it considered itself to be primarily a scientific research organisation for uncrewed space exploration in contrast to its American and Soviet counterparts. It is therefore not surprising that the first non-Soviet European in space was not an ESA astronaut on a European space craft; it was Czechoslovak Vladimír Remek who in 1978 became the first non-Soviet or American in space (the first man in space being Yuri Gagarin of the Soviet Union) – on a Soviet Soyuz spacecraft , followed by the Pole Mirosław Hermaszewski and East German Sigmund Jähn in the same year. This Soviet co-operation programme, known as Intercosmos , primarily involved the participation of Eastern bloc countries. In 1982, however, Jean-Loup Chrétien became the first non-Communist Bloc astronaut on a flight to the Soviet Salyut 7 space station. Because Chrétien did not officially fly into space as an ESA astronaut, but rather as a member of the French CNES astronaut corps, the German Ulf Merbold is considered the first ESA astronaut to fly into space. He participated in the STS-9 Space Shuttle mission that included the first use of the European-built Spacelab in 1983. STS-9 marked the beginning of an extensive ESA/NASA joint partnership that included dozens of space flights of ESA astronauts in the following years. Some of these missions with Spacelab were fully funded and organisationally and scientifically controlled by ESA (such as two missions by Germany and one by Japan) with European astronauts as full crew members rather than guests on board. Beside paying for Spacelab flights and seats on the shuttles, ESA continued its human space flight co-operation with the Soviet Union and later Russia, including numerous visits to Mir . During the latter half of the 1980s, European human space flights changed from being the exception to routine and therefore, in 1990, the European Astronaut Centre in Cologne , Germany was established. It selects and trains prospective astronauts and is responsible for the co-ordination with international partners, especially with regard to the International Space Station . As of 2006, the ESA astronaut corps officially included twelve members, including nationals from most large European countries except the United Kingdom. In 2008, ESA started to recruit new astronauts so that final selection would be due in spring 2009. Almost 10,000 people registered as astronaut candidates before registration ended in June 2008. 8,413 fulfilled the initial application criteria. Of the applicants, 918 were chosen to take part in the first stage of psychological testing, which narrowed down the field to 192. After two-stage psychological tests and medical evaluation in early 2009, as well as formal interviews, six new members of the European Astronaut Corps were selected – five men and one woman. [106] List of astronauts[ edit ] Crew vehicles[ edit ] In the 1980s, France pressed for an independent European crew launch vehicle. Around 1978, it was decided to pursue a reusable spacecraft model and starting in November 1987 a project to create a mini-shuttle by the name of Hermes was introduced. The craft was comparable to early proposals for the Space Shuttle and consisted of a small reusable spaceship that would carry 3 to 5 astronauts and 3 to 4 metric tons of payload for scientific experiments. With a total maximum weight of 21 metric tons it would have been launched on the Ariane 5 rocket, which was being developed at that time. It was planned solely for use in low Earth orbit space flights. The planning and pre-development phase concluded in 1991; the production phase was never fully implemented because at that time the political landscape had changed significantly. With the fall of the Soviet Union ESA looked forward to co-operation with Russia to build a next-generation space vehicle. Thus the Hermes programme was cancelled in 1995 after about 3 billion dollars had been spent. The Columbus space station programme had a similar fate. In the 21st century, ESA started new programmes in order to create its own crew vehicles, most notable among its various projects and proposals is Hopper , whose prototype by EADS , called Phoenix , has already been tested. While projects such as Hopper are neither concrete nor to be realised within the next decade, other possibilities for human spaceflight in co-operation with the Russian Space Agency have emerged. Following talks with the Russian Space Agency in 2004 and June 2005, [107] a co-operation between ESA and the Russian Space Agency was announced to jointly work on the Russian-designed Kliper , a reusable spacecraft that would be available for space travel beyond LEO (e.g. the moon or even Mars). It was speculated that Europe would finance part of it. A €50 million participation study for Kliper, which was expected to be approved in December 2005, was finally not approved by the ESA member states. The Russian state tender for the project was subsequently cancelled in 2006. In June 2006, ESA member states granted 15 million to the Crew Space Transportation System (CSTS) study, a two-year study to design a spacecraft capable of going beyond Low-Earth orbit based on the current Soyuz design. This project was pursued with Roskosmos instead of the cancelled Kliper proposal. A decision on the actual implementation and construction of the CSTS spacecraft was contemplated for 2008. In mid-2009 EADS Astrium was awarded a €21 million study into designing a crew vehicle based on the European ATV which is believed to now be the basis of the Advanced Crew Transportation System design. [108] In November 2012, ESA decided to join NASA's Orion programme . The ATV would form the basis of a propulsion unit for NASA's new crewed spacecraft. ESA may also seek to work with NASA on Orion's launch system as well in order to secure a seat on the spacecraft for its own astronauts. [109] In September 2014, ESA signed an agreement with Sierra Nevada Corporation for co-operation in Dream Chaser project. Further studies on the Dream Chaser for European Utilization or DC4EU project were funded, including the feasibility of launching a Europeanised Dream Chaser onboard Ariane 5. [110] [111] Cooperation with other countries and organisations[ edit ] ESA has signed co-operation agreements with the following states that currently neither plan to integrate as tightly with ESA institutions as Canada, nor envision future membership of ESA: Argentina, [112] Brazil, [113] China, [114] India [115] (for the Chandrayan mission), Russia [116] and Turkey . [88] Additionally, ESA has joint projects with the EUSPA of the European Union, NASA of the United States and is participating in the International Space Station together with the United States (NASA), Russia and Japan (JAXA). National space organisations of member states[ edit ] The Centre National d'Études Spatiales (CNES) (National Centre for Space Study) is the French government space agency (administratively, a "public establishment of industrial and commercial character"). Its headquarters are in central Paris. CNES is the main participant on the Ariane project. Indeed, CNES designed and tested all Ariane family rockets (mainly from its centre in Évry near Paris) The UK Space Agency is a partnership of the UK government departments which are active in space. Through the UK Space Agency, the partners provide delegates to represent the UK on the various ESA governing bodies. Each partner funds its own programme. The Italian Space Agency (Agenzia Spaziale Italiana or ASI) was founded in 1988 to promote, co-ordinate and conduct space activities in Italy. Operating under the Ministry of the Universities and of Scientific and Technological Research, the agency cooperates with numerous entities active in space technology and with the president of the Council of Ministers. Internationally, the ASI provides Italy's delegation to the Council of the European Space Agency and to its subordinate bodies. The German Aerospace Center (DLR) (German: Deutsches Zentrum für Luft- und Raumfahrt e. V.) is the national research centre for aviation and space flight of the Federal Republic of Germany and of other member states in the Helmholtz Association . Its extensive research and development projects are included in national and international cooperative programmes. In addition to its research projects, the centre is the assigned space agency of Germany bestowing headquarters of German space flight activities and its associates. The Instituto Nacional de Técnica Aeroespacial (INTA) (National Institute for Aerospace Technique) is a Public Research Organisation specialised in aerospace research and technology development in Spain. Among other functions, it serves as a platform for space research and acts as a significant testing facility for the aeronautic and space sector in the country. NASA[ edit ] ESA has a long history of collaboration with NASA . Since ESA's astronaut corps was formed, the Space Shuttle has been the primary launch vehicle used by ESA's astronauts to get into space through partnership programmes with NASA. In the 1980s and 1990s, the Spacelab programme was an ESA-NASA joint research programme that had ESA develop and manufacture orbital labs for the Space Shuttle for several flights on which ESA participate with astronauts in experiments. In robotic science mission and exploration missions, NASA has been ESA's main partner. Cassini–Huygens was a joint NASA-ESA mission, along with the Infrared Space Observatory , INTEGRAL , SOHO , and others. Also, the Hubble Space Telescope is a joint project of NASA and ESA. Future ESA-NASA joint projects include the James Webb Space Telescope and the proposed Laser Interferometer Space Antenna .[ citation needed ] NASA has supported ESA's MarcoPolo-R mission which landed on asteroid Bennu in October 2020 and is scheduled to return a sample to Earth for further analysis in 2023. [117] NASA and ESA will also likely join for a Mars sample-return mission . [118] In October 2020, the ESA entered into a memorandum of understanding (MOU) with NASA to work together on the Artemis program , which will provide an orbiting Lunar Gateway and also accomplish the first crewed lunar landing in 50 years, whose team will include the first woman on the Moon . Astronaut selection announcements are expected within two years of the 2024 scheduled launch date. [119] ESA also purchases seats on the NASA operated Commercial Crew Program . The first ESA astronaut to be on a Commercial Crew Program mission is Thomas Pesquet . Pesquet launched into space aboard Crew Dragon Endeavour on the Crew-2 mission. ESA also has seats on Crew-3 with Matthias Maurer and Crew-4 with Samantha Cristoforetti . SpaceX[ edit ] In 2023, following the successful launch of the Euclid telescope in July on a Falcon 9 rocket, ESA approached SpaceX to launch four Galileo communication satellites on two Falcon 9 rockets in 2024, however it would require approval from the European Commission and all member states of the European Union to proceed. [120] Cooperation with other space agencies[ edit ] Since China has invested more money into space activities, the Chinese Space Agency has sought international partnerships. Besides the Russian Space Agency , ESA is one of its most important partners. Both space agencies cooperated in the development of the Double Star Mission . [121] In 2017, ESA sent two astronauts to China for two weeks sea survival training with Chinese astronauts in Yantai , Shandong. [122] ESA entered into a major joint venture with Russia in the form of the CSTS , the preparation of French Guiana spaceport for launches of Soyuz-2 rockets and other projects. With India, ESA agreed to send instruments into space aboard the ISRO 's Chandrayaan-1 in 2008. [123] ESA is also co-operating with Japan, the most notable current project in collaboration with JAXA is the BepiColombo mission to Mercury . International Space Station[ edit ] ISS module Columbus at Kennedy Space Center's Space Station Processing Facility With regard to the International Space Station (ISS), ESA is not represented by all of its member states: [124] 11 of the 22 ESA member states currently participate in the project: Belgium, Denmark, France, Germany, Italy, Netherlands, Norway, Spain, Sweden, Switzerland and United Kingdom. Austria, Finland and Ireland chose not to participate, because of lack of interest or concerns about the expense of the project. Portugal, Luxembourg, Greece, the Czech Republic, Romania, Poland, Estonia and Hungary joined ESA after the agreement had been signed. ESA takes part in the construction and operation of the ISS , with contributions such as Columbus , a science laboratory module that was brought into orbit by NASA's STS-122 Space Shuttle mission, and the Cupola observatory module that was completed in July 2005 by Alenia Spazio for ESA. The current estimates for the ISS are approaching €100 billion in total (development, construction and 10 years of maintaining the station) of which ESA has committed to paying €8 billion. [125] About 90% of the costs of ESA's ISS share will be contributed by Germany (41%), France (28%) and Italy (20%). German ESA astronaut Thomas Reiter was the first long-term ISS crew member. ESA has developed the Automated Transfer Vehicle for ISS resupply. Each ATV has a cargo capacity of 7,667 kilograms (16,903 lb). [126] The first ATV, Jules Verne , was launched on 9 March 2008 and on 3 April 2008 successfully docked with the ISS. This manoeuvre, considered a major technical feat, involved using automated systems to allow the ATV to track the ISS, moving at 27,000 km/h, and attach itself with an accuracy of 2 cm. Five vehicles were launched before the program ended with the launch of the fifth ATV, Georges Lemaître , in 2014. [127] As of 2020, the spacecraft establishing supply links to the ISS are the Russian Progress and Soyuz , Japanese Kounotori (HTV) , and the United States vehicles Cargo Dragon 2 and Cygnus stemmed from the Commercial Resupply Services program. European Life and Physical Sciences research on board the International Space Station (ISS) is mainly based on the European Programme for Life and Physical Sciences in Space programme that was initiated in 2001.
IKAROS On 9 August 2004, ISAS successfully deployed two prototype solar sails from a sounding rocket. A clover-type sail was deployed at 122 km altitude and a fan type sail was deployed at 169 km altitude. Both sails used 7.5 micrometer -thick film. ISAS tested a solar sail again as a sub-payload to the Akari (ASTRO-F) mission on 22 February 2006. However the solar sail did not deploy fully. ISAS tested a solar sail again as a sub payload of the SOLAR-B launch at 23 September 2006, but contact with the probe was lost. The IKAROS solar sail was launched in May 2010 and successfully demonstrated solar sail technology in July. This made IKAROS the world's first spacecraft to successfully demonstrate solar sail technology in interplanetary space. The goal is to have a solar sail mission to Jupiter after 2020. [38] See also: Scientific research on the ISS The first Japanese astronomy mission was the X-ray satellite Hakucho (CORSA-b), which was launched in 1979. Later ISAS moved into solar observation, radio astronomy through space VLBI and infrared astronomy. Infrared astronomy[ edit ] ASTRO-E Japan's infrared astronomy began with the 15-cm IRTS telescope which was part of the SFU multipurpose satellite in 1995. ISAS also gave ground support for the ESA Infrared Space Observatory (ISO) infrared mission. JAXA's first infrared astronomy satellite was the Akari spacecraft, with the pre-launch designation ASTRO-F . This satellite was launched on 21 February 2006. Its mission is infrared astronomy with a 68 cm telescope. This is the first all sky survey since the first infrared mission IRAS in 1983. (A 3.6 kg nanosatellite named CUTE-1.7 was also released from the same launch vehicle.) [39] JAXA is also doing further R&D for increasing the performance of its mechanical coolers for its future infrared mission, SPICA . This would enable a warm launch without liquid helium. SPICA has the same size as the ESA Herschel Space Observatory mission, but is planned to have a temperature of just 4.5 K and will be much colder. Unlike Akari, which had a geocentric orbit , SPICA will be located at Sun–Earth L2 . The launch is expected in 2027 or 2028 on JAXA's new H3 Launch Vehicle , however the mission is not yet fully funded. ESA and NASA may also each contribute an instrument. [40] The SPICA mission was cancelled in 2020. See also: ASTRO-H and XRISM Starting from 1979 with Hakucho (CORSA-b), for nearly two decades Japan had achieved continuous observation. However, in the year 2000 the launch of ISAS's X-ray observation satellite, ASTRO-E failed (as it failed at launch it never received a proper name). Then on 10 July 2005, JAXA was finally able to launch a new X-ray astronomy mission named Suzaku (ASTRO-EII). This launch was important for JAXA, because in the five years since the launch failure of the original ASTRO-E satellite, Japan was without an x-ray telescope . Three instruments were included in this satellite: an X-ray spectrometer (XRS), an X-ray imaging spectrometer (XIS), and a hard X-ray detector (HXD). However, the XRS was rendered inoperable due to a malfunction which caused the satellite to lose its supply of liquid helium. The next JAXA x-ray mission is the Monitor of All-sky X-ray Image (MAXI) . MAXI continuously monitors astronomical X-ray objects over a broad energy band (0.5 to 30 keV). MAXI is installed on the Japanese external module of the ISS. [41] On 17 February 2016, Hitomi (ASTRO-H) was launched as the successor to Suzaku, which completed its mission a year before. Solar observation[ edit ] Japan's solar astronomy started in the early 1980s with the launch of the Hinotori (ASTRO-A) X-ray mission. The Hinode (SOLAR-B) spacecraft, the follow-on to the joint Japan/US/UK Yohkoh (SOLAR-A) spacecraft, was launched on 23 September 2006 by JAXA. [42] [43] A SOLAR-C can be expected sometime after 2020. However no details are worked out yet other than it will not be launched with the former ISAS's Mu rockets. Instead a H-2A from Tanegashima could launch it. As H-2A is more powerful, SOLAR-C could either be heavier or be stationed at L1 ( Lagrange point 1). Radio astronomy[ edit ] In 1998, Japan launched the HALCA (MUSES-B) mission, the world's first spacecraft dedicated to conduct space VLBI observations of pulsars, among others. To do so, ISAS set up a ground network around the world through international cooperation. The observation part of the mission lasted until 2003 and the satellite was retired at the end of 2005. In FY 2006, Japan funded the ASTRO-G as the succeeding mission. ASTRO-G was canceled in 2011. Communication, positioning and technology tests[ edit ] One of the primary duties of the former NASDA body was the testing of new space technologies, mostly in the field of communication. The first test satellite was ETS-I, launched in 1975. However, during the 1990s, NASDA was afflicted by problems surrounding the ETS-VI and COMETS missions. In February 2018, JAXA announced a research collaboration with Sony to test a laser communication system from the Kibo module in late 2018. [44] Testing of communication technologies remains to be one of JAXA's key duties in cooperation with NICT . Active Missions: INDEX , QZS-1 , SLATS , QZS-2 , QZS-3, QZS-4, QZS-1R Under Development: ETS-IX Retired: OICETS , ETS-VIII , WINDS i-Space : ETS-VIII, WINDS and QZS-1[ edit ] To upgrade Japan's communication technology the Japanese state launched the i-Space initiative with the ETS-VIII and WINDS missions. [45] ETS-VIII was launched on 18 December 2006. The purpose of ETS-VIII is to test communication equipment with two very large antennas and an atomic clock test. On 26 December both antennas were successfully deployed. This was not unexpected, since JAXA tested the deployment mechanism before with the LDREX-2 Mission, which was launched on 14 October with the European Ariane 5. The test was successful. On 23 February 2008, JAXA launched the Wideband InterNetworking engineering test and Demonstration Satellite ( WINDS ), also called "KIZUNA". WINDS aimed to facilitate experiments with faster satellite Internet connections. The launch, using H-IIA launch vehicle 14, took place from Tanegashima Space Center . [46] WINDS was decommissioned on 27 February 2019. [47] On 11 September 2010, JAXA launched QZS-1 (Michibiki-1), the first satellite of the Quasi Zenith Satellite System (QZSS), a subsystem of the global positioning system (GPS). Three more followed in 2017, and a replacement for QZS-1 is scheduled to launch in late 2021. A next-generation set of three satellites, able to operate independent of GPS, is scheduled to begin launching in 2023. OICETS and INDEX[ edit ] On 24 August 2005, JAXA launched the experimental satellites OICETS and INDEX on a Ukrainian Dnepr rocket . OICETS (Kirari) is a mission tasked with testing optical links with the European Space Agency (ESA) ARTEMIS satellite, which is around 40,000 km away from OICETS. The experiment was successful on 9 December, when the link could be established. In March 2006, JAXA could establish with OICETS the worldwide first optical links between a LEO satellite and a ground station first in Japan and in June 2006 with a mobile station in Germany. INDEX (Reimei) is a small 70 kg satellite for testing various equipment, and functions as an aurora observation mission as well. The Reimei satellite is currently in its extended mission phase. Earth observation program[ edit ] Japan's first Earth observation satellites were MOS-1a and MOS-1b launched in 1987 and 1990. During the 1990s, and the new millennium this NASDA program came under heavy fire, because both Adeos (Midori) and Adeos 2 (Midori 2) satellites failed after just ten months in orbit. ALOS[ edit ] MTSAT-1 In January 2006, JAXA successfully launched the Advanced Land Observation Satellite (ALOS/Daichi). Communication between ALOS and the ground station in Japan will be done through the Kodama Data Relay Satellite, which was launched during 2002. This project is under intense pressure due to the shorter than expected lifetime of the ADEOS II (Midori) Earth Observation Mission. For missions following Daichi, JAXA opted to separate it into a radar satellite ( ALOS-2 ) and an optical satellite ( ALOS-3 ). ALOS 2 SAR (Synthetic Aperture Radar) satellite was launched in May 2014. The ALOS 3 satellite was launched in March 2023. The satellite was lost in a launch failure. Rainfall observation[ edit ] Since Japan is an island nation and gets struck by typhoons every year, research about the dynamics of the atmosphere is a very important issue. For this reason Japan launched in 1997 the TRMM (Tropical Rainfall Measuring Mission) satellite in cooperation with NASA, to observe the tropical rainfall seasons. For further research NASDA had launched the ADEOS and ADEOS II missions in 1996 and 2003. However, due to various reasons,[ specify ] both satellites had a much shorter than expected life term. On 28 February 2014, a H-2A rocket launched the GPM Core Observatory , a satellite jointly developed by JAXA and NASA. The GPM mission is the successor to the TRMM mission, which by the time of the GPM launch had been noted as highly successful. JAXA provided the Global Precipitation Measurement /Dual-frequency Precipitation Radar (GPM/DPR) Instrument for this mission. Global Precipitation Measurement itself is a satellite constellation, whilst the GPM Core Observatory provides a new calibration standard for other satellites in the constellation. Other countries/agencies like France, India, ESA, etc. provides the sub-satellites. The aim of GPM is to measure global rainfall with unprecedented detail. Monitoring of carbon dioxide[ edit ] At the end of the 2008 fiscal year, JAXA launched the satellite GOSAT (Greenhouse Gas Observing SATellite) to help scientists determine and monitor the density distribution of carbon dioxide in the atmosphere . The satellite is being jointly developed by JAXA and Japan's Ministry of the Environment . JAXA is building the satellite while the Ministry is in charge of the data that will be collected. Since the number of ground-based carbon dioxide observatories cannot monitor enough of the world's atmosphere and are distributed unevenly throughout the globe, the GOSAT may be able to gather more accurate data and fill in the gaps on the globe where there are no observatories on the ground. Sensors for methane and other greenhouse gasses are also being considered for the satellite, although the plans are not yet finalized. The satellite weighs approximately 1650 kg and is expected to have a life span of five years. The successor satellite GOSAT 2 was launched in October 2018. GCOM series[ edit ] The next funded Earth-observation mission after GOSAT is the GCOM ( Global Change Observation Mission ) Earth-observation program as a successor to ADEOS II (Midori) and the Aqua mission. To reduce the risk and for a longer observation time the mission will be split into smaller satellites. Altogether GCOM will be a series of six satellites. The first satellite, GCOM-W (Shizuku), was launched on 17 May 2012 with the H-IIA. The second satellite, GCOM-C (Shikisai), was launched in 2017. Satellites for other agencies[ edit ] For weather observation Japan launched in February 2005 the Multi-Functional Transport Satellite 1R ( MTSAT-1R ). The success of this launch was critical for Japan, since the original MTSAT-1 could not be put into orbit because of a launch failure with the H-2 rocket in 1999. Since then Japan relied for weather forecasting on an old satellite which was already beyond its useful life term and on American systems. On 18 February 2006, JAXA, as head of the H-IIA at this time, successfully launched the MTSAT-2 aboard a H-2A rocket. MTSAT-2 is the backup to the MTSAT-1R. The MTSAT-2 uses the DS2000 satellite bus developed by Mitsubishi Electric. [48] The DS2000 is also used for the DRTS Kodama, ETS-VIII and the Superbird 7 communication satellite, making it the first commercial success for Japan. As a secondary mission both the MTSAT-1R and MTSAT-2 help to direct air traffic. Other JAXA satellites currently in use[ edit ] GEOTAIL magnetosphere observation satellite (since 1992) DRTS (Kodama) Data Relay Satellite, since 2002. (Projected Life Span is seven years) Ongoing joint missions with NASA are the Aqua Earth Observation Satellite, and the Global Precipitation Measurement (GPM) Core satellite.JAXA also provided the Light Particle Telescope (LPT) for the 2008 Jason 2 satellite by the French CNES . On 11 May 2018, JAXA deployed the first satellite developed in Kenya from the Japanese Experiment Module of the International Space Station. [49] The satellite, 1KUNS-PF , was created by the University of Nairobi .
Toggle the table of contents Canadian Space Agency From Wikipedia, the free encyclopedia Government agency March 1, 1989; 35 years ago (1989-03-01) Jurisdiction www.asc-csa.gc.ca The Canadian Space Agency (CSA; French : Agence spatiale canadienne, ASC) is the national space agency of Canada, established in 1990 by the Canadian Space Agency Act. History[ edit ] The origins of the Canadian upper atmosphere and space program can be traced back to the end of the Second World War . [5] Between 1945 and 1960, Canada undertook a number of small launcher and satellite projects under the aegis of defence research, including the development of the Black Brant rocket as well as series of advanced studies examining both orbital rendezvous and re-entry. [6] In 1957, scientists and engineers at the Canadian Defence Research Telecommunications Establishment (DRTE) under the leadership of John H. Chapman embarked on a project initially known simply as S-27 or the Topside Sounder Project. This work would soon lead to the development of Canada's first satellite known as Alouette 1 . With the launch of Alouette 1 in September 1962, Canada became the third country to put an artificial satellite into space. At the time, Canada only possessed upper atmospheric launch capabilities ( sounding rockets ), therefore, Alouette 1 was sent aloft by the American National Aeronautics and Space Administration (NASA) from Vandenberg Air Force Base in Lompoc, California . The technical excellence of the satellite, which lasted for ten years instead of the expected one, prompted the further study of the ionosphere with the joint Canadian-designed, U.S.-launched ISIS satellite program . This undertaking was designated an International Milestone of Electrical Engineering by IEEE in 1993. The launch of Anik A-1 in 1972 made Canada the first country in the world to establish its own domestic geostationary communication satellite network. [7] These and other space-related activities in the 1980s compelled the Canadian government to promulgate the Canadian Space Agency Act, which established the Canadian Space Agency. The act received royal assent on May 10, 1990, and came into force on December 14, 1990. [8] The mandate of the Canadian Space Agency is to promote the peaceful use and development of space, to advance the knowledge of space through science and to ensure that space science and technology provide social and economic benefits for Canadians. The Canadian Space Agency's mission statement says that the agency is committed to leading the development and application of space knowledge for the benefit of Canadians and humanity. In 1999, the CSA was moved from project-based to "A-base" funding and given a fixed annual budget of $300 million. [4] The actual budget varies from year to year due to additional earmarks and special projects. In 2009, Dr. Nicole Buckley was appointed chief scientist of life science. [9] April 12, 2007 – December 31, 2007— Larry J. Boisvert [14] January 1, 2008 - September 2, 2008— Guy Bujold September 2, 2008 – February 1, 2013— Steven MacLean [15] February 2, 2013 – August 5, 2013—Gilles Leclerc (interim)[ citation needed ] August 6, 2013 – November 3, 2014— Walter Natynczyk November 3, 2014 - March 9, 2015—Luc Brûlé, Interim [16] September 14, 2020 – present— Lisa Campbell [18] Cooperation with the European Space Agency[ edit ] The CSA has been a cooperating state of the European Space Agency (ESA) since the 1970s [19] [20] and has several formal and informal partnerships and collaborative programs with space agencies in other countries, such as NASA , ISRO , JAXA , and SNSA . Canada's collaboration with Europe in space activities predated both the European Space Agency and the Canadian Space Agency. [19] From 1968, Canada held observer status in the European Space Conference (ESC), a ministerial-level organization set up to determine future European space activities, and it continued in this limited role after ESA was created in 1975. [19] Since January 1, 1979, Canada has had the special status of a "Cooperating State" with the ESA, [20] paying for the privilege and also investing in working time and providing scientific instruments that are placed on ESA probes. Canada is allowed to participate in optional programs; it also has to contribute to the General Budget but not as much as associate membership entail. This status was unique at the time and remains so today. On 15 December 2010, the accord was renewed for a further 10 years, until 2020. [20] By virtue of this accord, Canada takes part in ESA deliberative bodies and decision-making and in ESA's programmes and activities. Canadian firms can bid for and receive contracts to work on programmes. The accord has a provision specifically ensuring a fair industrial return to Canada. The head of the Canadian delegation to ESA is the president of the Canadian Space Agency . As of February 2009, there are currently 30 Canadians that are employed as staff members at ESA. (Distributed over various ESA sites: 20 at ESTEC; 4 at ESOC; 4 at ESA HQ; 2 at ESRIN). Canadian space program[ edit ] Canadarm (right) during Space Shuttle mission STS-72 The Mobile Base System just before Canadarm2 installed it on the Mobile Transporter during STS-111 The Canadian space program is administered by the Canadian Space Agency. Canada has contributed technology, expertise and personnel to the world space effort, especially in collaboration with ESA and NASA . In addition to its astronauts and satellites , some of the most notable Canadian technological contributions to space exploration include the Canadarm on the Space Shuttle and Canadarm2 on the International Space Station. Canada's contribution to the International Space Station is the $1.3 billion Mobile Servicing System. This consists of Canadarm2 (SSRMS), Dextre (SPDM), mobile base system (MBS) and multiple robotics workstations that together make up the Mobile Servicing System on the ISS. The Canadarm, Canadarm2 and Dextre all employ the Advanced Space Vision System , which allows more efficient use of the robotic arms. Another Canadian technology of note is the Orbiter Boom Sensor System , which was an extension for the original Canadarm used to inspect the Space Shuttle's thermal protection system for damage while in orbit. [21] Before the Space Shuttle's retirement, the boom was modified for use with Canadarm2; STS-134 (the Space Shuttle program's penultimate mission) left it for use on the ISS. See also: Canadian Astronaut Corps There have been four recruiting campaigns for astronauts for the CSA. The first, in 1983 by the National Research Council , led to the selection of Roberta Bondar , Marc Garneau , Robert Thirsk , Ken Money , Bjarni Tryggvason and Steve MacLean . The second, in 1992, selected Chris Hadfield , Julie Payette , Dafydd Williams and Michael McKay . On May 13, 2009, it was announced after the completion of a third selection process that two new astronauts, Jeremy Hansen and David Saint-Jacques , had been chosen. [22] The latest recruitment campaign was launched in 2016, attracting 3,772 applicants for 2 candidates. [23] In 2017, Joshua Kutryk and Jennifer Sidey were chosen. [24] Nine Canadians have participated in 17 crewed missions in total: 14 NASA Space Shuttle missions (including one mission to Mir ) and 3 Roscosmos Soyuz missions. [25] [26] [27] Two former Canadian astronauts never flew in space: Michael McKay resigned due to medical reasons [28] and Ken Money resigned in 1992, eight years after his selection. [29] Canadian Space Agency astronauts
From Wikipedia, the free encyclopedia National space agency of the People's Republic of China This article is about the administrative office for China's civil space activities and international space cooperation. For the operator of China's state space programs, see China Aerospace Science and Technology Corporation . For the operator of China's human spaceflight program, see China Manned Space Agency . For other uses, see CNSA (disambiguation) . China National Space Administration 22 April 1993; 30 years ago (1993-04-22) Preceding agency [kʷɔ̄ːk.káːhɔ̏ːŋtʰīnkùːk] The China National Space Administration (CNSA) is a government agency of the People's Republic of China headquartered in Haidian, Beijing , responsible for civil space administration and international space cooperation. These responsibilities include organizing or leading foreign exchanges and cooperation in the aerospace field. [2] The CNSA is an administrative agency under the Ministry of Industry and Information Technology . [3] [ need quotation to verify ] Founded in 1993, CNSA has pioneered a number of achievements in space for China despite its relatively short history, including becoming the first space agency to land on the far side of the Moon with Chang'e 4 , bringing material back from the Moon with Chang'e 5 , and being the second agency who successfully landed a rover on Mars with Tianwen-1 . History[ edit ] CNSA is an agency created in 1993 when the Ministry of Aerospace Industry was split into CNSA and the China Aerospace Science and Technology Corporation (CASC). The former was to be responsible for policy, while the latter was to be responsible for execution. This arrangement proved somewhat unsatisfactory, as these two agencies were, in effect, one large agency, sharing both personnel and management. [6] As part of a massive restructuring in 1998, CASC was split into a number of smaller state-owned companies . The intention appeared to have been to create a system similar to that characteristic of Western defense procurement in which entities which are government agencies, setting operational policy, would then contract out their operational requirements to entities which were government-owned, but not government-managed. [6] Since the passage of the Wolf Amendment in 2011, NASA has been forced by Congress to implement a long-standing exclusion policy with CNSA ever since, though this has been periodically overcome. Function[ edit ] CNSA was established as a government institution to develop and fulfill China's due international obligations, with the approval by the 8th National People's Congress of China (NPC). The 9th NPC assigned CNSA as an internal structure of the Commission of Science, Technology and Industry for National Defense (COSTIND). CNSA assumes the following main responsibilities: signing governmental agreements in the space area on behalf of organizations, inter-governmental scientific and technical exchanges; and also being in charge of the enforcement of national space policies and managing the national space science, technology and industry. China has signed governmental space cooperation agreements with Argentina , Brazil , Chile , France, Germany, India, Italy, Pakistan , Russia, Ukraine , the United Kingdom, the United States, and some other countries. Significant achievements have been scored in the bilateral and multilateral and technology exchanges and cooperation. [6] The most recent administrator is Zhang Kejian . Wu Yanhua is vice-administrator and Tian Yulong is secretary general. [7] April 1993: Liu Jiyuan
From Wikipedia, the free encyclopedia Brazilian Space Agency Brazilian Space AgencyAgência Espacial Brasileira Agency overview 10 February 1994 [1] (formerly the Brazilian space program , 1961-1993) Type R$180 million / US$45 million (2019) [3] Website www.gov.br/aeb The Brazilian Space Agency ( Portuguese : Agência Espacial Brasileira; AEB) is the civilian authority in Brazil responsible for the country's space program . It operates a spaceport at Alcântara , and a rocket launch site at Barreira do Inferno . It is the largest and most prominent space agency in Latin America . The Brazilian Space Agency is the institutional successor of Brazil's space program , which had been managed by the Brazilian military until its transfer to civilian control on 10 February 1994. It suffered a major setback in 2003, when a rocket explosion killed 21 technicians. Brazil successfully launched its first rocket into space, the VSB-30 , on 23 October 2004 from the Alcântara Launch Center ; several other successful launches have followed. [4] [5] [6] Brazil was briefly a partner in the International Space Station , and in 2006, AEB astronaut Marcos Pontes became the first Brazilian and the first native Portuguese-speaker to go into space, when he arrived at the ISS for a week. During his trip, Pontes carried out eight experiments selected by the Brazilian Space Agency, including testing flight dynamics of saw blades in zero gravity environments. In June 2021, the AEB signed the Artemis Accords to the joint exploration of the Moon and Mars from 2024 as part of the Artemis program . [7] Technological Institute of Aeronautics (ITA). The Brazilian Space Agency's control room at the Alcântara Launch Center. The then president Jânio Quadros in 1960 established a commission that elaborated a national program for the space exploration. As a result of this work, in August 1961, the Organization Group of the National Commission of Space Activities ( Portuguese : Grupo de Organização da Comissão Nacional de Atividades Espaciais) was formed, operating in São José dos Campos , in the state of São Paulo . Its researchers participated in international projects in the areas of astronomy , geodesy , geomagnetism , and meteorology .[ citation needed ] The GOCNAE was replaced in April 1971 by the Institute for Space Research, currently called the National Institute for Space Research (INPE). Since the creation of the then Technical Center of Aeronautics (CTA), the current Department of Aerospace Science and Technology (DCTA) of the Brazilian Air Force , in 1946, the country has been following the international progress in the aerospace sector.[ citation needed ] With the creation of the Technological Institute of Aeronautics (ITA), a fully qualified institution was formed to train highly qualified human resources in areas of state-of-the-art technology. The DCTA, through the ITA and the Institute of Aeronautics and Space (IAE), play a key role in the consolidation of the Brazilian space program .[ citation needed ] In the early 1970s, the Brazilian Space Activities Commission (COBAE) was created - a body linked to the then General Staff of the Armed Forces (EMFA) - to coordinate and monitor the implementation of the space program. This coordinating role, in February 1994, was transferred to the Brazilian Space Agency. The creation of the AEB represents a change in government orientation by establishing a central coordinating body for the space program , reporting directly to the Presidency of the Republic .[ citation needed ] In 2011, Argentina 's defense minister, Arturo Puricelli , made a proposal to the Brazilian minister, Celso Amorim , for the creation of a unified South American space agency by the year 2025, according to the European Space Agency . [8] [9] In 2015, however, the Brazilian Space Agency and the Ministry of Defense rejected the Argentine proposal "because it understood that it would be an organ that would yield a lot of bureaucracy and few results like the 'confederation' proposed by the United States and never came to anything" and also because, according to the agency's adviser: "A regional space agency would reverberate in the Brazilian pocket that, due to its territorial size, would end up getting most of the invoice to pay." [10] VLS1-V03 rocket on a launching pad from Alcântara Launch Center . Alcântara Space Center[ edit ] 2°20′S 44°24′W﻿ / ﻿2.333°S 44.400°W﻿ / -2.333; -44.400 The Alcântara Launch Center ( Portuguese : Centro de Lançamento de Alcântara; CLA) is the main launch site and operational center of the Brazilian Space Agency. [11] It is located in the peninsula of Alcântara , in the state of Maranhão . [12] This region presents some excellent requirements, such as low population density , excellent security conditions and easiness of aerial and maritime access. [12] The most important factor is its closeness to the Equator - Alcântara is the closest launching base to the Equator. [11] This gives the launch site a significant advantage in launching geosynchronous satellites . [11] Barreira do Inferno Launch Center[ edit ] 5°55′30″S 35°9′47″W﻿ / ﻿5.92500°S 35.16306°W﻿ / -5.92500; -35.16306 The Barreira do Inferno Launch Center ( Portuguese : Centro de Lançamento da Barreira do Inferno; CLBI) is a rocket launch base of the Brazilian Space Agency. [13] It is located in the city of Parnamirim , in the state of Rio Grande do Norte . It is primarily used to launch sounding rockets and to support the Alcântara Launch Center. [14] VSB-30 , the Brazilian sub-orbital vehicle The Brazilian Space Agency has operated a series of sounding rockets. Main article: VLM (rocket) VLM , the future Brazilian three-stage satellite launcher Brazil has forged a cooperative arrangement with Germany to develop a dedicated micro-satellite launch vehicle. As a result, the VLM "Veiculo Lançador de Microsatelites" (Microsatellite Launch Vehicle) based on the S50 rocket engine is being studied, with the objective of orbiting satellites up to 150 kg in circular orbits ranging from 250 to 700 km. The first qualifying and test flight will be on 2022 from the Alcântara Space Center . [15] The second flight is scheduled for the following years as part of the SHEFEX mission, to be conducted also from Alcântara, in partnership with the German Aerospace Center (DLR) . [16] Main article: VLS-1 The VLS - Satellite Launch Vehicle ( Portuguese : Veículo Lançador de Satélites) was the Brazilian Space Agency's main satellite launch vehicle. [17] It is a four-stage rocket composed of a core and four strap-on motors. [18] The vehicle's first stage has four solid fuel motors derived from the Sonda sounding rockets . [18] It is intended to deploy 100 to 380 kg satellites into 200 to 1200 km orbit , or to deploy 75 to 275 kg payloads into 200 to 1000 km polar orbit . [18] The first 3 prototypes for the vehicle failed to launch, with the 3rd exploding on the launch pad in 2003 resulting in the deaths of 21 AEB personnel. The VLS-1 V4 prototype was expected a launch in 2013. [19] [20] After subsequent delays, the project was cancelled in 2016. [21] Southern Cross program[ edit ] The Brazilian Space Agency was developing a new family of launch vehicles in cooperation with the Russian Federal Space Agency . [22] [23] [24] The five rockets of the Southern Cross family will be based on Russia's Angara vehicle and liquid-propellant engines . [22] The first stage of the VLS Gama, Delta and Epsilon rockets was to be powered by a unit based on the RD-191 engine . [22] The second stage, which will be the same for all the Southern Cross rockets, will be driven by an engine based on the Molniya rocket . [22] The third stage will be a solid-propellant booster based on an upgraded version of the VLS-1 . [22] The program was named "Southern Cross" in reference to the Crux constellation, present on the flag of Brazil and composed of five stars. [25] Hence the names of the future launch vehicles: [25] VLS Alfa (light-weight rocket) The first rocket to be developed. As a direct modification of the VLS-1 of the original project, replacing the fourth and fifth stages by a single liquid fuel engine. It can place payloads in the range 200–400 kg in orbits up to 750 km. VLS Beta (light-weight rocket) Consisting of three stages without auxiliary thrusters. The first stage is a solid fuel propellant 40 tons, the second will have 30 tons of thrust engine and the latter will be 7.5 tons of thrust, with the same mixture "Kerolox". It can place payloads up to 800 kg in orbits up to 800 km. VLS Gama (light-weight rocket) The VLS Gama launcher is part of the light-weight class, but using the near-equatorial position of the Alcântara Launch Center , it can place almost 1 ton of payload into orbit up to 800 km. VLS Delta (medium-weight rocket) The VLS Delta launcher is a medium-weight rocket and differs from the Gamma by having four solid-propellant boosters attached to the first stage. [22] Its payload deliverable to a geostationary orbit is 2 tons. [22] VLS Epsilon (heavy-weight rocket) The VLS Epsilon launcher is a heavy-weight rocket with three identical units attached to the first stage. [22] It can place a 4 tons spacecraft in geostationary orbit , if it is launched from Alcântara. [22] The Brazilian government was planning to allocate $1 billion dollars for the project over six years. [22] It has already set aside $650 million for the construction of five launch pads able to handle up to 12 launches per year. [22] The program was scheduled to be completed by 2022. [25] However, it was cancelled by Brazilian President Dilma Rouseff. Instead, Brazil will focus on a series of smaller launch vehicles that appear to rely more on home-grown technology. 14-X[ edit ] 14-X in computer graphics Brazil is also investing in the domain of the hypersonic technology. The 14-X , a hypersonic glide vehicle (HGV) under development with the Brazilian Air Force , was conceived in 2007 and launched on 14 February 2022, [26] [27] by a VSB-30 rocket , reaching to 100,000 feet of altitude and a maximum speed of Mach 10. [28] A number of different engines [29] were developed for usage on the several launch vehicles: S-10-1 solid rocket engine. [30] Used on Sonda 1 . Thrust: 27 kN. S-10-2 solid rocket engine. [31] Used on Sonda 1. Thrust: 4.20 kN, burn time: 32 s. S-20 Avibras solid rocket engine. [32] Used on Sonda 2 and Sonda 3 .  Thrust:36 kN S-23 Avibrassolid rocket engine. [33] Used on Sonda 3M1.  Thrust:18 kN S-30 IAE solid rocket engine. [34] Used on Sonda 3, Sonda 3M1, Sonda 4 , VS-30 , VS-30/Orion and VSB-30 .  Thrust: 20.490 kN S-31 IAE solid rocket engine. [35] Used on VSB-30.  Thrust: 240 kN S-40TM IAE solid rocket engine. [36] Used on VLS-R1, VS-40 , VLS-1 and VLM-1 .  Thrust: 208.4 kN, isp=272s. S-43 IAE solid rocket engine. [37] Used on Sonda 4, VLS-R1 and VLS-1. Thrust: 303 kN, isp=265s S-43TM IAE solid rocket engine. [38] Used on VLS-R1, VLS-1 and VLM. Thrust: 321.7 kN, isp=276s S-44 IAE solid rocket engine. [39] Used on VLS-R1, VS-40, VLS-1 and VLM-1. Thrust:33.24 kN, isp=282s L5 (Estágio Líquido Propulsivo (EPL)) liquid fuel rocket engine. Tested on VS-30 and projected for use on VLS-Alfa . [40] L15 liquid fuel rocket engine. Projected for use on VS-15 . [41] Thrust: 15 kN L75 liquid fuel rocket engine, similar to the Russian RD-0109 . [42] Projected for use on VLS-Alfa , VLS-Beta , VLS-Omega , VLS-Gama and VLS-Epsilon . Thrust: 75 kN S-50 IAE solid rocket engine. Projected for use on VLM-1 and VS-50 . [40] [43] L1500 liquid fuel rocket engine. [42] Used on VLS-Beta , VLS-Omega , VLS-Gama and VLS-Epsilon . Thrust: 1500 kN Satellites[ edit ] The Brazilian Space Agency has several active satellites in orbit including imagery intelligence , military communications , reconnaissance and Earth observation satellites. Several others are currently in development. Brazilian Satellites PMM - "Plataforma Multimissão" (Multi-mission Platform) 2021– PMM - "Plataforma Multimissão" (Multi-mission Platform) 2020s PMM - "Plataforma Multimissão" (Multi-mission Platform) 2020s PMM - "Plataforma Multimissão" (Multi-mission Platform) 2020s Space weather ( EQUARS ) and X-ray space telescope ( MIRAX ) mission 2020s PMM - "Plataforma Multimissão" (Multi-mission Platform) 2020s Human spaceflight[ edit ] Marcos Pontes , a lieutenant colonel in the Brazilian Air Force , is an astronaut of the Brazilian Space Agency. [49] Pontes was the first Brazilian astronaut, [50] having launched with the Expedition 13 crew from the Baikonur Cosmodrome in Kazakhstan on March 29, 2006, aboard a Soyuz-TMA spacecraft. [51] Pontes docked with the International Space Station (ISS) on March 31, 2006, where he lived and worked for 9 days. [50] Pontes returned to Earth with the Expedition 12 crew, landing in Kazakhstan on April 8, 2006. [50] In January 2019, Pontes was nominated by the Brazilian President Jair Bolsonaro as Minister of Science, Technology, Innovation and Communication, a position he held until 2022. Name Main article: Aerospace Operations Command The Aerospace Operations Command ( Portuguese : Comando de Operações Aeroespaciais - COMAE) is a Brazilian air and space command created in 2017 [52] which is part of the Brazilian Air Force . The command is responsible for planning, coordinating, executing and controlling the country's air and space operations. [53] The Brazilian Navy and Brazilian Army also are part of the organization. Space operations center[ edit ] Brazilian President Jair Bolsonaro at the inauguration of the Space Operations Center in Brasília, June 2020. The Space Operations Center (Portuguese: Centro de Operações Espaciais, acronym COPE) is a facility established in 2020 subordinated to the Aerospace Operations Command, with the objective of operating and supervising the Brazilian satellites. [54] Bingo radio telescope[ edit ] The Bingo radio telescope called Baryon Acoustic Oscillations from Integrated Neutral Gas Observations, [55] is a project coordinated by the Ministry of Science, Technology and Innovation , Brazilian Space Agency, National Institute for Space Research (INPE) and international partners from Europe and China. The telescope will consist of two giant dishes, with 40 meters of diameter each, which will receive radiation from the space and project their spectrum in a series of metal detectors, called horns. Bingo will perform its detections in the radio band in the range of 960 to 1260 MHZ. [56] 80% of the Bingo parts came from the Brazilian industry. [55] China/CBERS[ edit ] China and Brazil have successfully cooperated in the field of space. [57] : 202 Among the most successful space cooperation projects were the development and launch of earth monitoring satellites. [57] : 202 The China–Brazil Earth Resources Satellite program (CBERS) is a technological cooperation program between Brazil and China which develops and operates Earth observation satellites . Brazil and China negotiated the CBERS project during two years (1986–1988), renewed in 1994 and again in 2004. As of 2023, the two countries have jointly developed six China-Brazil Earth Resource Satellites. [57] : 202 These projects have helped both Brazil and China develop their access to satellite imagery and promoted remote sending research. [57] : 202 Brazil and China's cooperation is a unique example of South-South cooperation between two developing countries in the field of space. [57] : 202 International Space Station[ edit ] ISS Express Logistics Carrier The Brazilian Space Agency is a bilateral partner of NASA in the International Space Station . [58] The agreement for the design, development, operation and use of Brazilian developed flight equipment and payloads for the Space Station was signed in 1997. [58] It includes the development of six items, among which are a Window Observational Research Facility and a Technology Experiment Facility. [49] In return, NASA will provide Brazil with access to its ISS facilities on-orbit, as well as a flight opportunity for one Brazilian astronaut during the course of the ISS program. [58] However, due to cost issues, the subcontractor Embraer was unable to provide the promised ExPrESS pallet, and Brazil left the programme in 2007. [59] [60] However, as a compromise, NASA have funded small Brazilian-made components for the Express Logistics Carrier-2 for the ISS, which were installed in 2009. [61] Ukraine/Ciclone 4[ edit ] On October 21, 2003, the Brazilian Space Agency and the State Space Agency of Ukraine established a cooperation agreement creating a joint venture space enterprise called Alcântara Cyclone Space. [62] The new company will focus on launching satellites from the Alcântara Launch Center using the Tsyklon-4 rocket. [63] The company will invest $160 million dollars in infrastructure for the new launch pad that will be constructed at the Alcântara Launch Center. In March 2009, the Brazilian Government increased its financial capital by US$ 50 million. [64] The first launch was planned for 2014 from the Alcantara Launch Center. [65] Brazil pulled out of the program in 2015. [66] Japan[ edit ] On November 8, 2010, National Institute for Space Research (INPE) and Japan Aerospace Exploration Agency (JAXA) signed a Letter of Intent regarding the Reducing Emissions from Deforestation and Forest Degradation in Developing Countries (REDD+) program. [67] Examples of the cooperation include the monitoring of illegal logging in the Amazon rainforest utilizing data from JAXA's ALOS satellite. Both Brazil and Japan are members of the Global Precipitation Measurement project. [68] Artemis program[ edit ] On 21 October 2020, Brazil was invited by the United States to join NASA 's Artemis Space Program . [69] On 15 June 2021, the country officially joined the program by signing the Artemis Accords with the U.S. and international partners, for the joint exploration of the Moon from 2024 and Mars and beyond in 2030s. The roles of Brazil include the development of a national lunar robot for the use in the future missions. [7] BRICS satellite constellation[ edit ] On 18 August 2021, AEB signed cooperation agreements with space agencies of BRICS (Brazil, Russia, India, China and South Africa), to the joint-development of a remote sensing satellite constellation, [70] aiming to help with common challenges for the mankind such as the climate change, major disasters and environmental protection. Ground stations located in Cuiabá in Brazil, Moscow Region in Russia, Shadnagar–Hyderabad in India, Sanya in China and Hartebeesthoek in South Africa will receive data from the satellite constellation. [71]
Toggle the table of contents South African National Space Agency 10 languages From Wikipedia, the free encyclopedia Space agency of the South African government South African National Space Agency (SANSA) SANSA logo 9 December 2010; 13 years ago (2010-12-09) Type Mr. Patrick Ndlovu, Chairperson of the Board Parent department www.sansa.org.za The South African National Space Agency (SANSA) is South Africa 's government agency responsible for the promotion and development of aeronautics and aerospace space research. It fosters cooperation in space-related activities and research in space science , seeks to advance scientific engineering through human capital , as well as the peaceful use of outer space , and supports the creation of an environment conducive to the industrial development of space technologies within the framework of national government. [2] SANSA was established on 9 December 2010 by the National Space Agency Act . [3] Currently, SANSA's main focusses include using data obtained from remote sensing through satellites and other projects to provide assessment on flooding , fires, resource management and environmental phenomena in South Africa and the African continent. [4] [5] [6] History[ edit ] Hartebeesthoek earth station SANSA was formed after an act of parliament was passed by acting President Kgalema Motlanthe in 2009. The agency was formed with the intent of consolidating space-related research, projects and research in South Africa and to assume the role as a regional centre for space research in Africa. Throughout the 1950s to 1970s lunar and interplanetary missions conducted by NASA had been supported from a tracking station at Hartebeesthoek where the first images of Mars were received from the Mariner IV spacecraft in the first successful flyby of the planet. [7] Other South African facilities also assisted in tracking satellites to determine the effects of the upper atmosphere on their orbits . [7] In 1980s work on the development of a launcher and a satellite had been in progress but was discontinued after 1994. In 1999, South Africa launched its first satellite, SUNSAT from Vandenberg Air Force Base in the US. A second satellite, SumbandilaSat , was launched from the Baikonur Cosmodrome in Kazakhstan in 2009. [8] SANSA's mission is to use space science and technology to: Deliver space-related services and products to the citizens of South Africa and the region. Support, guide and conduct research and development in space science and engineering and the practical application of the innovations they generate. Stimulate interest in science and develop human capacity in space science and technologies in South Africa. Create an environment that promotes industrial development. Nurture space-related partnerships to enhance South Africa's standing in the community of nations. South African National Earth Observation Strategy[ edit ] SANSA is a key contributor to the South African Earth Observation Strategy (SAEOS), for which the primary objective is "to coordinate the collection, assimilation and dissemination of Earth observation data, so that their full potential to support policy, decision-making, economic growth and sustainable development in South Africa can be realised." SANSA will provide space-based data platforms that focus on in-situ Earth observation measurements in collaboration with entities such as the South African Earth Observation Network (SAEON). SANSA Space Weather Centre[ edit ] SANSA Space Science is host to the only Space Weather [9] Regional Warning Centre in Africa, which operates as part of the International Space Environment Service (ISES). The Space Weather Centre provides an important service to the nation by monitoring the sun and its activity to provide information, early warnings and forecasts on space weather conditions. The space weather products and services are required primarily for communication and navigation systems, in the defence, aeronautics, navigation and communication sectors.
NASA satellite of the Explorer program Surface Water and Ocean Topography SWOT satellite Planned: 3 yearsElasped: 1 year, 3 months, 22 days Spacecraft properties Ka-band Radar Interferometer (KaRIn) (JPL) AMR-C Doppler Orbitography and Radiopositioning Integrated by Satellite (DORIS) (CNES) GPSP Global Positioning System Payload (JPL) LRA X-band Telecom (JPL) Alternate SWOT Mission Patch by NASA The Surface Water and Ocean Topography (SWOT) mission is a satellite altimeter jointly developed and operated by NASA and CNES , the French space agency, in partnership with the Canadian Space Agency (CSA) and UK Space Agency (UKSA). [2] The objectives of the mission are to make the first global survey of the Earth's surface water, to observe the fine details of the ocean surface topography , and to measure how terrestrial surface water bodies change over time. While past satellite missions like the Jason series altimeters ( TOPEX/Poseidon , Jason-1 , Jason-2 , Jason-3 ) have provided variation in river and lake water surface elevations at select locations, SWOT will provide the first truly global observations of changing water levels , stream slopes , and inundation extents in rivers, lakes, and floodplains. In the world's oceans, SWOT will observe ocean circulation at unprecedented scales of 15–25 km (9.3–15.5 mi), approximately an order of magnitude finer than current satellites. It does this using synthetic aperture radar interferometry [3] . Because it uses wide-swath altimetry technology, SWOT will almost completely observe the world's oceans and freshwater bodies with repeated high-resolution elevation measurements, allowing observations of variations. Context[ edit ] SWOT builds on a long-standing partnership between NASA and CNES to measure the surface of the ocean using radar altimetry. This partnership began with the TOPEX/Poseidon mission (launched in 1992) and continued with the Jason series. SWOT brings together the hydrology and oceanography communities and will extend the precise, high-resolution surface topography observations into the coastal and estuarine regions. Scientific objectives[ edit ] The mission's science goals are to: Provide sea surface heights and terrestrial water heights over a 120 km (75 mi) wide swath with a ±10 km (6.2 mi) gap at the nadir track. Over the deep oceans, provide sea surface heights within each swath with a posting every 2 km × 2 km (1.2 mi × 1.2 mi), and a precision not to exceed 2.7 cm (1.1 in) at 1 km × 1 km (0.62 mi × 0.62 mi), or 1.35 cm (0.53 in) at 2 km × 2 km (1.2 mi × 1.2 mi) when averaged over the area. Over land, download the raw data for ground processing and produce a water mask able to resolve 100 m (330 ft) wide rivers and 250 m × 250 m (820 ft × 820 ft) lakes and reservoirs. [4] Associated with this mask will be water level elevations with an accuracy of 10 cm (3.9 in) for water bodies whose non-vegetated surface area exceeds 1 km2 (0.39 sq mi). The slope accuracy is 1.7 cm/km (1.1 in/mi) over a maximum 10 km (6.2 mi) of flow distance. [5] The satellite will overfly Earth from 78° S to 78° N, covering at least 86% of the globe. SWOT retraces the same path over the Earth's surface every 21 days, during 292 orbits. [6] It is designed for a mission lifetime of three years. Applications[ edit ] SWOT is designed for the study and monitoring of inland waters and the oceans, such as: [7] Management of water-sharing issues The sharing of river water often causes friction between neighboring states, especially when there is no common technology for verification. SWOT will provide global information as input for systems monitoring transboundary river basins, including measurements of variations in reservoir water storage and estimates of river discharge. More accurate weather and climate forecasting SWOT will enable more accurate weather and climate forecasting, especially seasonally. The quality of weather and climate forecasting largely depends on numerical modeling that uses the state of the ocean surface and the hydrological conditions of catchment areas in their initial and boundary conditions. Managing freshwater for urban, industrial, and agricultural consumption Accurate knowledge of sources of available water is a key factor in decision-making for organizations involved in the distribution of water for agricultural, urban, and industrial needs. Data from SWOT will contribute at a global level by providing water supply services and distribution companies with information about major reservoirs and the largest rivers and catchment areas, thus enabling them to plan the management of water stocks further into the future. Improved flood modeling Flooding, whether from rivers overflowing their banks or in coastal regions, is among the most costly natural disasters. Altimetry data from the SWOT mission will make it possible to measure the 3-dimensional shape of flood waves, track floodwater levels, and improve measurements of local topographic details in floodplains. All of these will improve prediction capabilities for future floods. Coastal ocean dynamics Coastal ocean dynamics are important for many societal applications. They have smaller spatial and temporal scales than the dynamics of the open ocean and require finer-scale monitoring. SWOT will provide global, high-resolution observations in coastal regions for observing coastal currents and storm surges. While SWOT is not designed to monitor the fast temporal changes of the coastal processes, the swath coverage will allow us to characterize the spatial structure of their dynamics when they occur within the swath. Reducing environmental risk and contributing to public policy-making More generally, SWOT will help improve our knowledge of Earth's water cycle and ocean circulation, enhance our observation capacity by collecting unique data on water storage and fluxes and making them freely available, and help us better understand the physics that drives surface water and ocean dynamics. Water resources, natural risks (floods, climate change, hurricane forecasting, etc.), biodiversity, health (preventing the propagation of water-borne diseases), the agricultural sector, energy (including the management of electricity production and offshore gas and oil rigs), territorial development; all these areas and more stand to benefit from this new satellite mission. Sensor payload[ edit ] Diagram of SWOT data collection The primary instrument on SWOT is the Ka-band Radar Interferometer (KaRIn), which uses synthetic-aperture radar (SAR) technology, especially SAR interferometry . [8] Because SWOT operates at Ka-band 's relatively short wavelengths, 11–7 mm (27–43 GHz) – compared to the Ku-band Jason series, 25–17 mm (12–18 GHz) – and at near- nadir incidence angles (<5°), it is designed to be uniquely appropriate for measuring water surface elevations and inundation extents.[ citation needed ] The satellite will fly two radar antennas at either end of a 10 m (33 ft) mast, allowing it to measure the elevation of the surface across a 120 km (75 mi) wide swath. The new radar system is smaller than, but similar to, the one that flew on NASA's Shuttle Radar Topography Mission (SRTM), which made high-resolution measurements of Earth's land surface in 2000. [9] A conventional nadir radar altimeter will also be flown, and measure just beneath the satellite, as was done on the Topex/Poseidon , Jason series, and SARAL missions. It is a "Jason-class" altimeter. [10] History[ edit ] SWOT was developed by an international group of hydrologists and oceanographers to provide a better understanding of the world's oceans and its terrestrial surface waters. [11] It will give scientists their first comprehensive view of Earth's freshwater bodies from space and much more detailed measurements of the ocean surface than ever before. [12] By 2019 the mission hardware was under active construction, algorithms to produce hydrology and oceanography data products were under final development, and calibration/validation methods and post-launch activities were being finalized. The spacecraft launched on a SpaceX Falcon 9 on 16 December 2022. [1] Mating[ edit ] Mating of SWOT Payload on satellite platform made in Thales Alenia Space Cannes Center
Toggle the table of contents Digital elevation model 3D rendering of a DEM of Tithonium Chasma on Mars A digital elevation model (DEM) or digital surface model (DSM) is a 3D computer graphics representation of elevation data to represent terrain or overlaying objects, commonly of a planet , moon , or asteroid . A "global DEM" refers to a discrete global grid . DEMs are used often in geographic information systems (GIS), and are the most common basis for digitally produced relief maps .  A digital terrain model (DTM) represents specifically the ground surface while DEM and DSM may represent tree top canopy or building roofs. While a DSM may be useful for landscape modeling , city modeling and visualization applications, a DTM is often required for flood or drainage modeling, land-use studies , [1] geological applications, and other applications, [2] and in planetary science . Terminology[ edit ] Surfaces represented by a Digital Surface Model include buildings and other objects. Digital Terrain Models represent the bare ground. There is no universal usage of the terms digital elevation model (DEM), digital terrain model (DTM) and digital surface model (DSM) in scientific literature. In most cases the term digital surface model represents the earth's surface and includes all objects on it. In contrast to a DSM, the digital terrain model (DTM) represents the bare ground surface without any objects like plants and buildings (see the figure on the right). [3] [4] DEM is often used as a generic term for DSMs and DTMs, [5] only representing height information without any further definition about the surface. [6] Other definitions equalise the terms DEM and DTM, [7] equalise the terms DEM and DSM, [8] define the DEM as a subset of the DTM, which also represents other morphological elements, [9] or define a DEM as a rectangular grid and a DTM as a three-dimensional model ( TIN ). [10] Most of the data providers ( USGS , ERSDAC , CGIAR , Spot Image ) use the term DEM as a generic term for DSMs and DTMs. Some datasets such as SRTM or the ASTER GDEM are originally DSMs, although in forested areas, SRTM reaches into the tree canopy giving readings somewhere between a DSM and a DTM). DTMs are created from high resolution DSM datasets using complex algorithms to filter out buildings and other objects, a process known as "bare-earth extraction". [11] [12] In the following, the term DEM is used as a generic term for DSMs and DTMs. Types[ edit ] Heightmap of Earth's surface (including water and ice), rendered as an equirectangular projection with elevations indicated as normalized 8-bit grayscale, where lighter values indicate higher elevation A DEM can be represented as a raster (a grid of squares, also known as a heightmap when representing elevation) or as a vector-based triangular irregular network (TIN). [13] The TIN DEM dataset is also referred to as a primary (measured) DEM, whereas the Raster DEM is referred to as a secondary (computed) DEM. [14] The DEM could be acquired through techniques such as photogrammetry , lidar , IfSAR or InSAR , land surveying , etc. (Li et al. 2005). DEMs are commonly built using data collected using remote sensing techniques, but they may also be built from land surveying. Rendering[ edit ] Relief map of Spain's Sierra Nevada, showing use of both shading and false color as visualization tools to indicate elevation The digital elevation model itself consists of a matrix of numbers, but the data from a DEM is often rendered in visual form to make it understandable to humans.  This visualization may be in the form of a contoured topographic map , or could use shading and false color assignment (or "pseudo-color") to render elevations as colors (for example, using green for the lowest elevations, shading to red, with white for the highest elevation.). Visualizations are sometimes also done as oblique views, reconstructing a synthetic visual image of the terrain as it would appear looking down at an angle. In these oblique visualizations, elevations are sometimes scaled using " vertical exaggeration " in order to make subtle elevation differences more noticeable. [15] Some scientists, [16] [17] however, object to vertical exaggeration as misleading the viewer about the true landscape. Production[ edit ] Mappers may prepare digital elevation models in a number of ways, but they frequently use remote sensing rather than direct survey data. Older methods of generating DEMs often involve interpolating digital contour maps that may have been produced by direct survey of the land surface. This method is still used in mountain areas, where interferometry is not always satisfactory. Note that contour line data or any other sampled elevation datasets (by GPS or ground survey) are not DEMs, but may be considered digital terrain models. A DEM implies that elevation is available continuously at each location in the study area. Satellite mapping[ edit ] One powerful technique for generating digital elevation models is interferometric synthetic aperture radar where two passes of a radar satellite (such as RADARSAT-1 or TerraSAR-X or Cosmo SkyMed ), or a single pass if the satellite is equipped with two antennas (like the SRTM instrumentation), collect sufficient data to generate a digital elevation map tens of kilometers on a side with a resolution of around ten meters. [18] Other kinds of stereoscopic pairs can be employed using the digital image correlation method, where two optical images are acquired with different angles taken from the same pass of an airplane or an Earth Observation Satellite (such as the HRS instrument of SPOT5 or the VNIR band of ASTER ). [19] The SPOT 1 satellite (1986) provided the first usable elevation data for a sizeable portion of the planet's landmass, using two-pass stereoscopic correlation. Later, further data were provided by the European Remote-Sensing Satellite (ERS, 1991) using the same method, the Shuttle Radar Topography Mission (SRTM, 2000) using single-pass SAR and the Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER, 2000) instrumentation on the Terra satellite using double-pass stereo pairs. [19] The HRS instrument on SPOT 5 has acquired over 100 million square kilometers of stereo pairs. Planetary mapping[ edit ] MOLA digital elevation model showing the two hemispheres of Mars. This image appeared on the cover of Science magazine in May 1999. A tool of increasing value in planetary science has been use of orbital altimetry used to make digital elevation map of planets.  A primary tool for this is laser altimetry but radar altimetry is also used. [20] Planetary digital elevation maps made using laser altimetry include the Mars Orbiter Laser Altimeter (MOLA) mapping of Mars, [21] the Lunar Orbital Laser Altimeter (LOLA) [22] and Lunar Altimeter (LALT) mapping of the Moon, and the Mercury Laser Altimeter (MLA) mapping of Mercury. [23] In planetary mapping, each planetary body has a unique reference surface. [24] Methods for obtaining elevation data used to create DEMs[ edit ]
Toggle the table of contents Geographic information system From Wikipedia, the free encyclopedia System to capture, manage and present geographic data "GIS" redirects here. For other uses, see GIS (disambiguation) . ) Basic GIS concept A geographic information system (GIS) consists of integrated computer hardware and software that store, manage, analyze , edit, output, and visualize geographic data . [1] [2] Much of this often happens within a spatial database , however, this is not essential to meet the definition of a GIS. [1] In a broader sense, one may consider such a system also to include human users and support staff, procedures and workflows, the body of knowledge of relevant concepts and methods, and institutional organizations. The uncounted plural, geographic information systems, also abbreviated GIS, is the most common term for the industry and profession concerned with these systems. It is roughly synonymous with geoinformatics . The academic discipline that studies these systems and their underlying geographic principles, may also be abbreviated as GIS, but the unambiguous GIScience is more common. [3] GIScience is often considered a subdiscipline of geography within the branch of technical geography . Geographic information systems are utilized in multiple technologies, processes, techniques and methods. They are attached to various operations and numerous applications, that relate to: engineering, planning, management, transport/logistics, insurance, telecommunications, and business. [4] For this reason, GIS and location intelligence applications are at the foundation of location-enabled services, which rely on geographic analysis and visualization. GIS provides the capability to relate previously unrelated information, through the use of location as the "key index variable". Locations and extents that are found in the Earth's spacetime are able to be recorded through the date and time of occurrence, along with x, y, and z coordinates ; representing, longitude (x), latitude (y), and elevation (z). All Earth-based, spatial–temporal, location and extent references should be relatable to one another, and ultimately, to a "real" physical location or extent. This key characteristic of GIS has begun to open new avenues of scientific inquiry and studies. History and development[ edit ] While digital GIS dates to the mid-1960s, when Roger Tomlinson first coined the phrase "geographic information system", [5] many of the geographic concepts and methods that GIS automates date back decades earlier. E. W. Gilbert 's version (1958) of John Snow 's 1855 map of the Soho cholera outbreak showing the clusters of cholera cases in the London epidemic of 1854 One of the first known instances in which spatial analysis was used came from the field of epidemiology in the "Rapport sur la marche et les effets du choléra dans Paris et le département de la Seine " (1832). [6] French geographer and cartographer , Charles Picquet created a map outlining the forty-eight Districts in Paris , using halftone color gradients, to provide a visual representation for the number of reported deaths due to cholera per every 1,000 inhabitants. In 1854, John Snow , an epidemiologist and physician, was able to determine the source of a cholera outbreak in London through the use of spatial analysis. Snow achieved this through plotting the residence of each casualty on a map of the area, as well as the nearby water sources. Once these points were marked, he was able to identify the water source within the cluster that was responsible for the outbreak. This was one of the earliest successful uses of a geographic methodology in pinpointing the source of an outbreak in epidemiology. While the basic elements of topography and theme existed previously in cartography , Snow's map was unique due to his use of cartographic methods, not only to depict, but also to analyze clusters of geographically dependent phenomena. The early 20th century saw the development of photozincography , which allowed maps to be split into layers, for example one layer for vegetation and another for water. This was particularly used for printing contours – drawing these was a labour-intensive task but having them on a separate layer meant they could be worked on without the other layers to confuse the draughtsman . This work was initially drawn on glass plates, but later plastic film was introduced, with the advantages of being lighter, using less storage space and being less brittle, among others. When all the layers were finished, they were combined into one image using a large process camera. Once color printing came in, the layers idea was also used for creating separate printing plates for each color. While the use of layers much later became one of the typical features of a contemporary GIS, the photographic process just described is not considered a GIS in itself -– as the maps were just images with no database to link them to. Two additional developments are notable in the early days of GIS: Ian McHarg's publication "Design with Nature" [7] and its map overlay method and the introduction of a street network into the U.S. Census Bureau's DIME (Dual Independent Map Encoding) system. [8] The first publication detailing the use of computers to facilitate cartography was written by Waldo Tobler in 1959. [9] Further computer hardware development spurred by nuclear weapon research led to more widespread general-purpose computer "mapping" applications by the early 1960s. [10] In 1963 the world's first true operational GIS was developed in Ottawa, Ontario , Canada, by the federal Department of Forestry and Rural Development. Developed by Roger Tomlinson , it was called the Canada Geographic Information System (CGIS) and was used to store, analyze, and manipulate data collected for the Canada Land Inventory , an effort to determine the land capability for rural Canada by mapping information about soils , agriculture, recreation, wildlife, waterfowl , forestry and land use at a scale of 1:50,000. A rating classification factor was also added to permit analysis. [11] [12] CGIS was an improvement over "computer mapping" applications as it provided capabilities for data storage, overlay, measurement, and digitizing /scanning. It supported a national coordinate system that spanned the continent, coded lines as arcs having a true embedded topology and it stored the attribute and locational information in separate files. As a result of this, Tomlinson has become known as the "father of GIS", particularly for his use of overlays in promoting the spatial analysis of convergent geographic data. [13] CGIS lasted into the 1990s and built a large digital land resource database in Canada. It was developed as a mainframe -based system in support of federal and provincial resource planning and management. Its strength was continent-wide analysis of complex datasets . The CGIS was never available commercially. In 1964 Howard T. Fisher formed the Laboratory for Computer Graphics and Spatial Analysis at the Harvard Graduate School of Design (LCGSA 1965–1991), where a number of important theoretical concepts in spatial data handling were developed, and which by the 1970s had distributed seminal software code and systems, such as SYMAP, GRID, and ODYSSEY, to universities, research centers and corporations worldwide. [14] These programs were the first examples of general purpose GIS software that was not developed for a particular installation, and was very influential on future commercial software, such as Esri ARC/INFO , released in 1983. By the late 1970s two public domain GIS systems ( MOSS and GRASS GIS ) were in development, and by the early 1980s, M&S Computing (later Intergraph ) along with Bentley Systems Incorporated for the CAD platform, Environmental Systems Research Institute ( ESRI ), CARIS (Computer Aided Resource Information System), and ERDAS (Earth Resource Data Analysis System) emerged as commercial vendors of GIS software, successfully incorporating many of the CGIS features, combining the first generation approach to separation of spatial and attribute information with a second generation approach to organizing attribute data into database structures. [15] In 1986, Mapping Display and Analysis System (MIDAS), the first desktop GIS product [16] was released for the DOS operating system. This was renamed in 1990 to MapInfo for Windows when it was ported to the Microsoft Windows platform. This began the process of moving GIS from the research department into the business environment. By the end of the 20th century, the rapid growth in various systems had been consolidated and standardized on relatively few platforms and users were beginning to explore viewing GIS data over the Internet , requiring data format and transfer standards. More recently, a growing number of free, open-source GIS packages run on a range of operating systems and can be customized to perform specific tasks. The major trend of the 21st Century has been the integration of GIS capabilities with other Information technology and Internet infrastructure, such as relational databases , cloud computing , software as a service (SAAS), and mobile computing . [17] Main article: Geographic information system software The distinction must be made between a singular geographic information system, which is a single installation of software and data for a particular use, along with associated hardware, staff, and institutions (e.g., the GIS for a particular city government); and GIS software , a general-purpose application program that is intended to be used in many individual geographic information systems in a variety of application domains. [18] : 16 Starting in the late 1970s, many software packages have been created specifically for GIS applications. Esri's ArcGIS , which includes ArcGIS Pro and the legacy software ArcMap , currently dominates the GIS Market. Other examples of GIS include Autodesk and MapInfo Professional and open source programs such as QGIS , GRASS GIS , MapGuide , and Hadoop-GIS . [19] These and other desktop GIS applications include a full suite of capabilities for entering, managing, analyzing, and visualizing geographic data, and are designed to be used on their own. Starting in the late 1990s with the emergence of the Internet , as computer network technology progressed, GIS infrastructure and data began to move to servers , providing another mechanism for providing GIS capabilities. [20] : 216 This was facilitated by standalone software installed on a server, similar to other server software such as HTTP servers and relational database management systems , enabling clients to have access to GIS data and processing tools without having to install specialized desktop software. These networks are known as distributed GIS . [21] [22] This strategy has been extended through the Internet and development of cloud-based GIS platforms such as ArcGIS Online and GIS-specialized software as a service (SAAS). The use of the Internet to facilitate distributed GIS is known as Internet GIS . [21] [22] An alternative approach is the integration of some or all of these capabilities into other software or information technology architectures. One example is a spatial extension to Object-relational database software, which defines a geometry datatype so that spatial data can be stored in relational tables, and extensions to SQL for spatial analysis operations such as overlay . Another example is the proliferation of geospatial libraries and application programming interfaces (e.g., GDAL , Leaflet , D3.js ) that extend programming languages to enable the incorporation of GIS data and processing into custom software, including web mapping sites and location-based services in smartphones . Geospatial data management[ edit ] The core of any GIS is a database that contains representations of geographic phenomena, modeling their geometry (location and shape) and their properties or attributes. A GIS database may be stored in a variety of forms, such as a collection of separate data files or a single spatially-enabled relational database . Collecting and managing these data usually constitutes the bulk of the time and financial resources of a project, far more than other aspects such as analysis and mapping. [20] : 175 Aspects of geographic data[ edit ] GIS uses spatio-temporal ( space-time ) location as the key index variable for all other information. Just as a relational database containing text or numbers can relate many different tables using common key index variables, GIS can relate otherwise unrelated information by using location as the key index variable. The key is the location and/or extent in space-time. Any variable that can be located spatially, and increasingly also temporally, can be referenced using a GIS. Locations or extents in Earth space–time may be recorded as dates/times of occurrence, and x, y, and z coordinates representing, longitude , latitude , and elevation , respectively. These GIS coordinates may represent other quantified systems of temporo-spatial reference (for example, film frame number, stream gage station, highway mile-marker, surveyor benchmark, building address, street intersection, entrance gate, water depth sounding, POS or CAD drawing origin/units). Units applied to recorded temporal-spatial data can vary widely (even when using exactly the same data, see map projections ), but all Earth-based spatial–temporal location and extent references should, ideally, be relatable to one another and ultimately to a "real" physical location or extent in space–time. Related by accurate spatial information, an incredible variety of real-world and projected past or future data can be analyzed, interpreted and represented. [23] This key characteristic of GIS has begun to open new avenues of scientific inquiry into behaviors and patterns of real-world information that previously had not been systematically correlated . Main articles: Data model (GIS) and GIS file formats GIS data represents phenomena that exist in the real world, such as roads, land use, elevation, trees, waterways, and states. The most common types of phenomena that are represented in data can be divided into two conceptualizations: discrete objects (e.g., a house, a road) and continuous fields (e.g., rainfall amount or population density). [20] : 62–65 Other types of geographic phenomena, such as events (e.g., location of World War II battles), processes (e.g., extent of suburbanization ), and masses (e.g., types of soil in an area) are represented less commonly or indirectly, or are modeled in analysis procedures rather than data. Traditionally, there are two broad methods used to store data in a GIS for both kinds of abstractions mapping references: raster images and vector . Points, lines, and polygons represent vector data of mapped location attribute references. A new hybrid method of storing data is that of identifying point clouds, which combine three-dimensional points with RGB information at each point, returning a " 3D color image ". GIS thematic maps then are becoming more and more realistically visually descriptive of what they set out to show or determine. Data acquisition[ edit ] Example of hardware for mapping ( GPS and laser rangefinder ) and data collection ( rugged computer ). The current trend for geographical information system (GIS) is that accurate mapping and data analysis are completed while in the field. Depicted hardware ( field-map technology) is used mainly for forest inventories , monitoring and mapping. GIS data acquisition includes several methods for gathering spatial data into a GIS database, which can be grouped into three categories: primary data capture, the direct measurement phenomena in the field (e.g., remote sensing , the global positioning system ); secondary data capture, the extraction of information from existing sources that are not in a GIS form, such as paper maps, through digitization ; and data transfer , the copying of existing GIS data from external sources such as government agencies and private companies. All of these methods can consume significant time, finances, and other resources. [20] : 173 Primary data capture[ edit ] Survey data can be directly entered into a GIS from digital data collection systems on survey instruments using a technique called coordinate geometry (COGO) . Positions from a global navigation satellite system ( GNSS ) like Global Positioning System can also be collected and then imported into a GIS. A current trend in data collection gives users the ability to utilize field computers with the ability to edit live data using wireless connections or disconnected editing sessions. [24] Current trend is to utilize applications available on smartphones and PDAs - Mobile GIS. [25] This has been enhanced by the availability of low-cost mapping-grade GPS units with decimeter accuracy in real time. This eliminates the need to post process, import, and update the data in the office after fieldwork has been collected. This includes the ability to incorporate positions collected using a laser rangefinder . New technologies also allow users to create maps as well as analysis directly in the field, making projects more efficient and mapping more accurate. Remotely sensed data also plays an important role in data collection and consist of sensors attached to a platform. Sensors include cameras, digital scanners and lidar , while platforms usually consist of aircraft and satellites . In England in the mid 1990s, hybrid kite/balloons called helikites first pioneered the use of compact airborne digital cameras as airborne geo-information systems. Aircraft measurement software, accurate to 0.4 mm was used to link the photographs and measure the ground. Helikites are inexpensive and gather more accurate data than aircraft. Helikites can be used over roads, railways and towns where unmanned aerial vehicles (UAVs) are banned. Recently aerial data collection has become more accessible with miniature UAVs and drones. For example, the Aeryon Scout was used to map a 50-acre area with a ground sample distance of 1 inch (2.54 cm) in only 12 minutes. [26] The majority of digital data currently comes from photo interpretation of aerial photographs. Soft-copy workstations are used to digitize features directly from stereo pairs of digital photographs. These systems allow data to be captured in two and three dimensions, with elevations measured directly from a stereo pair using principles of photogrammetry . Analog aerial photos must be scanned before being entered into a soft-copy system, for high-quality digital cameras this step is skipped. Satellite remote sensing provides another important source of spatial data. Here satellites use different sensor packages to passively measure the reflectance from parts of the electromagnetic spectrum or radio waves that were sent out from an active sensor such as radar. Remote sensing collects raster data that can be further processed using different bands to identify objects and classes of interest, such as land cover. Secondary data capture[ edit ] Further information: Digitizing The most common method of data creation is digitization , where a hard copy map or survey plan is transferred into a digital medium through the use of a CAD program, and geo-referencing capabilities. With the wide availability of ortho-rectified imagery (from satellites, aircraft, Helikites and UAVs), heads-up digitizing is becoming the main avenue through which geographic data is extracted. Heads-up digitizing involves the tracing of geographic data directly on top of the aerial imagery instead of by the traditional method of tracing the geographic form on a separate digitizing tablet (heads-down digitizing). Heads-down digitizing, or manual digitizing, uses a special magnetic pen, or stylus, that feeds information into a computer to create an identical, digital map. Some tablets use a mouse-like tool, called a puck, instead of a stylus. [27] [28] The puck has a small window with cross-hairs which allows for greater precision and pinpointing map features. Though heads-up digitizing is more commonly used, heads-down digitizing is still useful for digitizing maps of poor quality. [28] Existing data printed on paper or PET film maps can be digitized or scanned to produce digital data. A digitizer produces vector data as an operator traces points, lines, and polygon boundaries from a map. Scanning a map results in raster data that could be further processed to produce vector data. When data is captured, the user should consider if the data should be captured with either a relative accuracy or absolute accuracy, since this could not only influence how information will be interpreted but also the cost of data capture. After entering data into a GIS, the data usually requires editing, to remove errors, or further processing. For vector data it must be made "topologically correct" before it can be used for some advanced analysis. For example, in a road network, lines must connect with nodes at an intersection. Errors such as undershoots and overshoots must also be removed. For scanned maps, blemishes on the source map may need to be removed from the resulting raster . For example, a fleck of dirt might connect two lines that should not be connected. Projections, coordinate systems, and registration[ edit ] Main article: Spatial reference system The earth can be represented by various models, each of which may provide a different set of coordinates (e.g., latitude, longitude, elevation) for any given point on the Earth's surface. The simplest model is to assume the earth is a perfect sphere. As more measurements of the earth have accumulated, the models of the earth have become more sophisticated and more accurate. In fact, there are models called datums that apply to different areas of the earth to provide increased accuracy, like North American Datum of 1983 for U.S. measurements, and the World Geodetic System for worldwide measurements. The latitude and longitude on a map made against a local datum may not be the same as one obtained from a GPS receiver . Converting coordinates from one datum to another requires a datum transformation such as a Helmert transformation , although in certain situations a simple translation may be sufficient. [29] In popular GIS software, data projected in latitude/longitude is often represented as a Geographic coordinate system . For example, data in latitude/longitude if the datum is the ' North American Datum of 1983' is denoted by 'GCS North American 1983'. Further information: Data quality While no digital model can be a perfect representation of the real world, it is important that GIS data be of a high quality. In keeping with the principle of homomorphism , the data must be close enough to reality so that the results of GIS procedures correctly correspond to the results of real world processes. This means that there is no single standard for data quality, because the necessary degree of quality depends on the scale and purpose of the tasks for which it is to be used. Several elements of data quality are important to GIS data: Accuracy The degree of similarity between a represented measurement and the actual value; conversely, error is the amount of difference between them. [18] : 623 In GIS data, there is concern for accuracy in representations of location (positional accuracy), property (attribute accuracy), and time. For example, the US 2020 Census says that the population of Houston on April 1, 2020 was 2,304,580; if it was actually 2,310,674, this would be an error and thus a lack of attribute accuracy. Precision The degree of refinement in a represented value. In a quantitative property, this is the number of significant digits in the measured value. [20] : 115 An imprecise value is vague or ambiguous, including a range of possible values. For example, if one were to say that the population of Houston on April 1, 2020 was "about 2.3 million," this statement would be imprecise, but likely accurate because the correct value (and many incorrect values) are included. As with accuracy, representations of location, property, and time can all be more or less precise. Resolution is a commonly used expression of positional precision, especially in raster data sets. Uncertainty A general acknowledgement of the presence of error and imprecision in geographic data. [20] : 99 That is, it is a degree of general doubt, given that it is difficult to know exactly how much error is present in a data set, although some form of estimate may be attempted (a confidence interval being such an estimate of uncertainty). This is sometimes used as a collective term for all or most aspects of data quality. Vagueness or fuzziness The degree to which an aspect (location, property, or time) of a phenomenon is inherently imprecise, rather than the imprecision being in a measured value. [20] : 103 For example, the spatial extent of the Houston metropolitan area is vague, as there are places on the outskirts of the city that are less connected to the central city (measured by activities such as commuting ) than places that are closer. Mathematical tools such as fuzzy set theory are commonly used to manage vagueness in geographic data. Completeness The degree to which a data set represents all of the actual features that it purports to include. [18] : 623 For example, if a layer of "roads in Houston " is missing some actual streets, it is incomplete. Currency The most recent point in time at which a data set claims to be an accurate representation of reality. This is a concern for the majority of GIS applications, which attempt to represent the world "at present," in which case older data is of lower quality. Consistency The degree to which the representations of the many phenomena in a data set correctly correspond with each other. [18] : 623 Consistency in topological relationships between spatial objects is an especially important aspect of consistency. [30] : 117 For example, if all of the lines in a street network were accidentally moved 10 meters to the East, they would be inaccurate but still consistent, because they would still properly connect at each intersection, and network analysis tools such as shortest path would still give correct results. Propagation of uncertainty The degree to which the quality of the results of Spatial analysis methods and other processing tools derives from the quality of input data. [30] : 118 For example, interpolation is a common operation used in many ways in GIS; because it generates estimates of values between known measurements, the results will always be more precise, but less certain (as each estimate has an unknown amount of error). GIS accuracy depends upon source data, and how it is encoded to be data referenced. Land surveyors have been able to provide a high level of positional accuracy utilizing the GPS -derived positions. [31] High-resolution digital terrain and aerial imagery, [32] powerful computers and Web technology are changing the quality, utility, and expectations of GIS to serve society on a grand scale, but nevertheless there are other source data that affect overall GIS accuracy like paper maps, though these may be of limited use in achieving the desired accuracy. In developing a digital topographic database for a GIS, topographical maps are the main source, and aerial photography and satellite imagery are extra sources for collecting data and identifying attributes which can be mapped in layers over a location facsimile of scale. The scale of a map and geographical rendering area representation type, or map projection , are very important aspects since the information content depends mainly on the scale set and resulting locatability of the map's representations. In order to digitize a map, the map has to be checked within theoretical dimensions, then scanned into a raster format, and resulting raster data has to be given a theoretical dimension by a rubber sheeting/warping technology process known as georeferencing . A quantitative analysis of maps brings accuracy issues into focus. The electronic and other equipment used to make measurements for GIS is far more precise than the machines of conventional map analysis. All geographical data are inherently inaccurate, and these inaccuracies will propagate through GIS operations in ways that are difficult to predict. [33] Raster-to-vector translation[ edit ] Data restructuring can be performed by a GIS to convert data into different formats. For example, a GIS may be used to convert a satellite image map to a vector structure by generating lines around all cells with the same classification, while determining the cell spatial relationships, such as adjacency or inclusion. More advanced data processing can occur with image processing , a technique developed in the late 1960s by NASA and the private sector to provide contrast enhancement, false color rendering and a variety of other techniques including use of two dimensional Fourier transforms . Since digital data is collected and stored in various ways, the two data sources may not be entirely compatible. So a GIS must be able to convert geographic data from one structure to another. In so doing, the implicit assumptions behind different ontologies and classifications require analysis. [34] Object ontologies have gained increasing prominence as a consequence of object-oriented programming and sustained work by Barry Smith and co-workers. Spatial ETL[ edit ] Spatial ETL tools provide the data processing functionality of traditional extract, transform, load (ETL) software, but with a primary focus on the ability to manage spatial data. They provide GIS users with the ability to translate data between different standards and proprietary formats, whilst geometrically transforming the data en route. These tools can come in the form of add-ins to existing wider-purpose software such as spreadsheets . Further information: Spatial analysis GIS spatial analysis is a rapidly changing field, and GIS packages are increasingly including analytical tools as standard built-in facilities, as optional toolsets, as add-ins or 'analysts'. In many instances these are provided by the original software suppliers (commercial vendors or collaborative non commercial development teams), while in other cases facilities have been developed and are provided by third parties. Furthermore, many products offer software development kits (SDKs), programming languages and language support, scripting facilities and/or special interfaces for developing one's own analytical tools or variants. The increased availability has created a new dimension to business intelligence termed " spatial intelligence " which, when openly delivered via intranet, democratizes access to geographic and social network data. Geospatial intelligence , based on GIS spatial analysis, has also become a key element for security. GIS as a whole can be described as conversion to a vectorial representation or to any other digitisation process. Geoprocessing is a GIS operation used to manipulate spatial data. A typical geoprocessing operation takes an input dataset , performs an operation on that dataset, and returns the result of the operation as an output dataset. Common geoprocessing operations include geographic feature overlay, feature selection and analysis, topology processing, raster processing, and data conversion. Geoprocessing allows for definition, management, and analysis of information used to form decisions. [35] Terrain analysis[ edit ] Hillshade model derived from a digital elevation model of the Valestra area in the northern Apennines (Italy) See also: Surface gradient Many geographic tasks involve the terrain , the shape of the surface of the earth, such as hydrology , earthworks , and biogeography . Thus, terrain data is often a core dataset in a GIS, usually in the form of a raster Digital elevation model (DEM) or a Triangulated irregular network (TIN). A variety of tools are available in most GIS software for analyzing terrain, often by creating derivative datasets that represent a specific aspect of the surface. Some of the most common include: Slope or grade is the steepness or gradient of a unit of terrain, usually measured as an angle in degrees or as a percentage. [36] Aspect can be defined as the direction in which a unit of terrain faces. Aspect is usually expressed in degrees from north. [37] Cut and fill is a computation of the difference between the surface before and after an excavation project to estimate costs. Hydrological modeling can provide a spatial element that other hydrological models lack, with the analysis of variables such as slope, aspect and watershed or catchment area . [38] Terrain analysis is fundamental to hydrology, since water always flows down a slope. [38] As basic terrain analysis of a digital elevation model (DEM) involves calculation of slope and aspect, DEMs are very useful for hydrological analysis. Slope and aspect can then be used to determine direction of surface runoff , and hence flow accumulation for the formation of streams, rivers and lakes. Areas of divergent flow can also give a clear indication of the boundaries of a catchment. Once a flow direction and accumulation matrix has been created, queries can be performed that show contributing or dispersal areas at a certain point. [38] More detail can be added to the model, such as terrain roughness, vegetation types and soil types, which can influence infiltration and evapotranspiration rates, and hence influencing surface flow. One of the main uses of hydrological modeling is in environmental contamination research . Other applications of hydrological modeling include groundwater and surface water mapping , as well as flood risk maps. Viewshed analysis predicts the impact that terrain has on the visibility between locations, which is especially important for wireless communications. Shaded relief is a depiction of the surface as if it were a three dimensional model lit from a given direction, which is very commonly used in maps. Most of these are generated using algorithms that are discrete simplifications of vector calculus . Slope, aspect, and surface curvature in terrain analysis are all derived from neighborhood operations using elevation values of a cell's adjacent neighbours. [39] Each of these is strongly affected by the level of detail in the terrain data, such as the resolution of a DEM, which should be chosen carefully. [40] Main article: Proximity analysis Distance is a key part of solving many geographic tasks, usually due to the friction of distance . Thus, a wide variety of analysis tools have analyze distance in some form, such as buffers , Voronoi or Thiessen polygons , Cost distance analysis , and network analysis . Data analysis[ edit ] It is difficult to relate wetlands maps to rainfall amounts recorded at different points such as airports, television stations, and schools. A GIS, however, can be used to depict two- and three-dimensional characteristics of the Earth's surface, subsurface, and atmosphere from information points. For example, a GIS can quickly generate a map with isopleth or contour lines that indicate differing amounts of rainfall. Such a map can be thought of as a rainfall contour map. Many sophisticated methods can estimate the characteristics of surfaces from a limited number of point measurements. A two-dimensional contour map created from the surface modeling of rainfall point measurements may be overlaid and analyzed with any other map in a GIS covering the same area. This GIS derived map can then provide additional information - such as the viability of water power potential as a renewable energy source. Similarly, GIS can be used to compare other renewable energy resources to find the best geographic potential for a region. [41] Additionally, from a series of three-dimensional points, or digital elevation model , isopleth lines representing elevation contours can be generated, along with slope analysis, shaded relief , and other elevation products. Watersheds can be easily defined for any given reach, by computing all of the areas contiguous and uphill from any given point of interest. Similarly, an expected thalweg of where surface water would want to travel in intermittent and permanent streams can be computed from elevation data in the GIS. Topological modeling[ edit ] A GIS can recognize and analyze the spatial relationships that exist within digitally stored spatial data. These topological relationships allow complex spatial modelling and analysis to be performed. Topological relationships between geometric entities traditionally include adjacency (what adjoins what), containment (what encloses what), and proximity (how close something is to something else). Main article: Transport network analysis Geometric networks are linear networks of objects that can be used to represent interconnected features, and to perform special spatial analysis on them. A geometric network is composed of edges, which are connected at junction points, similar to graphs in mathematics and computer science. Just like graphs, networks can have weight and flow assigned to its edges, which can be used to represent various interconnected features more accurately. Geometric networks are often used to model road networks and public utility networks, such as electric, gas, and water networks. Network modeling is also commonly employed in transportation planning , hydrology modeling, and infrastructure modeling. Main article: Map algebra An example of use of layers in a GIS application. In this example, the forest-cover layer (light green) forms the bottom layer, with the topographic layer (contour lines) over it. Next up is a standing water layer (pond, lake) and then a flowing water layer (stream, river), followed by the boundary layer and finally the road layer on top. The order is very important in order to properly display the final result. Note that the ponds are layered under the streams, so that a stream line can be seen overlying one of the ponds. Dana Tomlin coined the term "cartographic modeling" in his PhD dissertation (1983); he later used it in the title of his book, Geographic Information Systems and Cartographic Modeling (1990). [42] Cartographic modeling refers to a process where several thematic layers of the same area are produced, processed, and analyzed. Tomlin used raster layers, but the overlay method (see below) can be used more generally. Operations on map layers can be combined into algorithms, and eventually into simulation or optimization models. Main articles: Vector overlay and Map algebra The combination of several spatial datasets (points, lines, or polygons ) creates a new output vector dataset, visually similar to stacking several maps of the same region. These overlays are similar to mathematical Venn diagram overlays. A union overlay combines the geographic features and attribute tables of both inputs into a single new output. An intersect overlay defines the area where both inputs overlap and retains a set of attribute fields for each. A symmetric difference overlay defines an output area that includes the total area of both inputs except for the overlapping area. Data extraction is a GIS process similar to vector overlay, though it can be used in either vector or raster data analysis. Rather than combining the properties and features of both datasets, data extraction involves using a "clip" or "mask" to extract the features of one data set that fall within the spatial extent of another dataset. In raster data analysis, the overlay of datasets is accomplished through a process known as "local operation on multiple rasters" or " map algebra ", through a function that combines the values of each raster's matrix . This function may weigh some inputs more than others through use of an "index model" that reflects the influence of various factors upon a geographic phenomenon. Main article: Geostatistics Geostatistics is a branch of statistics that deals with field data, spatial data with a continuous index. It provides methods to model spatial correlation, and predict values at arbitrary locations (interpolation). When phenomena are measured, the observation methods dictate the accuracy of any subsequent analysis. Due to the nature of the data (e.g. traffic patterns in an urban environment; weather patterns over the Pacific Ocean ), a constant or dynamic degree of precision is always lost in the measurement. This loss of precision is determined from the scale and distribution of the data collection. To determine the statistical relevance of the analysis, an average is determined so that points (gradients) outside of any immediate measurement can be included to determine their predicted behavior. This is due to the limitations of the applied statistic and data collection methods, and interpolation is required to predict the behavior of particles, points, and locations that are not directly measurable. Interpolation is the process by which a surface is created, usually a raster dataset, through the input of data collected at a number of sample points. There are several forms of interpolation, each which treats the data differently, depending on the properties of the data set. In comparing interpolation methods, the first consideration should be whether or not the source data will change (exact or approximate). Next is whether the method is subjective, a human interpretation, or objective. Then there is the nature of transitions between points: are they abrupt or gradual. Finally, there is whether a method is global (it uses the entire data set to form the model), or local where an algorithm is repeated for a small section of terrain. Interpolation is a justified measurement because of a spatial autocorrelation principle that recognizes that data collected at any position will have a great similarity to, or influence of those locations within its immediate vicinity.
Laser[ edit ] 600–1000 nm lasers are most common for non-scientific applications. The maximum power of the laser is limited, or an automatic shut-off system which turns the laser off at specific altitudes is used in order to make it eye-safe for the people on the ground. One common alternative, 1550 nm lasers, are eye-safe at relatively high power levels since this wavelength is not strongly absorbed by the eye. A trade-off though is that current detector technology is less advanced, so these wavelengths are generally used at longer ranges with lower accuracies. They are also used for military applications because 1550 nm is not visible in night vision goggles , unlike the shorter 1000 nm infrared laser. Airborne topographic mapping lidars generally use 1064 nm diode-pumped YAG lasers, while bathymetric (underwater depth research) systems generally use 532 nm frequency-doubled diode pumped YAG lasers because 532 nm penetrates water with much less attenuation than 1064 nm. Laser settings include the laser repetition rate (which controls the data collection speed). Pulse length is generally an attribute of the laser cavity length, the number of passes required through the gain material (YAG, YLF , etc.), and Q-switch (pulsing) speed. Better target resolution is achieved with shorter pulses, provided the lidar receiver detectors and electronics have sufficient bandwidth. [6] Phased arrays[ edit ] A phased array can illuminate any direction by using a microscopic array of individual antennas. Controlling the timing (phase) of each antenna steers a cohesive signal in a specific direction. Phased arrays have been used in radar since the 1940s. The same technique can be used with light. On the order of a million optical antennas are used to see a radiation pattern of a certain size in a certain direction. The system is controlled by timing the precise flash. A single chip (or a few) replace a US$75,000 electromechanical system, drastically reducing costs. [30] Several companies are working on developing commercial solid-state lidar units. [31] The control system can change the shape of the lens to enable zoom in and zoom out functions. Specific sub-zones can be targeted at sub-second intervals. [30] Electromechanical lidar lasts for between 1,000 and 2,000 hours. By contrast, solid-state lidar can run for 100,000 hours. [30] Microelectromechanical machines[ edit ] Microelectromechanical mirrors (MEMS) are not entirely solid-state. However, their tiny form factor provides many of the same cost benefits. A single laser is directed to a single mirror that can be reoriented to view any part of the target field. The mirror spins at a rapid rate. However, MEMS systems generally operate in a single plane (left to right). To add a second dimension generally requires a second mirror that moves up and down. Alternatively, another laser can hit the same mirror from another angle. MEMS systems can be disrupted by shock/vibration and may require repeated calibration. [30] Scanner and optics[ edit ] Image development speed is affected by the speed at which they are scanned. Options to scan the azimuth and elevation include dual oscillating plane mirrors, a combination with a polygon mirror, and a dual axis scanner . Optic choices affect the angular resolution and range that can be detected. A hole mirror or a beam splitter are options to collect a return signal. Photodetector and receiver electronics[ edit ] Two main photodetector technologies are used in lidar: solid state photodetectors, such as silicon avalanche photodiodes , or photomultipliers . The sensitivity of the receiver is another parameter that has to be balanced in a lidar design. Position and navigation systems[ edit ] Lidar sensors mounted on mobile platforms such as airplanes or satellites require instrumentation to determine the absolute position and orientation of the sensor. Such devices generally include a Global Positioning System receiver and an inertial measurement unit (IMU). Sensor[ edit ] Lidar uses active sensors that supply their own illumination source. The energy source hits objects and the reflected energy is detected and measured by sensors. Distance to the object is determined by recording the time between transmitted and backscattered pulses and by using the speed of light to calculate the distance traveled. [32] Flash lidar allows for 3-D imaging because of the camera's ability to emit a larger flash and sense the spatial relationships and dimensions of area of interest with the returned energy. This allows for more accurate imaging because the captured frames do not need to be stitched together, and the system is not sensitive to platform motion. This results in less distortion. [33] 3-D imaging can be achieved using both scanning and non-scanning systems. "3-D gated viewing laser radar" is a non-scanning laser ranging system that applies a pulsed laser and a fast gated camera. Research has begun for virtual beam steering using Digital Light Processing (DLP) technology. Imaging lidar can also be performed using arrays of high speed detectors and modulation sensitive detector arrays typically built on single chips using complementary metal–oxide–semiconductor (CMOS) and hybrid CMOS/ Charge-coupled device (CCD) fabrication techniques. In these devices each pixel performs some local processing such as demodulation or gating at high speed, downconverting the signals to video rate so that the array can be read like a camera. Using this technique many thousands of pixels / channels may be acquired simultaneously. [34] High resolution 3-D lidar cameras use homodyne detection with an electronic CCD or CMOS shutter . [35] A coherent imaging lidar uses synthetic array heterodyne detection to enable a staring single element receiver to act as though it were an imaging array. [36] In 2014, Lincoln Laboratory announced a new imaging chip with more than 16,384 pixels, each able to image a single photon, enabling them to capture a wide area in a single image. An earlier generation of the technology with one fourth as many pixels was dispatched by the U.S. military after the January 2010 Haiti earthquake. A single pass by a business jet at 3,000 m (10,000 ft) over Port-au-Prince was able to capture instantaneous snapshots of 600 m (2,000 ft) squares of the city at a resolution of 30 cm (1 ft), displaying the precise height of rubble strewn in city streets. [37] The new system is ten times better, and could produce much larger maps more quickly. The chip uses indium gallium arsenide (InGaAs), which operates in the infrared spectrum at a relatively long wavelength that allows for higher power and longer ranges. In many applications, such as self-driving cars, the new system will lower costs by not requiring a mechanical component to aim the chip. InGaAs uses less hazardous wavelengths than conventional silicon detectors, which operate at visual wavelengths. [38] New technologies for infrared single-photon counting LIDAR are advancing rapidly, including arrays and cameras in a variety of semiconductor and superconducting platforms. [39] Flash lidar[ edit ] In flash lidar, the entire field of view is illuminated with a wide diverging laser beam in a single pulse. This is in contrast to conventional scanning lidar, which uses a collimated laser beam that illuminates a single point at a time, and the beam is raster scanned to illuminate the field of view point-by-point. This illumination method requires a different detection scheme as well. In both scanning and flash lidar, a time-of-flight camera is used to collect information about both the 3-D location and intensity of the light incident on it in every frame. However, in scanning lidar, this camera contains only a point sensor, while in flash lidar, the camera contains either a 1-D or a 2-D sensor array , each pixel of which collects 3-D location and intensity information. In both cases, the depth information is collected using the time of flight of the laser pulse (i.e., the time it takes each laser pulse to hit the target and return to the sensor), which requires the pulsing of the laser and acquisition by the camera to be synchronized. [40] The result is a camera that takes pictures of distance, instead of colors. [30] Flash lidar is especially advantageous, when compared to scanning lidar, when the camera, scene, or both are moving, since the entire scene is illuminated at the same time. With scanning lidar, motion can cause "jitter" from the lapse in time as the laser rasters over the scene. As with all forms of lidar, the onboard source of illumination makes flash lidar an active sensor. The signal that is returned is processed by embedded algorithms to produce a nearly instantaneous 3-D rendering of objects and terrain features within the field of view of the sensor. [41] The laser pulse repetition frequency is sufficient for generating 3-D videos with high resolution and accuracy. [40] [42] The high frame rate of the sensor makes it a useful tool for a variety of applications that benefit from real-time visualization, such as highly precise remote landing operations. [43] By immediately returning a 3-D elevation mesh of target landscapes, a flash sensor can be used to identify optimal landing zones in autonomous spacecraft landing scenarios. [44] Seeing at a distance requires a powerful burst of light. The power is limited to levels that do not damage human retinas. Wavelengths must not affect human eyes. However, low-cost silicon imagers do not read light in the eye-safe spectrum. Instead, gallium-arsenide imagers are required, which can boost costs to $200,000. [30] Gallium-arsenide is the same compound used to produce high-cost, high-efficiency solar panels usually used in space applications. Classification[ edit ] Based on orientation[ edit ] Lidar can be oriented to nadir , zenith , or laterally. For example, lidar altimeters look down, an atmospheric lidar looks up, and lidar-based collision avoidance systems are side-looking. Based on scanning mechanism[ edit ] Laser projections of lidars can be manipulated using various methods and mechanisms to produce a scanning effect: the standard spindle-type, which spins to give a 360-degree view; solid-state lidar, which has a fixed field of view, but no moving parts, and can use either MEMS or optical phased arrays to steer the beams; and flash lidar, which spreads a flash of light over a large field of view before the signal bounces back to a detector. [45] Based on platform[ edit ] Lidar applications can be divided into airborne and terrestrial types. [46] The two types require scanners with varying specifications based on the data's purpose, the size of the area to be captured, the range of measurement desired, the cost of equipment, and more. Spaceborne platforms are also possible, see satellite laser altimetry . Airborne[ edit ] Airborne lidar (also airborne laser scanning) is when a laser scanner, while attached to an aircraft during flight, creates a 3-D point cloud model of the landscape. This is currently the most detailed and accurate method of creating digital elevation models , replacing photogrammetry . One major advantage in comparison with photogrammetry is the ability to filter out reflections from vegetation from the point cloud model to create a digital terrain model which represents ground surfaces such as rivers, paths, cultural heritage sites, etc., which are concealed by trees. Within the category of airborne lidar, there is sometimes a distinction made between high-altitude and low-altitude applications, but the main difference is a reduction in both accuracy and point density of data acquired at higher altitudes. Airborne lidar can also be used to create bathymetric models in shallow water. [47] The main constituents of airborne lidar include digital elevation models (DEM) and digital surface models (DSM). The points and ground points are the vectors of discrete points while DEM and DSM are interpolated raster grids of discrete points. The process also involves capturing of digital aerial photographs. To interpret deep-seated landslides for example, under the cover of vegetation, scarps, tension cracks or tipped trees airborne lidar is used. Airborne lidar digital elevation models can see through the canopy of forest cover, perform detailed measurements of scarps, erosion and tilting of electric poles. [48] Airborne lidar data is processed using a toolbox called Toolbox for Lidar Data Filtering and Forest Studies (TIFFS) [49] for lidar data filtering and terrain study software. The data is interpolated to digital terrain models using the software. The laser is directed at the region to be mapped and each point's height above the ground is calculated by subtracting the original z-coordinate from the corresponding digital terrain model elevation. Based on this height above the ground the non-vegetation data is obtained which may include objects such as buildings, electric power lines, flying birds, insects, etc. The rest of the points are treated as vegetation and used for modeling and mapping. Within each of these plots, lidar metrics are calculated by calculating statistics such as mean, standard deviation, skewness, percentiles, quadratic mean, etc. [49] Lidar scanning performed with a multicopter UAV Multiple commercial lidar systems for unmanned aerial vehicles are currently on the market. These platforms can systematically scan large areas, or provide a cheaper alternative to manned aircraft for smaller scanning operations. [50] Airborne Lidar Bathymetric Technology-High-resolution multibeam lidar map showing spectacularly faulted and deformed seafloor geology, in shaded relief and coloured by depth[ citation needed ][ dubious – discuss ] Airborne lidar bathymetry[ edit ] The airborne lidar bathymetric technological system involves the measurement of time of flight of a signal from a source to its return to the sensor. The data acquisition technique involves a sea floor mapping component and a ground truth component that includes video transects and sampling. It works using a green spectrum (532 nm) laser beam. [51] Two beams are projected onto a fast rotating mirror, which creates an array of points. One of the beams penetrates the water and also detects the bottom surface of the water under favorable conditions. Water depth measurable by lidar depends on the clarity of the water and the absorption of the wavelength used. Water is most transparent to green and blue light, so these will penetrate deepest in clean water. [52] Blue-green light of 532 nm produced by frequency doubled solid-state IR laser output is the standard for airborne bathymetry. This light can penetrate water but pulse strength attenuates exponentially with distance traveled through the water. [51] Lidar can measure depths from about 0.9 to 40 m (3 to 131 ft), with vertical accuracy in the order of 15 cm (6 in). The surface reflection makes water shallower than about 0.9 m (3 ft) difficult to resolve, and absorption limits the maximum depth. Turbidity causes scattering and has a significant role in determining the maximum depth that can be resolved in most situations, and dissolved pigments can increase absorption depending on wavelength. [52] Other reports indicate that water penetration tends to be between two and three times Secchi depth. Bathymetric lidar is most useful in the 0–10 m (0–33 ft) depth range in coastal mapping. [51] On average in fairly clear coastal seawater lidar can penetrate to about 7 m (23 ft), and in turbid water up to about 3 m (10 ft). An average value found by Saputra et al, 2021, is for the green laser light to penetrate water about one and a half to two times Secchi depth in Indonesian waters. Water temperature and salinity have an effect on the refractive index which has a small effect on the depth calculation. [53] The data obtained shows the full extent of the land surface exposed above the sea floor. This technique is extremely useful as it will play an important role in the major sea floor mapping program. The mapping yields onshore topography as well as underwater elevations. Sea floor reflectance imaging is another solution product from this system which can benefit mapping of underwater habitats. This technique has been used for three-dimensional image mapping of California's waters using a hydrographic lidar. [54] Full-waveform lidar[ edit ] Airborne lidar systems were traditionally able to acquire only a few peak returns, while more recent systems acquire and digitize the entire reflected signal. [55] Scientists analysed the waveform signal for extracting peak returns using Gaussian decomposition . [56] Zhuang et al, 2017 used this approach for estimating aboveground biomass. [57] Handling the huge amounts of full-waveform data is difficult. Therefore, Gaussian decomposition of the waveforms is effective, since it reduces the data and is supported by existing workflows that support interpretation of 3-D point clouds . Recent studies investigated voxelisation . The intensities of the waveform samples are inserted into a voxelised space (3-D grayscale image) building up a 3-D representation of the scanned area. [55] Related metrics and information can then be extracted from that voxelised space. Structural information can be extracted using 3-D metrics from local areas and there is a case study that used the voxelisation approach for detecting dead standing Eucalypt trees in Australia. [58] Terrestrial[ edit ] Terrestrial applications of lidar (also terrestrial laser scanning) happen on the Earth's surface and can be either stationary or mobile. Stationary terrestrial scanning is most common as a survey method, for example in conventional topography, monitoring, cultural heritage documentation and forensics. [46] The 3-D point clouds acquired from these types of scanners can be matched with digital images taken of the scanned area from the scanner's location to create realistic looking 3-D models in a relatively short time when compared to other technologies. Each point in the point cloud is given the colour of the pixel from the image taken at the same location and direction as the laser beam that created the point. Mobile lidar (also mobile laser scanning) is when two or more scanners are attached to a moving vehicle to collect data along a path. These scanners are almost always paired with other kinds of equipment, including GNSS receivers and IMUs . One example application is surveying streets, where power lines, exact bridge heights, bordering trees, etc. all need to be taken into account. Instead of collecting each of these measurements individually in the field with a tachymeter , a 3-D model from a point cloud can be created where all of the measurements needed can be made, depending on the quality of the data collected. This eliminates the problem of forgetting to take a measurement, so long as the model is available, reliable and has an appropriate level of accuracy. Terrestrial lidar mapping involves a process of occupancy grid map generation . The process involves an array of cells divided into grids which employ a process to store the height values when lidar data falls into the respective grid cell. A binary map is then created by applying a particular threshold to the cell values for further processing. The next step is to process the radial distance and z-coordinates from each scan to identify which 3-D points correspond to each of the specified grid cell leading to the process of data formation. [59] Applications[ edit ] This mobile robot uses its lidar to construct a map and avoid obstacles. There are a wide variety of lidar applications, in addition to the applications listed below, as it is often mentioned in National lidar dataset programs. These applications are largely determined by the range of effective object detection; resolution, which is how accurately the lidar identifies and classifies objects; and reflectance confusion, meaning how well the lidar can see something in the presence of bright objects, like reflective signs or bright sun. [45] Companies are working to cut the cost of lidar sensors, currently anywhere from about US$1,200 to more than $12,000. Lower prices will make lidar more attractive for new markets. [60] Agriculture[ edit ] Lidar is used to analyze yield rates on agricultural fields. Agricultural robots have been used for a variety of purposes ranging from seed and fertilizer dispersions, sensing techniques as well as crop scouting for the task of weed control . Lidar can help determine where to apply costly fertilizer. It can create a topographical map of the fields and reveal slopes and sun exposure of the farmland. Researchers at the Agricultural Research Service used this topographical data with the farmland yield results from previous years, to categorize land into zones of high, medium, or low yield. [61] This indicates where to apply fertilizer to maximize yield. Lidar is now used to monitor insects in the field. The use of lidar can detect the movement and behavior of individual flying insects, with identification down to sex and species. [62] In 2017 a patent application was published on this technology in the United States, Europe, and China. [63] Another application is crop mapping in orchards and vineyards, to detect foliage growth and the need for pruning or other maintenance, detect variations in fruit production, or count plants. Lidar is useful in GNSS -denied situations, such as nut and fruit orchards, where foliage causes interference for agriculture equipment that would otherwise utilize a precise GNSS fix. Lidar sensors can detect and track the relative position of rows, plants, and other markers so that farming equipment can continue operating until a GNSS fix is reestablished. Plant species classification[ edit ] Controlling weeds requires identifying plant species. This can be done by using 3-D lidar and machine learning. [64] Lidar produces plant contours as a "point cloud" with range and reflectance values. This data is transformed, and features are extracted from it. If the species is known, the features are added as new data. The species is labelled and its features are initially stored as an example to identify the species in the real environment. This method is efficient because it uses a low-resolution lidar and supervised learning. It includes an easy-to-compute feature set with common statistical features which are independent of the plant size. [64] Archaeology[ edit ] Lidar has many uses in archaeology, including planning of field campaigns, mapping features under forest canopy, and overview of broad, continuous features indistinguishable from the ground. [65] Lidar can produce high-resolution datasets quickly and cheaply. Lidar-derived products can be easily integrated into a Geographic Information System (GIS) for analysis and interpretation. Lidar can also help to create high-resolution digital elevation models (DEMs) of archaeological sites that can reveal micro-topography that is otherwise hidden by vegetation. The intensity of the returned lidar signal can be used to detect features buried under flat vegetated surfaces such as fields, especially when mapping using the infrared spectrum. The presence of these features affects plant growth and thus the amount of infrared light reflected back. [66] For example, at Fort Beauséjour – Fort Cumberland National Historic Site, Canada, lidar discovered archaeological features related to the siege of the Fort in 1755. Features that could not be distinguished on the ground or through aerial photography were identified by overlaying hill shades of the DEM created with artificial illumination from various angles. Another example is work at Caracol by Arlen Chase and his wife Diane Zaino Chase . [67] In 2012, lidar was used to search for the legendary city of La Ciudad Blanca or "City of the Monkey God" in the La Mosquitia region of the Honduran jungle. During a seven-day mapping period, evidence was found of man-made structures. [68] [69] In June 2013, the rediscovery of the city of Mahendraparvata was announced. [70] In southern New England, lidar was used to reveal stone walls, building foundations, abandoned roads, and other landscape features obscured in aerial photography by the region's dense forest canopy. [71] [72] [73] In Cambodia, lidar data were used by Damian Evans and Roland Fletcher to reveal anthropogenic changes to Angkor landscape. [74] In 2012, lidar revealed that the Purépecha settlement of Angamuco in Michoacán , Mexico had about as many buildings as today's Manhattan; [75] while in 2016, its use in mapping ancient Maya causeways in northern Guatemala, revealed 17 elevated roads linking the ancient city of El Mirador to other sites. [76] [77] In 2018, archaeologists using lidar discovered more than 60,000 man-made structures in the Maya Biosphere Reserve , a "major breakthrough" that showed the Maya civilization was much larger than previously thought. [78] [79] [80] [81] [82] [83] [84] [85] [86] [87] [88] In 2024, archaeologists using lidar discovered the Upano Valley sites . [89] [90] Cruise Automation self-driving car with five Velodyne Lidar units on the roof Forecast 3-D Laser System using a SICK LMC lidar sensor Autonomous vehicles may use lidar for obstacle detection and avoidance to navigate safely through environments. [7] [91] The introduction of lidar was a pivotal occurrence that was the key enabler behind Stanley , the first autonomous vehicle to successfully complete the DARPA Grand Challenge . [92] Point cloud output from the lidar sensor provides the necessary data for robot software to determine where potential obstacles exist in the environment and where the robot is in relation to those potential obstacles. Singapore's Singapore-MIT Alliance for Research and Technology (SMART) is actively developing technologies for autonomous lidar vehicles. [93] The very first generations of automotive adaptive cruise control systems used only lidar sensors. Object detection for transportation systems[ edit ] In transportation systems, to ensure vehicle and passenger safety and to develop electronic systems that deliver driver assistance, understanding the vehicle and its surrounding environment is essential. Lidar systems play an important role in the safety of transportation systems. Many electronic systems which add to the driver assistance and vehicle safety such as Adaptive Cruise Control (ACC), Emergency Brake Assist, and Anti-lock Braking System (ABS) depend on the detection of a vehicle's environment to act autonomously or semi-autonomously. Lidar mapping and estimation achieve this. Basics overview: Current lidar systems use rotating hexagonal mirrors which split the laser beam. The upper three beams are used for vehicle and obstacles ahead and the lower beams are used to detect lane markings and road features. [94] The major advantage of using lidar is that the spatial structure is obtained and this data can be fused with other sensors such as radar , etc. to get a better picture of the vehicle environment in terms of static and dynamic properties of the objects present in the environment. Conversely, a significant issue with lidar is the difficulty in reconstructing point cloud data in poor weather conditions. In heavy rain, for example, the light pulses emitted from the lidar system are partially reflected off of rain droplets which adds noise to the data, called 'echoes'. [95] Below mentioned are various approaches of processing lidar data and using it along with data from other sensors through sensor fusion to detect the vehicle environment conditions. Obstacle detection and road environment recognition using lidar[ edit ] This method proposed by Kun Zhou et al. [96] not only focuses on object detection and tracking but also recognizes lane marking and road features. As mentioned earlier the lidar systems use rotating hexagonal mirrors that split the laser beam into six beams. The upper three layers are used to detect the forward objects such as vehicles and roadside objects. The sensor is made of weather-resistant material. The data detected by lidar are clustered to several segments and tracked by Kalman filter . Data clustering here is done based on characteristics of each segment based on object model, which distinguish different objects such as vehicles, signboards, etc. These characteristics include the dimensions of the object, etc. The reflectors on the rear edges of vehicles are used to differentiate vehicles from other objects. Object tracking is done using a two-stage Kalman filter considering the stability of tracking and the accelerated motion of objects [94] Lidar reflective intensity data is also used for curb detection by making use of robust regression to deal with occlusions. The road marking is detected using a modified Otsu method by distinguishing rough and shiny surfaces. [97] Advantages Roadside reflectors that indicate lane border are sometimes hidden due to various reasons. Therefore, other information is needed to recognize the road border. The lidar used in this method can measure the reflectivity from the object. Hence, with this data the road border can also be recognized. Also, the usage of a sensor with weather-robust head helps to detect the objects even in bad weather conditions. Canopy Height Model before and after flood is a good example. Lidar can detect highly detailed canopy height data as well as its road border. Lidar measurements help identify the spatial structure of the obstacle. This helps distinguish objects based on size and estimate the impact of driving over it. [98] Lidar systems provide better range and a large field of view, which helps in detecting obstacles on the curves. This is one of its major advantages over RADAR systems , which have a narrower field of view. The fusion of lidar measurement with different sensors makes the system robust and useful in real-time applications, since lidar dependent systems can't estimate the dynamic information about the detected object. [98] It has been shown that lidar can be manipulated, such that self-driving cars are tricked into taking evasive action. [99] Ecology and conservation[ edit ] Lidar imaging comparing old-growth forest (right) to a new plantation of trees (left) Lidar has also found many applications for mapping natural and managed landscapes such as forests, wetlands, [100] and grasslands. Canopy heights, biomass measurements, and leaf area can all be studied using airborne lidar systems. [101] [102] [103] [104] Similarly, lidar is also used by many industries, including Energy and Railroad, and the Department of Transportation as a faster way of surveying. Topographic maps can also be generated readily from lidar, including for recreational use such as in the production of orienteering maps. [105] Lidar has also been applied to estimate and assess the biodiversity of plants, fungi, and animals. [106] [107] [108] Using southern bull kelp in New Zealand, coastal lidar mapping data has been compared with population genomic evidence to form hypotheses regarding the occurrence and timing of prehistoric earthquake uplift events. [109] Forestry[ edit ] A typical workflow to derive forest information at individual tree or plot levels from lidar point clouds [110] Lidar systems have also been applied to improve forestry management. [111] Measurements are used to take inventory in forest plots as well as calculate individual tree heights, crown width and crown diameter. Other statistical analysis use lidar data to estimate total plot information such as canopy volume, mean, minimum and maximum heights, vegetation cover, biomass, and carbon density. [110] Aerial lidar has been used to map the bush fires in Australia in early 2020. The data was manipulated to view bare earth, and identify healthy and burned vegetation. [112] Geology and soil science[ edit ] High-resolution digital elevation maps generated by airborne and stationary lidar have led to significant advances in geomorphology (the branch of geoscience concerned with the origin and evolution of the Earth surface topography). The lidar abilities to detect subtle topographic features such as river terraces and river channel banks, glacial landforms, [113] to measure the land-surface elevation beneath the vegetation canopy, to better resolve spatial derivatives of elevation, and to detect elevation changes between repeat surveys have enabled many novel studies of the physical and chemical processes that shape landscapes. [114] In 2005 the Tour Ronde in the Mont Blanc massif became the first high alpine mountain on which lidar was employed to monitor the increasing occurrence of severe rock-fall over large rock faces allegedly caused by climate change and degradation of permafrost at high altitude. [115] Lidar is also used in structural geology and geophysics as a combination between airborne lidar and GNSS for the detection and study of faults , for measuring uplift . [116] The output of the two technologies can produce extremely accurate elevation models for terrain – models that can even measure ground elevation through trees. This combination was used most famously to find the location of the Seattle Fault in Washington , United States. [117] This combination also measures uplift at Mount St. Helens by using data from before and after the 2004 uplift. [118] Airborne lidar systems monitor glaciers and have the ability to detect subtle amounts of growth or decline. A satellite-based system, the NASA ICESat , includes a lidar sub-system for this purpose. The NASA Airborne Topographic Mapper [119] is also used extensively to monitor glaciers and perform coastal change analysis. The combination is also used by soil scientists while creating a soil survey . The detailed terrain modeling allows soil scientists to see slope changes and landform breaks which indicate patterns in soil spatial relationships.
Toggle the table of contents Photogrammetry From Wikipedia, the free encyclopedia Taking measurements using photography This article has an unclear citation style . The references used may be made clearer with a different or consistent style of citation and footnoting . (June 2019) ( ) Low altitude aerial photograph for use in photogrammetry. Location: Three Arch Bay , Laguna Beach, CA. Photogrammetry is the science and technology of obtaining reliable information about physical objects and the environment through the process of recording, measuring and interpreting photographic images and patterns of electromagnetic radiant imagery and other phenomena. [1] Photogrammetry of the headquarters of Fazenda do Pinhal, São Carlos-SP, Brazil While the invention of the method is attributed to Aimé Laussedat , [2] the term "photogrammetry" was coined by the Prussian architect Albrecht Meydenbauer, [3] which appeared in his 1867 article "Die Photometrographie." [4] Photogrammetry of the headquarters of Fazenda do Pinhal, São Carlos-SP, Brazil There are many variants of photogrammetry.  One example is the extraction of three-dimensional measurements from two-dimensional data (i.e. images); for example, the distance between two points that lie on a plane parallel to the photographic image plane can be determined by measuring their distance on the image, if the scale of the image is known.  Another is the extraction of accurate color ranges and values representing such quantities as albedo , specular reflection , metallicity , or ambient occlusion from photographs of materials for the purposes of physically based rendering . Close-range photogrammetry refers to the collection of photography from a lesser distance than traditional aerial (or orbital) photogrammetry. Photogrammetric analysis may be applied to one photograph, or may use high-speed photography and remote sensing to detect, measure and record complex 2D and 3D motion fields by feeding measurements and imagery analysis into computational models in an attempt to successively estimate, with increasing accuracy, the actual, 3D relative motions. From its beginning with the stereoplotters used to plot contour lines on topographic maps , it now has a very wide range of uses such as sonar , radar , and lidar . Methods[ edit ] A data model of photogrammetry [5] Tuure Leppänen, Reconstruction I: 2D image from a 3D model built with photogrammetry methods from hundreds of ground-level photos of a japanese garden Photogrammetry uses methods from many disciplines, including optics and projective geometry . Digital image capturing and photogrammetric processing includes several well defined stages, which allow the generation of 2D or 3D digital models of the object as an end product. [6] The data model on the right shows what type of information can go into and come out of photogrammetric methods. The 3D coordinates define the locations of object points in the 3D space . The image coordinates define the locations of the object points' images on the film or an electronic imaging device. The exterior orientation [7] of a camera defines its location in space and its view direction. The inner orientation defines the geometric parameters of the imaging process. This is primarily the focal length of the lens, but can also include the description of lens distortions. Further additional observations play an important role: With scale bars, basically a known distance of two points in space, or known fix points, the connection to the basic measuring units is created. Each of the four main variables can be an input or an output of a photogrammetric method. Algorithms for photogrammetry typically attempt to minimize the sum of the squares of errors over the coordinates and relative displacements of the reference points. This minimization is known as bundle adjustment and is often performed using the Levenberg–Marquardt algorithm . See also: Computer stereo vision A special case, called stereophotogrammetry, involves estimating the three-dimensional coordinates of points on an object employing measurements made in two or more photographic images taken from different positions (see stereoscopy ). Common points are identified on each image. A line of sight (or ray) can be constructed from the camera location to the point on the object.  It is the intersection of these rays ( triangulation ) that determines the three-dimensional location of the point. More sophisticated algorithms can exploit other information about the scene that is known a priori , for example symmetries , in some cases allowing reconstructions of 3D coordinates from only one camera position. Stereophotogrammetry is emerging as a robust non-contacting measurement technique to determine dynamic characteristics and mode shapes of non-rotating [8] [9] and rotating structures. [10] [11] The collection of images for the purpose of creating photogrammetric models can be called more properly, polyoscopy, after Pierre Seguin [12] Integration[ edit ] Photogrammetric data can be complemented with range data from other techniques.  Photogrammetry is more accurate in the x and y direction while range data are generally more accurate in the z direction [ citation needed ].  This range data can be supplied by techniques like LiDAR , laser scanners (using time of flight, triangulation or interferometry), white-light digitizers and any other technique that scans an area and returns x, y, z coordinates for multiple discrete points (commonly called " point clouds ").  Photos can clearly define the edges of buildings when the point cloud footprint can not.  It is beneficial to incorporate the advantages of both systems and integrate them to create a better product. A 3D visualization can be created by georeferencing the aerial photos [13] [14] and LiDAR data in the same reference frame, orthorectifying the aerial photos, and then draping the orthorectified images on top of the LiDAR grid. It is also possible to create digital terrain models and thus 3D visualisations using pairs (or multiples) of aerial photographs or satellite (e.g. SPOT satellite imagery). Techniques such as adaptive least squares stereo matching are then used to produce a dense array of correspondences which are transformed through a camera model to produce a dense array of x, y, z data which can be used to produce digital terrain model and orthoimage products. Systems which use these techniques, e.g. the ITG system, were developed in the 1980s and 1990s but have since been supplanted by LiDAR and radar-based approaches, although these techniques may still be useful in deriving elevation models from old aerial photographs or satellite images. Video of a 3D model of Horatio Nelson bust in Monmouth Museum , produced using photogrammetry Gibraltar 1 Neanderthal skull 3D wireframe model, created with 123d Catch Photogrammetry is used in fields such as topographic mapping , architecture , filmmaking , engineering , manufacturing , quality control , police investigation, cultural heritage , and geology . Archaeologists use it to quickly produce plans of large or complex sites, and meteorologists use it to determine the wind speed of tornadoes when objective weather data cannot be obtained. Photograph of person using controller to explore a 3D photogrammetry experience, Future Cities by DERIVE, recreating Tokyo It is also used to combine live action with computer-generated imagery in movies post-production ; The Matrix is a good example of the use of photogrammetry in film (details are given in the DVD extras). Photogrammetry was used extensively to create photorealistic environmental assets for video games including The Vanishing of Ethan Carter as well as EA DICE 's Star Wars Battlefront . [15] The main character of the game Hellblade: Senua's Sacrifice was derived from photogrammetric motion-capture models taken of actress Melina Juergens. [16] Photogrammetry is also commonly employed in collision engineering, especially with automobiles. When litigation for a collision occurs and engineers need to determine the exact deformation present in the vehicle, it is common for several years to have passed and the only evidence that remains is crash scene photographs taken by the police. Photogrammetry is used to determine how much the car in question was deformed, which relates to the amount of energy required to produce that deformation. The energy can then be used to determine important information about the crash (such as the velocity at time of impact). This article contains too many quotations . Please help summarize the quotations . Consider transferring direct quotations to Wikiquote or excerpts to Wikisource . (June 2019) Photomapping is the process of making a map with "cartographic enhancements" [17] that have been drawn from a photomosaic [18] that is "a composite photographic image of the ground," or more precisely, as a controlled photomosaic where "individual photographs are rectified for tilt and brought to a common scale (at least at certain control points)." Rectification of imagery is generally achieved by "fitting the projected images of each photograph to a set of four control points whose positions have been derived from an existing map or from ground measurements. When these rectified, scaled photographs are positioned on a grid of control points, a good correspondence can be achieved between them through skillful trimming and fitting and the use of the areas around the principal point where the relief displacements (which cannot be removed) are at a minimum." [17] "It is quite reasonable to conclude that some form of photomap will become the standard general map of the future." [19] They go on to suggest[ who? ] that, "photomapping would appear to be the only way to take reasonable advantage" of future data sources like high altitude aircraft and satellite imagery. Archaeology[ edit ] Using a pentop computer to photomap an archaeological excavation in the field Demonstrating the link between orthophotomapping and archaeology , [20] historic airphotos photos were used to aid in developing a reconstruction of the Ventura mission that guided excavations of the structure's walls. Pteryx UAV , a civilian UAV for aerial photography and photomapping with roll-stabilised camera head Overhead photography has been widely applied for mapping surface remains and excavation exposures at archaeological sites. Suggested platforms for capturing these photographs has included: War Balloons from World War I; [21] rubber meteorological balloons; [22] kites ; [22] [23] wooden platforms, metal frameworks, constructed over an excavation exposure; [22] ladders both alone and held together with poles or planks; three legged ladders; single and multi-section poles; [24] [25] bipods; [26] [27] [28] [29] tripods; [30] tetrapods, [31] [32] and aerial bucket trucks ("cherry pickers"). [33] Handheld, near-nadir, overhead digital photographs have been used with geographic information systems ( GIS ) to record excavation exposures. [34] [35] [36] [37] [38] Photogrammetry is increasingly being used in maritime archaeology because of the relative ease of mapping sites compared to traditional methods, allowing the creation of 3D maps which can be rendered in virtual reality . [39] 3D modeling[ edit ] A somewhat similar application is the scanning of objects to automatically make 3D models of them. Since photogrammetry relies on images, there are physical limitations when those images are of an object that has dark, shiny or clear surfaces. In those cases, the produced model often still contains gaps, so additional cleanup with software like MeshLab , netfabb or MeshMixer is often still necessary. [40] Alternatively, spray painting such objects with matte finish can remove any transparent or shiny qualities. Google Earth uses photogrammetry to create 3D imagery. [41] There is also a project called Rekrei that uses photogrammetry to make 3D models of lost/stolen/broken artifacts that are then posted online.
Toggle the table of contents Satellite imagery From Wikipedia, the free encyclopedia Images taken from an artificial satellite The first images from space were taken on the sub-orbital V-2 rocket flight launched by the U.S. on October 24, 1946. Satellite image of Fortaleza . Satellite images (also Earth observation imagery, spaceborne photography, or simply satellite photo) are images of Earth collected by imaging satellites operated by governments and businesses around the world. Satellite imaging companies sell images by licensing them to governments and businesses such as Apple Maps and Google Maps . Further information: First images of Earth from space The first crude image taken by the satellite Explorer 6 shows a sunlit area of the Central Pacific Ocean and its cloud cover. The photo was taken when the satellite was about 17,000 mi (27,000 km) above the surface of the Earth on August 14, 1959. At the time, the satellite was crossing Mexico. The first images from space were taken on sub-orbital flights . The U.S-launched V-2 flight on October 24, 1946, took one image every 1.5 seconds. With an apogee of 65 miles (105 km), these photos were from five times higher than the previous record, the 13.7 miles (22 km) by the Explorer II balloon mission in 1935. [1] The first satellite (orbital) photographs of Earth were made on August 14, 1959, by the U.S. Explorer 6 . [2] [3] The first satellite photographs of the Moon might have been made on October 6, 1959, by the Soviet satellite Luna 3 , on a mission to photograph the far side of the Moon. The Blue Marble photograph was taken from space in 1972, and has become very popular in the media and among the public. Also in 1972 the United States started the Landsat program , the largest program for acquisition of imagery of Earth from space. In 1977, the first real time satellite imagery was acquired by the United States's KH-11 satellite system. The most recent Landsat satellite, Landsat 9 , was launched on 27 September 2021. [4] The first television image of Earth from space transmitted by the TIROS-1 weather satellite in 1960. All satellite images produced by NASA are published by NASA Earth Observatory and are freely available to the public. Several other countries have satellite imaging programs, and a collaborative European effort launched the ERS and Envisat satellites carrying various sensors. There are also private companies that provide commercial satellite imagery. In the early 21st century satellite imagery became widely available when affordable, easy to use software with access to satellite imagery databases was offered by several companies and organizations. Uses[ edit ] Satellite photography can be used to produce composite images of an entire hemisphere ...or to map a small area of the Earth, such as this photo of the countryside of Haskell County , Kansas , United States. Satellite images have many applications in meteorology , oceanography , fishing , agriculture , biodiversity conservation , forestry , landscape , geology , cartography , regional planning , education , intelligence and warfare. Less mainstream uses include anomaly hunting , a criticized investigation technique involving the search of satellite images for unexplained phenomena. [5] Images can be in visible colors and in other spectra . There are also elevation maps , usually made by radar images. Image interpretation and analysis of satellite imagery is conducted using specialized remote sensing software . Data characteristics[ edit ] This section duplicates the scope of other articles, specifically Remote sensing#Data characteristics . Please discuss this issue and help introduce a summary style to the section by replacing the section with a link and a summary or by splitting the content into a new article. (February 2019) There are five types of resolution when discussing satellite imagery in remote sensing:  spatial, spectral, temporal, radiometric and geometric.  Campbell (2002) [6] defines these as follows: spatial resolution is defined as the pixel size of an image representing the size of the surface area (i.e. m2) being measured on the ground, determined by the sensors' instantaneous field of view (IFOV); spectral resolution is defined by the wavelength interval size (discrete segment of the Electromagnetic Spectrum) and number of intervals that the sensor is measuring; temporal resolution is defined by the amount of time (e.g. days) that passes between imagery collection periods for a given surface location Radiometric resolution is defined as the ability of an imaging system to record many levels of brightness (contrast for example) and to the effective bit-depth of the sensor (number of grayscale levels) and is typically expressed as 8-bit (0–255), 11-bit (0–2047), 12-bit (0–4095) or 16-bit (0–65,535). Geometric resolution refers to the satellite sensor's ability to effectively image a portion of the Earth's surface in a single pixel and is typically expressed in terms of Ground sample distance , or GSD. GSD is a term containing the overall optical and systemic noise sources and is useful for comparing how well one sensor can "see" an object on the ground within a single pixel. For example, the GSD of Landsat is ≈30m, which means the smallest unit that maps to a single pixel within an image is ≈30m x 30m. The latest commercial satellite (GeoEye 1) has a GSD of 0.41 m. This compares to a 0.3 m resolution obtained by some early military film based Reconnaissance satellite such as Corona .[ citation needed ] The resolution of satellite images varies depending on the instrument used and the altitude of the satellite's orbit. For example, the Landsat archive offers repeated imagery at 30 meter resolution for the planet, but most of it has not been processed from the raw data. Landsat 7 has an average return period of 16 days. For many smaller areas, images with resolution as fine as 41 cm can be available. [7] Satellite imagery is sometimes supplemented with aerial photography , which has higher resolution, but is more expensive per square meter. Satellite imagery can be combined with vector or raster data in a GIS provided that the imagery has been spatially rectified so that it will properly align with other data sets.
Toggle the table of contents Operational Land Imager From Wikipedia, the free encyclopedia Sensing instrument aboard the Landsat 8 satellite orbiting Earth OLI structure Satellite image of the Thames Estuary taken by OLI The Operational Land Imager (OLI) is a remote sensing instrument aboard Landsat 8 , built by Ball Aerospace & Technologies . Landsat 8 is the successor to Landsat 7 and was launched on February 11, 2013. [1] OLI is a push broom scanner that uses a four-mirror telescope with fixed mirrors. Overview and mission[ edit ] OLI operates alongside TIRS (Thermal Infrared Sensor) on board the LDCM. [2] The build and design of OLI differs from previous generations of instruments, while still maintaining data continuity with archived Landsat data from the last 40 years by keeping the same spectral and spatial resolutions of previous instruments. OLI aids the Landsat-8 mission in the imaging of Earth's surface and the collection of moderate resolution data that is used to monitor changing trends on the surface and evaluate how land usage changes over time. The images and data that OLI has helped collect have practical applications today in agriculture, mapping, and monitoring changes in snow, ice, and water. [3] Specifications and design[ edit ] OLI is a pushbroom sensor that operates in the visible (VIS) and short wave infrared (SWIR) spectral regions. [4] It has a swath width of 185-kilometer (115 mi), which means it can image the entire Earth over a repeating cycle of 16 days. [5] The OLI has nine spectral bands, including a panchromatic band: OLI Spectral Bands [6]
Toggle the table of contents Weather satellite Type of satellite designed to record the state of the Earth's atmosphere Not to be confused with Atmospheric satellite . GOES-16, a United States weather satellite of the meteorological-satellite service A weather satellite or meteorological satellite is a type of Earth observation satellite that is primarily used to monitor the weather and climate of the Earth.  Satellites can be polar orbiting (covering the entire Earth asynchronously), or geostationary (hovering over the same spot on the equator ). [1] While primarily used to detect the development and movement of storm systems and other cloud patterns, meteorological satellites can also detect other phenomena such as city lights, fires, effects of pollution, auroras , sand and dust storms , snow cover, ice mapping, boundaries of ocean currents , and energy flows. Other types of environmental information are collected using weather satellites. Weather satellite images helped in monitoring the volcanic ash cloud from Mount St. Helens and activity from other volcanoes such as Mount Etna . [2] Smoke from fires in the western United States such as Colorado and Utah have also been monitored. El Niño and its effects on weather are monitored daily from satellite images.  The Antarctic ozone hole is mapped from weather satellite data.  Collectively, weather satellites flown by the U.S., Europe, India, China, Russia, and Japan provide nearly continuous observations for a global weather watch. Further information: First images of Earth from space The first television image of Earth from space from the TIROS-1 weather satellite in 1960 A mosaic of photographs of the United States from the ESSA-9 weather satellite, taken on June 26, 1969 As early as 1946, the idea of cameras in orbit to observe the weather was being developed.  This was due to sparse data observation coverage and the expense of using cloud cameras on rockets.  By 1958, the early prototypes for TIROS and Vanguard (developed by the Army Signal Corps) were created. [3] The first weather satellite, Vanguard 2 , was launched on February 17, 1959. [4] It was designed to measure cloud cover and resistance, but a poor axis of rotation and its elliptical orbit kept it from collecting a notable amount of useful data.  The Explorer VI and VII satellites also contained weather-related experiments. [3] The first weather satellite to be considered a success was TIROS-1 , launched by NASA on April 1, 1960. [5] TIROS operated for 78 days and proved to be much more successful than Vanguard 2.  TIROS paved the way for the Nimbus program , whose technology and findings are the heritage of most of the Earth-observing satellites NASA and NOAA have launched since then.  Beginning with the Nimbus 3 satellite in 1969, temperature information through the tropospheric column began to be retrieved by satellites from the eastern Atlantic and most of the Pacific Ocean, which led to significant improvements to weather forecasts . [6] The ESSA and NOAA polar orbiting satellites followed suit from the late 1960s onward.  Geostationary satellites followed, beginning with the ATS and SMS series in the late 1960s and early 1970s, then continuing with the GOES series from the 1970s onward.  Polar orbiting satellites such as QuikScat and TRMM began to relay wind information near the ocean's surface starting in the late 1970s, with microwave imagery which resembled radar displays, which significantly improved the diagnoses of tropical cyclone strength, intensification, and location during the 2000s and 2010s. The DSCOVR satellite, owned by NOAA, was launched in 2015 and became the first deep space satellite that can observe and predict space weather. It can detect potentially dangerous weather such as solar wind and geomagnetic storms . This is what has given humanity the capability to make accurate and preemptive space weather forecasts since the late 2010s. [7] In Europe, the first Meteosat geostationary operational meteorological satellite, Meteosat-1, was launched in 1977 on a Delta launch vehicle. The satellite was a spin-stabilised cylindrical design, 2.1m in diameter and 3.2m tall, rotating at approx. 100 rpm and carrying the Meteosat Visible and Infrared Imager (MVIRI) instrument. Successive Meteosat first generation satellites were launched, on European Ariane-4 launchers from Kourou in French Guyana, up to and including Meteosat-7 which acquired data from 1997 until 2017, operated initially by the European Space Agency and later, from 1995, by the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT). The Meteosat Second Generation (MSG) satellites - also spin stabilised although physically larger and twice the mass of the first generation - were developed by ESA with European industry and in cooperation with EUMETSAT who then operate the satellites from their headquarters in Darmstadt, Germany with this same approach followed for all subsequent European meteorological satellites. Meteosat-8, the first MSG satellite, was launched in 2002 on an Ariane-5 launcher, carrying the Spinning Enhanced Visible and Infrared Imager (SEVIRI) and Geostationary Earth Radiation Budget (GERB) instruments, along with payloads to support the COSPAS-SARSAT Search and Rescue (SAR) and ARGOS Data Collection Platform (DCP) missions. SEVIRI provided an increased number of spectral channels over MVIRI and imaged the full-Earth disc at double the rate. Meteosat-9 was launched to complement Meteosat-8 in 2005, with the second pair consisting of Meteosat-10 and Meteosat-11 launched in 2012 and 2015, respectively. The Meteosat Third Generation (MTG) programme launched its first satellite in 2022, and featured a number of changes over its predecessors in support of its mission to gather data for weather forecasting and climate monitoring. The MTG satellites are three-axis stabilised rather than spin stabilised, giving greater flexibility in satellite and instrument design. The MTG system features separate Imager and Sounder satellite models that share the same satellite bus, with a baseline of three satellites - two Imagers and one Sounder - forming the operational configuration. The imager satellites carry the Flexible Combined Imager (FCI), succeeding MVIRI and SEVIRI to give even greater resolution and spectral coverage, scanning the full Earth disc every ten minutes, as well as a new Lightning Imager (LI) payload. The sounder satellites carry the Infrared Sounder (IRS) and Ultra-violet Visible Near-infrared (UVN) instruments. UVN is part of the European Commission 's Copernicus programme and fulfils the Sentinel-4 mission to monitor air quality, trace gases and aerosols over Europe hourly at high spatial resolution. Two MTG satellites - one Imager and one Sounder - will operate in close proximity from the 0-deg geostationary location over western Africa to observe the eastern Atlantic Ocean, Europe, Africa and the Middle East, while a second imager satellite will operate from 9.5-deg East to perform a Rapid Scanning mission over Europe. MTG continues Meteosat support to the ARGOS and Search and Rescue missions. MTG-I1 launched in one of the last Ariane-5 launches, with the subsequent satellites planned to launch in Ariane-6 when it enters service. In 2006, the first European low-Earth orbit operational meteorological satellite, Metop -A was launched into a Sun-synchronous orbit at 817 km altitude by a Soyuz launcher from Baikonur, Kazakhstan. This operational satellite - which forms the space segment of the Eumetsat Polar System (EPS) - built on the heritage from ESA's ERS and Envisat experimental missions, and was followed at six-year intervals by Metop-B and Metop-C - the latter launched from French Guyana in a "Europeanised" Soyuz. Each carry thirteen different passive and active instruments ranging in design from imagers and sounders to a scatterometer and a radio-occultation instrument. The satellite service module is based on the SPOT-5 bus, while the payload suite is a combination of new and heritage instruments from both Europe and the US under the Initial Joint Polar System agreement between EUMETSAT and NOAA. A second generation of Metop satellites (Metop-SG) is in advanced development with launch of the first satellite foreseen in 2025. As with MTG, Metop-SG will launch on Ariane-6 and comprise two satellite models to be operated in pairs in replacement of the single first generation satellites to continue the EPS mission. Observation[ edit ] These meteorological-satellite service , however, see more than clouds and cloud systems Observation is typically made via different 'channels' of the electromagnetic spectrum , in particular, the visible and infrared portions. Some of these channels include: [8] [9] Visible and Near Infrared: 0.6–1.6 μm – for recording cloud cover during the day Infrared: 3.9–7.3 μm (water vapor), 8.7–13.4 μm (thermal imaging) Visible spectrum[ edit ] Visible-light images from weather satellites during local daylight hours are easy to interpret even by the average person, clouds, cloud systems such as fronts and tropical storms, lakes, forests, mountains, snow ice, fires, and pollution such as smoke, smog, dust and haze are readily apparent.  Even wind can be determined by cloud patterns, alignments and movement from successive photos. [10] Infrared spectrum[ edit ] The thermal or infrared images recorded by sensors called scanning radiometers enable a trained analyst to determine cloud heights and types, to calculate land and surface water temperatures, and to locate ocean surface features. Infrared satellite imagery can be used effectively for tropical cyclones with a visible eye pattern, using the Dvorak technique , where the difference between the temperature of the warm eye and the surrounding cold cloud tops can be used to determine its intensity (colder cloud tops generally indicate a more intense storm). [11] Infrared pictures depict ocean eddies or vortices and map currents such as the Gulf Stream which are valuable to the shipping industry. Fishermen and farmers are interested in knowing land and water temperatures to protect their crops against frost or increase their catch from the sea. Even El Niño phenomena can be spotted. Using color-digitized techniques, the gray shaded thermal images can be converted to color for easier identification of desired information. The geostationary Himawari 8 satellite's first true-colour composite PNG image The geostationary GOES-17 satellite's Level 1B Calibrated Radiances - True Colour Composite PNG image Each meteorological satellite is designed to use one of two different classes of orbit: geostationary and polar orbiting . Geostationary[ edit ] "geostationary meteorological satellite" redirects here. For the Japanese satellites called "Geostationary Meteorological Satellite", see Himawari (satellite) . Geostationary weather satellites orbit the Earth above the equator at altitudes of 35,880 km (22,300 miles).  Because of this orbit , they remain stationary with respect to the rotating Earth and thus can record or transmit images of the entire hemisphere below continuously with their visible-light and infrared sensors. The news media use the geostationary photos in their daily weather presentation as single images or made into movie loops. These are also available on the city forecast pages of www.noaa.gov (example Dallas, TX). [12] Several geostationary meteorological spacecraft are in operation. The United States' GOES series has three in operation: GOES-15 , GOES-16 and GOES-17 . GOES-16 and-17 remain stationary over the Atlantic and Pacific Oceans, respectively. [13] GOES-15 was retired in early July 2019. [14] The satellite GOES 13 that was previously owned by the National Oceanic and Atmospheric Association (NOAA) was transferred to the U.S. Space Force in 2019 and renamed the EWS-G1; becoming the first geostationary weather satellite to be owned and operated by the U.S. Department of Defense. [15] Russia 's new-generation weather satellite Elektro-L No.1 operates at 76°E over the Indian Ocean. The Japanese have the MTSAT -2 located over the mid Pacific at 145°E and the Himawari 8 at 140°E. The Europeans have four in operation, Meteosat -8 (3.5°W) and Meteosat-9 (0°) over the Atlantic Ocean and have Meteosat-6 (63°E) and Meteosat-7 (57.5°E) over the Indian Ocean. China currently has four Fengyun (风云) geostationary satellites (FY-2E at 86.5°E, FY-2F at 123.5°E, FY-2G at 105°E and FY-4A at 104.5 °E) operated. [16] India also operates geostationary satellites called INSAT which carry instruments for meteorological purposes. Polar orbiting[ edit ] Computer-controlled motorized parabolic dish antenna for tracking LEO weather satellites. Polar orbiting weather satellites circle the Earth at a typical altitude of 850 km (530 miles) in a north to south (or vice versa) path, passing over the poles in their continuous flight.  Polar orbiting weather satellites are in sun-synchronous orbits , which means they are able to observe any place on Earth and will view every location twice each day with the same general lighting conditions due to the near-constant local solar time . Polar orbiting weather satellites offer a much better resolution than their geostationary counterparts due their closeness to the Earth. The United States has the NOAA series of polar orbiting meteorological satellites, presently NOAA-15, NOAA-18 and NOAA-19 ( POES ) and NOAA-20 ( JPSS ). Europe has the Metop -A, Metop -B and Metop -C satellites operated by EUMETSAT . Russia has the Meteor and RESURS series of satellites. China has FY -3A, 3B and 3C. India has polar orbiting satellites as well. DMSP[ edit ] Turnstile antenna for reception of 137 MHz LEO weather satellite transmissions The United States Department of Defense 's Meteorological Satellite ( DMSP ) can "see" the best of all weather vehicles with its ability to detect objects almost as 'small' as a huge oil tanker .  In addition, of all the weather satellites in orbit, only DMSP can "see" at night in the visual.  Some of the most spectacular photos have been recorded by the night visual sensor; city lights, volcanoes , fires, lightning, meteors , oil field burn-offs, as well as the Aurora Borealis and Aurora Australis have been captured by this 720 kilometres (450 mi) high space vehicle's low moonlight sensor. At the same time, energy use and city growth can be monitored since both major and even minor cities, as well as highway lights, are conspicuous.  This informs astronomers of light pollution . The New York City Blackout of 1977 was captured by one of the night orbiter DMSP space vehicles. In addition to monitoring city lights, these photos are a life saving asset in the detection and monitoring of fires.  Not only do the satellites see the fires visually day and night, but the thermal and infrared scanners on board these weather satellites detect potential fire sources below the surface of the Earth where smoldering occurs.  Once the fire is detected, the same weather satellites provide vital information about wind that could fan or spread the fires.  These same cloud photos from space tell the firefighter when it will rain. Some of the most dramatic photos showed the 600 Kuwaiti oil fires that the fleeing Army of Iraq started on February 23, 1991.  The night photos showed huge flashes, far outstripping the glow of large populated areas.  The fires consumed huge quantities of oil; the last was doused on November 6, 1991. Uses[ edit ] Infrared image of storms over the central United States from the GOES-17 satellite Snowfield monitoring, especially in the Sierra Nevada , can be helpful to the hydrologist keeping track of available snowpack for runoff vital to the watersheds of the western United States.  This information is gleaned from existing satellites of all agencies of the U.S. government (in addition to local, on-the-ground measurements).  Ice floes, packs, and bergs can also be located and tracked from weather spacecraft. Even pollution whether it is nature-made or human-made can be pinpointed.  The visual and infrared photos show effects of pollution from their respective areas over the entire earth.  Aircraft and rocket pollution, as well as condensation trails , can also be spotted.  The ocean current and low level wind information gleaned from the space photos can help predict oceanic oil spill coverage and movement. Almost every summer, sand and dust from the Sahara Desert in Africa drifts across the equatorial regions of the Atlantic Ocean.  GOES-EAST photos enable meteorologists to observe, track and forecast this sand cloud.  In addition to reducing visibilities and causing respiratory problems, sand clouds suppress hurricane formation by modifying the solar radiation balance of the tropics. Other dust storms in Asia and mainland China are common and easy to spot and monitor, with recent examples of dust moving across the Pacific Ocean and reaching North America. In remote areas of the world with few local observers, fires could rage out of control for days or even weeks and consume huge areas before authorities are alerted.  Weather satellites can be a valuable asset in such situations. Nighttime photos also show the burn-off in gas and oil fields.  Atmospheric temperature and moisture profiles have been taken by weather satellites since 1969. [17]
Toggle the table of contents Geographic information science Printable version From Wikipedia, the free encyclopedia Geographic information science (GIScience, GISc) or geoinformation science is a scientific discipline at the crossroads of computational science , social science , and natural science that studies geographic information , including how it represents phenomena in the real world, how it represents the way humans understand the world, and how it can be captured, organized , and analyzed . It is a sub-field of geography , specifically part of technical geography . [1] [2] [3] It has applications to both physical geography and human geography , although its techniques can be applied to many other fields of study as well as many different industries . As a field of study or profession, it can be contrasted with geographic information systems (GIS), which are the actual repositories of geospatial data, the software tools for carrying out relevant tasks, and the profession of GIS users. That said, one of the major goals of GIScience is to find practical ways to improve GIS data, software, and professional practice; it is more focused on how gis is applied in real life as opposed to being a geographic information system tool in and of itself. The field is also sometimes called geographical information science. British geographer Michael Goodchild defined this area in the 1990s and summarized its core interests, including spatial analysis , visualization, and the representation of uncertainty. [4] GIScience is conceptually related to geomatics , information science , computer science , and data science , but it claims the status of an independent scientific discipline. [5] Recent developments in the field have expanded its focus to include studies on human dynamics in hybrid physical-virtual worlds, quantum GIScience, the development of smart cities , and the social and environmental impacts of technological innovations. [6] These advancements indicate a growing intersection of GIScience with contemporary societal and technological issues. Overlapping disciplines are: geocomputation , geoinformatics , geomatics and geovisualization . [7] Other related terms are geographic data science (after data science ) [8] [9] and geographic information science and technology (GISci&T), [10] with job titles geospatial information scientists and technologists. [11] Definitions[ edit ] This section needs expansion. You can help by adding to it . (September 2015) Since its inception in the 1990s, the boundaries between GIScience and cognate disciplines are contested, and different communities might disagree on what GIScience is and what it studies. In particular, Goodchild stated that "information science can be defined as the systematic study according to scientific principles of the nature and properties of information. Geographic information science is the subset of information science that is about geographic information." [12] Another influential definition is that by geographic information scientist (GIScientist) David Mark , which states: Geographic Information Science (GIScience) is the basic research field that seeks to redefine geographic concepts and their use in the context of geographic information systems. GIScience also examines the impacts of GIS on individuals and society, and the influences of society on GIS. GIScience re-examines some of the most fundamental themes in traditional spatially oriented fields such as geography, cartography, and geodesy, while incorporating more recent developments in cognitive and information science. It also overlaps with and draws from more specialized research fields such as computer science, statistics, mathematics, and psychology, and contributes to progress in those fields. It supports research in political science and anthropology, and draws on those fields in studies of geographic information and society. [13] In 2009, Goodchild summarized the history of GIScience and its achievements and open challenges. [14]
Toggle the table of contents International Charter 'Space and Major Disasters' 8 languages (Redirected from International Charter Space and Major Disasters ) Non-binding multinatinal alliance The International Charter "Space and Major Disasters" is a non-binding charter which provides for the charitable and humanitarian acquisition and transmission of satellite data to relief organizations in the event of major disasters.  Initiated by the European Space Agency and the French space agency CNES after the UNISPACE III conference held in Vienna , Austria, in July 1999, it officially came into operation on November 1, 2000, after the Canadian Space Agency signed onto the charter on October 20, 2000.  Their space assets were then, respectively, ERS and ENVISAT , SPOT and Formosat , and RADARSAT . The assorted satellite assets of various corporate, national, and international space agencies and entities provide for humanitarian coverage which is wide albeit contingent.  First activated for landslide in Slovenia in November 2000, [1] the Charter has since brought space assets into play for numerous floods, earthquakes, oil spills, forest fires, tsunamis, major snowfalls, volcanic eruptions, hurricanes and landslides, [2] [3] and furthermore (and unusually) for the search for Malaysia Airlines Flight 370 [4] and for the 2014 West Africa Ebola outbreak . [5] As of 2015, fifteen space agencies are signatories; dozens of satellites are available with image resolutions ranging from 8 kilometres (5 mi) per pixel to about 0.3 metres (1 ft) per pixel. [6] As of August 2018, it had had 579 activations, from 125 countries, and had 17 members, which contributed 34 satellites. [7] It won the prestigious William T. Pecora Award in 2017. [8] Successive signatories and satellite assets[ edit ] United States National Oceanic and Atmospheric Administration ( NOAA ) — ( POES ), ( GOES ) and Indian Space Research Organization ( ISRO ) (September 2001) [9] —  (the Indian Remote Sensing satellite series) February 2005 – Japan Aerospace Exploration Agency ( JAXA ) [9] — ( ALOS ) ? 2005 – United States Geological Survey ( USGS ) as part of the U.S. team  — ( Landsat , Quickbird , GeoEye 1 ) November 2005 – The British space agency BNSC ( UK-DMC ) with the company DMCii May 2007 – China National Space Administration ( CNSA ) — (the FY, SJ, ZY satellite series) ? – The Turkish space agency TUBITAK — ( BILSAT-1 ) ? – The British company BNSC/Surrey Satellite Technology Limited — ( UK-DMC ) 2012 German Aerospace Center ( DLR ) - ( TerraSAR-X , TanDEM-X ) 2012 Korea Aerospace Research Institute ( KARI ) - (Arirang 3, 3A, 5) 2012 Instituto Nacional de Pesquisas Espaciais, Brazil ( INPE ) 2012 European Organisation for the Exploitation of Meteorological Satellites ( EUMETSAT ) [10] 2016 Bolivarian Agency for Space Activities (ABAE) - ( VRSS-1 ) As of 2012, [update] the live satellites and their instrumentalities were: The high resolution and very high resolution radar sensors of ENVISAT (decommissioned in April), RISAT-1 , RADARSAT -1 & 2, TerraSAR-X and TanDEM-X ; the high resolution and very high resolution optical sensors of SPOT satellites 4  & 5, Pleiades , Landsat 5 & 7, PROBA 1, UK-DMC 2 , KOMPSAT-2 , IRS-P5 , Resourcesat-2 , Oceansat-2 , Cartosat-2 , IMS-1 , and RapidEye ; the medium and low resolution optical sensors of POES , GOES , and SAC-C . Furthermore, specific agreements with other entities, including corporations, allow the Charter access to additional products of high and very high resolution from satellites such as the Formosat series, GeoEye , IKONOS , QuickBird , and WorldView . [10] In 2014, the charter was activated 41 times for disasters in 30 countries. In that year the live satellites and their instrumentalities included: The high and very high resolution radar sensors of Risat-1, RADARSAT -2, TerraSAR-X , and TanDEM-X and Sentinel-1A ; the optical high and very high resolution sensors of UK-DMC 2, Landsat 7 and 8, SPOT series 5, 6, and 7, Pléiades 1A and 1B, PROBA 1, SJ-9A, GF-1, KOMPSAT-2 , IRS-P5 ( Cartosat -1), Cartosat-2, Resourcesat-2 , Oceansat-2 , RapidEye , Kanopus -V, and Resurs-P , and the HDTV camera mounted the Kibo module of the International Space Station ; and the medium and low optical sensors of POES , GOES , FY-3C, the Metop series, the first two Meteosat generations, and Meteor-M . Specific agreements with other entities allow for the usage of the Formosat , GeoEye , IKONOS , QuickBird , and WorldViews satellites, which have high and very high resolution. Archival data from defunct satellites such as ALOS , ENVISAT , ERS , CBERS , IRS-1C , Astra 1D , IRS P4, P6, IMS-1 , RADARSAT -1, SAC-C, SPOT 1-3 & 4, UK-DMC , Landsat -5 and NigeriaSat are also available. [13] Major events resulting in activation[ edit ] This is very much a partial list; the 500th activation of the Charter was on 1 August 2016. [14] 2004: The Charter was activated for the 2004 Indian Ocean earthquake and tsunami by the Indian Space Research Organisation ( ISRO ). [15] 14 January 2010:  The French Civil Protection authorities, Public Safety Canada , the American Earthquake Hazards Programme of U.S. Geological Survey and the UN Stabilisation Mission in Haiti requested a post-event map of Haiti, two days after the 2010 Haiti earthquake , via the Space and Major Disasters Charter. [16] [17] 22 February 2011: Both COGIC (French Civil Protection) [18] and U.S. Geological Survey requested the activation of the Charter on the behalf of MCDEM New Zealand, thus readily providing satellite imagery for aid and rescue services following the 2011 Christchurch earthquake . [19] 12 March 2011: Japan, through its space agency JAXA , requested the activation of the Charter to help in managing the aftermath of 2011 Tōhoku earthquake and tsunami . [20] 8 November 2013: The Charter was activated by Philippine authorities as super-typhoon Haiyan made landfall. [21] 11 March 2014: The Charter was activated by Chinese authorities to aid in the search for Malaysia Airlines Flight 370 which disappeared on March 8, 2014, en route from Kuala Lumpur International Airport to Beijing Capital International Airport. [4] [22] As of 2023, the aircraft was still lost. 4 May 2016: Public Safety Canada activated the Charter for the Fort McMurray Wildfire . [25] 13 August 2016: The Charter was activated by the U.S. Geological Survey in response to the 2016 Louisiana floods . [26] 24 August 2017: The Charter was activated by the U.S. Geological Survey for Hurricane Harvey . [27] 5 September 2017: The Charter was activated by the Comisión Nacional de Emergencias in the Dominican Republic for Hurricane Irma ; [28] Haiti and the United States followed suit shortly thereafter. [29] [30] 19 September 2017: The Charter was activated in the aftermath of 2017 Central Mexico earthquake . [31] 20 September 2017. The Charter was activated by the Comisión Nacional de Emergencias CNE in the Dominican Republic for Hurricane Maria . [32]
Toggle the table of contents Environmental monitoring From Wikipedia, the free encyclopedia Monitoring of the quality of the environment Part of a series on e Environmental monitoring describes the processes and activities that need to take place to characterize and monitor the quality of the environment. Environmental monitoring is used in the preparation of environmental impact assessments , as well as in many circumstances in which human activities carry a risk of harmful effects on the natural environment . All monitoring strategies and programs have reasons and justifications which are often designed to establish the current status of an environment or to establish trends in environmental parameters. In all cases, the results of monitoring will be reviewed, analyzed statistically , and published. The design of a monitoring program must therefore have regard to the final use of the data before monitoring starts. Environmental monitoring includes monitoring of air quality , soils and water quality . Air quality monitoring[ edit ] Air quality monitoring station in Italy Further information: Air pollution measurement , Air quality index , Continuous emissions monitoring system , Particulate matter sampler , and Portable emissions measurement system Air pollutants are atmospheric substances—both naturally occurring and anthropogenic —which may potentially have a negative impact on the environment and organism health. With the evolution of new chemicals and industrial processes has come the introduction or elevation of pollutants in the atmosphere, as well as environmental research and regulations, increasing the demand for air quality monitoring. [1] Air quality monitoring is challenging to enact as it requires the effective integration of multiple environmental data sources, which often originate from different environmental networks and institutions. [2] These challenges require specialized observation equipment and tools to establish air pollutant concentrations, including sensor networks, geographic information system (GIS) models, and the Sensor Observation Service (SOS), a web service for querying real-time sensor data. [2] Air dispersion models that combine topographic, emissions, and meteorological data to predict air pollutant concentrations are often helpful in interpreting air monitoring data. Additionally, consideration of anemometer data in the area between sources and the monitor often provides insights on the source of the air contaminants recorded by an air pollution monitor. Air quality monitors are operated by citizens, [3] [4] [5] regulatory agencies, [6] [7] non-governmental organisations [8] and researchers [9] to investigate air quality and the effects of air pollution. Interpretation of ambient air monitoring data often involves a consideration of the spatial and temporal representativeness [10] of the data gathered, and the health effects associated with exposure to the monitored levels. [11] If the interpretation reveals concentrations of multiple chemical compounds, a unique "chemical fingerprint" of a particular air pollution source may emerge from analysis of the data. [12] Air sampling[ edit ] Passive or "diffusive" air sampling depends on meteorological conditions such as wind to diffuse air pollutants to a sorbent medium. Passive samplers, such as diffusion tubes , have the advantage of typically being small, quiet, and easy to deploy, and they are particularly useful in air quality studies that determine key areas for future continuous monitoring. [13] Air pollution can also be assessed by biomonitoring with organisms that bioaccumulate air pollutants, such as lichens , mosses, fungi, and other biomass. [14] [15] One of the benefits of this type of sampling is how quantitative information can be obtained via measurements of accumulated compounds, representative of the environment from which they came. However, careful considerations must be made in choosing the particular organism, how it's dispersed, and relevance to the pollutant. [15] Other sampling methods include the use of a denuder , [16] [17] needle trap devices, and microextraction techniques. [18] Further information: Environmental soil science Soil monitoring involves the collection and/or analysis of soil and its associated quality , constituents , and physical status to determine or guarantee its fitness for use. Soil faces many threats, including compaction , contamination , organic material loss, biodiversity loss , slope stability issues, erosion , salinization , and acidification . Soil monitoring helps characterize these threats and other potential risks to the soil, surrounding environments, animal health, and human health. [19] Assessing these threats and other risks to soil can be challenging due to a variety of factors, including soil's heterogeneity and complexity, scarcity of toxicity data, lack of understanding of a contaminant's fate, and variability in levels of soil screening. [19] This requires a risk assessment approach and analysis techniques that prioritize environmental protection, risk reduction, and, if necessary, remediation methods. [19] Soil monitoring plays a significant role in that risk assessment, not only aiding in the identification of at-risk and affected areas but also in the establishment of base background values of soil. [19] Soil monitoring has historically focused on more classical conditions and contaminants, including toxic elements (e.g., mercury , lead , and arsenic ) and persistent organic pollutants (POPs). [19] Historically, testing for these and other aspects of soil, however, has had its own set of challenges, as sampling in most cases is of a destructive in nature, requiring multiple samples over time. Additionally, procedural and analytical errors may be introduced due to variability among references and methods, particularly over time. [20] However, as analytical techniques evolve and new knowledge about ecological processes and contaminant effects disseminate, the focus of monitoring will likely broaden over time and the quality of monitoring will continue to improve. [19] Soil sampling[ edit ] The two primary types of soil sampling are grab sampling and composite sampling. Grab sampling involves the collection of an individual sample at a specific time and place, while composite sampling involves the collection of a homogenized mixture of multiple individual samples at either a specific place over different times or multiple locations at a specific time. [21] Soil sampling may occur both at shallow ground levels or deep in the ground, with collection methods varying by level collected from. Scoops, augers, core barrel, and solid-tube samplers, and other tools are used at shallow ground levels, whereas split-tube, solid-tube, or hydraulic methods may be used in deep ground. [22] Monitoring programs[ edit ] A portable X-ray fluorescence (XRF) analyzer can be used in the field for testing soils for metal contamination. Soil contamination monitoring[ edit ] Further information: Soil contamination Soil contamination monitoring helps researchers identify patterns and trends in contaminant deposition, movement, and effect. Human-based pressures such as tourism, industrial activity, urban sprawl , construction work, and inadequate agriculture/forestry practices can contribute to and make worse soil contamination and lead to the soil becoming unfit for its intended use. Both inorganic and organic pollutants may make their way to the soil, having a wide variety of detrimental effects. Soil contamination monitoring is therefore important to identify risk areas, set baselines, and identify contaminated zones for remediation. Monitoring efforts may range from local farms to nationwide efforts, such as those made by China in the late 2000s, [19] providing details such as the nature of contaminants, their quantity, effects, concentration patterns, and remediation feasibility. [23] Monitoring and analytical equipment will ideally will have high response times, high levels of resolution and automation, and a certain degree of self-sufficiency. [24] Chemical techniques may be used to measure toxic elements and POPs using chromatography and spectrometry , geophysical techniques may assess physical properties of large terrains, and biological techniques may use specific organisms to gauge not only contaminant level but also byproducts of contaminant biodegradation. These techniques and others are increasingly becoming more efficient, and laboratory instrumentation is becoming more precise, resulting in more meaningful monitoring outcomes. [25] Soil erosion monitoring[ edit ] Further information: Soil erosion Soil erosion monitoring helps researchers identify patterns and trends in soil and sediment movement. Monitoring programs have varied over the years, from long-term academic research on university plots to reconnaissance-based surveys of biogeoclimatic areas. In most methods, however, the general focus is on identifying and measuring all the dominant erosion processes in a given area. [26] Additionally, soil erosion monitoring may attempt to quantify the effects of erosion on crop productivity, though challenging "because of the many complexities in the relationship between soils and plants and their management under a variable climate." [27] Soil salinity monitoring[ edit ] Further information: Soil salinity Soil salinity monitoring helps researchers identify patterns and trends in soil salt content. Both the natural process of seawater intrusion and the human-induced processes of inappropriate soil and water management can lead to salinity problems in soil, with up to one billion hectares of land affected globally (as of 2013). [28] Salinity monitoring at the local level may look closely at the root zone to gauge salinity impact and develop management options, whereas at the regional and national level salinity monitoring may help with identifying areas at-risk and aiding policymakers in tackling the issue before it spreads. [28] The monitoring process itself may be performed using technologies such as remote sensing and geographic information systems (GIS) to identify salinity via greenness, brightness, and whiteness at the surface level. Direct analysis of soil up close, including the use of electromagnetic induction techniques, may also be used to monitor soil salinity. [28] Water quality monitoring[ edit ] Electrofishing survey methods use a mild electric shock to temporarily stun fish for capture, identification and counting. The fish are then returned to the water unharmed. Design of environmental monitoring programmes[ edit ] Water quality monitoring is of little use without a clear and unambiguous definition of the reasons for the monitoring and the objectives that it will satisfy. Almost all monitoring (except perhaps remote sensing ) is in some part invasive of the environment under study and extensive and poorly planned monitoring carries a risk of damage to the environment. This may be a critical consideration in wilderness areas or when monitoring very rare organisms or those that are averse to human presence. Some monitoring techniques, such as gill netting fish to estimate populations, can be very damaging, at least to the local population and can also degrade public trust in scientists carrying out the monitoring. Almost all mainstream environmentalism monitoring projects form part of an overall monitoring strategy or research field, and these field and strategies are themselves derived from the high levels objectives or aspirations of an organisation. Unless individual monitoring projects fit into a wider strategic framework, the results are unlikely to be published and the environmental understanding produced by the monitoring will be lost. [29] [30] Chemical[ edit ] Analyzing water samples for pesticides The range of chemical parameters that have the potential to affect any ecosystem is very large and in all monitoring programmes it is necessary to target a suite of parameters based on local knowledge and past practice for an initial review. The list can be expanded or reduced based on developing knowledge and the outcome of the initial surveys. Freshwater environments have been extensively studied for many years and there is a robust understanding of the interactions between chemistry and the environment across much of the world. However, as new materials are developed and new pressures come to bear, revisions to monitoring programmes will be required. In the last 20 years acid rain , synthetic hormone analogues, halogenated hydrocarbons , greenhouse gases and many others have required changes to monitoring strategies. Biological[ edit ] In ecological monitoring, the monitoring strategy and effort is directed at the plants and animals in the environment under review and is specific to each individual study. However, in more generalised environmental monitoring, many animals act as robust indicators of the quality of the environment that they are experiencing or have experienced in the recent past. [31] One of the most familiar examples is the monitoring of numbers of Salmonid fish such as brown trout or Atlantic salmon in river systems and lakes to detect slow trends in adverse environmental effects. The steep decline in salmonid fish populations was one of the early indications of the problem that later became known as acid rain . In recent years much more attention has been given to a more holistic approach in which the ecosystem health is assessed and used as the monitoring tool itself. [32] It is this approach that underpins the monitoring protocols of the Water Framework Directive in the European Union . Radiological[ edit ] Radiation monitoring involves the measurement of radiation dose or radionuclide contamination for reasons related to the assessment or control of exposure to ionizing radiation or radioactive substances, and the interpretation of the results. [33] The 'measurement' of dose often means the measurement of a dose equivalent quantity as a proxy (i.e. substitute) for a dose quantity that cannot be measured directly. Also, sampling may be involved as a preliminary step to measurement of the content of radionuclides in environmental media. The methodological and technical details of the design and operation of monitoring programmes and systems for different radionuclides, environmental media and types of facility are given in IAEA Safety Guide RS–G-1.8 [34] and in IAEA Safety Report No. 64. [35] Radiation monitoring is often carried out using networks of fixed and deployable sensors such as the US Environmental Protection Agency 's Radnet and the SPEEDI network in Japan. Airborne surveys are also made by organizations like the Nuclear Emergency Support Team . Microbiological[ edit ] Bacteria and viruses are the most commonly monitored groups of microbiological organisms and even these are only of great relevance where water in the aquatic environment is subsequently used as drinking water or where water contact recreation such as swimming or canoeing is practised. Although pathogens are the primary focus of attention, the principal monitoring effort is almost always directed at much more common indicator species such as Escherichia coli , [36] supplemented by overall coliform bacteria counts. The rationale behind this monitoring strategy is that most human pathogens originate from other humans via the sewage stream. Many sewage treatment plants have no sterilisation final stage and therefore discharge an effluent which, although having a clean appearance, still contains many millions of bacteria per litre, the majority of which are relatively harmless coliform bacteria. Counting the number of harmless (or less harmful) sewage bacteria allows a judgement to be made about the probability of significant numbers of pathogenic bacteria or viruses being present. Where E. coli or coliform levels exceed pre-set trigger values, more intensive monitoring including specific monitoring for pathogenic species is then initiated. Populations[ edit ] Monitoring strategies can produce misleading answers when relaying on counts of species or presence or absence of particular organisms if there is no regard to population size. Understanding the populations dynamics of an organism being monitored is critical. As an example if presence or absence of a particular organism within a 10 km square is the measure adopted by a monitoring strategy, then a reduction of population from 10,000 per square to 10 per square will go unnoticed despite the very significant impact experienced by the organism. Monitoring programmes[ edit ] All scientifically reliable environmental monitoring is performed in line with a published programme. The programme may include the overall objectives of the organisation, references to the specific strategies that helps deliver the objective and details of specific projects or tasks within those strategies the key feature of any programme is the listing of what is being monitored and how that monitoring is to take place and the time-scale over which it should all happen. Typically, and often as an appendix, a monitoring programme will provide a table of locations, dates and sampling methods that are proposed and which, if undertaken in full, will deliver the published monitoring programme. There are a number of commercial software packages which can assist with the implementation of the programme, monitor its progress and flag up inconsistencies or omissions but none of these can provide the key building block which is the programme itself. Environmental monitoring data management systems[ edit ] Given the multiple types and increasing volumes and importance of monitoring data, commercial software Environmental Data Management Systems (EDMS) or E-MDMS are increasingly in common use by regulated industries. They provide a means of managing all monitoring data in a single central place.  Quality validation, compliance checking, verifying all data has been received, and sending alerts are generally automated. Typical interrogation functionality enables comparison of data sets both temporarily and spatially. They will also generate regulatory and other reports. Sampling methods[ edit ] There are a wide range of sampling methods which depend on the type of environment, the material being sampled and the subsequent analysis of the sample. At its simplest a sample can be filling a clean bottle with river water and submitting it for conventional chemical analysis. At the more complex end, sample data may be produced by complex electronic sensing devices taking sub-samples over fixed or variable time periods. Sampling methods include judgmental sampling, simple random sampling, stratified sampling , systematic and grid sampling, adaptive cluster sampling , grab samples, semi-continuous monitoring and continuous, passive sampling , remote surveillance, remote sensing , biomonitoring and other sampling methods. Judgmental sampling[ edit ] In judgmental sampling, the selection of sampling units (i.e., the number and location and/or timing of collecting samples) is based on knowledge of the feature or condition under investigation and on professional judgment. Judgmental sampling is distinguished from probability-based sampling in that inferences are based on professional judgment, not statistical scientific theory. Therefore, conclusions about the target population are limited and depend entirely on the validity and accuracy of professional judgment; probabilistic statements about parameters are not possible. As described in subsequent chapters, expert judgment may also be used in conjunction with other sampling designs to produce effective sampling for defensible decisions. [40] Simple random sampling[ edit ] In simple random sampling, particular sampling units (for example, locations and/or times) are selected using random numbers, and all possible selections of a given number of units are equally likely. For example, a simple random sample of a set of drums can be taken by numbering all the drums and randomly selecting numbers from that list or by sampling an area by using pairs of random coordinates. This method is easy to understand, and the equations for determining sample size are relatively straightforward. Simple random sampling is most useful when the population of interest is relatively homogeneous; i.e., no major patterns of contamination or “hot spots” are expected. The main advantages of this design are: It provides statistically unbiased estimates of the mean, proportions, and variability. It is easy to understand and easy to implement. Sample size calculations and data analysis are very straightforward. In some cases, implementation of a simple random sample can be more difficult than some other types of designs (for example, grid samples) because of the difficulty of precisely identifying random geographic locations. Additionally, simple random sampling can be more costly than other plans if difficulties in obtaining samples due to location causes an expenditure of extra effort. [40] Stratified sampling[ edit ] In stratified sampling , the target population is separated into non-overlapping strata, or subpopulations that are known or thought to be more homogeneous (relative to the environmental medium or the contaminant), so that there tends to be less variation among sampling units in the same stratum than among sampling units in different strata. Strata may be chosen on the basis of spatial or temporal proximity of the units, or on the basis of preexisting information or professional judgment about the site or process. Advantages of this sampling design are that it has potential for achieving greater precision in estimates of the mean and variance, and that it allows computation of reliable estimates for population subgroups of special interest. Greater precision can be obtained if the measurement of interest is strongly correlated with the variable used to make the strata. [40] Systematic and grid sampling[ edit ] In systematic and grid sampling, samples are taken at regularly spaced intervals over space or time. An initial location or time is chosen at random, and then the remaining sampling locations are defined so that all locations are at regular intervals over an area (grid) or time (systematic). Examples Systematic Grid Sampling - Square Grid Systematic Grid Sampling - Triangular Grids of systematic grids include square, rectangular, triangular, or radial grids. Cressie, 1993. In random systematic sampling, an initial sampling location (or time) is chosen at random and the remaining sampling sites are specified so that they are located according to a regular pattern. Random systematic sampling is used to search for hot spots and to infer means, percentiles, or other parameters and is also useful for estimating spatial patterns or trends over time. This design provides a practical and easy method for designating sample locations and ensures uniform coverage of a site, unit, or process. [40] Ranked set sampling is an innovative design that can be highly useful and cost efficient in obtaining better estimates of mean concentration levels in soil and other environmental media by explicitly incorporating the professional judgment of a field investigator or a field screening measurement method to pick specific sampling locations in the field. Ranked set sampling uses a two-phase sampling design that identifies sets of field locations, utilizes inexpensive measurements to rank locations within each set, and then selects one location from each set for sampling. In ranked set sampling, m sets (each of size r) of field locations are identified using simple random sampling. The locations are ranked independently within each set using professional judgment or inexpensive, fast, or surrogate measurements. One sampling unit from each set is then selected (based on the observed ranks) for subsequent measurement using a more accurate and reliable (hence, more expensive) method for the contaminant of interest. Relative to simple random sampling, this design results in more representative samples and so leads to more precise estimates of the population parameters. Ranked set sampling is useful when the cost of locating and ranking locations in the field is low compared to laboratory measurements. It is also appropriate when an inexpensive auxiliary variable (based on expert knowledge or measurement) is available to rank population units with respect to the variable of interest. To use this design effectively, it is important that the ranking method and analytical method are strongly correlated. [40] Adaptive cluster sampling[ edit ] In adaptive cluster sampling , samples are taken using simple random sampling, and additional samples are taken at locations where measurements exceed some threshold value. Several additional rounds of sampling and analysis may be needed. Adaptive cluster sampling tracks the selection probabilities for later phases of sampling so that an unbiased estimate of the population mean can be calculated despite oversampling of certain areas. An example application of adaptive cluster sampling is delineating the borders of a plume of contamination. Adaptive sampling is useful for estimating or searching for rare characteristics in a population and is appropriate for inexpensive, rapid measurements. It enables delineating the boundaries of hot spots, while also using all data collected with appropriate weighting to give unbiased estimates of the population mean. [40] [41] Grab samples[ edit ] Collecting a grab sample on a stream Grab samples are samples taken of a homogeneous material, usually water , in a single vessel. Filling a clean bottle with river water is a very common example. Grab samples provide a good snap-shot view of the quality of the sampled environment at the point of sampling and at the time of sampling. Without additional monitoring, the results cannot be extrapolated to other times or to other parts of the river, lake or ground-water. [41] : 3 In order to enable grab samples or rivers to be treated as representative, repeat transverse and longitudinal transect surveys taken at different times of day and times of year are required to establish that the grab-sample location is as representative as is reasonably possible. For large rivers such surveys should also have regard to the depth of the sample and how to best manage the sampling locations at times of flood and drought. [41] : 8–9 In lakes grab samples are relatively simple to take using depth samplers which can be lowered to a pre-determined depth and then closed trapping a fixed volume of water from the required depth. In all but the shallowest lakes, there are major changes in the chemical composition of lake water at different depths, especially during the summer months when many lakes stratify into a warm, well oxygenated upper layer ( epilimnion ) and a cool de-oxygenated lower layer ( hypolimnion ). In the open seas marine environment grab samples can establish a wide range of base-line parameters such as salinity and a range of cation and anion concentrations. However, where changing conditions are an issue such as near river or sewage discharges, close to the effects of volcanism or close to areas of freshwater input from melting ice, a grab sample can only give a very partial answer when taken on its own. Semi-continuous monitoring and continuous[ edit ] An automated sampling station and data logger (to record temperature, specific conductance, and dissolved oxygen levels) There is a wide range of specialized sampling equipment available that can be programmed to take samples at fixed or variable time intervals or in response to an external trigger. For example, an autosampler can be programmed to start taking samples of a river at 8-minute intervals when the rainfall intensity rises above 1 mm / hour. The trigger in this case may be a remote rain gauge communicating with the sampler by using cell phone or meteor burst [42] technology. Samplers can also take individual discrete samples at each sampling occasion or bulk up samples into composite so that in the course of one day, such a sampler might produce 12 composite samples each composed of 6 sub-samples taken at 20-minute intervals. Continuous or quasi-continuous monitoring involves having an automated analytical facility close to the environment being monitored so that results can, if required, be viewed in real time. Such systems are often established to protect important water supplies such as in the River Dee regulation system but may also be part of an overall monitoring strategy on large strategic rivers where early warning of potential problems is essential. Such systems routinely provide data on parameters such as pH , dissolved oxygen , conductivity , turbidity and ammonia using sondes. [43] It is also possible to operate gas liquid chromatography with mass spectrometry technologies (GLC/MS) to examine a wide range of potential organic pollutants. In all examples of automated bank-side analysis there is a requirement for water to be pumped from the river into the monitoring station. Choosing a location for the pump inlet is equally as critical as deciding on the location for a river grab sample. The design of the pump and pipework also requires careful design to avoid artefacts being introduced through the action of pumping the water. Dissolved oxygen concentration is difficult to sustain through a pumped system and GLC/MS facilities can detect micro-organic contaminants from the pipework and glands . Main article: Passive sampling The use of passive samplers greatly reduces the cost and the need of infrastructure on the sampling location. Passive samplers are semi-disposable and can be produced at a relatively low cost, thus they can be employed in great numbers, allowing for a better cover and more data being collected. Due to being small the passive sampler can also be hidden, and thereby lower the risk of vandalism. Examples of passive sampling devices are the diffusive gradients in thin films (DGT) sampler, Chemcatcher , polar organic chemical integrative sampler (POCIS), semipermeable membrane devices (SPMDs), stabilized liquid membrane devices (SLMDs), and an air sampling pump . Remote surveillance[ edit ] Although on-site data collection using electronic measuring equipment is common-place, many monitoring programmes also use remote surveillance and remote access to data in real time. This requires the on-site monitoring equipment to be connected to a base station via either a telemetry network, land-line, cell phone network or other telemetry system such as Meteor burst. The advantage of remote surveillance is that many data feeds can come into a single base station for storing and analysis. It also enable trigger levels or alert levels to be set for individual monitoring sites and/or parameters so that immediate action can be initiated if a trigger level is exceeded. The use of remote surveillance also allows for the installation of very discrete monitoring equipment which can often be buried, camouflaged or tethered at depth in a lake or river with only a short whip aerial protruding. Use of such equipment tends to reduce vandalism and theft when monitoring in locations easily accessible by the public. Main article: Remote sensing Environmental remote sensing uses UAV , aircraft or satellites to monitor the environment using multi-channel sensors. There are two kinds of remote sensing. Passive sensors detect natural radiation that is emitted or reflected by the object or surrounding area being observed. Reflected sunlight is the most common source of radiation measured by passive sensors and in environmental remote sensing, the sensors used are tuned to specific wavelengths from far infrared through visible light frequencies to the far ultraviolet . The volumes of data that can be collected are very large and require dedicated computational support. The output of data analysis from remote sensing are false colour images which differentiate small differences in the radiation characteristics of the environment being monitored. With a skilful operator choosing specific channels it is possible to amplify differences which are imperceptible to the human eye. In particular it is possible to discriminate subtle changes in chlorophyll a and chlorophyll b concentrations in plants and show areas of an environment with slightly different nutrient regimes. Active remote sensing emits energy and uses a passive sensor to detect and measure the radiation that is reflected or backscattered from the target. LIDAR is often used to acquire information about the topography of an area, especially when the area is large and manual surveying would be prohibitively expensive or difficult. Remote sensing makes it possible to collect data on dangerous or inaccessible areas. Remote sensing applications include monitoring deforestation in areas such as the Amazon Basin , the effects of climate change on glaciers and Arctic and Antarctic regions, and depth sounding of coastal and ocean depths. Orbital platforms collect and transmit data from different parts of the electromagnetic spectrum , which in conjunction with larger scale aerial or ground-based sensing and analysis, provides information to monitor trends such as El Niño and other natural long and short term phenomena. Other uses include different areas of the earth sciences such as natural resource management , land use planning and conservation. [44] Main article: Aquatic biomonitoring The use of living organisms as monitoring tools has many advantages. Organisms living in the environment under study are constantly exposed to the physical, biological and chemical influences of that environment. Organisms that have a tendency to accumulate chemical species can often accumulate significant quantities of material from very low concentrations in the environment. Mosses have been used by many investigators to monitor heavy metal concentrations because of their tendency to selectively adsorb heavy metals. [45] [46] Similarly, eels have been used to study halogenated organic chemicals, as these are adsorbed into the fatty deposits within the eel. [47] Other sampling methods[ edit ] Ecological sampling requires careful planning to be representative and as noninvasive as possible. For grasslands and other low growing habitats the use of a quadrat – a 1-metre square frame – is often used with the numbers and types of organisms growing within each quadrat area counted [48] Sediments and soils require specialist sampling tools to ensure that the material recovered is representative. Such samplers are frequently designed to recover a specified volume of material and may also be designed to recover the sediment or soil living biota as well [49] such as the Ekman grab sampler. Data interpretations[ edit ] The interpretation of environmental data produced from a well designed monitoring programme is a large and complex topic addressed by many publications. Regrettably it is sometimes the case that scientists approach the analysis of results with a pre-conceived outcome in mind and use or misuse statistics to demonstrate that their own particular point of view is correct. Statistics remains a tool that is equally easy to use or to misuse to demonstrate the lessons learnt from environmental monitoring. Environmental quality indices[ edit ] Since the start of science-based environmental monitoring, a number of quality indices have been devised to help classify and clarify the meaning of the considerable volumes of data involved. Stating that a river stretch is in "Class B" is likely to be much more informative than stating that this river stretch has a mean BOD of 4.2, a mean dissolved oxygen of 85%, etc. In the UK the Environment Agency formally employed a system called General Quality Assessment (GQA) which classified rivers into six quality letter bands from A to F based on chemical criteria [50] and on biological criteria. [51] The Environment Agency and its devolved partners in Wales (Countryside Council for Wales, CCW) and Scotland (Scottish Environmental Protection Agency, SEPA) now employ a system of biological, chemical and physical classification for rivers and lakes that corresponds with the EU Water Framework Directive. [52]
Toggle the table of contents Agricultural science From Wikipedia, the free encyclopedia Academic field within biology "Crop Science" redirects here. For the journal, see Crop Science (journal) . For the company, see Bayer CropScience . e Agricultural science (or agriscience for short [1] ) is a broad multidisciplinary field of biology that encompasses the parts of exact, natural, economic and social sciences that are used in the practice and understanding of agriculture . Professionals of the agricultural science are called agricultural scientists or agriculturists . In 1843, John Bennet Lawes and Joseph Henry Gilbert began a set of long-term field experiments at Rothamsted Research in England, some of which are still running as of 2018. [3] [4] [5] In the United States, a scientific revolution in agriculture began with the Hatch Act of 1887 , which used the term "agricultural science". [6] [7] The Hatch Act was driven by farmers' interest in knowing the constituents of early artificial fertilizer. The Smith–Hughes Act of 1917 shifted agricultural education back to its vocational roots, but the scientific foundation had been built. [8] For the next 44 years after 1906, federal expenditures on agricultural research in the United States outpaced private expenditures. [9] : xxi Prominent agricultural scientists[ edit ] Scope[ edit ] Agriculture, agricultural science, and agronomy are often confused.[ by whom? ] However, they cover different concepts: Agriculture is the set of activities that transform the environment for the production of animals and plants for human use. Agriculture concerns techniques, including the application of agronomic research. Agronomy is research and development related to studying and improving plant-based crops. Soil forming factors and soil degradation[ edit ] Agricultural sciences include research and development on: [10] [11] Improving agricultural productivity in terms of quantity and quality (e.g., selection of drought-resistant crops and animals, development of new pesticides , yield-sensing technologies, simulation models of crop growth, in-vitro cell culture techniques) Minimizing the effects of pests ( weeds , insects , pathogens , mollusks , nematodes ) on crop or animal production systems. Transformation of primary products into end-consumer products (e.g., production, preservation, and packaging of dairy products ) Theoretical production ecology , relating to crop production modeling Traditional agricultural systems, sometimes termed subsistence agriculture , which feed most of the poorest people in the world.  These systems are of interest as they sometimes retain a level of integration with natural ecological systems greater than that of industrial agriculture , which may be more sustainable than some modern agricultural systems. Food production and demand on a global basis, with special attention paid to the major producers, such as China, India, Brazil, the US and the EU. Various sciences relating to agricultural resources and the environment (e.g. soil science, agroclimatology); biology of agricultural crops and animals (e.g. crop science, animal science and their included sciences, e.g. ruminant nutrition, farm animal welfare); such fields as agricultural economics and rural sociology; various disciplines encompassed in agricultural engineering .
Foresters of the Austral University of Chile in the Valdivian forests of San Pablo de Tregua , Chile Foresters work for the timber industry , government agencies, conservation groups , local authorities, urban parks boards, citizens' associations, and private landowners . The forestry profession includes a wide diversity of jobs, with educational requirements ranging from college bachelor's degrees to PhDs for highly specialized work. Industrial foresters plan forest regeneration starting with careful harvesting. Urban foresters manage trees in urban green spaces . Foresters work in tree nurseries growing seedlings for woodland creation or regeneration projects. Foresters improve tree genetics . Forest engineers develop new building systems. Professional foresters measure and model the growth of forests with tools like geographic information systems . Foresters may combat insect infestation, disease, forest and grassland wildfire , but increasingly allow these natural aspects of forest ecosystems to run their course when the likelihood of epidemics or risk of life or property are low. Increasingly, foresters participate in wildlife conservation planning and watershed protection. Foresters have been mainly concerned with timber management, especially reforestation ,  forests at prime conditions, and fire control. [15] Forestry plans[ edit ] Foresters develop and implement forest management plans relying on mapped resources, inventories showing an area's topographical features as well as its distribution of trees (by species) and other plant covers. Plans also include landowner objectives, roads, culverts , proximity to human habitation, water features and hydrological conditions, and soil information. Forest management plans typically include recommended silvicultural treatments and a timetable for their implementation. Application of digital maps in Geographic Information systems (GIS) that extracts and integrates different information about forest terrains, soil type and tree covers, etc. using, e.g. laser scanning enhances forest management plans in modern systems. [16] Forest management plans include recommendations to achieve the landowner's objectives and desired future conditions for the property subject to ecological, financial, logistical (e.g. access to resources), and other constraints.  On some properties, plans focus on producing quality wood products for processing or sale. Hence, tree species, quantity, and form, all central to the value of harvested products quality and quantity, tend to be important components of silvicultural plans. Good management plans include consideration of future conditions of the stand after any recommended harvests treatments, including future treatments (particularly in intermediate stand treatments), and plans for natural or artificial regeneration after final harvests. The objectives of landowners and leaseholders influence plans for harvest and subsequent site treatment. In Britain, plans featuring "good forestry practice" must always consider the needs of other stakeholders such as nearby communities or rural residents living within or adjacent to woodland areas. Foresters consider tree felling and environmental legislation when developing plans. Plans instruct the sustainable harvesting and replacement of trees. [17] They indicate whether road building or other forest engineering operations are required. Agriculture and forest leaders are also trying to understand how the climate change legislation will affect what they do. The information gathered will provide the data that will determine the role of agriculture and forestry in a new climate change regulatory system. [15] Forestry as a science[ edit ] Over the past centuries, forestry was regarded as a separate science. With the rise of ecology and environmental science , there has been a reordering in the applied sciences. In line with this view, forestry is a primary land-use science comparable with agriculture . [18] Under these headings, the fundamentals behind the management of natural forests comes by way of natural ecology. Forests or tree plantations, those whose primary purpose is the extraction of forest products, are planned and managed to utilize a mix of ecological and agroecological principles. [19] In many regions of the world there is considerable conflict between forest practices and other societal priorities such as water quality, watershed preservation, sustainable fishing, conservation, and species preservation. [20] Genetic diversity in forestry[ edit ] The provenance of forest reproductive material used to plant forests has a great influence on how the trees develop, hence why it is important to use forest reproductive material of good quality and of high genetic diversity . [21] More generally, all forest management practices, including in natural regeneration systems , may impact the genetic diversity of trees. The term genetic diversity describes the differences in DNA sequence between individuals as distinct from variation caused by environmental influences. The unique genetic composition of an individual (its genotype ) will determine its performance (its phenotype ) at a particular site. [22] Genetic diversity is needed to maintain the vitality of forests and to provide resilience to pests and diseases . Genetic diversity also ensures that forest trees can survive, adapt and evolve under changing environmental conditions. Furthermore, genetic diversity is the foundation of biological diversity at species and ecosystem levels. Forest genetic resources are therefore important to consider in forest management. [21] Genetic diversity in forests is threatened by forest fires , pests and diseases, habitat fragmentation , poor silvicultural practices and inappropriate use of forest reproductive material. About 98 million hectares of forest were affected by fire in 2015; this was mainly in the tropical domain, where fire burned about 4 percent of the total forest area in that year. More than two-thirds of the total forest area affected was in Africa and South America. Insects, diseases and severe weather events damaged about 40 million hectares of forests in 2015, mainly in the temperate and boreal domains. [23] Furthermore, the marginal populations of many tree species are facing new threats due to the effects of climate change . [21] Most countries in Europe have recommendations or guidelines for selecting species and provenances that can be used in a given site or zone. [22] History and background[ edit ] The preindustrial age has been dubbed by Werner Sombart and others as the 'wooden age', as timber and firewood were the basic resources for energy, construction and housing. The development of modern forestry is closely connected with the rise of capitalism , the economy as a science and varying notions of land use and property. [24] Roman Latifundiae , large agricultural estates, were quite successful in maintaining the large supply of wood that was necessary for the Roman Empire. [25] Large deforestations came with the decline of the Romans. [25] However  already in the 5th century, monks in the then Byzantine Romagna on the Adriatic coast, were able to establish stone pine plantations to provide fuelwood and food . [26] This was the beginning of the massive forest mentioned by Dante Alighieri in his 1308 poem Divine Comedy . [26] Similar sustainable formal forestry practices were developed by the Visigoths in the 7th century when, faced with the ever-increasing shortage of wood, they instituted a code concerned with the preservation of oak and pine forests. [26] The use and management of many forest resources has a long history in China as well, dating back to the Han dynasty and taking place under the landowning gentry . A similar approach was used in Japan. It was also later written about by the Ming dynasty Chinese scholar Xu Guangqi (1562–1633). In Europe, land usage rights in medieval and early modern times allowed different users to access forests and pastures. Plant litter and resin extraction were important, as pitch (resin) was essential for the caulking of ships, falking and hunting rights, firewood and building, timber gathering in wood pastures , and grazing animals in forests. The notion of " commons " (German "Allmende") refers to the underlying traditional legal term of common land . The idea of enclosed private property came about during modern times. However, most hunting rights were retained by members of the nobility which preserved the right of the nobility to access and use common land for recreation, like fox hunting . Early modern forestry development[ edit ] Forestry work in Austria Hans Carl von Carlowitz , German miner Systematic management of forests for a sustainable yield of timber began in Portugal in the 13th century when King Afonso III planted the Pinhal do Rei (King's Pine Forest) near Leiria to prevent coastal erosion and soil degradation , and as a sustainable source for timber used in naval construction. [27] His successor King Denis of Portugal continued the practice and the forest exists still today. [28] Forest management also flourished in the German states in the 14th century, e.g. in Nuremberg , [29] and in 16th-century Japan . [30] Typically, a forest was divided into specific sections and mapped; the harvest of timber was planned with an eye to regeneration. As timber rafting allowed for connecting large continental forests, as in south western Germany, via Main, Neckar, Danube and Rhine with the coastal cities and states, early modern forestry and remote trading were closely connected. Large firs in the black forest were called „Holländer“, as they were traded to the Dutch ship yards. Large timber rafts on the Rhine were 200 to 400m in length, 40m in width and consisted of several thousand logs. The crew consisted of 400 to 500 men, including shelter, bakeries, ovens and livestock stables. [31] Timber rafting infrastructure allowed for large interconnected networks all over continental Europe and is still of importance in Finland. Starting with the 16th century, enhanced world maritime trade , a boom in housing construction in Europe, and the success and further Berggeschrey (rushes) of the mining industry increased timber consumption sharply. The notion of 'Nachhaltigkeit', sustainability in forestry, is closely connected to the work of Hans Carl von Carlowitz (1645–1714), a mining administrator in Saxony . His book Sylvicultura oeconomica, oder haußwirthliche Nachricht und Naturmäßige Anweisung zur wilden Baum-Zucht (1713) was the first comprehensive treatise about sustainable yield forestry. [32] In the UK, and, to an extent, in continental Europe, the enclosure movement and the Clearances favored  strictly enclosed private property. [33] The Agrarian reformers, early economic writers and scientists tried to get rid of the traditional commons. [34] At the time, an alleged tragedy of the commons together with fears of a Holznot , an imminent wood shortage played a watershed role in the controversies about cooperative land use patterns. [35] The practice of establishing tree plantations in the British Isles was promoted by John Evelyn , though it had already acquired some popularity. Louis XIV 's minister Jean-Baptiste Colbert 's oak Forest of Tronçais , planted for the future use of the French Navy , matured as expected in the mid-19th century: "Colbert had thought of everything except the steamship," Fernand Braudel observed. [36] Colbert's vision of forestry management was encoded in the French forestry Ordinance of 1669 , which proved to be an influential management system throughout Europe. [37] In parallel, schools of forestry were established beginning in the late 18th century in Hesse , Russia , Austria-Hungary , Sweden , France and elsewhere in Europe. Forest conservation and early globalization[ edit ] Further information: forest conservation Starting from the 1750s modern scientific forestry was developed in France and the German speaking countries in the context of natural history scholarship and state administration inspired by physiocracy and cameralism . [38] Its main traits were centralized management by professional foresters, the adherence to sustainable yield concepts with a bias towards fuelwood and timber production, artificial afforestation, and a critical view of pastoral and agricultural uses of forests. [39] During the late 19th and early 20th centuries, forest preservation programs were established in British India , the United States , and Europe. Many foresters were either from continental Europe (like Sir Dietrich Brandis ), or educated there (like Gifford Pinchot ). Sir Dietrich Brandis is considered the father of tropical forestry, European concepts and practices had to be adapted in tropical and semi-arid climate zones. The development of plantation forestry was one of the (controversial) answers to the specific challenges in the tropical colonies. The enactment and evolution of forest laws and binding regulations occurred in most Western nations in the 20th century in response to growing conservation concerns and the increasing technological capacity of logging companies. Tropical forestry is a separate branch of forestry which deals mainly with equatorial forests that yield woods such as teak and mahogany . Forest and landscape restoration[ edit ] Further information: Forest restoration Forest and landscape restoration (FLR) is defined as a process that aims to regain ecological functionality and enhance human well-being in deforested or degraded landscapes. [40] FLR has been developed as a response to the growing degradation and loss of forest and land, which resulted in declined biodiversity and ecosystem services. [40] Effective FLR will support the achievement of the Sustainable Development Goals . [40] The United Nations Decade on Ecosystem Restoration (2021–2030) provides the opportunity to restore hundreds of millions of hectares of degraded forests and other ecosystems. [40] Mechanization[ edit ] Forestry mechanization was always in close connection to metal working and the development of mechanical tools to cut and transport timber to its destination. [41] Rafting belongs to the earliest means of transport. Steel saws came up in the 15th century. The 19th century widely increased the availability of steel for whipsaws and introduced forest railways and railways in general for transport and as forestry customer. Further human induced changes, however, came since World War II, respectively in line with the "1950s syndrome". [42] The first portable chainsaw was invented in 1918 in Canada , but large impact of mechanization in forestry started after World War II. [43] Forestry harvesters are among the most recent developments. Although drones, planes , laser scanning , satellites and robots also play a part in forestry. Early journals which are still present[ edit ]
Toggle the table of contents Oceanography From Wikipedia, the free encyclopedia Study of physical, chemical, and biological processes in the ocean Thermohaline circulation Oceanography (from Ancient Greek ὠκεανός (ōkeanós) ' ocean ', and γραφή (graphḗ) ' writing '), also known as oceanology, sea science, ocean science, and marine science, is the scientific study of the oceans. It is an Earth science , which covers a wide range of topics, including ecosystem dynamics; ocean currents , waves , and geophysical fluid dynamics ; plate tectonics and seabed geology; and fluxes of various chemical substances and physical properties within the ocean and across its boundaries. These diverse topics reflect multiple disciplines that oceanographers utilize to glean further knowledge of the world ocean , including astronomy , biology , chemistry , geography , geology , hydrology , meteorology and physics . Paleoceanography studies the history of the oceans in the geologic past. An oceanographer is a person who studies many matters concerned with oceans, including marine geology , physics , chemistry , and biology . Early history[ edit ] Humans first acquired knowledge of the waves and currents of the seas and oceans in pre-historic times. Observations on tides were recorded by Aristotle and Strabo in 384–322 BC. [1] Early exploration of the oceans was primarily for cartography and mainly limited to its surfaces and of the animals that fishermen brought up in nets, though depth soundings by lead line were taken. The Portuguese campaign of Atlantic navigation is the earliest example of a systematic scientific large project, sustained over many decades, studying the currents and winds of the Atlantic. The work of Pedro Nunes (1502–1578) is remembered in the navigation context for the determination of the loxodromic curve: the shortest course between two points on the surface of a sphere represented onto a two-dimensional map. [2] [3] When he published his "Treatise of the Sphere" (1537), mostly a commentated translation of earlier work by others, he included a treatise on geometrical and astronomic methods of navigation. There he states clearly that Portuguese navigations were not an adventurous endeavour: "nam se fezeram indo a acertar: mas partiam os nossos mareantes muy ensinados e prouidos de estromentos e regras de astrologia e geometria que sam as cousas que os cosmographos ham dadar apercebidas (...) e leuaua cartas muy particularmente rumadas e na ja as de que os antigos vsauam" (were not done by chance: but our seafarers departed well taught and provided with instruments and rules of astrology (astronomy) and geometry which were matters the cosmographers would provide (...) and they took charts with exact routes and no longer those used by the ancient). [4] His credibility rests on being personally involved in the instruction of pilots and senior seafarers from 1527 onwards by Royal appointment, along with his recognized competence as mathematician and astronomer. [2] The main problem in navigating back from the south of the Canary Islands (or south of Boujdour ) by sail alone, is due to the change in the regime of winds and currents: the North Atlantic gyre and the Equatorial counter current [5] will push south along the northwest bulge of Africa, while the uncertain winds where the Northeast trades meet the Southeast trades (the doldrums) [6] leave a sailing ship to the mercy of the currents. Together, prevalent current and wind make northwards progress very difficult or impossible. It was to overcome this problem and clear the passage to India around Africa as a viable maritime trade route, that a systematic plan of exploration was devised by the Portuguese. The return route from regions south of the Canaries became the ' volta do largo' or 'volta do mar '. The 'rediscovery' of the Azores islands in 1427 is merely a reflection of the heightened strategic importance of the islands, now sitting on the return route from the western coast of Africa (sequentially called 'volta de Guiné' and 'volta da Mina'); and the references to the Sargasso Sea (also called at the time 'Mar da Baga'), to the west of the Azores , in 1436, reveals the western extent of the return route. [7] This is necessary, under sail, to make use of the southeasterly and northeasterly winds away from the western coast of Africa, up to the northern latitudes where the westerly winds will bring the seafarers towards the western coasts of Europe. [8] The secrecy involving the Portuguese navigations, with the death penalty for the leaking of maps and routes, concentrated all sensitive records in the Royal Archives, completely destroyed by the Lisbon earthquake of 1775 . However, the systematic nature of the Portuguese campaign, mapping the currents and winds of the Atlantic, is demonstrated by the understanding of the seasonal variations, with expeditions setting sail at different times of the year taking different routes to take account of seasonal predominate winds. This happens from as early as late 15th century and early 16th: Bartolomeu Dias followed the African coast on his way south in August 1487, while Vasco da Gama would take an open sea route from the latitude of Sierra Leone , spending 3 months in the open sea of the South Atlantic to profit from the southwards deflection of the southwesterly on the Brazilian side (and the Brazilian current going southward) - Gama departed in July 1497); and Pedro Alvares Cabral , departing March 1500) took an even larger arch to the west, from the latitude of Cape Verde, thus avoiding the summer monsoon (which would have blocked the route taken by Gama at the time he set sail). [9] Furthermore, there were systematic expeditions pushing into the western Northern Atlantic (Teive, 1454; Vogado, 1462; Teles, 1474; Ulmo, 1486). [10] The documents relating to the supplying of ships, and the ordering of sun declination tables for the southern Atlantic for as early as 1493–1496, [11] all suggest a well-planned and systematic activity happening during the decade long period between Bartolomeu Dias finding the southern tip of Africa, and Gama's departure; additionally, there are indications of further travels by Bartolomeu Dias in the area. [7] The most significant consequence of this systematic knowledge was the negotiation of the Treaty of Tordesillas in 1494, moving the line of demarcation 270 leagues to the west (from 100 to 370 leagues west of the Azores), bringing what is now Brazil into the Portuguese area of domination. The knowledge gathered from open sea exploration allowed for the well-documented extended periods of sail without sight of land, not by accident but as pre-determined planned route; for example, 30 days for Bartolomeu Dias culminating on Mossel Bay , the 3 months Gama spent in the South Atlantic to use the Brazil current (southward), or the 29 days Cabral took from Cape Verde up to landing in Monte Pascoal , Brazil. The Danish expedition to Arabia 1761–67 can be said to be the world's first oceanographic expedition, as the ship Grønland had on board a group of scientists, including naturalist Peter Forsskål , who was assigned an explicit task by the king, Frederik V , to study and describe the marine life in the open sea, including finding the cause of mareel , or milky seas. For this purpose, the expedition was equipped with nets and scrapers, specifically designed to collect samples from the open waters and the bottom at great depth. [12] Although Juan Ponce de León in 1513 first identified the Gulf Stream , and the current was well known to mariners, Benjamin Franklin made the first scientific study of it and gave it its name. Franklin measured water temperatures during several Atlantic crossings and correctly explained the Gulf Stream's cause. Franklin and Timothy Folger printed the first map of the Gulf Stream in 1769–1770. [13] [14] 1799 map of the currents in the Atlantic and Indian Oceans , by James Rennell Information on the currents of the Pacific Ocean was gathered by explorers of the late 18th century, including James Cook and Louis Antoine de Bougainville . James Rennell wrote the first scientific textbooks on oceanography, detailing the current flows of the Atlantic and Indian oceans. During a voyage around the Cape of Good Hope in 1777, he mapped "the banks and currents at the Lagullas ". He was also the first to understand the nature of the intermittent current near the Isles of Scilly , (now known as Rennell's Current). [15] Sir James Clark Ross took the first modern sounding in deep sea in 1840, and Charles Darwin published a paper on reefs and the formation of atolls as a result of the second voyage of HMS Beagle in 1831–1836. Robert FitzRoy published a four-volume report of Beagle's three voyages. In 1841–1842 Edward Forbes undertook dredging in the Aegean Sea that founded marine ecology. The first superintendent of the United States Naval Observatory (1842–1861), Matthew Fontaine Maury devoted his time to the study of marine meteorology, navigation , and charting prevailing winds and currents. His 1855 textbook Physical Geography of the Sea was one of the first comprehensive oceanography studies. Many nations sent oceanographic observations to Maury at the Naval Observatory, where he and his colleagues evaluated the information and distributed the results worldwide. [16] Modern oceanography[ edit ] Knowledge of the oceans remained confined to the topmost few fathoms of the water and a small amount of the bottom, mainly in shallow areas. Almost nothing was known of the ocean depths. The British Royal Navy 's efforts to chart all of the world's coastlines in the mid-19th century reinforced the vague idea that most of the ocean was very deep, although little more was known. As exploration ignited both popular and scientific interest in the polar regions and Africa , so too did the mysteries of the unexplored oceans. HMS Challenger undertook the first global marine research expedition in 1872. The seminal event in the founding of the modern science of oceanography was the 1872–1876 Challenger expedition . As the first true oceanographic cruise, this expedition laid the groundwork for an entire academic and research discipline. [17] In response to a recommendation from the Royal Society , the British Government announced in 1871 an expedition to explore world's oceans and conduct appropriate scientific investigation. Charles Wyville Thomson and Sir John Murray launched the Challenger expedition . Challenger , leased from the Royal Navy, was modified for scientific work and equipped with separate laboratories for natural history and chemistry . [18] Under the scientific supervision of Thomson, Challenger travelled nearly 70,000 nautical miles (130,000 km) surveying and exploring. On her journey circumnavigating the globe, [18] 492 deep sea soundings, 133 bottom dredges, 151 open water trawls and 263 serial water temperature observations were taken. [19] Around 4,700 new species of marine life were discovered. The result was the Report Of The Scientific Results of the Exploring Voyage of H.M.S. Challenger during the years 1873–76. Murray, who supervised the publication, described the report as "the greatest advance in the knowledge of our planet since the celebrated discoveries of the fifteenth and sixteenth centuries". He went on to found the academic discipline of oceanography at the University of Edinburgh , which remained the centre for oceanographic research well into the 20th century. [20] Murray was the first to study marine trenches and in particular the Mid-Atlantic Ridge , and map the sedimentary deposits in the oceans. He tried to map out the world's ocean currents based on salinity and temperature observations, and was the first to correctly understand the nature of coral reef development. In the late 19th century, other Western nations also sent out scientific expeditions (as did private individuals and institutions). The first purpose-built oceanographic ship, Albatros, was built in 1882. In 1893, Fridtjof Nansen allowed his ship, Fram, to be frozen in the Arctic ice. This enabled him to obtain oceanographic, meteorological and astronomical data at a stationary spot over an extended period. Writer and geographer John Francon Williams FRGS commemorative plaque, Clackmannan Cemetery 2019 In 1881 the geographer John Francon Williams published a seminal book, Geography of the Oceans. [21] [22] [23] Between 1907 and 1911 Otto Krümmel published the Handbuch der Ozeanographie, which became influential in awakening public interest in oceanography. [24] The four-month 1910 North Atlantic expedition headed by John Murray and Johan Hjort was the most ambitious research oceanographic and marine zoological project ever mounted until then, and led to the classic 1912 book The Depths of the Ocean. The first acoustic measurement of sea depth was made in 1914. Between 1925 and 1927 the "Meteor" expedition gathered 70,000 ocean depth measurements using an echo sounder, surveying the Mid-Atlantic Ridge. In 1934, Easter Ellen Cupp , the first woman to have earned a PhD (at Scripps) in the United States, completed a major work on diatoms [25] that remained the standard taxonomy in the field until well after her death in 1999. In 1940, Cupp was let go from her position at Scripps. Sverdrup specifically commended Cupp as a conscientious and industrious worker and commented that his decision was no reflection on her ability as a scientist. Sverdrup used the instructor billet vacated by Cupp to employ Marston Sargent,a biologist studying marine algae, which was not a new research program at Scripps. Financial pressures did not prevent Sverdrup from retaining the services of two other young post-doctoral students, Walter Munk and Roger Revelle . Cupp's partner, Dorothy Rosenbury, found her a position teaching high school, where she remained for the rest of her career. (Russell, 2000) Sverdrup, Johnson and Fleming published The Oceans in 1942, [26] which was a major landmark. The Sea (in three volumes, covering physical oceanography, seawater and geology) edited by M.N. Hill was published in 1962, while Rhodes Fairbridge 's Encyclopedia of Oceanography was published in 1966. The Great Global Rift, running along the Mid Atlantic Ridge, was discovered by Maurice Ewing and Bruce Heezen in 1953 and mapped by Heezen and Marie Tharp using bathymetric data; in 1954 a mountain range under the Arctic Ocean was found by the Arctic Institute of the USSR. The theory of seafloor spreading was developed in 1960 by Harry Hammond Hess . The Ocean Drilling Program started in 1966. Deep-sea vents were discovered in 1977 by Jack Corliss and Robert Ballard in the submersible DSV Alvin . In the 1950s, Auguste Piccard invented the bathyscaphe and used the bathyscaphe Trieste to investigate the ocean's depths. The United States nuclear submarine Nautilus made the first journey under the ice to the North Pole in 1958. In 1962 the FLIP (Floating Instrument Platform), a 355-foot (108 m) spar buoy, was first deployed. In 1968, Tanya Atwater led the first all-woman oceanographic expedition. Until that time, gender policies restricted women oceanographers from participating in voyages to a significant extent. From the 1970s, there has been much emphasis on the application of large scale computers to oceanography to allow numerical predictions of ocean conditions and as a part of overall environmental change prediction. Early techniques included analog computers (such as the Ishiguro Storm Surge Computer ) generally now replaced by numerical methods (e.g. SLOSH .) An oceanographic buoy array was established in the Pacific to allow prediction of El Niño events. 1990 saw the start of the World Ocean Circulation Experiment (WOCE) which continued until 2002. Geosat seafloor mapping data became available in 1995. Study of the oceans is critical to understanding shifts in Earth's energy balance along with related global and regional changes in climate , the biosphere and biogeochemistry . The atmosphere and ocean are linked because of evaporation and precipitation as well as thermal flux (and solar insolation ). Recent studies have advanced knowledge on ocean acidification , ocean heat content , ocean currents , sea level rise , the oceanic carbon cycle , the water cycle , Arctic sea ice decline , coral bleaching , marine heatwaves , extreme weather , coastal erosion and many other phenomena in regards to ongoing climate change and climate feedbacks . In general, understanding the world ocean through further scientific study enables better stewardship and sustainable utilization of Earth's resources. [27] The Intergovernmental Oceanographic Commission reports that 1.7% of the total national research expenditure of its members is focused on ocean science. [28] Branches[ edit ] Oceanographic frontal systems on the Southern Hemisphere The Applied Marine Physics Building at the University of Miami 's Rosenstiel School of Marine, Atmospheric, and Earth Science on Virginia Key , in September 2007 The study of oceanography is divided into these five branches:
Toggle the table of contents Limnology From Wikipedia, the free encyclopedia Science of inland aquatic ecosystems Limnology ( /lɪmˈnɒlədʒi/ lim-NOL-ə-jee ; from Ancient Greek λίμνη (límnē) 'lake', and -λογία ( -logía ) 'study of') is the study of inland aquatic ecosystems . [1] The study of limnology includes aspects of the biological , chemical , physical , and geological characteristics of fresh and saline , natural and man-made bodies of water . This includes the study of lakes , reservoirs , ponds , rivers , springs , streams , wetlands , and groundwater . [2] Water systems are often categorized as either running ( lotic ) or standing ( lentic ). [3] Limnology includes the study of the drainage basin, movement of water through the basin and biogeochemical changes that occur en route. A more recent sub-discipline of limnology, termed landscape limnology , studies, manages, and seeks to conserve these ecosystems using a landscape perspective, by explicitly examining connections between an aquatic ecosystem and its drainage basin . Recently, the need to understand global inland waters as part of the Earth system created a sub-discipline called global limnology. [4] This approach considers processes in inland waters on a global scale, like the role of inland aquatic ecosystems in global biogeochemical cycles . [5] [6] [7] [8] [9] Limnology is closely related to aquatic ecology and hydrobiology , which study aquatic organisms and their interactions with the abiotic (non-living) environment. While limnology has substantial overlap with freshwater-focused disciplines (e.g., freshwater biology ), it also includes the study of inland salt lakes. History[ edit ] The term limnology was coined by François-Alphonse Forel (1841–1912) who established the field with his studies of Lake Geneva . Interest in the discipline rapidly expanded, and in 1922 August Thienemann (a German zoologist) and Einar Naumann (a Swedish botanist) co-founded the International Society of Limnology (SIL, from Societas Internationalis Limnologiae ).  Forel's original definition of limnology, "the oceanography of lakes", was expanded to encompass the study of all inland waters, [2] and influenced Benedykt Dybowski 's work on Lake Baikal . Physical properties[ edit ] Physical properties of aquatic ecosystems are determined by a combination of heat, currents, waves and other seasonal distributions of environmental conditions. [13] The morphometry of a body of water depends on the type of feature (such as a lake, river, stream, wetland, estuary etc.) and the structure of the earth surrounding the body of water. Lakes , for instance, are classified by their formation, and zones of lakes are defined by water depth. [14] [15] River and stream system morphometry is driven by underlying geology of the area as well as the general velocity of the water. [13] Stream morphometry is also influenced by topography (especially slope) as well as precipitation patterns and other factors such as vegetation and land development. Connectivity between streams and lakes relates to the landscape drainage density , lake surface area and lake shape . [15] Other types of aquatic systems which fall within the study of limnology are estuaries . Estuaries are bodies of water classified by the interaction of a river and the ocean or sea. [13] Wetlands vary in size, shape, and pattern however the most common types, marshes, bogs and swamps, often fluctuate between containing shallow, freshwater and being dry depending on the time of year. [13] The volume and quality of water in underground aquifers rely on the vegetation cover, which fosters recharge and aids in maintaining water quality. [16] Light interactions[ edit ] Light zonation is the concept of how the amount of sunlight penetration into water influences the structure of a body of water. [13] These zones define various levels of productivity within an aquatic ecosystems such as a lake. For instance, the depth of the water column which sunlight is able to penetrate and where most plant life is able to grow is known as the photic or euphotic zone. The rest of the water column which is deeper and does not receive sufficient amounts of sunlight for plant growth is known as the aphotic zone . [13] The amount of solar energy present underwater and the spectral quality of the light that are present at various depths have a significant impact on the behavior of many aquatic organisms. For example, zooplankton's vertical migration is influenced by solar energy levels. [16] Thermal stratification[ edit ] Similar to light zonation, thermal stratification or thermal zonation is a way of grouping parts of the water body within an aquatic system based on the temperature of different lake layers. The less turbid the water, the more light is able to penetrate, and thus heat is conveyed deeper in the water. [17] Heating declines exponentially with depth in the water column, so the water will be warmest near the surface but progressively cooler as moving downwards. There are three main sections that define thermal stratification in a lake. The epilimnion is closest to the water surface and absorbs long- and shortwave radiation to warm the water surface. During cooler months, wind shear can contribute to cooling of the water surface. The thermocline is an area within the water column where water temperatures rapidly decrease. [17] The bottom layer is the hypolimnion , which tends to have the coldest water because its depth restricts sunlight from reaching it. [17] In temperate lakes, fall-season cooling of surface water results in turnover of the water column, where the thermocline is disrupted, and the lake temperature profile becomes more uniform. In cold climates, when water cools below 4oC (the temperature of maximum density) many lakes can experience an inverse thermal stratification in winter. [18] These lakes are often dimictic , with a brief spring overturn in addition to longer fall overturn. The relative thermal resistance is the energy needed to mix these strata of different temperatures. [19] Lake Heat Budget[ edit ] An annual heat budget, also shown as θa, is the total amount of heat needed to raise the water from its minimum winter temperature to its maximum summer temperature. This can be calculated by integrating the area of the lake at each depth interval (Az) multiplied by the difference between the summer (θsz) and winter (θwz) temperatures or {\displaystyle \displaystyle \int } Chemical properties[ edit ] The chemical composition of water in aquatic ecosystems is influenced by natural characteristics and processes including precipitation , underlying soil and bedrock in the drainage basin , erosion , evaporation , and sedimentation . [13] All bodies of water have a certain composition of both organic and inorganic elements and compounds. Biological reactions also affect the chemical properties of water. In addition to natural processes, human activities strongly influence the chemical composition of aquatic systems and their water quality. [17] Allochthonous sources of carbon or nutrients come from outside the aquatic system (such as plant and soil material).  Carbon sources from within the system, such as algae and the microbial breakdown of aquatic particulate organic carbon , are autochthonous. In aquatic food webs, the portion of biomass derived from allochthonous material is then named "allochthony". [20] In streams and small lakes, allochthonous sources of carbon are dominant while in large lakes and the ocean, autochthonous sources dominate. [21] Oxygen and carbon dioxide[ edit ] Dissolved oxygen and dissolved carbon dioxide are often discussed together due their coupled role in respiration and photosynthesis . Dissolved oxygen concentrations can be altered by physical, chemical, and biological processes and reaction. Physical processes including wind mixing can increase dissolved oxygen concentrations, particularly in surface waters of aquatic ecosystems. Because dissolved oxygen solubility is linked to water temperatures, changes in temperature affect dissolved oxygen concentrations as warmer water has a lower capacity to "hold" oxygen as colder water. [22] Biologically, both photosynthesis and aerobic respiration affect dissolved oxygen concentrations. [17] Photosynthesis by autotrophic organisms , such as phytoplankton and aquatic algae , increases dissolved oxygen concentrations while simultaneously reducing carbon dioxide concentrations, since carbon dioxide is taken up during photosynthesis. [22] All aerobic organisms in the aquatic environment take up dissolved oxygen during aerobic respiration, while carbon dioxide is released as a byproduct of this reaction. Because photosynthesis is light-limited, both photosynthesis and respiration occur during the daylight hours, while only respiration occurs during dark hours or in dark portions of an ecosystem. The balance between dissolved oxygen production and consumption is calculated as the aquatic metabolism rate . [23] Lake cross-sectional diagram of the factors influencing lake metabolic rates and concentration of dissolved gases within lakes. Processes in gold text consume oxygen and produce carbon dioxide while processes in green text produce oxygen and consume carbon dioxide. Vertical changes in the concentrations of dissolved oxygen are affected by both wind mixing of surface waters and the balance between photosynthesis and respiration of organic matter . These vertical changes, known as profiles, are based on similar principles as thermal stratification and light penetration. As light availability decreases deeper in the water column, photosynthesis rates also decrease, and less dissolved oxygen is produced. This means that dissolved oxygen concentrations generally decrease as you move deeper into the body of water because of photosynthesis is not replenishing dissolved oxygen that is being taken up through respiration. [17] During periods of thermal stratification, water density gradients prevent oxygen-rich surface waters from mixing with deeper waters. Prolonged periods of stratification can result in the depletion of bottom-water dissolved oxygen; when dissolved oxygen concentrations are below 2 milligrams per liter, waters are considered hypoxic . [22] When dissolved oxygen concentrations are approximately 0 milligrams per liter, conditions are anoxic . Both hypoxic and anoxic waters reduce available habitat for organisms that respire oxygen, and contribute to changes in other chemical reactions in the water. [22] Nitrogen and phosphorus[ edit ] Nitrogen and phosphorus are ecologically significant nutrients in aquatic systems. Nitrogen is generally present as a gas in aquatic ecosystems however most water quality studies tend to focus on nitrate , nitrite and ammonia levels. [13] Most of these dissolved nitrogen compounds follow a seasonal pattern with greater concentrations in the fall and winter months compared to the spring and summer . [13] Phosphorus has a different role in aquatic ecosystems as it is a limiting factor in the growth of phytoplankton because of generally low concentrations in the water. [13] Dissolved phosphorus is also crucial to all living things, is often very limiting to primary productivity in freshwater, and has its own distinctive ecosystem cycling . [17] Lake George , New York , United States, an oligotrophic lake Role in ecology[ edit ] Lakes "are relatively easy to sample, because they have clear-cut boundaries (compared to terrestrial ecosystems) and because field experiments are relatively easy to perform.", which make then especially useful for ecologists who try to understand ecological dynamics. [24] Lake trophic classification[ edit ] One way to classify lakes (or other bodies of water) is with the trophic state index . [2] An oligotrophic lake is characterized by relatively low levels of primary production and low levels of nutrients . A eutrophic lake has high levels of primary productivity due to very high nutrient levels. Eutrophication of a lake can lead to algal blooms . Dystrophic lakes have high levels of humic matter and typically have yellow-brown, tea-coloured waters. [2] These categories do not have rigid specifications; the classification system can be seen as more of a spectrum encompassing the various levels of aquatic productivity.[ citation needed ] Tropical limnology[ edit ] Tropical limnology is a unique and important subfield of limnology that focuses on the distinct physical, chemical, biological, and cultural aspects of freshwater systems in tropical regions . [25] The physical and chemical properties of tropical aquatic environments are different from those in temperate regions , with warmer and more stable temperatures, higher nutrient levels, and more complex ecological interactions. [25] Moreover, the biodiversity of tropical freshwater systems is typically higher, human impacts are often more severe, and there are important cultural and socioeconomic factors that influence the use and management of these systems. [25] Professional organizations[ edit ] People who study limnology are called limnologists. These scientists largely study the characteristics of inland fresh-water systems such as lakes, rivers, streams, ponds and wetlands. They may also study non-oceanic bodies of salt water, such as the Great Salt Lake. There are many professional organizations related to limnology and other aspects of the aquatic science, including the Association for the Sciences of Limnology and Oceanography , the Asociación Ibérica de Limnología , the International Society of Limnology , the Polish Limnological Society , the Society of Canadian Limnologists, and the Freshwater Biological Association .[ citation needed ]
Toggle the table of contents Meteorology From Wikipedia, the free encyclopedia Interdisciplinary scientific study of the atmosphere focusing on weather forecasting This article is about the study of weather. For the treatise by Aristotle, see Meteorology (Aristotle) . For the science of measurement, see Metrology . For the study of meteors, see Meteoritics . e Meteorology is a branch of the atmospheric sciences (which include atmospheric chemistry and physics) with a major focus on weather forecasting . The study of meteorology dates back millennia , though significant progress in meteorology did not begin until the 18th century. The 19th century saw modest progress in the field after weather observation networks were formed across broad regions. Prior attempts at prediction of weather depended on historical data. It was not until after the elucidation of the laws of physics, and more particularly in the latter half of the 20th century, the development of the computer (allowing for the automated solution of a great many modelling equations) that significant breakthroughs in weather forecasting were achieved. An important branch of weather forecasting is marine weather forecasting as it relates to maritime and coastal safety, in which weather effects also include atmospheric interactions with large bodies of water. Meteorological phenomena are observable weather events that are explained by the science of meteorology. Meteorological phenomena are described and quantified by the variables of Earth's atmosphere: temperature, air pressure, water vapour , mass flow , and the variations and interactions of these variables, and how they change over time. Different spatial scales are used to describe and predict weather on local, regional, and global levels. Meteorology, climatology , atmospheric physics , and atmospheric chemistry are sub-disciplines of the atmospheric sciences . Meteorology and hydrology compose the interdisciplinary field of hydrometeorology . The interactions between Earth's atmosphere and its oceans are part of a coupled ocean-atmosphere system. Meteorology has application in many diverse fields such as the military, energy production, transport, agriculture, and construction. The word meteorology is from the Ancient Greek μετέωρος metéōros (meteor) and -λογία -logia ( -(o)logy ), meaning "the study of things high in the air". Parhelion (sundog) in Savoie Early attempts at predicting weather were often related to prophecy and divining , and were sometimes based on astrological ideas. Ancient religions believed meteorological phenomena to be under the control of the gods. [1] The ability to predict rains and floods based on annual cycles was evidently used by humans at least from the time of agricultural settlement if not earlier. Early approaches to predicting weather were based on astrology and were practiced by priests. The Egyptians had rain-making rituals as early as 3500 BC. [1] Ancient Indian Upanishads contain mentions of clouds and seasons . [2] The Samaveda mentions sacrifices to be performed when certain phenomena were noticed. [3] Varāhamihira 's classical work Brihatsamhita, written about 500 AD, [2] provides evidence of weather observation. Cuneiform inscriptions on Babylonian tablets included associations between thunder and rain. The Chaldeans differentiated the 22° and 46° halos . [3] The ancient Greeks were the first to make theories about the weather. Many natural philosophers studied the weather. However, as meteorological instruments did not exist, the inquiry was largely qualitative, and could only be judged by more general theoretical speculations. [4] Herodotus states that Thales predicted the solar eclipse of 585 BC. He studied Babylonian equinox tables. [5] According to Seneca, he gave the explanation that the cause of the Nile 's annual floods was due to northerly winds hindering its descent by the sea. [6] Anaximander and Anaximenes thought that thunder and lightning was caused by air smashing against the cloud, thus kindling the flame. Early meteorological theories generally considered that there was a fire-like substance in the atmosphere. Anaximander defined wind as a flowing of air, but this was not generally accepted for centuries. [7] A theory to explain summer hail was first proposed by Anaxagoras . He observed that air temperature decreased with increasing height and that clouds contain moisture. He also noted that heat caused objects to rise, and therefore the heat on a summer day would drive clouds to an altitude where the moisture would freeze. [8] Empedocles theorized on the change of the seasons. He believed that fire and water opposed each other in the atmosphere, and when fire gained the upper hand, the result was summer, and when water did, it was winter. Democritus also wrote about the flooding of the Nile. He said that during the summer solstice, snow in northern parts of the world melted. This would cause vapors to form clouds, which would cause storms when driven to the Nile by northerly winds, thus filling the lakes and the Nile. [9] Hippocrates inquired into the effect of weather on health. Eudoxus claimed that bad weather followed four-year periods, according to Pliny. [10] Aristotelian meteorology[ edit ] These early observations would form the basis for Aristotle 's Meteorology , written in 350 BC. [11] [12] Aristotle is considered the founder of meteorology. [13] One of the most impressive achievements described in the Meteorology is the description of what is now known as the hydrologic cycle . His work would remain an authority on meteorology for nearly 2,000 years. [14] The book De Mundo (composed before 250 BC or between 350 and 200 BC) noted: [15] If the flashing body is set on fire and rushes violently to the Earth it is called a thunderbolt; if it is only half of fire, but violent also and massive, it is called a meteor; if it is entirely free from fire, it is called a smoking bolt. They are all called 'swooping bolts' because they swoop down upon the Earth. Lightning is sometimes smoky and is then called 'smoldering lightning"; sometimes it darts quickly along and is then said to be vivid. At other times, it travels in crooked lines, and is called forked lightning. When it swoops down upon some object it is called 'swooping lightning' After Aristotle, progress in meteorology stalled for a long time. Theophrastus compiled a book on weather forecasting, called the Book of Signs, as well as On Winds. He gave hundreds of signs for weather phenomena for a period up to a year. [16] His system was based on dividing the year by the setting and the rising of the Pleiad, halves into solstices and equinoxes, and the continuity of the weather for those periods. He also divided months into the new moon, fourth day, eighth day and full moon, in likelihood of a change in the weather occurring. The day was divided into sunrise, mid-morning, noon, mid-afternoon and sunset, with corresponding divisions of the night, with change being likely at one of these divisions. [17] Applying the divisions and a principle of balance in the yearly weather, he came up with forecasts like that if a lot of rain falls in the winter, the spring is usually dry. Rules based on actions of animals are also present in his work, like that if a dog rolls on the ground, it is a sign of a storm. Shooting stars and the Moon were also considered significant. However, he made no attempt to explain these phenomena, referring only to the Aristotelian method. [18] The work of Theophrastus remained a dominant influence in weather forecasting for nearly 2,000 years. [19] Meteorology after Aristotle[ edit ] Meteorology continued to be studied and developed over the centuries, but it was not until the Renaissance in the 14th to 17th centuries that significant advancements were made in the field. Scientists such as Galileo and Descartes introduced new methods and ideas, leading to the scientific revolution in meteorology. Speculation on the cause of the flooding of the Nile ended when Eratosthenes , according to Proclus , stated that it was known that man had gone to the sources of the Nile and observed the rains, although interest in its implications continued. [20] During the era of Roman Greece and Europe, scientific interest in meteorology waned. In the 1st century BC, most natural philosophers claimed that the clouds and winds extended up to 111 miles, but Posidonius thought that they reached up to five miles, after which the air is clear, liquid and luminous. He closely followed Aristotle's theories. By the end of the second century BC, the center of science shifted from Athens to Alexandria , home to the ancient Library of Alexandria . In the 2nd century AD, Ptolemy 's Almagest dealt with meteorology, because it was considered a subset of astronomy. He gave several astrological weather predictions. [21] He constructed a map of the world divided into climatic zones by their illumination, in which the length of the Summer solstice increased by half an hour per zone between the equator and the Arctic. [22] Ptolemy wrote on the atmospheric refraction of light in the context of astronomical observations. [23] In 25 AD, Pomponius Mela , a Roman geographer, formalized the climatic zone system. [24] In 63–64 AD, Seneca wrote Naturales quaestiones . It was a compilation and synthesis of ancient Greek theories. However, theology was of foremost importance to Seneca, and he believed that phenomena such as lightning were tied to fate. [25] The second book(chapter) of Pliny 's Natural History covers meteorology. He states that more than twenty ancient Greek authors studied meteorology. He did not make any personal contributions, and the value of his work is in preserving earlier speculation, much like Seneca's work. [26] Twilight at Baker Beach From 400 to 1100, scientific learning in Europe was preserved by the clergy. Isidore of Seville devoted a considerable attention to meteorology in Etymologiae , De ordine creaturum and De natura rerum. Bede the Venerable was the first Englishman to write about the weather in De Natura Rerum in 703. The work was a summary of then extant classical sources. However, Aristotle's works were largely lost until the twelfth century, including Meteorologica. Isidore and Bede were scientifically minded, but they adhered to the letter of Scripture . [27] Islamic civilization translated many ancient works into Arabic which were transmitted and translated in western Europe to Latin. [28] In the 9th century, Al-Dinawari wrote the Kitab al-Nabat (Book of Plants), in which he deals with the application of meteorology to agriculture during the Arab Agricultural Revolution . He describes the meteorological character of the sky, the planets and constellations , the sun and moon , the lunar phases indicating seasons and rain, the anwa ( heavenly bodies of rain), and atmospheric phenomena such as winds, thunder, lightning, snow, floods, valleys, rivers, lakes. [29] [30] In 1021, Alhazen showed that atmospheric refraction is also responsible for twilight in Opticae thesaurus ; he estimated that twilight begins when the sun is 19 degrees below the horizon , and also used a geometric determination based on this to estimate the maximum possible height of the Earth's atmosphere as 52,000 passim (about 49 miles, or 79 km). [31] Adelard of Bath was one of the early translators of the classics. He also discussed meteorological topics in his Quaestiones naturales. He thought dense air produced propulsion in the form of wind. He explained thunder by saying that it was due to ice colliding in clouds, and in Summer it melted. In the thirteenth century, Aristotelian theories reestablished dominance in meteorology. For the next four centuries, meteorological work by and large was mostly commentary . It has been estimated over 156 commentaries on the Meteorologica were written before 1650. [32] Experimental evidence was less important than appeal to the classics and authority in medieval thought. In the thirteenth century, Roger Bacon advocated experimentation and the mathematical approach. In his Opus majus , he followed Aristotle's theory on the atmosphere being composed of water, air, and fire, supplemented by optics and geometric proofs. He noted that Ptolemy's climatic zones had to be adjusted for topography . [33] St. Albert the Great was the first to propose that each drop of falling rain had the form of a small sphere, and that this form meant that the rainbow was produced by light interacting with each raindrop. [34] Roger Bacon was the first to calculate the angular size of the rainbow. He stated that a rainbow summit cannot appear higher than 42 degrees above the horizon. [35] In the late 13th century and early 14th century, Kamāl al-Dīn al-Fārisī and Theodoric of Freiberg were the first to give the correct explanations for the primary rainbow phenomenon. Theoderic went further and also explained the secondary rainbow. [36] By the middle of the sixteenth century, meteorology had developed along two lines: theoretical science based on Meteorologica, and astrological weather forecasting. The pseudoscientific prediction by natural signs became popular and enjoyed protection of the church and princes. This was supported by scientists like Johannes Muller , Leonard Digges , and Johannes Kepler . However, there were skeptics. In the 14th century, Nicole Oresme believed that weather forecasting was possible, but that the rules for it were unknown at the time. Astrological influence in meteorology persisted until the eighteenth century. [37] Gerolamo Cardano 's De Subilitate (1550) was the first work to challenge fundamental aspects of Aristotelian theory. Cardano maintained that there were only three basic elements- earth, air, and water. He discounted fire because it needed material to spread and produced nothing. Cardano thought there were two kinds of air: free air and enclosed air. The former destroyed inanimate things and preserved animate things, while the latter had the opposite effect. [38] Rene Descartes 's Discourse on the Method (1637) typifies the beginning of the scientific revolution in meteorology. His scientific method had four principles: to never accept anything unless one clearly knew it to be true; to divide every difficult problem into small problems to tackle; to proceed from the simple to the complex, always seeking relationships; to be as complete and thorough as possible with no prejudice. [39] In the appendix Les Meteores, he applied these principles to meteorology. He discussed terrestrial bodies and vapors which arise from them, proceeding to explain the formation of clouds from drops of water, and winds, clouds then dissolving into rain, hail and snow. He also discussed the effects of light on the rainbow. Descartes hypothesized that all bodies were composed of small particles of different shapes and interwovenness. All of his theories were based on this hypothesis. He explained the rain as caused by clouds becoming too large for the air to hold, and that clouds became snow if the air was not warm enough to melt them, or hail if they met colder wind. Like his predecessors, Descartes's method was deductive, as meteorological instruments were not developed and extensively used yet. He introduced the Cartesian coordinate system to meteorology and stressed the importance of mathematics in natural science. His work established meteorology as a legitimate branch of physics. [40] In the 18th century, the invention of the thermometer and barometer allowed for more accurate measurements of temperature and pressure, leading to a better understanding of atmospheric processes. This century also saw the birth of the first meteorological society, the Societas Meteorologica Palatina in 1780. [41] In the 19th century, advances in technology such as the telegraph and photography led to the creation of weather observing networks and the ability to track storms. Additionally, scientists began to use mathematical models to make predictions about the weather. The 20th century saw the development of radar and satellite technology, which greatly improved the ability to observe and track weather systems. In addition, meteorologists and atmospheric scientists started to create the first weather forecasts and temperature predictions. [42] In the 20th and 21st centuries, with the advent of computer models and big data, meteorology has become increasingly dependent on numerical methods and computer simulations. This has greatly improved weather forecasting and climate predictions. Additionally, meteorology has expanded to include other areas such as air quality, atmospheric chemistry, and climatology. The advancement in observational, theoretical and computational technologies has enabled ever more accurate weather predictions and understanding of weather pattern and air pollution. In current time, with the advancement in weather forecasting and satellite technology, meteorology has become an integral part of everyday life, and is used for many purposes such as aviation, agriculture, and disaster management.[ citation needed ] Instruments and classification scales[ edit ] See also: Beaufort scale , Celsius , and Fahrenheit A hemispherical cup anemometer In 1441, King Sejong 's son, Prince Munjong of Korea, invented the first standardized rain gauge . [43] These were sent throughout the Joseon dynasty of Korea as an official tool to assess land taxes based upon a farmer's potential harvest. In 1450, Leone Battista Alberti developed a swinging-plate anemometer , and was known as the first anemometer. [44] In 1607, Galileo Galilei constructed a thermoscope . In 1611, Johannes Kepler wrote the first scientific treatise on snow crystals: "Strena Seu de Nive Sexangula (A New Year's Gift of Hexagonal Snow)." [45] In 1643, Evangelista Torricelli invented the mercury barometer . [44] In 1662, Sir Christopher Wren invented the mechanical, self-emptying, tipping bucket rain gauge. In 1714, Gabriel Fahrenheit created a reliable scale for measuring temperature with a mercury-type thermometer . [46] In 1742, Anders Celsius , a Swedish astronomer, proposed the "centigrade" temperature scale, the predecessor of the current Celsius scale. [47] In 1783, the first hair hygrometer was demonstrated by Horace-Bénédict de Saussure . In 1802–1803, Luke Howard wrote On the Modification of Clouds, in which he assigns cloud types Latin names. [48] In 1806, Francis Beaufort introduced his system for classifying wind speeds . [49] Near the end of the 19th century the first cloud atlases were published, including the International Cloud Atlas , which has remained in print ever since. The April 1960 launch of the first successful weather satellite , TIROS-1 , marked the beginning of the age where weather information became available globally. Atmospheric composition research[ edit ] In 1648, Blaise Pascal rediscovered that atmospheric pressure decreases with height, and deduced that there is a vacuum above the atmosphere. [50] In 1738, Daniel Bernoulli published Hydrodynamics, initiating the Kinetic theory of gases and established the basic laws for the theory of gases. [51] In 1761, Joseph Black discovered that ice absorbs heat without changing its temperature when melting. In 1772, Black's student Daniel Rutherford discovered nitrogen , which he called phlogisticated air, and together they developed the phlogiston theory . [52] In 1777, Antoine Lavoisier discovered oxygen and developed an explanation for combustion. [53] In 1783, in Lavoisier's essay "Reflexions sur le phlogistique," [54] he deprecates the phlogiston theory and proposes a caloric theory . [55] [56] In 1804, John Leslie observed that a matte black surface radiates heat more effectively than a polished surface, suggesting the importance of black-body radiation . In 1808, John Dalton defended caloric theory in A New System of Chemistry and described how it combines with matter, especially gases; he proposed that the heat capacity of gases varies inversely with atomic weight . In 1824, Sadi Carnot analyzed the efficiency of steam engines using caloric theory; he developed the notion of a reversible process and, in postulating that no such thing exists in nature, laid the foundation for the second law of thermodynamics . In 1716, Edmund Halley suggested that aurorae are caused by "magnetic effluvia" moving along the Earth's magnetic field lines. Research into cyclones and air flow[ edit ] General circulation of the Earth's atmosphere: The westerlies and trade winds are part of the Earth's atmospheric circulation. Main articles: Coriolis effect and Prevailing winds In 1494, Christopher Columbus experienced a tropical cyclone, which led to the first written European account of a hurricane. [57] In 1686, Edmund Halley presented a systematic study of the trade winds and monsoons and identified solar heating as the cause of atmospheric motions. [58] In 1735, an ideal explanation of global circulation through study of the trade winds was written by George Hadley . [59] In 1743, when Benjamin Franklin was prevented from seeing a lunar eclipse by a hurricane , he decided that cyclones move in a contrary manner to the winds at their periphery. [60] Understanding the kinematics of how exactly the rotation of the Earth affects airflow was partial at first. Gaspard-Gustave Coriolis published a paper in 1835 on the energy yield of machines with rotating parts, such as waterwheels. [61] In 1856, William Ferrel proposed the existence of a circulation cell in the mid-latitudes, and the air within deflected by the Coriolis force resulting in the prevailing westerly winds. [62] Late in the 19th century, the motion of air masses along isobars was understood to be the result of the large-scale interaction of the pressure gradient force and the deflecting force. By 1912, this deflecting force was named the Coriolis effect. [63] Just after World War I, a group of meteorologists in Norway led by Vilhelm Bjerknes developed the Norwegian cyclone model that explains the generation, intensification and ultimate decay (the life cycle) of mid-latitude cyclones , and introduced the idea of fronts , that is, sharply defined boundaries between air masses . [64] The group included Carl-Gustaf Rossby (who was the first to explain the large scale atmospheric flow in terms of fluid dynamics ), Tor Bergeron (who first determined how rain forms) and Jacob Bjerknes . Observation networks and weather forecasting[ edit ] Cloud classification by altitude of occurrence This "Hyetographic or Rain Map of the World" was first published 1848 by Alexander Keith Johnston . This "Hyetographic or Rain Map of Europe" was also published in 1848 as part of "The Physical Atlas". See also: History of surface weather analysis In the late 16th century and first half of the 17th century a range of meteorological instruments were invented – the thermometer , barometer , hydrometer , as well as wind and rain gauges. In the 1650s natural philosophers started using these instruments to systematically record weather observations. Scientific academies established weather diaries and organised observational networks. [65] In 1654, Ferdinando II de Medici established the first weather observing network, that consisted of meteorological stations in Florence , Cutigliano , Vallombrosa , Bologna , Parma , Milan , Innsbruck , Osnabrück , Paris and Warsaw . The collected data were sent to Florence at regular time intervals. [66] In the 1660s Robert Hooke of the Royal Society of London sponsored networks of weather observers. Hippocrates ' treatise Airs, Waters, and Places had linked weather to disease. Thus early meteorologists attempted to correlate weather patterns with epidemic outbreaks, and the climate with public health. [65] During the Age of Enlightenment meteorology tried to rationalise traditional weather lore, including astrological meteorology. But there were also attempts to establish a theoretical understanding of weather phenomena. Edmond Halley and George Hadley tried to explain trade winds . They reasoned that the rising mass of heated equator air is replaced by an inflow of cooler air from high latitudes. A flow of warm air at high altitude from equator to poles in turn established an early picture of circulation. Frustration with the lack of discipline among weather observers, and the poor quality of the instruments, led the early modern nation states to organise large observation networks. Thus, by the end of the 18th century, meteorologists had access to large quantities of reliable weather data. [65] In 1832, an electromagnetic telegraph was created by Baron Schilling . [67] The arrival of the electrical telegraph in 1837 afforded, for the first time, a practical method for quickly gathering surface weather observations from a wide area. [68] This data could be used to produce maps of the state of the atmosphere for a region near the Earth's surface and to study how these states evolved through time. To make frequent weather forecasts based on these data required a reliable network of observations, but it was not until 1849 that the Smithsonian Institution began to establish an observation network across the United States under the leadership of Joseph Henry . [69] Similar observation networks were established in Europe at this time. The Reverend William Clement Ley was key in understanding of cirrus clouds and early understandings of Jet Streams . [70] Charles Kenneth Mackinnon Douglas , known as 'CKM' Douglas read Ley's papers after his death and carried on the early study of weather systems. [71] Nineteenth century researchers in meteorology were drawn from military or medical backgrounds, rather than trained as dedicated scientists. [72] In 1854, the United Kingdom government appointed Robert FitzRoy to the new office of Meteorological Statist to the Board of Trade with the task of gathering weather observations at sea. FitzRoy's office became the United Kingdom Meteorological Office in 1854, the second oldest national meteorological service in the world (the Central Institution for Meteorology and Geodynamics (ZAMG) in Austria was founded in 1851 and is the oldest weather service in the world). The first daily weather forecasts made by FitzRoy's Office were published in The Times newspaper in 1860. The following year a system was introduced of hoisting storm warning cones at principal ports when a gale was expected. FitzRoy coined the term "weather forecast" and tried to separate scientific approaches from prophetic ones. [73] Over the next 50 years, many countries established national meteorological services. The India Meteorological Department (1875) was established to follow tropical cyclone and monsoon . [74] The Finnish Meteorological Central Office (1881) was formed from part of Magnetic Observatory of Helsinki University . [75] Japan's Tokyo Meteorological Observatory, the forerunner of the Japan Meteorological Agency , began constructing surface weather maps in 1883. [76] The United States Weather Bureau (1890) was established under the United States Department of Agriculture . The Australian Bureau of Meteorology (1906) was established by a Meteorology Act to unify existing state meteorological services. [77] [78] Numerical weather prediction[ edit ] Main article: Numerical weather prediction A meteorologist at the console of the IBM 7090 in the Joint Numerical Weather Prediction Unit. c. 1965 In 1904, Norwegian scientist Vilhelm Bjerknes first argued in his paper Weather Forecasting as a Problem in Mechanics and Physics that it should be possible to forecast weather from calculations based upon natural laws . [79] [80] It was not until later in the 20th century that advances in the understanding of atmospheric physics led to the foundation of modern numerical weather prediction . In 1922, Lewis Fry Richardson published "Weather Prediction By Numerical Process," [81] after finding notes and derivations he worked on as an ambulance driver in World War I. He described how small terms in the prognostic fluid dynamics equations that govern atmospheric flow could be neglected, and a numerical calculation scheme that could be devised to allow predictions. Richardson envisioned a large auditorium of thousands of people performing the calculations. However, the sheer number of calculations required was too large to complete without electronic computers, and the size of the grid and time steps used in the calculations led to unrealistic results. Though numerical analysis later found that this was due to numerical instability . Starting in the 1950s, numerical forecasts with computers became feasible. [82] The first weather forecasts derived this way used barotropic (single-vertical-level) models, and could successfully predict the large-scale movement of midlatitude Rossby waves , that is, the pattern of atmospheric lows and highs . [83] In 1959, the UK Meteorological Office received its first computer, a Ferranti Mercury . [84] In the 1960s, the chaotic nature of the atmosphere was first observed and mathematically described by Edward Lorenz , founding the field of chaos theory . [85] These advances have led to the current use of ensemble forecasting in most major forecasting centers, to take into account uncertainty arising from the chaotic nature of the atmosphere. [86] Mathematical models used to predict the long term weather of the Earth ( climate models ), have been developed that have a resolution today that are as coarse as the older weather prediction models. These climate models are used to investigate long-term climate shifts, such as what effects might be caused by human emission of greenhouse gases . Further information: Meteorologist Meteorologists are scientists who study and work in the field of meteorology. [87] The American Meteorological Society publishes and continually updates an authoritative electronic Meteorology Glossary. [88] Meteorologists work in government agencies , private consulting and research services, industrial enterprises, utilities, radio and television stations , and in education . In the United States, meteorologists held about 10,000 jobs in 2018. [89] Although weather forecasts and warnings are the best known products of meteorologists for the public, weather presenters on radio and television are not necessarily professional meteorologists. They are most often reporters with little formal meteorological training, using unregulated titles such as weather specialist or weatherman. The American Meteorological Society and National Weather Association issue "Seals of Approval" to weather broadcasters who meet certain requirements but this is not mandatory to be hired by the media. Main article: Meteorological instrumentation Satellite image of Hurricane Hugo with a polar low visible at the top of the image Each science has its own unique sets of laboratory equipment. In the atmosphere, there are many things or qualities of the atmosphere that can be measured. Rain, which can be observed, or seen anywhere and anytime was one of the first atmospheric qualities measured historically. Also, two other accurately measured qualities are wind and humidity. Neither of these can be seen but can be felt. The devices to measure these three sprang up in the mid-15th century and were respectively the rain gauge , the anemometer, and the hygrometer. Many attempts had been made prior to the 15th century to construct adequate equipment to measure the many atmospheric variables. Many were faulty in some way or were simply not reliable. Even Aristotle noted this in some of his work as the difficulty to measure the air. Sets of surface measurements are important data to meteorologists. They give a snapshot of a variety of weather conditions at one single location and are usually at a weather station , a ship or a weather buoy . The measurements taken at a weather station can include any number of atmospheric observables. Usually, temperature, pressure , wind measurements, and humidity are the variables that are measured by a thermometer, barometer, anemometer, and hygrometer, respectively. [90] Professional stations may also include air quality sensors ( carbon monoxide , carbon dioxide , methane , ozone , dust , and smoke ), ceilometer (cloud ceiling), falling precipitation sensor, flood sensor , lightning sensor , microphone ( explosions , sonic booms , thunder ), pyranometer / pyrheliometer / spectroradiometer (IR/Vis/UV photodiodes ), rain gauge / snow gauge , scintillation counter ( background radiation , fallout , radon ), seismometer ( earthquakes and tremors), transmissometer (visibility), and a GPS clock for data logging . Upper air data are of crucial importance for weather forecasting. The most widely used technique is launches of radiosondes . Supplementing the radiosondes a network of aircraft collection is organized by the World Meteorological Organization . Remote sensing , as used in meteorology, is the concept of collecting data from remote weather events and subsequently producing weather information. The common types of remote sensing are Radar , Lidar , and satellites (or photogrammetry ). Each collects data about the atmosphere from a remote location and, usually, stores the data where the instrument is located. Radar and Lidar are not passive because both use EM radiation to illuminate a specific portion of the atmosphere. [91] Weather satellites along with more general-purpose Earth-observing satellites circling the earth at various altitudes have become an indispensable tool for studying a wide range of phenomena from forest fires to El Niño . Spatial scales[ edit ] The study of the atmosphere can be divided into distinct areas that depend on both time and spatial scales. At one extreme of this scale is climatology. In the timescales of hours to days, meteorology separates into micro-, meso-, and synoptic scale meteorology. Respectively, the geospatial size of each of these three scales relates directly with the appropriate timescale. Other subclassifications are used to describe the unique, local, or broad effects within those subclasses. Scales of Atmospheric Motion Systems [92] Type of motion
Toggle the table of contents Climatology From Wikipedia, the free encyclopedia Scientific study of climate, defined as weather conditions averaged over a period of time "Climate Research" redirects here. For the journal of that name, see Climate Research (journal) . e Climatology (from Greek κλίμα, klima, "slope"; and -λογία, -logia ) or climate science is the scientific study of Earth's climate , typically defined as weather conditions averaged over a period of at least 30 years. [1] Climate concerns the atmospheric condition during an extended to indefinite period of time; weather is the condition of the atmosphere during a relative brief period of time. The main topics of research are the study of climate variability , mechanisms of climate changes and modern climate change . [2] [3] This topic of study is regarded as part of the atmospheric sciences and a subdivision of physical geography , which is one of the Earth sciences . Climatology includes some aspects of oceanography and biogeochemistry . The main methods employed by climatologists are the analysis of observations and modelling of the physical processes that determine climate. Short term weather forecasting can be interpreted in terms of knowledge of longer-term phenomena of climate, for instance climatic cycles such as the El Niño–Southern Oscillation (ENSO), the Madden–Julian oscillation (MJO), the North Atlantic oscillation (NAO), the Arctic oscillation (AO), the Pacific decadal oscillation (PDO), and the Interdecadal Pacific Oscillation (IPO). Climate models are used for a variety of purposes from studying the dynamics of the weather and climate system to predictions of future climate. [4] Further information: History of climate change science The Greeks began the formal study of climate; in fact the word climate is derived from the Greek word klima, meaning "slope", referring to the slope or inclination of the Earth's axis. Arguably the most influential classic text concerning climate was On Airs, Water and Places [5] written by Hippocrates about 400 BCE . This work commented on the effect of climate on human health and cultural differences between Asia and Europe. [5] This idea that climate controls which populations excel depending on their climate, or climatic determinism , remained influential throughout history. [5] Chinese scientist Shen Kuo (1031–1095) inferred that climates naturally shifted over an enormous span of time, after observing petrified bamboos found underground near Yanzhou (modern Yan'an , Shaanxi province), a dry-climate area unsuitable at that time for the growth of bamboo. [6] The invention of thermometers and barometers during the Scientific Revolution allowed for systematic recordkeeping, that began as early as 1640–1642 in England. [5] Early climate researchers include Edmund Halley , who published a map of the trade winds in 1686 after a voyage to the southern hemisphere. Benjamin Franklin (1706–1790) first mapped the course of the Gulf Stream for use in sending mail from North America to Europe. Francis Galton (1822–1911) invented the term anticyclone . [7] Helmut Landsberg (1906–1985) fostered the use of statistical analysis in climatology. During the early 20th century, climatology mostly emphasized the description of regional climates. This descriptive climatology was mainly an applied science, giving farmers and other interested people statistics about what the normal weather was and how great chances were of extreme events. [8] To do this, climatologists had to define a climate normal , or an average of weather and weather extremes over a period of typically 30 years. [9] While scientists knew of past climate change such as the ice ages , the concept of climate as changing only very gradually was useful for descriptive climatology. This started to change during the decades that followed, and while the history of climate change science started earlier, climate change only became one of the main topics of study for climatologists during the 1970s and afterward. [10] Subfields[ edit ] Map of the average temperature over 30 years. Data sets formed from the long-term average of historical weather parameters are sometimes called a "climatology". Various subtopics of climatology study different aspects of climate. There are different categorizations of the sub-topics of climatology. The American Meteorological Society for instance identifies descriptive climatology, scientific climatology and applied climatology as the three subcategories of climatology, a categorization based on the complexity and the purpose of the research. [11] Applied climatologists apply their expertise to different industries such as manufacturing and agriculture . [12] Paleoclimatology is the attempt to reconstruct and understand past climates by examining records such as ice cores and tree rings ( dendroclimatology ). Paleotempestology uses these same records to help determine hurricane frequency over millennia. Historical climatology is the study of climate as related to human history and is thus concerned mainly with the last few thousand years. Boundary-layer climatology concerns exchanges in water, energy and momentum near surfaces. [13] Further identified subtopics are physical climatology, dynamic climatology, tornado climatology , regional climatology, bioclimatology , and synoptic climatology. The study of the hydrological cycle over long time scales is sometimes termed hydroclimatology, in particular when studying the effects of climate change on the water cycle. [11] Methods[ edit ] The study of contemporary climates incorporates meteorological data accumulated over many years, such as records of rainfall, temperature and atmospheric composition. Knowledge of the atmosphere and its dynamics is also embodied in models , either statistical or mathematical , which help by integrating different observations and testing how well they match. Modeling is used for understanding past, present and potential future climates. Climate research is made difficult by the large scale, long time periods, and complex processes which govern climate. Climate is governed by physical principles which can be expressed as differential equations . These equations are coupled and nonlinear, so that approximate solutions are obtained by using numerical methods to create global climate models . Climate is sometimes modeled as a stochastic process but this is generally accepted as an approximation to processes that are otherwise too complicated to analyze. Climate data[ edit ] The collection of a long record of climate variables is essential for the study of climate. Climatology deals with the aggregate data that meteorologists have recorded. [14] Scientists use both direct and indirect observations of the climate, from Earth observing satellites and scientific instrumentation such as a global network of thermometers , to prehistoric ice extracted from glaciers . [15] As measuring technology changes over time, records of data often cannot be compared directly. As cities are generally warmer than the areas surrounding, urbanization has made it necessary to constantly correct data for this urban heat island effect. [16] Main article: Climate models Climate models use quantitative methods to simulate the interactions of the atmosphere, oceans, land surface, and ice.  They are used for a variety of purposes from study of the dynamics of the weather and climate system to projections of future climate.  All climate models balance, or very nearly balance, incoming energy as short wave (including visible) electromagnetic radiation to the Earth with outgoing energy as long wave (infrared) electromagnetic radiation from the Earth. Any unbalance results in a change of the average temperature of the Earth. Most climate models include the radiative effects of greenhouse gases such as carbon dioxide . These models predict a trend of increase of surface temperatures , as well as a more rapid increase of temperature at higher latitudes. Models can range from relatively simple to complex: A simple radiant heat transfer model that treats the Earth as a single point and averages outgoing energy. This can be expanded vertically (radiative-convective models), or horizontally. Coupled atmosphere–ocean– sea ice global climate models discretise and solve the full equations for mass and energy transfer and radiant exchange. Earth system models further include the biosphere. Additionally, they are available with different resolutions ranging from >100 km to 1 km. High resolutions in global climate models are computational very demanding and only few global datasets exists. Examples are ICON [17] or mechanistically downscaled data such as CHELSA (Climatologies at high resolution for the Earth's land surface areas). [18] [19] Topics of research[ edit ] Topics that climatologists study comprise three main categories: climate variability , mechanisms of climatic change, and modern changes of climate. [20] Climatological processes[ edit ] Various factors affect the average state of the atmosphere at a particular location. For instance, midlatitudes will have a pronounced seasonal cycle of temperature whereas tropical regions show little variation of temperature over a year. [21] Another major variable of climate is continentality: the distance to major water bodies such as oceans . Oceans act as a moderating factor, so that land close to it has typically less difference of temperature between winter and summer than areas further from it. [22] The atmosphere interacts with other parts of the climate system , with winds generating ocean currents that transport heat around the globe. [23] Climate classification[ edit ] Classification is an important method of simplifying complicated processes. Different climate classifications have been developed over the centuries, with the first ones in Ancient Greece . How climates are classified depends on what the application is. A wind energy producer will require different information (wind) in a classification than someone more interested in agriculture, for whom precipitation and temperature are more important. [24] The most widely used classification, the Köppen climate classification , was developed during the late nineteenth century and is based on vegetation. It uses monthly data concerning temperature and precipitation . [25] Climate variability[ edit ] There are different types of variability: recurring patterns of temperature or other climate variables. They are quantified with different indices. Much in the way the Dow Jones Industrial Average , which is based on the stock prices of 30 companies, is used to represent the fluctuations of stock prices in general, climate indices are used to represent the essential elements of climate. Climate indices are generally devised with the twin objectives of simplicity and completeness, and each index typically represents the status and timing of the climate factor it represents. By their very nature, indices are simple, and combine many details into a generalized, overall description of the atmosphere or ocean which can be used to characterize the factors which effect the global climate system. El Niño–Southern Oscillation (ENSO) is a coupled ocean-atmosphere phenomenon in the Pacific Ocean responsible for much of the global variability of temperature, [23] and has a cycle between two and seven years. [26] The North Atlantic oscillation is a mode of variability that is mainly contained to the lower atmosphere, the troposphere . The layer of atmosphere above, the stratosphere is also capable of creating its own variability, most importantly the Madden–Julian oscillation (MJO), which has a cycle of approximately 30 to 60 days. The Interdecadal Pacific oscillation can create changes in the Pacific Ocean and lower atmosphere on decadal time scales. Climate change[ edit ] Climate change occurs when changes of Earth's climate system result in new weather patterns that remain for an extended period of time. This duration of time can be as brief as a few decades to as long as millions of years. The climate system receives nearly all of its energy from the sun. The climate system also gives off energy to outer space . The balance of incoming and outgoing energy, and the passage of the energy through the climate system, determines Earth's energy budget . When the incoming energy is greater than the outgoing energy, earth's energy budget is positive and the climate system is warming. If more energy goes out, the energy budget is negative and earth experiences cooling. [27] Climate change also influences the average sea level . Modern climate change is caused largely by the human emissions of greenhouse gas from the burning of fossil fuel which increases global mean surface temperatures . Increasing temperature is only one aspect of modern climate change, which also includes observed changes of precipitation , storm tracks and cloudiness. Warmer temperatures are causing further changes of the climate system , such as the widespread melt of glaciers , sea level rise and shifts of flora and fauna. [28] Differences with meteorology[ edit ] In contrast to meteorology , which emphasises short term weather systems lasting no more than a few weeks, climatology studies the frequency and trends of those systems. It studies the periodicity of weather events over years to millennia, as well as changes of long-term average weather patterns in relation to atmospheric conditions. Climatologists study both the nature of climates – local, regional or global – and the natural or human-induced factors that cause climates to change. Climatology considers the past and can help predict future climate change . Phenomena of climatological interest include the atmospheric boundary layer , circulation patterns , heat transfer ( radiative , convective and latent ), interactions between the atmosphere and the oceans and land surface (particularly vegetation, land use and topography ), and the chemical and physical composition of the atmosphere. Use in weather forecasting[ edit ] Main article: Weather forecasting A relative difficult method of forecast, the analog technique requires remembering a previous weather event which is expected to be mimicked by an upcoming event.  What makes it a difficult technique is that there is rarely a perfect analog for an event of the future. [29] Some refer to this type of forecasting as pattern recognition, which remains a useful method of estimating rainfall over data voids such as oceans using knowledge of how satellite imagery relates to precipitation rates over land, [30] as well as the forecasting of precipitation amounts and distribution of the future.  A variation of this theme, used for medium range forecasting, is known as teleconnections , when systems in other locations are used to help determine the location of a system within the regime surrounding. [31] One method of using teleconnections are by using climate indices such as ENSO-related phenomena. [32]
Toggle the table of contents Hydrology From Wikipedia, the free encyclopedia Science of the movement, distribution, and quality of water on Earth and other planets For other uses, see Hydrology (disambiguation) . Rain over a Scottish catchment . Understanding the cycling of water into, through, and out of catchments is a key element of hydrology. Hydrology (from Ancient Greek ὕδωρ (húdōr) 'water', and -λογία ( -logía ) 'study of') is the scientific study of the movement, distribution, and management of water on Earth and other planets, including the water cycle , water resources , and drainage basin sustainability. A practitioner of hydrology is called a hydrologist. Hydrologists are scientists studying earth or environmental science , civil or environmental engineering , and physical geography . [1] Using various analytical methods and scientific techniques, they collect and analyze data to help solve water related problems such as environmental preservation , natural disasters , and water management . [1] Hydrology subdivides into surface water hydrology, groundwater hydrology (hydrogeology), and marine hydrology. Domains of hydrology include hydrometeorology , surface hydrology , hydrogeology , drainage-basin management, and water quality . Oceanography and meteorology are not included because water is only one of many important aspects within those fields. Hydrological research can inform environmental engineering, policy , and planning . Chemical hydrology is the study of the chemical characteristics of water. Ecohydrology is the study of interactions between organisms and the hydrologic cycle. Hydrogeology is the study of the presence and movement of groundwater . Hydrogeochemistry is the study of how terrestrial water dissolves minerals weathering and this effect on water chemistry. Hydroinformatics is the adaptation of information technology to hydrology and water resources applications. Hydrometeorology is the study of the transfer of water and energy between land and water body surfaces and the lower atmosphere. Isotope hydrology is the study of the isotopic signatures of water. Surface hydrology is the study of hydrologic processes that operate at or near Earth's surface. Drainage basin management covers water storage, in the form of reservoirs, and floods protection. Water quality includes the chemistry of water in rivers and lakes, both of pollutants and natural solutes. Real-time flood forecasting , flood warning , Flood Frequency Analysis Designing irrigation schemes and managing agricultural productivity. Part of the hazard module in catastrophe modeling . Designing sewers and urban drainage systems. Analyzing the impacts of antecedent moisture on sanitary sewer systems. Predicting geomorphologic changes, such as erosion or sedimentation . Assessing the impacts of natural and anthropogenic environmental change on water resources . Assessing contaminant transport risk and establishing environmental policy guidelines. Estimating the water resource potential of river basins. History[ edit ] This section includes a list of references , related reading , or external links , but its sources remain unclear because it lacks inline citations . Please help improve this section by introducing more precise citations. (April 2012) ( ) The Roman aqueduct at Caesarea Maritima , bringing water from the wetter Carmel mountains to the settlement. Hydrology has been subject to investigation and engineering for millennia. Ancient Egyptians were one of the first to employ hydrology in their engineering and agriculture, inventing a form of water management known as basin irrigation. [2] Mesopotamian towns were protected from flooding with high earthen walls. Aqueducts were built by the Greeks and Romans , while history shows that the Chinese built irrigation and flood control works. The ancient Sinhalese used hydrology to build complex irrigation works in Sri Lanka , also known for the invention of the Valve Pit which allowed construction of large reservoirs, anicuts and canals which still function. Marcus Vitruvius , in the first century BC, described a philosophical theory of the hydrologic cycle, in which precipitation falling in the mountains infiltrated the Earth's surface and led to streams and springs in the lowlands. [3] With the adoption of a more scientific approach, Leonardo da Vinci and Bernard Palissy independently reached an accurate representation of the hydrologic cycle. It was not until the 17th century that hydrologic variables began to be quantified. Pioneers of the modern science of hydrology include Pierre Perrault , Edme Mariotte and Edmund Halley . By measuring rainfall, runoff, and drainage area, Perrault showed that rainfall was sufficient to account for the flow of the Seine. Mariotte combined velocity and river cross-section measurements to obtain a discharge value, again in the Seine. Halley showed that the evaporation from the Mediterranean Sea was sufficient to account for the outflow of rivers flowing into the sea. [4] Advances in the 18th century included the Bernoulli piezometer and Bernoulli's equation , by Daniel Bernoulli , and the Pitot tube , by Henri Pitot . The 19th century saw development in groundwater hydrology, including Darcy's law , the Dupuit-Thiem well formula, and Hagen- Poiseuille 's capillary flow equation. Rational analyses began to replace empiricism in the 20th century, while governmental agencies began their own hydrological research programs. Of particular importance were Leroy Sherman's unit hydrograph , the infiltration theory of Robert E. Horton , and C.V. Theis' aquifer test/equation describing well hydraulics. Since the 1950s, hydrology has been approached with a more theoretical basis than in the past, facilitated by advances in the physical understanding of hydrological processes and by the advent of computers and especially geographic information systems (GIS). (See also GIS and hydrology ) Main article: Water cycle The central theme of hydrology is that water circulates throughout the Earth through different pathways and at different rates. The most vivid image of this is in the evaporation of water from the ocean, which forms clouds. These clouds drift over the land and produce rain. The rainwater flows into lakes, rivers, or aquifers. The water in lakes, rivers, and aquifers then either evaporates back to the atmosphere or eventually flows back to the ocean, completing a cycle. Water changes its state of being several times throughout this cycle. The areas of research within hydrology concern the movement of water between its various states, or within a given state, or simply quantifying the amounts in these states in a given region. Parts of hydrology concern developing methods for directly measuring these flows or amounts of water, while others concern modeling these processes either for scientific knowledge or for making a prediction in practical applications. Groundwater[ edit ] Building a map of groundwater contours Ground water is water beneath Earth's surface, often pumped for drinking water. [1] Groundwater hydrology ( hydrogeology ) considers quantifying groundwater flow and solute transport. [5] Problems in describing the saturated zone include the characterization of aquifers in terms of flow direction, groundwater pressure and, by inference, groundwater depth (see: aquifer test ). Measurements here can be made using a piezometer . Aquifers are also described in terms of hydraulic conductivity, storativity and transmissivity. There are a number of geophysical methods [6] for characterizing aquifers. There are also problems in characterizing the vadose zone (unsaturated zone). [7] Main article: Infiltration (hydrology) Infiltration is the process by which water enters the soil. Some of the water is absorbed, and the rest percolates down to the water table . The infiltration capacity, the maximum rate at which the soil can absorb water, depends on several factors. The layer that is already saturated provides a resistance that is proportional to its thickness, while that plus the depth of water above the soil provides the driving force ( hydraulic head ). Dry soil can allow rapid infiltration by capillary action ; this force diminishes as the soil becomes wet. Compaction reduces the porosity and the pore sizes. Surface cover increases capacity by retarding runoff, reducing compaction and other processes. Higher temperatures reduce viscosity , increasing infiltration. [8] : 250–275 Main article: Soil moisture Soil moisture can be measured in various ways; by capacitance probe , time domain reflectometer or Tensiometer . Other methods include solute sampling and geophysical methods. [9] Surface water flow[ edit ] A flood hydrograph showing stage for the Shawsheen River at Wilmington. Hydrology considers quantifying surface water flow and solute transport, although the treatment of flows in large rivers is sometimes considered as a distinct topic of hydraulics or hydrodynamics. Surface water flow can include flow both in recognizable river channels and otherwise. Methods for measuring flow once the water has reached a river include the stream gauge (see: discharge ), and tracer techniques. Other topics include chemical transport as part of surface water, sediment transport and erosion. One of the important areas of hydrology is the interchange between rivers and aquifers. Groundwater/surface water interactions in streams and aquifers can be complex and the direction of net water flux (into surface water or into the aquifer) may vary spatially along a stream channel and over time at any particular location, depending on the relationship between stream stage and groundwater levels. Precipitation and evaporation[ edit ] A standard NOAA rain gauge In some considerations, hydrology is thought of as starting at the land-atmosphere boundary [10] and so it is important to have adequate knowledge of both precipitation and evaporation. Precipitation can be measured in various ways: disdrometer for precipitation characteristics at a fine time scale; radar for cloud properties, rain rate estimation, hail and snow detection; rain gauge for routine accurate measurements of rain and snowfall; satellite for rainy area identification, rain rate estimation, land-cover/land-use, and soil moisture, for example. Evaporation is an important part of the water cycle. It is partly affected by humidity, which can be measured by a sling psychrometer . It is also affected by the presence of snow, hail, and ice and can relate to dew, mist and fog. Hydrology considers evaporation of various forms: from water surfaces; as transpiration from plant surfaces in natural and agronomic ecosystems. Direct measurement of evaporation can be obtained using Simon's evaporation pan . Detailed studies of evaporation involve boundary layer considerations as well as momentum, heat flux, and energy budgets. Main article: Remote sensing Estimates of changes in water storage around the Tigris and Euphrates Rivers, measured by NASA's GRACE satellites. The satellites measure tiny changes in gravitational acceleration, which can then be processed to reveal movement of water due to changes in its total mass. Remote sensing of hydrologic processes can provide information on locations where in situ sensors may be unavailable or sparse. It also enables observations over large spatial extents. Many of the variables constituting the terrestrial water balance, for example surface water storage, soil moisture , precipitation , evapotranspiration , and snow and ice , are measurable using remote sensing at various spatial-temporal resolutions and accuracies. [11] Sources of remote sensing include land-based sensors, airborne sensors and satellite sensors which can capture microwave , thermal and near-infrared data or use lidar , for example. Main article: Water quality In hydrology, studies of water quality concern organic and inorganic compounds, and both dissolved and sediment material. In addition, water quality is affected by the interaction of dissolved oxygen with organic material and various chemical transformations that may take place. Measurements of water quality may involve either in-situ methods, in which analyses take place on-site, often automatically, and laboratory-based analyses and may include microbiological analysis . Integrating measurement and modelling[ edit ] Budget analyses
Toggle the table of contents Soil science From Wikipedia, the free encyclopedia Study of soil as a natural resource on the surface of Earth A soil scientist examining horizons within a soil profile Soil science is the study of soil as a natural resource on the surface of the Earth including soil formation , classification and mapping ; physical , chemical , biological , and fertility properties of soils; and these properties in relation to the use and management of soils . [1] Sometimes terms which refer to branches of soil science, such as pedology (formation, chemistry, morphology, and classification of soil) and edaphology (how soils interact with living things, especially plants), are used as if synonymous with soil science. The diversity of names associated with this discipline is related to the various associations concerned.  Indeed, engineers, agronomists , chemists , geologists , physical geographers , ecologists , biologists , microbiologists , silviculturists , sanitarians , archaeologists , and specialists in regional planning , all contribute to further knowledge of soils and the advancement of the soil sciences. [1] Soil scientists have raised concerns about how to preserve soil and arable land in a world with a growing population, possible future water crisis , increasing per capita food consumption , and land degradation . [2] Fields of study[ edit ] Soil occupies the pedosphere , one of Earth's spheres that the geosciences use to organize the Earth conceptually. This is the conceptual perspective of pedology and edaphology , the two main branches of soil science. Pedology is the study of soil in its natural setting. Edaphology is the study of soil in relation to soil-dependent uses. Both branches apply a combination of soil physics , soil chemistry , and soil biology . Due to the numerous interactions between the biosphere , atmosphere and hydrosphere that are hosted within the pedosphere, more integrated, less soil-centric concepts are also valuable. Many concepts essential to understanding soil come from individuals not identifiable strictly as soil scientists. This highlights the interdisciplinary nature of soil concepts. Research[ edit ] Exploring the diversity and dynamics of soil continues to yield fresh discoveries and insights. New avenues of soil research are compelled by a need to understand soil in the context of climate change , [3] [4] greenhouse gases , and carbon sequestration . [3] Interest in maintaining the planet's biodiversity and in exploring past cultures has also stimulated renewed interest in achieving a more refined understanding of soil. This section is an excerpt from Soil survey .[ edit ] Soil survey , or soil mapping, is the  process of classifying soil types and other soil properties in a given area and geo-encoding such information. Main article: soil classification Map of global soil regions from the USDA In 1998, the World Reference Base for Soil Resources (WRB) replaced the FAO soil classification as the international soil classification system. The currently valid version of WRB is the 4th edition, 2022. [5] The FAO soil classification, in turn, borrowed from modern soil classification concepts, including USDA soil taxonomy . WRB is based mainly on soil morphology as an expression of pedogenesis . A major difference with USDA soil taxonomy is that soil climate is not part of the system, except insofar as climate influences soil profile characteristics. Many other classification schemes exist, including vernacular systems. The structure in vernacular systems is either nominal (giving unique names to soils or landscapes) or descriptive (naming soils by their characteristics such as red, hot, fat, or sandy). Soils are distinguished by obvious characteristics, such as physical appearance (e.g., color , texture , landscape position), performance (e.g., production capability, flooding), and accompanying vegetation. [6] A vernacular distinction familiar to many is classifying texture as heavy or light. Light soil content and better structure take less effort to turn and cultivate. Light soils do not necessarily weigh less than heavy soils on an air dry basis, nor do they have more porosity . History[ edit ] The earliest known soil classification system comes from China, appearing in the book Yu Gong (5th century BCE), where the soil was divided into three categories and nine classes, depending on its color, texture and hydrology. [7] Contemporaries Friedrich Albert Fallou (the German founder of modern soil science) and Vasily Dokuchaev (the Russian founder of modern soil science) are both credited with being among the first to identify soil as a resource whose distinctness and complexity deserved to be separated conceptually from geology and crop production and treated as a whole. As a founding father of soil science, Fallou has primacy in time. Fallou was working on the origins of soil before Dokuchaev was born; however Dokuchaev's work was more extensive and is considered to be the more significant to modern soil theory than Fallou's. Previously, soil had been considered a product of chemical transformations of rocks, a dead substrate from which plants derive nutritious elements. Soil and bedrock were in fact equated. Dokuchaev considers the soil as a natural body having its own genesis and its own history of development, a body with complex and multiform processes taking place within it. The soil is considered as different from bedrock. The latter becomes soil under the influence of a series of soil-formation factors (climate, vegetation, country, relief and age). According to him, soil should be called the "daily" or outward horizons of rocks regardless of the type; they are changed naturally by the common effect of water, air and various kinds of living and dead organisms. [8] A 1914 encyclopedic definition: "the different forms of earth on the surface of the rocks, formed by the breaking down or weathering of rocks". [9] serves to illustrate the historic view of soil which persisted from the 19th century. Dokuchaev's late 19th century soil concept developed in the 20th century to one of soil as earthy material that has been altered by living processes. [10] A corollary concept is that soil without a living component is simply a part of Earth's outer layer. Further refinement of the soil concept is occurring in view of an appreciation of energy transport and transformation within soil. The term is popularly applied to the material on the surface of the Earth's moon and Mars, a usage acceptable within a portion of the scientific community. Accurate to this modern understanding of soil is Nikiforoff's 1959 definition of soil as the "excited skin of the sub aerial part of the Earth's crust ". [11] Areas of practice[ edit ] Academically, soil scientists tend to be drawn to one of five areas of specialization: microbiology , pedology , edaphology , physics , or chemistry . Yet the work specifics are very much dictated by the challenges facing our civilization's desire to sustain the land that supports it, and the distinctions between the sub-disciplines of soil science often blur in the process. Soil science professionals commonly stay current in soil chemistry, soil physics, soil microbiology, pedology, and applied soil science in related disciplines One interesting effort drawing in soil scientists in the U.S. as of 2004 [update] is the Soil Quality Initiative. Central to the Soil Quality Initiative is developing indices of soil health and then monitoring them in a way that gives us long term (decade-to-decade) feedback on our performance as stewards of the planet. The effort includes understanding the functions of soil microbiotic crusts and exploring the potential to sequester atmospheric carbon in soil organic matter . The concept of agriculture in relation to soil quality , however, has not been without its share of controversy and criticism, including critiques by Nobel Laureate Norman Borlaug and World Food Prize Winner Pedro Sanchez . A more traditional role for soil scientists has been to map soils. Most every area in the United States now has a published soil survey , which includes interpretive tables as to how soil properties support or limit activities and uses. An internationally accepted soil taxonomy allows uniform communication of soil characteristics and soil functions . National and international soil survey efforts have given the profession unique insights into landscape scale functions. The landscape functions that soil scientists are called upon to address in the field seem to fall roughly into six areas: Land-based treatment of wastes
See also: Emergence Complexity is understood as a large computational effort needed to piece together numerous interacting parts exceeding the iterative memory capacity of the human mind. Global patterns of biological diversity are complex. This biocomplexity stems from the interplay among ecological processes that operate and influence patterns at different scales that grade into each other, such as transitional areas or ecotones spanning landscapes. Complexity stems from the interplay among levels of biological organization as energy, and matter is integrated into larger units that superimpose onto the smaller parts. "What were wholes on one level become parts on a higher one." [98] : 209 Small scale patterns do not necessarily explain large scale phenomena, otherwise captured in the expression (coined by Aristotle) 'the sum is greater than the parts'. [99] [100] [E] "Complexity in ecology is of at least six distinct types: spatial, temporal, structural, process, behavioral, and geometric." [101] : 3 From these principles, ecologists have identified emergent and self-organizing phenomena that operate at different environmental scales of influence, ranging from molecular to planetary, and these require different explanations at each integrative level . [48] [102] Ecological complexity relates to the dynamic resilience of ecosystems that transition to multiple shifting steady-states directed by random fluctuations of history. [9] [103] Long-term ecological studies provide important track records to better understand the complexity and resilience of ecosystems over longer temporal and broader spatial scales. These studies are managed by the International Long Term Ecological Network (LTER). [104] The longest experiment in existence is the Park Grass Experiment , which was initiated in 1856. [105] Another example is the Hubbard Brook study , which has been in operation since 1960. [106] Main article: Holism Holism remains a critical part of the theoretical foundation in contemporary ecological studies. Holism addresses the biological organization of life that self-organizes into layers of emergent whole systems that function according to non-reducible properties. This means that higher-order patterns of a whole functional system, such as an ecosystem , cannot be predicted or understood by a simple summation of the parts. [107] "New properties emerge because the components interact, not because the basic nature of the components is changed." [5] : 8 Ecological studies are necessarily holistic as opposed to reductionistic . [36] [102] [108] Holism has three scientific meanings or uses that identify with ecology: 1) the mechanistic complexity of ecosystems, 2) the practical description of patterns in quantitative reductionist terms where correlations may be identified but nothing is understood about the causal relations without reference to the whole system, which leads to 3) a metaphysical hierarchy whereby the causal relations of larger systems are understood without reference to the smaller parts. Scientific holism differs from mysticism that has appropriated the same term. An example of metaphysical holism is identified in the trend of increased exterior thickness in shells of different species. The reason for a thickness increase can be understood through reference to principles of natural selection via predation without the need to reference or understand the biomolecular properties of the exterior shells. [109] Relation to evolution[ edit ] Main article: Evolutionary ecology Ecology and evolutionary biology are considered sister disciplines of the life sciences. Natural selection , life history , development , adaptation , populations , and inheritance are examples of concepts that thread equally into ecological and evolutionary theory. Morphological, behavioural, and genetic traits, for example, can be mapped onto evolutionary trees to study the historical development of a species in relation to their functions and roles in different ecological circumstances. In this framework, the analytical tools of ecologists and evolutionists overlap as they organize, classify, and investigate life through common systematic principles, such as phylogenetics or the Linnaean system of taxonomy . [110] The two disciplines often appear together, such as in the title of the journal Trends in Ecology and Evolution . [111] There is no sharp boundary separating ecology from evolution, and they differ more in their areas of applied focus. Both disciplines discover and explain emergent and unique properties and processes operating across different spatial or temporal scales of organization. [36] [48] While the boundary between ecology and evolution is not always clear, ecologists study the abiotic and biotic factors that influence evolutionary processes, [112] [113] and evolution can be rapid, occurring on ecological timescales as short as one generation. [114] Main article: Behavioural ecology Social display and colour variation in differently adapted species of chameleons (Bradypodion spp.). Chameleons change their skin colour to match their background as a behavioural defence mechanism and also use colour to communicate with other members of their species, such as dominant (left) versus submissive (right) patterns shown in the three species (A-C) above. [115] All organisms can exhibit behaviours. Even plants express complex behaviour, including memory and communication. [116] Behavioural ecology is the study of an organism's behaviour in its environment and its ecological and evolutionary implications. Ethology is the study of observable movement or behaviour in animals. This could include investigations of motile sperm of plants, mobile phytoplankton , zooplankton swimming toward the female egg, the cultivation of fungi by weevils , the mating dance of a salamander , or social gatherings of amoeba . [117] [118] [119] [120] [121] Adaptation is the central unifying concept in behavioural ecology. [122] Behaviours can be recorded as traits and inherited in much the same way that eye and hair colour can. Behaviours can evolve by means of natural selection as adaptive traits conferring functional utilities that increases reproductive fitness. [123] [124] Mutualism: Leafhoppers (Eurymela fenestrata) are protected by ants (Iridomyrmex purpureus) in a mutualistic relationship. The ants protect the leafhoppers from predators and stimulate feeding in the leafhoppers, and in return, the leafhoppers feeding on plants exude honeydew from their anus that provides energy and nutrients to tending ants. [125] Predator-prey interactions are an introductory concept into food-web studies as well as behavioural ecology. [126] Prey species can exhibit different kinds of behavioural adaptations to predators, such as avoid, flee, or defend. Many prey species are faced with multiple predators that differ in the degree of danger posed. To be adapted to their environment and face predatory threats, organisms must balance their energy budgets as they invest in different aspects of their life history, such as growth, feeding, mating, socializing, or modifying their habitat. Hypotheses posited in behavioural ecology are generally based on adaptive principles of conservation, optimization, or efficiency. [33] [112] [127] For example, "[t]he threat-sensitive predator avoidance hypothesis predicts that prey should assess the degree of threat posed by different predators and match their behaviour according to current levels of risk" [128] or "[t]he optimal flight initiation distance occurs where expected postencounter fitness is maximized, which depends on the prey's initial fitness, benefits obtainable by not fleeing, energetic escape costs, and expected fitness loss due to predation risk." [129] Elaborate sexual displays and posturing are encountered in the behavioural ecology of animals. The birds-of-paradise , for example, sing and display elaborate ornaments during courtship . These displays serve a dual purpose of signalling healthy or well-adapted individuals and desirable genes. The displays are driven by sexual selection as an advertisement of quality of traits among suitors . [130] Cognitive ecology[ edit ] Cognitive ecology integrates theory and observations from evolutionary ecology and neurobiology , primarily cognitive science , in order to understand the effect that animal interaction with their habitat has on their cognitive systems and how those systems restrict behavior within an ecological and evolutionary framework. [131] "Until recently, however, cognitive scientists have not paid sufficient attention to the fundamental fact that cognitive traits evolved under particular natural settings. With consideration of the selection pressure on cognition, cognitive ecology can contribute intellectual coherence to the multidisciplinary study of cognition." [132] [133] As a study involving the 'coupling' or interactions between organism and environment, cognitive ecology is closely related to enactivism , [131] a field based upon the view that "...we must see the organism and environment as bound together in reciprocal specification and selection...". [134] Main article: Social ecology (academic field) Social-ecological behaviours are notable in the social insects , slime moulds , social spiders , human society , and naked mole-rats where eusocialism has evolved. Social behaviours include reciprocally beneficial behaviours among kin and nest mates [119] [124] [135] and evolve from kin and group selection. Kin selection explains altruism through genetic relationships, whereby an altruistic behaviour leading to death is rewarded by the survival of genetic copies distributed among surviving relatives. The social insects, including ants , bees , and wasps are most famously studied for this type of relationship because the male drones are clones that share the same genetic make-up as every other male in the colony. [124] In contrast, group selectionists find examples of altruism among non-genetic relatives and explain this through selection acting on the group; whereby, it becomes selectively advantageous for groups if their members express altruistic behaviours to one another. Groups with predominantly altruistic members survive better than groups with predominantly selfish members. [124] [136] Main article: Coevolution Bumblebees and the flowers they pollinate have coevolved so that both have become dependent on each other for survival. Parasitism: A harvestman arachnid being parasitized by mites . The harvestman is being consumed, while the mites benefit from traveling on and feeding off of their host. Ecological interactions can be classified broadly into a host and an associate relationship. A host is any entity that harbours another that is called the associate. [137] Relationships between species that are mutually or reciprocally beneficial are called mutualisms . Examples of mutualism include fungus-growing ants employing agricultural symbiosis, bacteria living in the guts of insects and other organisms, the fig wasp and yucca moth pollination complex, lichens with fungi and photosynthetic algae , and corals with photosynthetic algae. [138] [139] If there is a physical connection between host and associate, the relationship is called symbiosis . Approximately 60% of all plants, for example, have a symbiotic relationship with arbuscular mycorrhizal fungi living in their roots forming an exchange network of carbohydrates for mineral nutrients . [140] Indirect mutualisms occur where the organisms live apart. For example, trees living in the equatorial regions of the planet supply oxygen into the atmosphere that sustains species living in distant polar regions of the planet. This relationship is called commensalism because many others receive the benefits of clean air at no cost or harm to trees supplying the oxygen. [5] [141] If the associate benefits while the host suffers, the relationship is called parasitism . Although parasites impose a cost to their host (e.g., via damage to their reproductive organs or propagules , denying the services of a beneficial partner), their net effect on host fitness is not necessarily negative and, thus, becomes difficult to forecast. [142] [143] Co-evolution is also driven by competition among species or among members of the same species under the banner of reciprocal antagonism, such as grasses competing for growth space. The Red Queen Hypothesis , for example, posits that parasites track down and specialize on the locally common genetic defense systems of its host that drives the evolution of sexual reproduction to diversify the genetic constituency of populations responding to the antagonistic pressure. [144] [145] Main article: Biogeography Biogeography (an amalgamation of biology and geography) is the comparative study of the geographic distribution of organisms and the corresponding evolution of their traits in space and time. [146] The Journal of Biogeography was established in 1974. [147] Biogeography and ecology share many of their disciplinary roots. For example, the theory of island biogeography , published by the Robert MacArthur and Edward O. Wilson in 1967 [148] is considered one of the fundamentals of ecological theory. [149] Biogeography has a long history in the natural sciences concerning the spatial distribution of plants and animals. Ecology and evolution provide the explanatory context for biogeographical studies. [146] Biogeographical patterns result from ecological processes that influence range distributions, such as migration and dispersal . [149] and from historical processes that split populations or species into different areas. The biogeographic processes that result in the natural splitting of species explain much of the modern distribution of the Earth's biota. The splitting of lineages in a species is called vicariance biogeography and it is a sub-discipline of biogeography. [150] There are also practical applications in the field of biogeography concerning ecological systems and processes. For example, the range and distribution of biodiversity and invasive species responding to climate change is a serious concern and active area of research in the context of global warming . [151] [152] r/K selection theory[ edit ] Main article: r/K selection theory A population ecology concept is r/K selection theory, [D] one of the first predictive models in ecology used to explain life-history evolution . The premise behind the r/K selection model is that natural selection pressures change according to population density . For example, when an island is first colonized, density of individuals is low. The initial increase in population size is not limited by competition, leaving an abundance of available resources for rapid population growth. These early phases of population growth experience density-independent forces of natural selection, which is called r-selection. As the population becomes more crowded, it approaches the island's carrying capacity, thus forcing individuals to compete more heavily for fewer available resources. Under crowded conditions, the population experiences density-dependent forces of natural selection, called K-selection. [153] In the r/K-selection model, the first variable r is the intrinsic rate of natural increase in population size and the second variable K is the carrying capacity of a population. [33] Different species evolve different life-history strategies spanning a continuum between these two selective forces. An r-selected species is one that has high birth rates, low levels of parental investment, and high rates of mortality before individuals reach maturity. Evolution favours high rates of fecundity in r-selected species. Many kinds of insects and invasive species exhibit r-selected characteristics . In contrast, a K-selected species has low rates of fecundity, high levels of parental investment in the young, and low rates of mortality as individuals mature. Humans and elephants are examples of species exhibiting K-selected characteristics, including longevity and efficiency in the conversion of more resources into fewer offspring. [148] [154] Main article: Molecular ecology The important relationship between ecology and genetic inheritance predates modern techniques for molecular analysis. Molecular ecological research became more feasible with the development of rapid and accessible genetic technologies, such as the polymerase chain reaction (PCR) . The rise of molecular technologies and the influx of research questions into this new ecological field resulted in the publication Molecular Ecology in 1992. [155] Molecular ecology uses various analytical techniques to study genes in an evolutionary and ecological context. In 1994, John Avise also played a leading role in this area of science with the publication of his book, Molecular Markers, Natural History and Evolution. [156] Newer technologies opened a wave of genetic analysis into organisms once difficult to study from an ecological or evolutionary standpoint, such as bacteria, fungi, and nematodes . Molecular ecology engendered a new research paradigm for investigating ecological questions considered otherwise intractable. Molecular investigations revealed previously obscured details in the tiny intricacies of nature and improved resolution into probing questions about behavioural and biogeographical ecology. [156] For example, molecular ecology revealed promiscuous sexual behaviour and multiple male partners in tree swallows previously thought to be socially monogamous . [157] In a biogeographical context, the marriage between genetics, ecology, and evolution resulted in a new sub-discipline called phylogeography . [158] Main article: Human ecology The history of life on Earth has been a history of interaction between living things and their surroundings. To a large extent, the physical form and the habits of the earth's vegetation and its animal life have been molded by the environment. Considering the whole span of earthly time, the opposite effect, in which life actually modifies its surroundings, has been relatively slight. Only within the moment of time represented by the present century has one species man acquired significant power to alter the nature of his world. Rachel Carson, "Silent Spring" [159] Ecology is as much a biological science as it is a human science. [5] Human ecology is an interdisciplinary investigation into the ecology of our species. "Human ecology may be defined: (1) from a bioecological standpoint as the study of man as the ecological dominant in plant and animal communities and systems; (2) from a bioecological standpoint as simply another animal affecting and being affected by his physical environment; and (3) as a human being, somehow different from animal life in general, interacting with physical and modified environments in a distinctive and creative way. A truly interdisciplinary human ecology will most likely address itself to all three." [160] : 3 The term was formally introduced in 1921, but many sociologists, geographers, psychologists, and other disciplines were interested in human relations to natural systems centuries prior, especially in the late 19th century. [160] [161] The ecological complexities human beings are facing through the technological transformation of the planetary biome has brought on the Anthropocene . The unique set of circumstances has generated the need for a new unifying science called coupled human and natural systems that builds upon, but moves beyond the field of human ecology. [107] Ecosystems tie into human societies through the critical and all-encompassing life-supporting functions they sustain. In recognition of these functions and the incapability of traditional economic valuation methods to see the value in ecosystems, there has been a surge of interest in social - natural capital , which provides the means to put a value on the stock and use of information and materials stemming from ecosystem goods and services . Ecosystems produce, regulate, maintain, and supply services of critical necessity and beneficial to human health (cognitive and physiological), economies, and they even provide an information or reference function as a living library giving opportunities for science and cognitive development in children engaged in the complexity of the natural world. Ecosystems relate importantly to human ecology as they are the ultimate base foundation of global economics as every commodity, and the capacity for exchange ultimately stems from the ecosystems on Earth. [107] [162] [163] [164]
Toggle the table of contents Biogeography From Wikipedia, the free encyclopedia Study of the distribution of species and ecosystems in geographic space and through geological time Frontispiece to Alfred Russel Wallace 's book The Geographical Distribution of Animals Part of a series on e Biogeography is the study of the distribution of species and ecosystems in geographic space and through geological time . Organisms and biological communities often vary in a regular fashion along geographic gradients of latitude , elevation , isolation and habitat area . [1] Phytogeography is the branch of biogeography that studies the distribution of plants. Zoogeography is the branch that studies distribution of animals. Mycogeography is the branch that studies distribution of fungi, such as mushrooms . Knowledge of spatial variation in the numbers and types of organisms is as vital to us today as it was to our early human ancestors , as we adapt to heterogeneous but geographically predictable environments . Biogeography is an integrative field of inquiry that unites concepts and information from ecology , evolutionary biology , taxonomy , geology , physical geography , palaeontology , and climatology . [2] [3] Modern biogeographic research combines information and ideas from many fields, from the physiological and ecological constraints on organismal dispersal to geological and climatological phenomena operating at global spatial scales and evolutionary time frames. The short-term interactions within a habitat and species of organisms describe the ecological application of biogeography. Historical biogeography describes the long-term, evolutionary periods of time for broader classifications of organisms. [4] Early scientists, beginning with Carl Linnaeus , contributed to the development of biogeography as a science. Introduction[ edit ] The patterns of species distribution across geographical areas can usually be explained through a combination of historical factors such as: speciation , extinction , continental drift , and glaciation . Through observing the geographic distribution of species, we can see associated variations in sea level , river routes, habitat, and river capture . Additionally, this science considers the geographic constraints of landmass areas and isolation, as well as the available ecosystem energy supplies. Over periods of ecological changes, biogeography includes the study of plant and animal species in: their past and/or present living refugium habitat ; their interim living sites; and/or their survival locales. [11] As writer David Quammen put it, "...biogeography does more than ask Which species? and Where. It also asks Why? and, what is sometimes more crucial, Why not?." [12] Modern biogeography often employs the use of Geographic Information Systems (GIS), to understand the factors affecting organism distribution, and to predict future trends in organism distribution. [13] Often mathematical models and GIS are employed to solve ecological problems that have a spatial aspect to them. [14] Biogeography is most keenly observed on the world's islands . These habitats are often much more manageable areas of study because they are more condensed than larger ecosystems on the mainland. [15] Islands are also ideal locations because they allow scientists to look at habitats that new invasive species have only recently colonized and can observe how they disperse throughout the island and change it. They can then apply their understanding to similar but more complex mainland habitats. Islands are very diverse in their biomes , ranging from the tropical to arctic climates. This diversity in habitat allows for a wide range of species study in different parts of the world. One scientist who recognized the importance of these geographic locations was Charles Darwin , who remarked in his journal "The Zoology of Archipelagoes will be well worth examination". [15] Two chapters in On the Origin of Species were devoted to geographical distribution. 18th century[ edit ] The first discoveries that contributed to the development of biogeography as a science began in the mid-18th century, as Europeans explored the world and described the biodiversity of life. During the 18th century most views on the world were shaped around religion and for many natural theologists, the bible. Carl Linnaeus , in the mid-18th century, improved our classifications of organisms through the exploration of undiscovered territories by his students and disciples. When he noticed that species were not as perpetual as he believed, he developed the Mountain Explanation to explain the distribution of biodiversity; when Noah's ark landed on Mount Ararat and the waters receded, the animals dispersed throughout different elevations on the mountain. This showed different species in different climates proving species were not constant. [4] Linnaeus' findings set a basis for ecological biogeography. Through his strong beliefs in Christianity, he was inspired to classify the living world, which then gave way to additional accounts of secular views on geographical distribution. [10] He argued that the structure of an animal was very closely related to its physical surroundings. This was important to a George Louis Buffon's rival theory of distribution. [10] Closely after Linnaeus, Georges-Louis Leclerc, Comte de Buffon observed shifts in climate and how species spread across the globe as a result. He was the first to see different groups of organisms in different regions of the world. Buffon saw similarities between some regions which led him to believe that at one point continents were connected and then water separated them and caused differences in species. His hypotheses were described in his work, the 36 volume Histoire Naturelle, générale et particulière , in which he argued that varying geographical regions would have different forms of life. This was inspired by his observations comparing the Old and New World, as he determined distinct variations of species from the two regions. Buffon believed there was a single species creation event, and that different regions of the world were homes for varying species, which is an alternate view than that of Linnaeus. Buffon's law eventually became a principle of biogeography by explaining how similar environments were habitats for comparable types of organisms. [10] Buffon also studied fossils which led him to believe that the Earth was over tens of thousands of years old, and that humans had not lived there long in comparison to the age of the Earth. [4] 19th century[ edit ] Following the period of exploration came the Age of Enlightenment in Europe, which attempted to explain the patterns of biodiversity observed by Buffon and Linnaeus. At the birth of the 19th century, Alexander von Humboldt, known as the "founder of plant geography", [4] developed the concept of physique generale to demonstrate the unity of science and how species fit together. As one of the first to contribute empirical data to the science of biogeography through his travel as an explorer, he observed differences in climate and vegetation. The Earth was divided into regions which he defined as tropical, temperate, and arctic and within these regions there were similar forms of vegetation. [4] This ultimately enabled him to create the isotherm, which allowed scientists to see patterns of life within different climates. [4] He contributed his observations to findings of botanical geography by previous scientists, and sketched this description of both the biotic and abiotic features of the Earth in his book, Cosmos . [10] Augustin de Candolle contributed to the field of biogeography as he observed species competition and the several differences that influenced the discovery of the diversity of life. He was a Swiss botanist and created the first Laws of Botanical Nomenclature in his work, Prodromus. [16] He discussed plant distribution and his theories eventually had a great impact on Charles Darwin , who was inspired to consider species adaptations and evolution after learning about botanical geography. De Candolle was the first to describe the differences between the small-scale and large-scale distribution patterns of organisms around the globe. [10] Several additional scientists contributed new theories to further develop the concept of biogeography. Charles Lyell developed the Theory of Uniformitarianism after studying fossils. This theory explained how the world was not created by one sole catastrophic event, but instead from numerous creation events and locations. [17] Uniformitarianism also introduced the idea that the Earth was actually significantly older than was previously accepted. Using this knowledge, Lyell concluded that it was possible for species to go extinct. [18] Since he noted that Earth's climate changes, he realized that species distribution must also change accordingly. Lyell argued that climate changes complemented vegetation changes, thus connecting the environmental surroundings to varying species. This largely influenced Charles Darwin in his development of the theory of evolution. [10] Charles Darwin was a natural theologist who studied around the world, and most importantly in the Galapagos Islands . Darwin introduced the idea of natural selection, as he theorized against previously accepted ideas that species were static or unchanging. His contributions to biogeography and the theory of evolution were different from those of other explorers of his time, because he developed a mechanism to describe the ways that species changed. His influential ideas include the development of theories regarding the struggle for existence and natural selection. Darwin's theories started a biological segment to biogeography and empirical studies, which enabled future scientists to develop ideas about the geographical distribution of organisms around the globe. [10] Alfred Russel Wallace studied the distribution of flora and fauna in the Amazon Basin and the Malay Archipelago in the mid-19th century. His research was essential to the further development of biogeography, and he was later nicknamed the "father of Biogeography". Wallace conducted fieldwork researching the habits, breeding and migration tendencies, and feeding behavior of thousands of species. He studied butterfly and bird distributions in comparison to the presence or absence of geographical barriers. His observations led him to conclude that the number of organisms present in a community was dependent on the amount of food resources in the particular habitat. [10] Wallace believed species were dynamic by responding to biotic and abiotic factors. He and Philip Sclater saw biogeography as a source of support for the theory of evolution as they used Darwin's conclusion to explain how biogeography was similar to a record of species inheritance. [10] Key findings, such as the sharp difference in fauna either side of the Wallace Line , and the sharp difference that existed between North and South America prior to their relatively recent faunal interchange , can only be understood in this light. Otherwise, the field of biogeography would be seen as a purely descriptive one. [4] 20th and 21st century[ edit ] Schematic distribution of fossils on Pangea according to Wegener Moving on to the 20th century, Alfred Wegener introduced the Theory of Continental Drift in 1912, though it was not widely accepted until the 1960s. [4] This theory was revolutionary because it changed the way that everyone thought about species and their distribution around the globe. The theory explained how continents were formerly joined in one large landmass, Pangea , and slowly drifted apart due to the movement of the plates below Earth's surface. The evidence for this theory is in the geological similarities between varying locations around the globe, the geographic distribution of some fossils (including the mesosaurs ) on various continents, and the jigsaw puzzle shape of the landmasses on Earth. Though Wegener did not know the mechanism of this concept of Continental Drift, this contribution to the study of biogeography was significant in the way that it shed light on the importance of environmental and geographic similarities or differences as a result of climate and other pressures on the planet. Importantly, late in his career Wegener recognised that testing his theory required measurement of continental movement rather than inference from fossils species distributions. [19] In 1958 paleontologist Paul S. Martin published A Biogeography of Reptiles and Amphibians in the Gómez Farias Region, Tamaulipas, Mexico, which has been described as "ground-breaking" [20] : 35 p. and "a classic treatise in historical biogeography". [21] : 311 p. Martin applied several disciplines including ecology , botany , climatology , geology , and Pleistocene dispersal routes to examine the herpetofauna of a relatively small and largely undisturbed area, but ecologically complex, situated on the threshold of temperate – tropical (nearctic and neotropical) regions, including semiarid lowlands at 70 meters elevation and the northernmost cloud forest in the western hemisphere at over 2200 meters. [20] [21] [22] Biologist Edward O. Wilson , coauthored The Theory of Island Biogeography , which helped in stimulating much research on this topic in the late 20th and 21st. centuries. The publication of The Theory of Island Biogeography by Robert MacArthur and E.O. Wilson in 1967 [23] showed that the species richness of an area could be predicted in terms of such factors as habitat area, immigration rate and extinction rate. This added to the long-standing interest in island biogeography . The application of island biogeography theory to habitat fragments spurred the development of the fields of conservation biology and landscape ecology . [24] Classic biogeography has been expanded by the development of molecular systematics , creating a new discipline known as phylogeography . This development allowed scientists to test theories about the origin and dispersal of populations, such as island endemics . For example, while classic biogeographers were able to speculate about the origins of species in the Hawaiian Islands , phylogeography allows them to test theories of relatedness between these populations and putative source populations on various continents, notably in Asia and North America . [15] Biogeography continues as a point of study for many life sciences and geography students worldwide, however it may be under different broader titles within institutions such as ecology or evolutionary biology. In recent years, one of the most important and consequential developments in biogeography has been to show how multiple organisms, including mammals like monkeys and reptiles like squamates , overcame barriers such as large oceans that many biogeographers formerly believed were impossible to cross. [25] See also Oceanic dispersal . Modern applications[ edit ] Biogeographic regions of Europe Biogeography now incorporates many different fields including but not limited to physical geography, geology, botany and plant biology, zoology, general biology, and modelling. A biogeographer's main focus is on how the environment and humans affect the distribution of species as well as other manifestations of Life such as species or genetic diversity. Biogeography is being applied to biodiversity conservation and planning, projecting global environmental changes on species and biomes, projecting the spread of infectious diseases, invasive species, and for supporting planning for the establishment of crops. Technological evolving and advances have allowed for generating a whole suite of predictor variables for biogeographic analysis, including satellite imaging and processing of the Earth. [26] Two main types of satellite imaging that are important within modern biogeography are Global Production Efficiency Model (GLO-PEM) and Geographic Information Systems (GIS). GLO-PEM uses satellite-imaging gives "repetitive, spatially contiguous, and time specific observations of vegetation". These observations are on a global scale. [27] GIS can show certain processes on the earth's surface like whale locations, sea surface temperatures , and bathymetry. [28] Current scientists also use coral reefs to delve into the history of biogeography through the fossilized reefs. Two global information systems are either dedicated to, or have strong focus on, biogeography (in the form of the spatial location of observations of organisms), namely the Global Biodiversity Information Facility (GBIF: 2.57 billion species occurrence records reported as at August 2023) [29] and, for marine species only, the Ocean Biodiversity Information System (OBIS, originally the Ocean Biogeographic Information System: 116 million species occurrence records reported as at August 2023), [30] while at a national scale, similar compilations of species occurrence records also exist such as the U.K. National Biodiversity Network , the Atlas of Living Australia , and many others. In the case of the oceans, in 2017 Costello et al. analyzed the distribution of 65,000 species of marine animals and plants as then documented in OBIS, and used the results to distinguish 30 distinct marine realms, split between continental-shelf and offshore deep-sea areas. [31] Since it is self evident that compilations of species occurrence records cannot cover with any completeness, areas that have received either limited or no sampling, a number of methods have been developed to produce arguably more complete "predictive" or "modelled" distributions for species based on their associated environmental or other preferences (such as availability of food or other habitat requirements); this approach is known as either Environmental niche modelling (ENM) or Species distribution modelling (SDM). Depending on the reliability of the source data and the nature of the models employed (including the scales for which data are available), maps generated from such models may then provide better representations of the "real" biogeographic distributions of either individual species, groups of species, or biodiversity as a whole, however it should also be borne in mind that historic or recent human activities (such as hunting of great whales , or other human-induced exterminations) may have altered present-day species distributions from their potential "full" ecological footprint. Examples of predictive maps produced by niche modelling methods based on either GBIF (terrestrial) or OBIS (marine, plus some freshwater) data are the former Lifemapper project at the University of Kansas (now continued as a part of BiotaPhy [32] ) and AquaMaps , which as at 2023 contain modelled distributions for around 200,000 terrestrial, and 33,000 species of teleosts , marine mammals and invertebrates, respectively. [32] [33] One advantage of ENM/SDM is that in addition to showing current (or even past) modelled distributions, insertion of changed parameters such as the anticipated effects of climate change can also be used to show potential changes in species distributions that may occur in the future based on such scenarios. [34] See also: Dinosaur paleobiogeography Distribution of four Permian and Triassic fossil groups used as biogeographic evidence for continental drift, and land bridging Paleobiogeography goes one step further to include paleogeographic data and considerations of plate tectonics . Using molecular analyses and corroborated by fossils , it has been possible to demonstrate that perching birds evolved first in the region of Australia or the adjacent Antarctic (which at that time lay somewhat further north and had a temperate climate). From there, they spread to the other Gondwanan continents and Southeast Asia – the part of Laurasia then closest to their origin of dispersal – in the late Paleogene , before achieving a global distribution in the early Neogene . [35] Not knowing that at the time of dispersal, the Indian Ocean was much narrower than it is today, and that South America was closer to the Antarctic, one would be hard pressed to explain the presence of many "ancient" lineages of perching birds in Africa, as well as the mainly South American distribution of the suboscines . Paleobiogeography also helps constrain hypotheses on the timing of biogeographic events such as vicariance and geodispersal , and provides unique information on the formation of regional biotas. For example, data from species-level phylogenetic and biogeographic studies tell us that the Amazonian teleost fauna accumulated in increments over a period of tens of millions of years, principally by means of allopatric speciation, and in an arena extending over most of the area of tropical South America (Albert & Reis 2011). In other words, unlike some of the well-known insular faunas ( Galapagos finches , Hawaiian drosophilid flies, African rift lake cichlids ), the species-rich Amazonian ichthyofauna is not the result of recent adaptive radiations . [36] For freshwater organisms, landscapes are divided naturally into discrete drainage basins by watersheds , episodically isolated and reunited by erosional processes. In regions like the Amazon Basin (or more generally Greater Amazonia, the Amazon basin, Orinoco basin, and Guianas ) with an exceptionally low (flat) topographic relief, the many waterways have had a highly reticulated history over geological time . In such a context, stream capture is an important factor affecting the evolution and distribution of freshwater organisms. Stream capture occurs when an upstream portion of one river drainage is diverted to the downstream portion of an adjacent basin. This can happen as a result of tectonic uplift (or subsidence ), natural damming created by a landslide , or headward or lateral erosion of the watershed between adjacent basins. [36] Concepts and fields[ edit ] Biogeography is a synthetic science, related to geography , biology , soil science , geology , climatology , ecology and evolution . Some fundamental concepts in biogeography include: allopatric speciation – the splitting of a species by evolution of geographically isolated populations evolution – change in genetic composition of a population extinction – disappearance of a species dispersal – movement of populations away from their point of origin, related to migration endemic areas geodispersal – the erosion of barriers to biotic dispersal and gene flow, that permit range expansion and the merging of previously isolated biotas
Toggle the table of contents Atmospheric physics From Wikipedia, the free encyclopedia Sub-field of physics dealing with the atmosphere's structure, composition, and motion e Within the atmospheric sciences , atmospheric physics is the application of physics to the study of the atmosphere . Atmospheric physicists attempt to model Earth's atmosphere and the atmospheres of the other planets using fluid flow equations, radiation budget , and energy transfer processes in the atmosphere (as well as how these tie into boundary systems such as the oceans). In order to model weather systems, atmospheric physicists employ elements of scattering theory , wave propagation models, cloud physics , statistical mechanics and spatial statistics which are highly mathematical and related to physics. It has close links to meteorology and climatology and also covers the design and construction of instruments for studying the atmosphere and the interpretation of the data they provide, including remote sensing instruments. At the dawn of the space age and the introduction of sounding rockets, aeronomy became a subdiscipline concerning the upper layers of the atmosphere, where dissociation and ionization are important. Main article: Remote sensing Remote sensing is the small or large-scale acquisition of information of an object or phenomenon, by the use of either recording or real-time sensing device(s) that is not in physical or intimate contact with the object (such as by way of aircraft , spacecraft , satellite , buoy , or ship ). In practice, remote sensing is the stand-off collection through the use of a variety of devices for gathering information on a given object or area which gives more information than sensors at individual sites might convey. [1] Thus, Earth observation or weather satellite collection platforms, ocean and atmospheric observing weather buoy platforms, monitoring of a pregnancy via ultrasound , magnetic resonance imaging (MRI), positron-emission tomography (PET), and space probes are all examples of remote sensing. In modern usage, the term generally refers to the use of imaging sensor technologies including but not limited to the use of instruments aboard aircraft and spacecraft, and is distinct from other imaging-related fields such as medical imaging . There are two kinds of remote sensing. Passive sensors detect natural radiation that is emitted or reflected by the object or surrounding area being observed. Reflected sunlight is the most common source of radiation measured by passive sensors. Examples of passive remote sensors include film photography , infrared, charge-coupled devices , and radiometers . Active collection, on the other hand, emits energy in order to scan objects and areas whereupon a sensor then detects and measures the radiation that is reflected or backscattered from the target. radar , lidar , and SODAR are examples of active remote sensing techniques used in atmospheric physics where the time delay between emission and return is measured, establishing the location, height, speed and direction of an object. [2] Remote sensing makes it possible to collect data on dangerous or inaccessible areas. Remote sensing applications include monitoring deforestation in areas such as the Amazon Basin , the effects of climate change on glaciers and Arctic and Antarctic regions, and depth sounding of coastal and ocean depths. Military collection during the Cold War made use of stand-off collection of data about dangerous border areas. Remote sensing also replaces costly and slow data collection on the ground, ensuring in the process that areas or objects are not disturbed. Orbital platforms collect and transmit data from different parts of the electromagnetic spectrum , which in conjunction with larger scale aerial or ground-based sensing and analysis, provides researchers with enough information to monitor trends such as El Niño and other natural long and short term phenomena. Other uses include different areas of the earth sciences such as natural resource management , agricultural fields such as land usage and conservation, and national security and overhead, ground-based and stand-off collection on border areas. [3] Radiation[ edit ] This is a diagram of the seasons.  In addition to the density of incident light, the dissipation of light in the atmosphere is greater when it falls at a shallow angle. See also: Radiation and Effect of sun angle on climate Atmospheric physicists typically divide radiation into solar radiation (emitted by the sun) and terrestrial radiation (emitted by Earth's surface and atmosphere). Solar radiation contains variety of wavelengths. Visible light has wavelengths between 0.4 and 0.7 micrometers. [4] Shorter wavelengths are known as the ultraviolet (UV) part of the spectrum, while longer wavelengths are grouped into the infrared portion of the spectrum. [5] Ozone is most effective in absorbing radiation around 0.25 micrometers, [6] where UV-c rays lie in the spectrum. This increases the temperature of the nearby stratosphere . Snow reflects 88% of UV rays, [6] while sand reflects 12%, and water reflects only 4% of incoming UV radiation. [6] The more glancing the angle is between the atmosphere and the sun 's rays, the more likely that energy will be reflected or absorbed by the atmosphere . [7] Terrestrial radiation is emitted at much longer wavelengths than solar radiation. This is because Earth is much colder than the sun.   Radiation is emitted by Earth across a range of wavelengths, as formalized in Planck's law . The wavelength of maximum energy is around 10 micrometers. Main article: Cloud physics Cloud physics is the study of the physical processes that lead to the formation, growth and precipitation of clouds . Clouds are composed of microscopic droplets of water (warm clouds), tiny crystals of ice, or both (mixed phase clouds). Under suitable conditions, the droplets combine to form precipitation , where they may fall to the earth. [8] The precise mechanics of how a cloud forms and grows is not completely understood, but scientists have developed theories explaining the structure of clouds by studying the microphysics of individual droplets. Advances in radar and satellite technology have also allowed the precise study of clouds on a large scale.
Toggle the table of contents Atmospheric chemistry From Wikipedia, the free encyclopedia Branch of atmospheric science in which the chemistry of the atmosphere is studied This article is about field of academic study. For composition of the Earth's atmosphere, see Atmosphere of Earth . e Atmospheric chemistry is a branch of atmospheric science in which the chemistry of the Earth's atmosphere and that of other planets is studied. [1] It is a multidisciplinary approach of research and draws on environmental chemistry , physics , meteorology , computer modeling , oceanography , geology and volcanology and other disciplines. Research is increasingly connected with other areas of study such as climatology . The composition and chemistry of the Earth's atmosphere is of importance for several reasons, but primarily because of the interactions between the atmosphere and living organisms . The composition of the Earth's atmosphere changes as result of natural processes such as volcano emissions, lightning and bombardment by solar particles from corona . It has also been changed by human activity and some of these changes are harmful to human health, crops and ecosystems. Examples of problems which have been addressed by atmospheric chemistry include acid rain , ozone depletion , photochemical smog , greenhouse gases and global warming . Atmospheric chemists seek to understand the causes of these problems, and by obtaining a theoretical understanding of them, allow possible solutions to be tested and the effects of changes in government policy evaluated. Atmospheric composition[ edit ] Visualisation of composition by volume of Earth's atmosphere. Water vapour is not included as it is highly variable. Each tiny cube (such as the one representing krypton) has one millionth of the volume of the entire block. Data is from NASA Langley . The composition of common nitrogen oxides in dry air vs. temperature Chemical composition of atmosphere according to altitude . [2] Axis: Altitude (km),  Content of volume (%). Average composition of dry atmosphere ( mole fractions ) Gas Highly variable (about 0–3%);typically makes up about 1% Notes The mean molecular mass of dry air is 28.97 g/mol. *The content of the gas may undergo significant variations from time to time or from place to place. [a]The concentration of CO2 and CH4 vary by season and location. [b]CO2 here is from 1975, but has been increasing by about 2–3 ppm annually (see Carbon dioxide in Earth's atmosphere ). Trace gas composition[ edit ] Besides the more major components listed above, Earth's atmosphere also has many trace gas species that vary significantly depending on nearby sources and sinks. These trace gases can include compounds such as CFCs/HCFCs which are particularly damaging to the ozone layer, and H2S which has a characteristic foul odor of rotten eggs and can be smelt in concentrations as low as 0.47 ppb. Some approximate amounts near the surface of some additional gases are listed below. In addition to gases, the atmosphere contains particulates as aerosol , which includes for example droplets, ice crystals, bacteria, and dust. Composition (ppt by volume unless otherwise stated) Gas Clean continental, Seinfeld & Pandis (2016) [4] Simpson et al. (2010) [5] History[ edit ] Schematic of chemical and transport processes related to atmospheric composition The ancient Greeks regarded air as one of the four elements . The first scientific studies of atmospheric composition began in the 18th century, as chemists such as Joseph Priestley , Antoine Lavoisier and Henry Cavendish made the first measurements of the composition of the atmosphere.[ citation needed ] In the late 19th and early 20th centuries interest shifted towards trace constituents with very small concentrations. One particularly important discovery for atmospheric chemistry was the discovery of ozone by Christian Friedrich Schönbein in 1840. [6] In the 20th century atmospheric science moved on from studying the composition of air to a consideration of how the concentrations of trace gases in the atmosphere have changed over time and the chemical processes which create and destroy compounds in the air. Two particularly important examples of this were the explanation by Sydney Chapman and Gordon Dobson of how the ozone layer is created and maintained, and the explanation of photochemical smog by Arie Jan Haagen-Smit . Further studies on ozone issues led to the 1995 Nobel Prize in Chemistry award shared between Paul Crutzen , Mario Molina and Frank Sherwood Rowland . [7] In the 21st century the focus is now shifting again. Atmospheric chemistry is increasingly studied as one part of the Earth system . Instead of concentrating on atmospheric chemistry in isolation the focus is now on seeing it as one part of a single system with the rest of the atmosphere , biosphere and geosphere . An especially important driver for this is the links between chemistry and climate such as the effects of changing climate on the recovery of the ozone hole and vice versa but also interaction of the composition of the atmosphere with the oceans and terrestrial ecosystems .[ citation needed ] Carbon dioxide in Earth's atmosphere if half of anthropogenic CO2 emissions [8] [9] are not absorbed( NASA simulation ; 9 November 2015) Nitrogen dioxide 2014 - global air quality levels(released 14 December 2015) [10] Methodology[ edit ] Observations, lab measurements, and modeling are the three central elements in atmospheric chemistry. Progress in atmospheric chemistry is often driven by the interactions between these components and they form an integrated whole. For example, observations may tell us that more of a chemical compound exists than previously thought possible. This will stimulate new modelling and laboratory studies which will increase our scientific understanding to a point where the observations can be explained.[ citation needed ] Observation[ edit ] Observations of atmospheric chemistry are essential to our understanding. Routine observations of chemical composition tell us about changes in atmospheric composition over time. One important example of this is the Keeling Curve - a series of measurements from 1958 to today which show a steady rise in of the concentration of carbon dioxide (see also ongoing measurements of atmospheric CO2 ). Observations of atmospheric chemistry are made in observatories such as that on Mauna Loa and on mobile platforms such as aircraft (e.g. the UK's Facility for Airborne Atmospheric Measurements ), ships and balloons. Observations of atmospheric composition are increasingly made by satellites with important instruments such as GOME and MOPITT giving a global picture of air pollution and chemistry. Surface observations have the advantage that they provide long term records at high time resolution but are limited in the vertical and horizontal space they provide observations from. Some surface based instruments e.g. LIDAR can provide concentration profiles of chemical compounds and aerosol but are still restricted in the horizontal region they can cover. Many observations are available on line in Atmospheric Chemistry Observational Databases .[ citation needed ] Laboratory studies[ edit ] Measurements made in the laboratory are essential to our understanding of the sources and sinks of pollutants and naturally occurring compounds. These experiments are performed in controlled environments that allow for the individual evaluation of specific chemical reactions or the assessment of properties of a particular atmospheric constituent. [11] Types of analysis that are of interest includes both those on gas-phase reactions, as well as heterogeneous reactions that are relevant to the formation and growth of aerosols . Also of high importance is the study of atmospheric photochemistry which quantifies how the rate in which molecules are split apart by sunlight and what resulting products are. In addition, thermodynamic data such as Henry's law coefficients can also be obtained.[ citation needed ]
Toggle the table of contents Geophysics From Wikipedia, the free encyclopedia Physics of the Earth and its vicinity For the journal, see Geophysics (journal) . Not to be confused with Physical geography . Age of the sea floor. Much of the dating information comes from magnetic anomalies. [1] Computer simulation of the Earth's magnetic field in a period of normal polarity between reversals [2] Geophysics ( /ˌdʒiːoʊˈfɪzɪks/ ) is a subject of natural science concerned with the physical processes and physical properties of the Earth and its surrounding space environment, and the use of quantitative methods for their analysis. Geophysicists, who usually study geophysics, physics , or one of the Earth sciences at the graduate level, complete investigations across a wide range of scientific disciplines. The term geophysics classically refers to solid earth applications: Earth's shape ; its gravitational , magnetic fields , and electromagnetic fields ; its internal structure and composition ; its dynamics and their surface expression in plate tectonics , the generation of magmas , volcanism and rock formation. [3] However, modern geophysics organizations and pure scientists use a broader definition that includes the water cycle including snow and ice; fluid dynamics of the oceans and the atmosphere ; electricity and magnetism in the ionosphere and magnetosphere and solar-terrestrial physics ; and analogous problems associated with the Moon and other planets. [3] [4] [5] [6] [7] [8] Although geophysics was only recognized as a separate discipline in the 19th century, its origins date back to ancient times. The first magnetic compasses were made from lodestones , while more modern magnetic compasses played an important role in the history of navigation. The first seismic instrument was built in 132 AD. Isaac Newton applied his theory of mechanics to the tides and the precession of the equinox ; and instruments were developed to measure the Earth's shape, density and gravity field, as well as the components of the water cycle. In the 20th century, geophysical methods were developed for remote exploration of the solid Earth and the ocean, and geophysics played an essential role in the development of the theory of plate tectonics. Geophysics is applied to societal needs, such as mineral resources , mitigation of natural hazards and environmental protection . [4] In exploration geophysics , geophysical survey data are used to analyze potential petroleum reservoirs and mineral deposits, locate groundwater, find archaeological relics, determine the thickness of glaciers and soils, and assess sites for environmental remediation . Physical phenomena[ edit ] Geophysics is a highly interdisciplinary subject, and geophysicists contribute to every area of the Earth sciences , while some geophysicists conduct research in the planetary sciences . To provide a more clear idea on what constitutes geophysics, this section describes phenomena that are studied in physics and how they relate to the Earth and its surroundings. Geophysicists also investigate the physical processes and properties of the Earth, its fluid layers, and magnetic field along with the near-Earth environment in the Solar System , which includes other planetary bodies. Further information: Physical geodesy and Gravimetry The gravitational pull of the Moon and Sun gives rise to two high tides and two low tides every lunar day, or every 24 hours and 50 minutes. Therefore, there is a gap of 12 hours and 25 minutes between every high tide and between every low tide. [9] Gravitational forces make rocks press down on deeper rocks, increasing their density as the depth increases. [10] Measurements of gravitational acceleration and gravitational potential at the Earth's surface and above it can be used to look for mineral deposits (see gravity anomaly and gravimetry ). [11] The surface gravitational field provides information on the dynamics of tectonic plates . The geopotential surface called the geoid is one definition of the shape of the Earth. The geoid would be the global mean sea level if the oceans were in equilibrium and could be extended through the continents (such as with very narrow canals). [12] A model of thermal convection in the Earth's mantle . The thin red columns are mantle plumes . The Earth is cooling, and the resulting heat flow generates the Earth's magnetic field through the geodynamo and plate tectonics through mantle convection . [13] The main sources of heat are: primordial heat due to Earth's cooling and radioactivity in the planets upper crust. [14] There is also some contributions from phase transitions . Heat is mostly carried to the surface by thermal convection , although there are two thermal boundary layers – the core–mantle boundary and the lithosphere – in which heat is transported by conduction . [15] Some heat is carried up from the bottom of the mantle by mantle plumes . The heat flow at the Earth's surface is about 4.2 × 1013 W, and it is a potential source of geothermal energy. [16] Main article: Seismology Illustration of the deformations of a block by body waves and surface waves (see seismic wave ) Seismic waves are vibrations that travel through the Earth's interior or along its surface. [17] The entire Earth can also oscillate in forms that are called normal modes or free oscillations of the Earth . Ground motions from waves or normal modes are measured using seismographs . If the waves come from a localized source such as an earthquake or explosion, measurements at more than one location can be used to locate the source. The locations of earthquakes provide information on plate tectonics and mantle convection. [18] [19] Recording of seismic waves from controlled sources provides information on the region that the waves travel through. If the density or composition of the rock changes, waves are reflected. Reflections recorded using Reflection Seismology can provide a wealth of information on the structure of the earth up to several kilometers deep and are used to increase our understanding of the geology as well as to explore for oil and gas. [11] Changes in the travel direction, called refraction , can be used to infer the deep structure of the Earth . [19] Earthquakes pose a risk to humans . Understanding their mechanisms, which depend on the type of earthquake (e.g., intraplate or deep focus ), can lead to better estimates of earthquake risk and improvements in earthquake engineering . [20] Electricity[ edit ] Although we mainly notice electricity during thunderstorms , there is always a downward electric field near the surface that averages 120 volts per meter. [21] Relative to the solid Earth, the ionization of the planet's atmosphere is a result of the galactic cosmic rays penetrating it, which leaves it with a net positive charge. [22] A current of about 1800 amperes flows in the global circuit. [21] It flows downward from the ionosphere over most of the Earth and back upwards through thunderstorms. The flow is manifested by lightning below the clouds and sprites above. A variety of electric methods are used in geophysical survey. Some measure spontaneous potential , a potential that arises in the ground because of human-made or natural disturbances. Telluric currents flow in Earth and the oceans. They have two causes: electromagnetic induction by the time-varying, external-origin geomagnetic field and motion of conducting bodies (such as seawater) across the Earth's permanent magnetic field. [23] The distribution of telluric current density can be used to detect variations in electrical resistivity of underground structures. Geophysicists can also provide the electric current themselves (see induced polarization and electrical resistivity tomography ). Electromagnetic waves[ edit ] Electromagnetic waves occur in the ionosphere and magnetosphere as well as in Earth's outer core . Dawn chorus is believed to be caused by high-energy electrons that get caught in the Van Allen radiation belt . Whistlers are produced by lightning strikes. Hiss may be generated by both. Electromagnetic waves may also be generated by earthquakes (see seismo-electromagnetics ). In the highly conductive liquid iron of the outer core, magnetic fields are generated by electric currents through electromagnetic induction. Alfvén waves are magnetohydrodynamic waves in the magnetosphere or the Earth's core. In the core, they probably have little observable effect on the Earth's magnetic field, but slower waves such as magnetic Rossby waves may be one source of geomagnetic secular variation . [24] Further information: Earth's magnetic field , Aeromagnetic survey , and Paleomagnetism The Earth's magnetic field protects the Earth from the deadly solar wind and has long been used for navigation. It originates in the fluid motions of the outer core. [24] The magnetic field in the upper atmosphere gives rise to the auroras . [26] Earth's dipole axis (pink line) is tilted away from the rotational axis (blue line). The Earth's field is roughly like a tilted dipole , but it changes over time (a phenomenon called geomagnetic secular variation). Mostly the geomagnetic pole stays near the geographic pole , but at random intervals averaging 440,000 to a million years or so, the polarity of the Earth's field reverses. These geomagnetic reversals , analyzed within a Geomagnetic Polarity Time Scale , contain 184 polarity intervals in the last 83 million years, with change in frequency over time, with the most recent brief complete reversal of the Laschamp event occurring 41,000 years ago during the last glacial period . Geologists observed geomagnetic reversal recorded in volcanic rocks, through magnetostratigraphy correlation (see natural remanent magnetization ) and their signature can be seen as parallel linear magnetic anomaly stripes on the seafloor. These stripes provide quantitative information on seafloor spreading , a part of plate tectonics. They are the basis of magnetostratigraphy , which correlates magnetic reversals with other stratigraphies to construct geologic time scales. [27] In addition, the magnetization in rocks can be used to measure the motion of continents. [24] Further information: Radiometric dating Example of a radioactive decay chain (see Radiometric dating ) Radioactive decay accounts for about 80% of the Earth's internal heat , powering the geodynamo and plate tectonics. [28] The main heat-producing isotopes are potassium-40 , uranium-238 , uranium-235, and thorium-232 . [29] Radioactive elements are used for radiometric dating , the primary method for establishing an absolute time scale in geochronology . Unstable isotopes decay at predictable rates, and the decay rates of different isotopes cover several orders of magnitude, so radioactive decay can be used to accurately date both recent events and events in past geologic eras . [30] Radiometric mapping using ground and airborne gamma spectrometry can be used to map the concentration and distribution of radioisotopes near the Earth's surface, which is useful for mapping lithology and alteration. [31] [32] Main article: Geophysical fluid dynamics Fluid motions occur in the magnetosphere, atmosphere , ocean, mantle and core. Even the mantle, though it has an enormous viscosity , flows like a fluid over long time intervals. This flow is reflected in phenomena such as isostasy , post-glacial rebound and mantle plumes . The mantle flow drives plate tectonics and the flow in the Earth's core drives the geodynamo. [24] Geophysical fluid dynamics is a primary tool in physical oceanography and meteorology . The rotation of the Earth has profound effects on the Earth's fluid dynamics, often due to the Coriolis effect . In the atmosphere, it gives rise to large-scale patterns like Rossby waves and determines the basic circulation patterns of storms. In the ocean, they drive large-scale circulation patterns as well as Kelvin waves and Ekman spirals at the ocean surface. [33] In the Earth's core, the circulation of the molten iron is structured by Taylor columns . [24] Waves and other phenomena in the magnetosphere can be modeled using magnetohydrodynamics . Main article: Mineral physics The physical properties of minerals must be understood to infer the composition of the Earth's interior from seismology , the geothermal gradient and other sources of information. Mineral physicists study the elastic properties of minerals; their high-pressure phase diagrams , melting points and equations of state at high pressure; and the rheological properties of rocks, or their ability to flow. Deformation of rocks by creep make flow possible, although over short times the rocks are brittle. The viscosity of rocks is affected by temperature and pressure, and in turn, determines the rates at which tectonic plates move. [10] Water is a very complex substance and its unique properties are essential for life. [34] Its physical properties shape the hydrosphere and are an essential part of the water cycle and climate . Its thermodynamic properties determine evaporation and the thermal gradient in the atmosphere . The many types of precipitation involve a complex mixture of processes such as coalescence , supercooling and supersaturation . [35] Some precipitated water becomes groundwater , and groundwater flow includes phenomena such as percolation , while the conductivity of water makes electrical and electromagnetic methods useful for tracking groundwater flow. Physical properties of water such as salinity have a large effect on its motion in the oceans. [33] The many phases of ice form the cryosphere and come in forms like ice sheets , glaciers , sea ice , freshwater ice, snow, and frozen ground (or permafrost ). [36] Regions of the Earth[ edit ] Size and form of the Earth[ edit ] Contrary to popular belief, the earth is not entirely spherical but instead generally exhibits an ellipsoid shape- which is a result of the centrifugal forces the planet generates due to its' constant motion [37] . These forces cause the planets diameter to bulge towards the Equator and results in the ellipsoid shape [37] . Earth's shape is constantly changing, and different factors including glacial isostatic rebound (large ice sheets melting causing the Earth's crust to the rebound due to the release of the pressure [38] ), geological features such as mountains or ocean trenches , tectonic plate dynamics, and natural disasters can further distort the planet's shape [37] . Structure of the interior[ edit ] Main article: Structure of Earth Seismic velocities and boundaries in the interior of the Earth sampled by seismic waves Evidence from seismology , heat flow at the surface, and mineral physics is combined with the Earth's mass and moment of inertia to infer models of the Earth's interior – its composition, density, temperature, pressure. For example, the Earth's mean specific gravity (5.515) is far higher than the typical specific gravity of rocks at the surface (2.7–3.3), implying that the deeper material is denser. This is also implied by its low moment of inertia ( 0.33 M R2, compared to 0.4 M R2 for a sphere of constant density). However, some of the density increase is compression under the enormous pressures inside the Earth. The effect of pressure can be calculated using the Adams–Williamson equation . The conclusion is that pressure alone cannot account for the increase in density. Instead, we know that the Earth's core is composed of an alloy of iron and other minerals. [10] Reconstructions of seismic waves in the deep interior of the Earth show that there are no S-waves in the outer core. This indicates that the outer core is liquid, because liquids cannot support shear. The outer core is liquid, and the motion of this highly conductive fluid generates the Earth's field. Earth's inner core , however, is solid because of the enormous pressure. [12] Reconstruction of seismic reflections in the deep interior indicates some major discontinuities in seismic velocities that demarcate the major zones of the Earth: inner core , outer core , mantle, lithosphere and crust . The mantle itself is divided into the upper mantle , transition zone, lower mantle and D′′ layer. Between the crust and the mantle is the Mohorovičić discontinuity . [12] The seismic model of the Earth does not by itself determine the composition of the layers. For a complete model of the Earth, mineral physics is needed to interpret seismic velocities in terms of composition. The mineral properties are temperature-dependent, so the geotherm must also be determined. This requires physical theory for thermal conduction and convection and the heat contribution of radioactive elements . The main model for the radial structure of the interior of the Earth is the preliminary reference Earth model (PREM). Some parts of this model have been updated by recent findings in mineral physics (see post-perovskite ) and supplemented by seismic tomography . The mantle is mainly composed of silicates , and the boundaries between layers of the mantle are consistent with phase transitions. [10] The mantle acts as a solid for seismic waves, but under high pressures and temperatures, it deforms so that over millions of years it acts like a liquid. This makes plate tectonics possible. Main article: Magnetosphere Schematic of Earth's magnetosphere. The solar wind flows from left to right. If a planet's magnetic field is strong enough, its interaction with the solar wind forms a magnetosphere. Early space probes mapped out the gross dimensions of the Earth's magnetic field, which extends about 10 Earth radii towards the Sun. The solar wind, a stream of charged particles, streams out and around the terrestrial magnetic field, and continues behind the magnetic tail , hundreds of Earth radii downstream. Inside the magnetosphere, there are relatively dense regions of solar wind particles called the Van Allen radiation belts. [26]
Toggle the table of contents Physical geography From Wikipedia, the free encyclopedia Study of processes and patterns in the natural environment This article is about the academic discipline. For the peer-reviewed journal, see Physical Geography (journal) . "Physiography" redirects here. The term may also refer to geomorphology . NASA true-color image of the Earth's surface and atmosphere. Part of a series on e Physical geography (also known as physiography) is one of the three main branches of geography . [1] [2] [3] [4] [5] Physical geography is the branch of natural science which deals with the processes and patterns in the natural environment such as the atmosphere , hydrosphere , biosphere , and geosphere . This focus is in contrast with the branch of human geography , which focuses on the built environment , and technical geography , which focuses on using, studying, and creating tools to obtain, analyze, interpret, and understand spatial information. [4] [5] [6] The three branches have significant overlap, however. A natural arch . Physical geography can be divided into several branches or related fields, as follows: Geomorphology [7] [8] is concerned with understanding the surface of the Earth and the processes by which it is shaped, both at the present as well as in the past. Geomorphology as a field has several sub-fields that deal with the specific landforms of various environments, e.g. desert geomorphology and fluvial geomorphology; however, these sub-fields are united by the core processes which cause them, mainly tectonic or climatic processes. Geomorphology seeks to understand landform history and dynamics, and predict future changes through a combination of field observation, physical experiment, and numerical modeling ( Geomorphometry ). Early studies in geomorphology are the foundation for pedology, one of two main branches of soil science . Meander formation. Hydrology [7] [8] is predominantly concerned with the amounts and quality of water moving and accumulating on the land surface and in the soils and rocks near the surface and is typified by the hydrological cycle . Thus the field encompasses water in rivers , lakes , aquifers and to an extent glaciers , in which the field examines the process and dynamics involved in these bodies of water. Hydrology has historically had an important connection with engineering and has thus developed a largely quantitative method in its research; however, it does have an earth science side that embraces the systems approach. Similar to most fields of physical geography it has sub-fields that examine the specific bodies of water or their interaction with other spheres e.g. limnology and ecohydrology . Glaciology is the study of glaciers and ice sheets , or more commonly the cryosphere or ice and phenomena that involve ice. Glaciology groups the latter (ice sheets) as continental glaciers and the former (glaciers) as alpine glaciers. Although research in the areas is similar to research undertaken into both the dynamics of ice sheets and glaciers, the former tends to be concerned with the interaction of ice sheets with the present climate and the latter with the impact of glaciers on the landscape. Glaciology also has a vast array of sub-fields examining the factors and processes involved in ice sheets and glaciers e.g. snow hydrology and glacial geology . Biogeography [7] [8] is the science which deals with geographic patterns of species distribution and the processes that result in these patterns. Biogeography emerged as a field of study as a result of the work of Alfred Russel Wallace , although the field prior to the late twentieth century had largely been viewed as historic in its outlook and descriptive in its approach. The main stimulus for the field since its founding has been that of evolution , plate tectonics and the theory of island biogeography. The field can largely be divided into five sub-fields: island biogeography , paleobiogeography, phylogeography , zoogeography and phytogeography . Climatology [7] [8] is the study of the climate , scientifically defined as weather conditions averaged over a long period of time. Climatology examines both the nature of micro (local) and macro (global) climates and the natural and anthropogenic influences on them. The field is also sub-divided largely into the climates of various regions and the study of specific phenomena or time periods e.g. tropical cyclone rainfall climatology and paleoclimatology . Soil geography deals with the distribution of soils across the terrain . This discipline, between geography and soil science, is fundamental to both physical geography and pedology . [9] [10] [11] Pedology is the study of soils in their natural environment. It deals with pedogenesis , soil morphology , soil classification . Soil geography studies the spatial distribution of soils as it relates to topography , climate (water, air, temperature), soil life (micro-organisms, plants, animals) and mineral materials within soils ( biogeochemical cycles ). Palaeogeography [7] is a cross-disciplinary study that examines the preserved material in the stratigraphic record to determine the distribution of the continents through geologic time. Almost all the evidence for the positions of the continents comes from geology in the form of fossils or paleomagnetism . The use of these data has resulted in evidence for continental drift , plate tectonics , and supercontinents . This, in turn, has supported palaeogeographic theories such as the Wilson cycle . Coastal geography is the study of the dynamic interface between the ocean and the land, incorporating both the physical geography (i.e. coastal geomorphology, geology, and oceanography) and the human geography of the coast. It involves an understanding of coastal weathering processes, particularly wave action, sediment movement and weathering, and also the ways in which humans interact with the coast. Coastal geography, although predominantly geomorphological in its research, is not just concerned with coastal landforms, but also the causes and influences of sea level change . Oceanography [7] is the branch of physical geography that studies the Earth's oceans and seas. It covers a wide range of topics, including marine organisms and ecosystem dynamics ( biological oceanography ); ocean currents, waves, and geophysical fluid dynamics ( physical oceanography ); plate tectonics and the geology of the sea floor ( geological oceanography ); and fluxes of various chemical substances and physical properties within the ocean and across its boundaries ( chemical oceanography ). These diverse topics reflect multiple disciplines that oceanographers blend to further knowledge of the world ocean and understanding of processes within it. Quaternary science [8] is an interdisciplinary field of study focusing on the Quaternary period, which encompasses the last 2.6 million years. The field studies the last ice age and the recent interstadial the Holocene and uses proxy evidence to reconstruct the past environments during this period to infer the climatic and environmental changes that have occurred. Landscape ecology is a sub-discipline of ecology and geography that address how spatial variation in the landscape affects ecological processes such as the distribution and flow of energy, materials, and individuals in the environment (which, in turn, may influence the distribution of landscape "elements" themselves such as hedgerows). The field was largely funded by the German geographer Carl Troll . Landscape ecology typically deals with problems in an applied and holistic context. The main difference between biogeography and landscape ecology is that the latter is concerned with how flows or energy and material are changed and their impacts on the landscape whereas the former is concerned with the spatial patterns of species and chemical cycles. Geomatics is the field of gathering, storing, processing, and delivering geographic information, or spatially referenced information. Geomatics includes geodesy (scientific discipline that deals with the measurement and representation of the earth, its gravitational field, and other geodynamic phenomena, such as crustal motion, oceanic tides, and polar motion), cartography , geographical information science (GIS) and remote sensing (the short or large-scale acquisition of information of an object or phenomenon, by the use of either recording or real-time sensing devices that are not in physical or intimate contact with the object). Environmental geography is a branch of geography that analyzes the spatial aspects of interactions between humans and the natural world. The branch bridges the divide between human and physical geography and thus requires an understanding of the dynamics of geology, meteorology, hydrology, biogeography, and geomorphology, as well as the ways in which human societies conceptualize the environment. Although the branch was previously more visible in research than at present with theories such as environmental determinism linking society with the environment. It has largely become the domain of the study of environmental management or anthropogenic influences. Journals and literature[ edit ] Main category: Geography Journals Mental geography and earth science journals communicate and document the results of research carried out in universities and various other research institutions. Most journals cover a specific publish the research within that field, however unlike human geographers, physical geographers tend to publish in inter-disciplinary journals rather than predominantly geography journal; the research is normally expressed in the form of a scientific paper . Additionally, textbooks, books, and communicate research to laypeople, although these tend to focus on environmental issues or cultural dilemmas. Examples of journals that publish articles from physical geographers are:
Toggle the table of contents Human geography From Wikipedia, the free encyclopedia Study of cultures, communities, and activities of peoples of the world Original mapping by John Snow showing the clusters of cholera cases in the London epidemic of 1854, which is a classical case of using human geography Human geography or anthropogeography is the branch of geography which studies spatial relationships between human communities, cultures, economies, and their interactions with the environment, examples of which include urban sprawl and urban redevelopment . [1] It analyzes spatial interdependencies between social interactions and the environment through qualitative and quantitative methods. [2] [3] This multidisciplinary approach draws from sociology, anthropology, economics, and environmental science, contributing to a comprehensive understanding of the intricate connections that shape lived spaces. [4] History[ edit ] This article includes a list of references , related reading , or external links , but its sources remain unclear because it lacks inline citations . Please help improve this article by introducing more precise citations. (October 2022) ( See also: History of geography The Royal Geographical Society was founded in England in 1830. [5] The first professor of geography in the United Kingdom was appointed in 1883, [6] and the first major geographical intellect to emerge in the UK was Halford John Mackinder , appointed professor of geography at the London School of Economics in 1922. [6] The National Geographic Society was founded in the United States in 1888 and began publication of the National Geographic magazine which became, and continues to be, a great popularizer of geographic information. The society has long supported geographic research and education on geographical topics. The Association of American Geographers was founded in 1904 and was renamed the American Association of Geographers in 2016 to better reflect the increasingly international character of its membership. One of the first examples of geographic methods being used for purposes other than to describe and theorize the physical properties of the earth is John Snow's map of the 1854 Broad Street cholera outbreak . Though Snow was primarily a physician and a pioneer of epidemiology rather than a geographer, his map is probably one of the earliest examples of health geography . The now fairly distinct differences between the subfields of physical and human geography developed at a later date. The connection between both physical and human properties of geography is most apparent in the theory of environmental determinism , made popular in the 19th century by Carl Ritter and others, and has close links to the field of evolutionary biology of the time. Environmental determinism is the theory that people's physical, mental and moral habits are directly due to the influence of their natural environment. However, by the mid-19th century, environmental determinism was under attack for lacking methodological rigor associated with modern science, and later as a means to justify racism and imperialism . A similar concern with both human and physical aspects is apparent during the later 19th and first half of the 20th centuries focused on regional geography . The goal of regional geography, through something known as regionalisation , was to delineate space into regions and then understand and describe the unique characteristics of each region through both human and physical aspects. With links to possibilism and cultural ecology some of the same notions of causal effect of the environment on society and culture remain with environmental determinism. By the 1960s, however, the quantitative revolution led to strong criticism of regional geography. Due to a perceived lack of scientific rigor in an overly descriptive nature of the discipline, and a continued separation of geography from its two subfields of physical and human geography and from geology , geographers in the mid-20th century began to apply statistical and mathematical models in order to solve spatial problems. [1] Much of the development during the quantitative revolution is now apparent in the use of geographic information systems ; the use of statistics, spatial modeling, and positivist approaches are still important to many branches of human geography. Well-known geographers from this period are Fred K. Schaefer , Waldo Tobler , William Garrison , Peter Haggett , Richard J. Chorley , William Bunge , and Torsten Hägerstrand . From the 1970s, a number of critiques of the positivism now associated with geography emerged. Known under the term ' critical geography ,' these critiques signaled another turning point in the discipline. Behavioral geography emerged for some time as a means to understand how people made perceived spaces and places and made locational decisions. The more influential 'radical geography' emerged in the 1970s and 1980s. It draws heavily on Marxist theory and techniques and is associated with geographers such as David Harvey and Richard Peet . Radical geographers seek to say meaningful things about problems recognized through quantitative methods, [7] provide explanations rather than descriptions, put forward alternatives and solutions, and be politically engaged, [8] rather than using the detachment associated with positivists. (The detachment and objectivity of the quantitative revolution was itself critiqued by radical geographers as being a tool of capital). Radical geography and the links to Marxism and related theories remain an important part of contemporary human geography (See: Antipode ). Critical geography also saw the introduction of 'humanistic geography', associated with the work of Yi-Fu Tuan , which pushed for a much more qualitative approach in methodology. The changes under critical geography have led to contemporary approaches in the discipline such as feminist geography , new cultural geography , settlement geography , "demonic" geographies, and the engagement with postmodern and post-structural theories and philosophies.
Toggle the table of contents Environmental science From Wikipedia, the free encyclopedia The integrated, quantitative, and interdisciplinary approach to the study of environmental systems. "Environmental research" redirects here. For the Elsevier journal, see Environmental Research . Environmental science is an interdisciplinary academic field that integrates physics , biology , and geography (including ecology , chemistry , plant science , zoology , mineralogy , oceanography , limnology , soil science , geology and physical geography , and atmospheric science ) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment . [1] Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems . [2] Environmental studies incorporates more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect.[ citation needed ] Environmental scientists seek to understand the earth's  physical, chemical, biological, and geological processes, and to use that knowledge to understand how issues such as alternative energy systems, pollution control and mitigation, natural resource management , and the effects of global warming and climate change influence and affect the natural systems and processes of earth. Environmental issues almost always include an interaction of physical, chemical, and biological processes. Environmental scientists bring a systems approach to the analysis of environmental problems. Key elements of an effective environmental scientist include the ability to relate space, and time relationships as well as quantitative analysis. Environmental science came alive as a substantive, active field of scientific investigation in the 1960s and 1970s driven by (a) the need for a multi-disciplinary approach to analyze complex environmental problems, (b) the arrival of substantive environmental laws requiring specific environmental protocols of investigation and (c) the growing public awareness of a need for action in addressing environmental problems. Events that spurred this development included the publication of Rachel Carson 's landmark environmental book Silent Spring [3] along with major environmental issues becoming very public, such as the 1969 Santa Barbara oil spill , and the Cuyahoga River of Cleveland, Ohio, "catching fire" (also in 1969), and helped increase the visibility of environmental issues and create this new field of study. See also: Glossary of environmental science In common usage, "environmental science" and "ecology" are often used interchangeably, but technically, ecology refers only to the study of organisms and their interactions with each other as well as how they interrelate with environment. Ecology could be considered a subset of environmental science, which also could involve purely chemical or public health issues (for example) ecologists would be unlikely to study. In practice, there are considerable similarities between the work of ecologists and other environmental scientists.  There is substantial overlap between ecology and environmental science with the disciplines of fisheries, forestry, and wildlife.[ citation needed ] Ancient civilizations[ edit ] Historical concern for environmental issues is well documented in archives around the world. [4] Ancient civilizations were mainly concerned with what is now known as environmental science insofar as it related to agriculture and natural resources. Scholars believe that early interest in the environment began around 6000 BCE when ancient civilizations in Israel and Jordan collapsed due to deforestation. [5] As a result, in 2700 BCE the first legislation limiting deforestation was established in Mesopotamia . [5] Two hundred years later, in 2500 BCE, a community residing in the Indus River Valley observed the nearby river system in order to improve sanitation. [5] This involved manipulating the flow of water to account for public health. In the Western Hemisphere, numerous ancient Central American city-states collapsed around 1500 BCE due to soil erosion from intensive agriculture. [5] Those remaining from these civilizations took greater attention to the impact of farming practices on the sustainability of the land and its stable food production. Furthermore, in 1450 BCE the Minoan civilization on the Greek island of Crete declined due to deforestation and the resulting environmental degradation of natural resources. [5] Pliny the Elder somewhat addressed the environmental concerns of ancient civilizations in the text Naturalis Historia , written between 77 and 79 ACE, which provided an overview of many related subsets of the discipline. [6] Although warfare and disease were of primary concern in ancient society, environmental issues played a crucial role in the survival and power of different civilizations. As more communities recognized the importance of the natural world to their long-term success, an interest in studying the environment came into existence.[ citation needed ] Beginnings of environmental science[ edit ] 18th century[ edit ] In 1735, the concept of binomial nomenclature is introduced by Carolus Linnaeus as a way to classify all living organisms, influenced by earlier works of Aristotle . [6] His text, Systema Naturae , represents one of the earliest culminations of knowledge on the subject, providing a means to identify different species based partially on how they interact with their environment.[ citation needed ] 19th century[ edit ] In the 1820s, scientists were studying the properties of gases, particularly those in the Earth's atmosphere and their interactions with heat from the Sun. [7] Later that century, studies suggested that the Earth had experienced an Ice Age and that warming of the Earth was partially due to what are now known as greenhouse gases (GHG). [7] The greenhouse effect was introduced, although climate science was not yet recognized as an important topic in environmental science due to minimal industrialization and lower rates of greenhouse gas emissions at the time.[ citation needed ] Rachel Carson published her groundbreaking novel, Silent Spring, in 1962, bringing the study of environmental science to the forefront of society. Former President Richard Nixon visits the site of the 1969 Santa Barbara oil spill, which received intense media coverage and inspired a multitude of environmental legislation. A team of British researchers found a hole in the ozone layer forming over Antarctica, the discovery of which would later influence the Montreal Protocol in 1987. 20th century[ edit ] In the 1900s, the discipline of environmental science as it is known today began to take shape. The century is marked by significant research, literature, and international cooperation in the field. In the early 20th century, criticism from dissenters downplayed the effects of global warming . [7] At this time, few researchers were studying the dangers of fossil fuels . After a 1.3 degrees Celsius temperature anomaly was found in the Atlantic Ocean in the 1940s, however, scientists renewed their studies of gaseous heat trapping from the greenhouse effect (although only carbon dioxide and water vapor were known to be greenhouse gases then). [7] Nuclear development following the Second World War allowed environmental scientists to intensively study the effects of carbon and make advancements in the field. [7] Further knowledge from archaeological evidence brought to light the changes in climate over time, particularly ice core sampling . [7] Environmental science was brought to the forefront of society in 1962 when Rachel Carson published an influential piece of environmental literature, Silent Spring . [8] Carson's writing led the American public to pursue environmental safeguards, such as bans on harmful chemicals like the insecticide DDT . [8] Another important work, The Tragedy of the Commons , was published by Garrett Hardin in 1968 in response to accelerating natural degradation. [6] In 1969, environmental science once again became a household term after two striking disasters: Ohio's Cuyahoga River caught fire due to the amount of pollution in its waters and a Santa Barbara oil spill endangered thousands of marine animals, both receiving prolific media coverage. [8] Consequently, the United States passed an abundance of legislation, including the Clean Water Act and the Great Lakes Water Quality Agreement . [8] The following year, in 1970, the first ever Earth Day was celebrated worldwide and the United States Environmental Protection Agency (EPA) was formed, legitimizing the study of environmental science in government policy. [8] In the next two years, the United Nations created the United Nations Environment Programme (UNEP) in Stockholm, Sweden to address global environmental degradation . [9] Much of the interest in environmental science throughout the 1970s and the 1980s was characterized by major disasters and social movements. In 1978, hundreds of people were relocated from Love Canal , New York after carcinogenic pollutants were found to be buried underground near residential areas. [8] The next year, in 1979, the nuclear power plant on Three Mile Island in Pennsylvania suffered a meltdown and raised concerns about the dangers of radioactive waste and the safety of nuclear energy. [8] In response to landfills and toxic waste often disposed of near their homes, the official Environmental Justice Movement was started by a Black community in North Carolina in 1982. [8] Two years later, the toxic methyl isocyanate gas was released to the public from a power plant disaster in Bhopal , India, harming hundreds of thousands of people living near the disaster site, the effects of which are still felt today. [8] In a groundbreaking discovery in 1985, a British team of researchers studying Antarctica found evidence of a hole in the ozone layer , inspiring global agreements banning the use of chlorofluorocarbons (CFCs), which were previously used in nearly all aerosols and refrigerants. [7] Notably, in 1986, the meltdown at the Chernobyl nuclear power plant in Ukraine released radioactive waste to the public, leading to international studies on the ramifications of environmental disasters. [8] Over the next couple of years, the Brundtland Commission (previously known as the World Commission on Environment and Development) published a report titled Our Common Future and the Montreal Protocol formed the International Panel on Climate Change (IPCC) as international communication focused on finding solutions for climate change and degradation. [9] In the late 1980s, the Exxon Valdez company was fined for spilling large quantities of crude oil off the coast of Alaska and the resulting cleanup, involving the work of environmental scientists. [8] After hundreds of oil wells were burned in combat in 1991, warfare between Iraq and Kuwait polluted the surrounding atmosphere just below the air quality threshold environmental scientist s believed was life-threatening. [8] 21st century[ edit ] The Paris Agreement (formerly the Kyoto Protocol) is adopted in 2016. Nearly every country in the United Nations has signed the treaty, which aims to reduce greenhouse gas emissions. Many niche disciplines of environmental science have emerged over the years, although climatology is one of the most known topics. Since the 2000s, environmental scientists have focused on modeling the effects of climate change and encouraging global cooperation to minimize potential damages. In 2002, the Society for the Environment as well as the Institute of Air Quality Management were founded to share knowledge and develop solutions around the world. [9] Later, in 2008, the United Kingdom became the first country to pass legislation (the Climate Change Act ) that aims to reduce carbon dioxide output to a specified threshold. [9] In 2016 the Kyoto Protocol became the Paris Agreement , which sets concrete goals to reduce greenhouse gas emissions and restricts Earth's rise in temperature to a 2 degrees Celsius maximum. [9] The agreement is one of the most expansive international efforts to limit the effects of global warming to date. Most environmental disasters in this time period involve crude oil pollution or the effects of rising temperatures. In 2010, BP was responsible for the largest American oil spill in the Gulf of Mexico, known as the Deepwater Horizon spill , which killed a number of the company's workers and released large amounts of crude oil into the water. [8] Furthermore, throughout this century, much of the world has been ravaged by widespread wildfires and water scarcity , prompting regulations on the sustainable use of natural resources as determined by environmental scientists. [8] A false color composite of the greater Boston area, created using remote sensing technology, reveals otherwise not visible characteristics about the land cover and the health of the surrounding ecosystems. The 21st century is marked by significant technological advancements. New technology in environmental science has transformed how researchers gather information about various topics in the field. Research in engines, fuel efficiency , and decreasing emissions from vehicles since the times of the Industrial Revolution has reduced the amount of carbon and other pollutants into the atmosphere. [10] Furthermore, investment in researching and developing clean energy (i.e. wind, solar, hydroelectric, and geothermal power) has significantly increased in recent years, indicating the beginnings of the divestment from fossil fuel use . [10] Geographic information systems (GIS) are used to observe sources of air or water pollution through satellites and digital imagery analysis. [10] This technology allows for advanced farming techniques like precision agriculture as well as monitoring water usage in order to set market prices. [10] In the field of water quality, developed strains of natural and manmade bacteria contribute to bioremediation , the treatment of wastewaters for future use. [10] This method is more eco-friendly and cheaper than manual cleanup or treatment of wastewaters. [10] Most notably, the expansion of computer technology has allowed for large data collection, advanced analysis, historical archives, public awareness of environmental issues, and international scientific communication. [11] The ability to crowdsource on the Internet, for example, represents the process of collectivizing knowledge from researchers around the world to create increased opportunity for scientific progress. [11] With crowdsourcing , data is released to the public for personal analyses which can later be shared as new information is found. [11] Another technological development, blockchain technology, monitors and regulates global fisheries. [11] By tracking the path of fish through global markets, environmental scientists can observe whether certain species are being overharvested to the point of extinction. [11] Additionally, remote sensing allows for the detection of features of the environment without physical intervention. [11] The resulting digital imagery is used to create increasingly accurate models of environmental processes, climate change , and much more. Advancements to remote sensing technology are particularly useful in locating the nonpoint sources of pollution and analyzing ecosystem health through image analysis across the electromagnetic spectrum . Lastly, thermal imaging technology is used in wildlife management to catch and discourage poachers and other illegal wildlife traffickers from killing endangered animals, proving useful for conservation efforts. [11] Artificial intelligence has also been used to predict the movement of animal populations and protect the habitats of wildlife. [11]
Toggle the table of contents Environmental policy From Wikipedia, the free encyclopedia Government efforts protecting the natural environment e Environmental policy is the commitment of an organization or government to the laws, regulations, and other policy mechanisms concerning environmental issues . These issues generally include air and water pollution , waste management , ecosystem management , maintenance of biodiversity , the management of natural resources , wildlife and endangered species . [1] For example, concerning environmental policy, the implementation of an eco-energy-oriented policy at a global level to address the issues of global warming and climate changes could be addressed. [2] Policies concerning energy or regulation of toxic substances including pesticides and many types of industrial waste are part of the topic of environmental policy. This policy can be deliberately taken to influence human activities and thereby prevent undesirable effects on the biophysical environment and natural resources, as well as to make sure that changes in the environment do not have unacceptable effects on humans. [3] Definition[ edit ] One way is to describe environmental policy is that it comprises two major terms: environment and policy . Environment refers to the physical ecosystems, but can also take into consideration the social dimension (quality of life, health) and an economic dimension (resource management, biodiversity). [4] Policy can be defined as a "course of action or principle adopted or proposed by a government, party, business or individual". [5] Thus, environmental policy tends to focus on problems arising from human impact on the environment , which is important to human society by having a (negative) impact on human values.  Such human values are often labeled as good health or the 'clean and green' environment.  In practice, policy analysts provide a wide variety of types of information to the public decision-making process. [6] The concept of environmental policy was first used in the 1960s to recognise that all environmental problems, like the environment itself, are interconnected. Addressing environmental problems effectively (such as air, water, and soil pollution) requires looking at their connections and underlying and common sources, and how policies addressing particular problems can have spill-over effects on other problems and policies. "The environment" thus became a focus for public policy and environmental policy the term to refer to the way environmental issues were addressed more or less comprehensively. [7] Environmental issues typically addressed by environmental policy include (but are not limited to) air and water pollution , waste management , ecosystem management, biodiversity protection, the protection of natural resources , wildlife and endangered species , and the management of these natural resources for future generations. Relatively recently, environmental policy has also attended to the communication of environmental issues. [8] Environmental policies often address issues in one of three dimensions of the environment: ecological (for instance, policies aimed at protecting a particular species or natural areas), resource (for instance, related to energy, land, water), and the human environment (the environment modified or shaped by humans, for instance, urban planning, pollution). [9] Environmental policy-making is often highly fragmented, although environmental policy analysts have long pointed out the need for the development of more comprehensive and integrated environmental policies. [10] [11] [12] In contrast to environmental policy, ecological policy addresses issues that focus on achieving benefits (both monetary and non monetary) from the non human ecological world.  Broadly included in ecological policy is natural resource management (fisheries, forestry, wildlife, range, biodiversity, and at-risk species).  This specialized area of policy possesses its own distinctive features. [13] History[ edit ] As pointed out by environmental historians , environmental problems have long afflicted human societies and led to collective efforts to address these problems. [14] Some longstanding problems have been the hunting of animals to extinction, soil erosion and salinisation (because of over-irrigation), and the adverse effects of some practices on human health (wood fires, unhygienic practices). [15] [16] [17] In some cases, these practices contributed to the collapse of societies. [18] In the 19th century, the growing impact of human development and practices on the environment became increasingly apparent. Deforestation, the decline and extinction of birds, the uglification of landscapes and cities, large-scale mining (notably of coal), industrial pollution, and urban squalor led to growing awareness and appreciation of the importance of nature. Some seminal thinkers on these matters were George Perkins Marsh , Henry David Thoreau , and John Muir . In Europe, a positive view of nature was promoted by the Romanticist movement of poets, authors and artists from the early 18th century, a movement that lamented the despoliation of nature by industrialism. Building on these early forms of concern about nature, organisations aimed at the preservation of forests, birds and landscapes emerged in the United States , the United Kingdom , Europe , Australia and New Zealand in the late 19th and the early 20th centuries, these efforts, combined with concerns about dwindling timber supplies, were instrumental in the establishment of the first nature reserves, national and forest parks and to changes in forestry laws. Concerns about pollution and its threat to humans as well as nature has provided another major stimulus for the development of environmental policies. In 1863, in the United Kingdom, health problems arising from the release of harmful chemicals led to the adoption of the Alkali Act and the creation of the Alkali Inspectorate. [19] In 1956, the Clean Air Act 1956 was adopted in the wake of London 's Great Smog of 1952 that is believed to have killed 12,000 people. Concerns about the effects of pollution fuelled notably by the publication, in 1962, of Rachel Carson ’s Silent Spring, sparked the beginning of the modern environmental movement. It also marked the start of “the environment” becoming a concern of public policy, as pointed out by Caldwell in 1963. [20] These growing concerns, as well as the growing publicity about environmental problems and accidents, forced governments to introduce or strengthen laws and policies aimed at enhancing environmental protection. Earth Day founder Gaylord Nelson , then a U.S. Senator from Wisconsin, after witnessing the ravages of the 1969 massive oil spill in Santa Barbara, California, became famous for his environmental work. Administrator Ruckelshaus was confirmed by the Senate on December 2, 1970, which is the traditional date used as the birth of the United States Environmental Protection Agency (EPA). Five months earlier, in July 1970, President Nixon had signed Reorganization Plan No. 3 calling for the establishment of EPA. At the time, environmental policy was a bipartisan issue and the efforts of the United States of America made it an early environmental leader. [21] [22] During this period, legislation was passed to regulate pollutants that go into the air, water tables, and solid waste disposal. President Nixon signed the Clean Air Act in 1970. In many countries, governments created environment ministries, departments or agencies, and appointed ministers of or for the environment. The world's first minister of the environment was the British Politician Peter Walker from the Conservative Party in 1970. In the European Union, the very first Environmental Action Programme was adopted by national government representatives in July 1973 during the first meeting of the Council of Environmental Ministers. [23] Since then an increasingly dense network of legislation has developed, which now extends to all areas of environmental protection including air pollution control, water protection and waste policy but also nature conservation and the control of chemicals, biotechnology and other industrial risks. EU environmental policy has thus become a core area of European politics. Despite commonalities between countries in the development of environmental policies and institutions, they have also adopted different approaches in this area. In the 1970s, the field of Comparative Environmental Politics and Policy emerged to compare the environmental policies and institutions of countries aimed at explaining differences and similarities. Some of the pioneers in this area were Lennart Lundqvist [24] [25] and Cynthia Enloe. [26] Rationale[ edit ] As documented by environmental historians , human societies have always impacted their environment, often with adverse consequences for themselves and the rest of nature. Their failure to (timely) recognise and address these problems has been a contributing factor to their decline and collapse. [27] [28] Although particular environmental problems like soil erosion, growing resource scarcity, air and water pollution increasingly became the subject of concern and government regulation in the 19th century, these were seen and addressed as separate issues. [29] [30] The shortcomings of this reactive and fragmented approach received growing recognition during the 1960s and early 1970s, the first wave of environmentalism . This was reflected in the creation, in many countries, of environmental agencies, policies and legislation with the aim of taking a more comprehensive and integrated approach to environmental issues. [31] [32] [33] In 1972, the need for this was also recognised at the international level at the United Nations Conference on the Human Environment , which led to the creation of the United Nations Environment Programme . [34] [35] Thus, growing environmental awareness and concern provided the main rationale for the adoption of environmental policies and institutions by governments. Environmental protection became a focus of public policy. [7] This rationale for environmental policy is broader than that provided by some interpretations based on economic theories. The rationale for governmental involvement in the environment is often attributed to market failure in the form of forces beyond the control of one person, including the free rider problem and the tragedy of the commons . An example of an externality is when a factory produces waste pollution which may be discharged into a river, ultimately contaminating water. The cost of such action is paid by society at large when they must clean the water before drinking it and is external to the costs of the polluter. The free rider problem occurs when the private marginal cost of taking action to protect the environment is greater than the private marginal benefit, but the social marginal cost is less than the social marginal benefit. The tragedy of the commons is the condition that, because no one person owns the commons, each individual has an incentive to utilize common resources as much as possible. Without governmental involvement, the commons is overused.  Examples of tragedies of the commons are overfishing and overgrazing . [36] [37] The “market failure” rationale for environmental policy has been criticised for its implicit assumptions about the drivers of human behaviour, which are considered to be rooted in the idea that societies are nothing but collections of self-interested “utility-maximising” individuals. [38] [39] As Elinor Ostrom has demonstrated, [40] this is not supported by evidence on how societies actually make resource decisions. The market-failure theory also assumes that “markets” have, or should have precedence over governments in collective decision-making, which is an ideological position that was challenged by Karl Polanyi whose historical analysis shows how the idea of a self-regulating market was politically created. He added that "Such an institution could not exist for any length of time without annihilating the human and natural substance of society." [41] By contrast, ecological economists argue that economic policies should be developed within a theoretical framework that recognises the biophysical reality. The economic system is a sub-system of the biophysical environmental system on which humans and other species depend for their well-being and survival. [42] [43] The need for grounding environmental policy on ecological principles has also been recognised by many environmental policy analysts, sometimes under the label of ecological rationality and/or environmental integration. [44] [45] [46] From this perspective, political, economic, and other systems, as well as policies, need to be “greened” to make them ecologically rational. [47] [48] [9] Environmental policy approaches: instruments, problems, and issues[ edit ] In practice, governments have adopted a wide range of approaches to the development and implementation of environmental policies. To a large extent, differences in approaches have been influenced and shaped by the particular political, economic and social context of a country or polity (like the European Union or the United Nations). The differences in approaches, the reasons behind them, and their results have been the subject of research in the fields of comparative environmental politics and policy. [49] [50] [51] [52] But the study of problems and issues associated with environmental policy development has also been influenced by general public policy theories and analyses. [53] [54] [55] [56] Contributions on this front have been influenced by different academic disciplines, notably economics, public policy, and environmental studies, but also by political-ideological views, politics, and economic interests, among others through “think tanks”. [57] [ [58] [59] [60] Thus, the design of environmental policy and the choice of policy instruments is always political and not just a matter determined by technical and efficiency considerations advanced by scientists, economists or other experts. [61] [56] As Majone has argued: “Policy instruments are seldom ideologically neutral” and “cannot be neatly separated from goals.” [56] The choice of policy instruments always occurs in a political context. Differences in ideological preferences of governments and political actors, and in national policy styles, have been argued to strongly influence a government’s approach to policy design, including the choice of instruments. [61] [62] [63] [64] Although many different policy instruments can be identified, and many ways of classifying them have been put forward, [65] [61] [66] [67] very broadly, a minimalist approach distinguishes three kinds or categories of policy instruments: regulation, economic instruments, and normative or “hortatory” approaches. These have also been referred to as “sticks, carrots and sermons”. [65] [67] Vedung, based on Majone’s classification of power, argues that the main difference underlying these categories is the degree of coercion (authoritative force) involved. [67] Regulation has been a traditional and predominant approach to policymaking in many policy areas and countries. [68] [69] [70] It relies foremost on adopting rules (often backed up by legislation), to prohibit, impose or circumscribe human behaviour and practices. In the environmental policy area, this includes, for instance, the imposition of limits or standards for air and water pollution, car emissions, the regulation or banning of the use of hazardous substances, the phasing out of ozone-depleting substances, waste disposal, and laws to protect endangered species and natural areas. [71] [72] [56] [73] [74] Regulation is often derogatorily referred to by detractors as a top-down, “command and control” approach as it leaves target groups with little if any control over the way(s) environmental activities or goals must be pursued. Since the 1980s, with the rise of neoliberalism in many countries and the associated redefinition of the role of the state (centred on the notion of governance rather than government), regulation has been touted as ineffective and inefficient, sparking a move toward deregulation and the adoption by many governments of “new” policy instruments, notably market instruments and voluntary agreements, also in the realm of environmental policy. [75] [76] [77] Economic instruments involve the imposition or use of economic incentives, including (environmental) taxes, tax exemptions, fees, subsidies, and the creation of markets and rights for trading in substances, pollutants, resources, or activities, such as for SO2, CO2 (carbon or greenhouse gas emissions), water, and tradeable fisheries quota. They are based on the assumption that behaviour and practices are foremost driven by rationality, self-interest and economic considerations and that these motivations can be harnessed for environmental purposes. Decision-making studies cast doubt on these premises. Often, decisions are reached based on irrational influences, unconscious biases, illogical assumptions, and the desire to avoid or create ambiguity and uncertainty. [78] [79] [80] [81] Market-based policy instruments also have their supporters and detractors. Among the detractors, for example, some environmentalists contend that a more radical, overarching approach is needed than a set of specific initiatives, to deal with climate change . For example, energy efficiency measures may actually increase energy consumption in the absence of a cap on fossil fuel use, as people might drive more fuel-efficient cars. To combat this result, Aubrey Meyer calls for a 'framework-based market' of Contraction and Convergence . [82] The Cap and Share and the Sky Trust are proposals based on the idea. In the case of corporations, it is assumed that such tools make it financially rewarding to engage in efficient environmental management that also improves business and organizational performance  They also encourage businesses to become more transparent about their environmental performance by publishing data and reporting. For economic instruments to function, some form(s) of regulation are needed that involve policy design, for instance, related to the choice and level of taxation, who pays, who qualifies for rights or permits, and the rules on which trading, and a “market” depend for their functioning. For example, the implementation of greener public purchasing programs relies on a combination of regulation and economic incentives. Normative ("hortatory”) instruments (“sermons”) rely on persuasion and information. [83] They include, among others, campaigns aimed at raising public awareness and enhancing knowledge of environmental problems, call upon people to change their behaviour and practices (like taking up recycling, reducing waste, the use of water and energy, and using public transport), and voluntary agreements between governments and businesses. They share the aim of encouraging people to do “the right thing”, to change their behaviour and practices, and to accept individual or group responsibility for addressing issues. Agreements between the government and private firms and commitments made by firms independent of government requirements are examples of voluntary environmental measures. [83] Environmental Impact Assessment is a tool that relies foremost on the gathering of knowledge and information about (potential) environmental effects. It originated in the United States but has been adopted in many countries to analyse and assess the potential impacts of projects. Usually undertaken by experts, it is based on the assumption that an objective assessment of effects is possible, and that the knowledge generated will persuade decision-makers to make changes to proposals to mitigate or prevent adverse environmental effects. [84] How EIA rules and processes are designed and implemented depends on regulation and is influenced by the political context. [85] Eccleston and March argue that although policymakers normally have access to reasonably accurate environmental information, political and economic factors are important and often lead to policy decisions that rank environmental priorities of secondary importance.[Reference needed] The effectiveness of hortatory instruments has also been under debate. [83] Policies relying foremost on such instruments may amount to little more than symbolic policies, implying that governments have little or no intention to effectively address an issue while creating the impression of taking it seriously. [86] Such policies rely more on rhetoric than action. In the environmental realm, sustainable development policies or strategies are often used for this purpose if these are not translated into clear and specific objectives, timeframes and measures. [87] Yet, hortatory policy instruments are often preferred by governments and other actors as they are seen as a way of recognising and sharing collective responsibility, possibly avoiding the need for regulation and/or economic instruments. They are thus often used as a first step towards addressing environmental problems. [67] However, these tools are often combined with some form of legislation and regulation, for instance, in the case of labelling of consumer products (product information), waste disposal and recycling. There has been much debate about the relative merits of the various kinds of policy instruments. Market instruments are often held up and used as a more efficient and cost-effective, alternative to regulation. Yet, many analysts have pointed out that regulation, economic incentives, “market” instruments, and environmental taxation and subsidies can achieve the same results. For instance, as Kemp and Pontoglio argue, policy instruments cannot be usefully ranked with regard to their effects on eco-innovation, “the often expressed view that market-based approaches such as pollution taxes and emission trading systems are better for promoting eco-innovation is not brought out by the case study literature or by survey analysis”, and there is actually more evidence that regulations stimulate radical innovation more than market-based instruments. [88] It has also been argued that If the government can anticipate new technology or is able to react to it optimally, regulatory policies by virtue of administered prices (taxes) and policies by setting quantities (issuing tradable permits) are (almost) equivalent. [89] More generally, the performance of economic instruments in dealing with environmental problems has been a mixed bag, referred to by Hahn as “not very impressive” [78] , and has led Tietenberg to conclude that they are “no panacea”. [90] Different instruments are sometimes combined in a policy mix to address a particular environmental problem. Since environmental issues have many aspects, several policy instruments may be required to adequately address each one. Ideally, government policies are carefully formulated so that the individual measures do not undermine one another or create a rigid and cost-ineffective framework. Overlapping policies result in unnecessary administrative costs, increasing the cost of implementation. To help governments realize their policy goals, the OECD Environment Directorate, for example, collects data on the efficiency and consequences of environmental policies implemented by the national governments. Their website provides a database detailing countries' experiences with their environmental policies. The United Nations Economic Commission for Europe , through UNECE, and the OECD’s Environmental Performance Reviews , evaluate progress made by its member countries in improving their environmental policies. However, although regulation, taxation and market instruments can be equally (in-) effective, they may differ significantly in the allocation and distribution of (potential) costs and benefits, with the allocation of tradeable (“property”) rights potentially generating significant profits to those who receive such rights. [91] [92] They are, therefore, generally much preferred by affected resource users and industries, which explains their popularity since the rise of neoliberalism. This has led analysts to point out that there are many other important aspects to the choice of policy instruments than their efficiency and cost-effectiveness, such as distributional, ethical and political aspects, and their appropriateness for addressing environmental problems. [93] [94] [95] [61] [96] [56] [97] Environmental policy analysis[ edit ] How environmental policies are made, how effective they are, and how they can or should be improved, has become the subject of considerable research and debate. In the academic realm, these questions are commonly addressed under the label of environmental policy analysis. Environmental policy analysis is a broad field comprising different approaches to explaining and developing environmental policy. The first type has been referred to in the policy literature as the analysis of policy and the second as the analysis for policy. [98] Many approaches are derived from the broader field of public policy analysis which emerged as a scientific enterprise after WWII. [99] While policy analysis as a decision-making tool continued to be applied in the business sector, the study of public policy, defined broadly as “What governments do, why they do it, and what difference it makes, [100] became an important strand in political science. This variety, which has been classified into analycentric, policy process, and meta-policy categories, has also manifested itself in the area of environmental policy analysis which developed since the 1960s. [4] The analycentric or rational approach[ edit ] The analycentric approach to environmental policy analysis, which focuses on particular issues and uses mostly quantitative methods to identify “optimal” (cost-effective or efficient) solutions, has been the prevalent way to address environmental problems, both by governments and businesses. It is also often depicted as the rational or scientific approach to and for policy development. While scientific analyses and (preferably) quantitative data provide knowledge of the more immediate sources or causes of environmental problems, such as forms of pollution and climate change, policy prescriptions are based on setting goals, objectives and targets and the identification of the most cost-effective and efficient means by assessing alternative options. Technological innovation, more efficient management, and economic instruments such as cost-benefit analysis, [9] [101] environmental taxes, [102] [103] and tradeable permit schemes (market creation) [78] [90] have been among the preferred means in this approach. The analycentric or rational approach has been critiqued on various grounds. [104] [105] [106] [107] First, it assumes that there is adequate knowledge and agreement on the causes of problems and the goals to be achieved. Second, the approach (for policy) ignores the way policies are developed in (political) practice. Third, the preferred means are often based on questionable assumptions notably about human behaviour. Many of the limitations of the rational approach were already acknowledged by an early proponent, Herbert Simon, who argued that “limited rationality” provided a more realistic basis for decision-making. [108] This view has also been expressed by advocates of more comprehensive and integrated environmental policy development, who argued that looking at problems in isolation (on a one-by-one basis) ignores the linkages between environmental problems and their causes. [109] [110] In the late 1980s, “green planning” and the adoption of sustainable development strategies, in particular, received support in academic circles and among many governments as rational, goal-based policy approaches aimed at overcoming the limitations of the fragmented analycentric approach. [111] [112] [113] [114] [115] The policy process approach[ edit ] The policy process approach emphasises the role and importance of politics and power in policy development. It aims foremost at better understanding how policies are made and put into practice. It commonly involves identifying a variable number of steps, including problem definition and agenda setting, the formulation and selection of policy options, implementation, and evaluation. [4] [116] These are conceived as being parts of a policy cycle, as existing policies are reviewed and changed for political reasons and/or because they are deemed to be unsatisfactory. The various stages have become the focus of much research, generating insights into why and how policies have been developed and implemented, with variable outcomes and effectiveness. These studies show that policy development is more about the role of and interplay between conflicting interests than the result of rational analysis and finding and adopting (optimal) solutions to problems. One of the main schools of thought on this front is that of incrementalism, which argues that policy change often occurs in small steps that accommodate conflicting interests. [117] [118] [106] Policy process analysis has also been applied to environmental policy in its different stages. It has been used, for instance, to clarify why environmental issues have had difficulty reaching or staying on the public and political agendas. [119] [120] [121] More recently, research has revealed the role and power of businesses, notably the oil industry, in downplaying the risks associated with climate change or “climate denial.” [122] [123] [124] “Think tanks” and the media have been used to sow scepticism about the science behind environmental and other problems, to redefine issues, and to avert policies that threaten the interests of businesses. [125] [126] [127] [128] Policy process analyses also include studies of the variety of actors and their influence on government decision-making. Although pluralism, the idea that not one group dominates all decision-making in modern societies, has long been the prevailing school of thought in political science, [129] [130] it has been contested by elite theories that assign predominant power to elites in different areas or sectors of decision-making. [131] [132] [133] To what extent environmental groups have had influence on government decisions and policies continues to be a subject of debate. Some argue that Non-Governmental organizations have the greatest influence on environmental policies. [134] These days, many countries are facing huge environmental, social, and economic impacts of rapid population growth, development, and natural resource constraints. As NGOs try to help countries to tackle these issues more successfully, a lack of understanding about their role in civil society and the public perception that the government alone is responsible for the well-being of its citizens and residents makes NGOs tasks more difficult to achieve.  NGOs such as Greenpeace and World Wildlife Fund can help tackling issues by conducting research to facilitate policy development, building institutional capacity, and facilitating independent dialogue with civil society to help people live more sustainable lifestyles. The need for a legal framework to recognize NGOs and enable them to access more diverse funding sources, high-level support/endorsement from local figureheads, and engaging NGOs in policy development and implementation is more important as environmental issues continue to increase. [135] It has been argued that notwithstanding Reagan's efforts to undo environmental regulation in the US, the effects have been limited as environmental interests were already strongly entrenched. [136] Under President Trump, again, many environmental regulations have been dismantled or were scheduled to be rolled back. [137] [138] [139] Other research suggests that many environmental policies adopted by governments are designed to be weak and largely ineffective as business interests use their power to influence or even shape these policies, also at the international level. [9] [140] International organizations have also made great impacts on environmental policies by creating programmes such as the United Nations Environment Programme and hosting conferences such as the United Nations Earth Summit to address environmental issues. UNEP is the leading global environmental authority tasked with policy guidance for environmental programs. The UNEP monitors environmental aspects, such as waste management, energy use, greenhouse gas inventory, and water use to promote environmental sustainability and address environmental issues. [141] The role of science and scientists in policy environmental policy development has been another focus of research. Scientists have been instrumental in discovering many environmental problems, from the damaging effects of the use of pesticides, [142] the depletion of the ozone layer , the greenhouse effect , all kinds of pollution, among others. In this respect, they have often provided legitimacy and support to the raising of concerns by the environmental movement, although they have often been reluctant to get involved in environmental activism out of fear of compromising their scientific credibility. [143] Nonetheless, scientists have played a significant role pushing environmental issues onto the international agenda, together with international ENGOs, in what have been referred to as “epistemic communities.” [144] However, to what extent science can be “value-free” has been a subject of debate. [145] [146] [147] Science and scientists always operate in a political-economic context that circumscribes their role, research and its effects. [148] This raises the question of scientific integrity, especially when scientists are paid to serve commercial and political interests. [149] [128] [150] [151] The meta-policy approach[ edit ] Meta-policy research focuses on the ways policy development is influenced or shaped by contextual factors, including political institutions and systems, socio-cultural patterns, economic systems, knowledge frameworks, discourses, and the changes therein. The latter may involve deliberate changes to the formal and non-formal institutions through which policy analysis, development, decision-making, and implementation occur, such as the introduction of rules for cost-benefit analysis, risk analysis, consultation and accountability requirements, and organisational change. [4] How environmental problems are interpreted and defined directly affects the development of environmental policies, at all stages of the policy cycle, from problem recognition, and the formulation of policy options, to decision-making, implementation and policy evaluation. However, much (meta-policy) research has been undertaken on what influences or shapes these views and interpretations. For instance, there is a large body of research that looks at whether societies have moved or are moving towards “post-materialist” values, [152] [153] or to a New Environmental Paradigm. [154] [155] More broadly, the link between dominant worldviews and the way the environment is treated has been a focus of much debate. [156] [157] [158] [iv] The rise and growing support for the environmental movement is often seen as a driver towards “greener” societies. [159] [160] If such socio-cultural trends hold, this is expected to lead governments to adopt stronger environmental policies. Other meta-policy research focuses on the different “environmental discourses” and how they compete for dominance in societies and worldwide. [161] [162] [163] [164] The power to influence or shape people's view of the world has been referred to as “cognitive power”. [165] The role of intellectuals, opinion leaders, and the media in shaping and advancing the dominant views and ideologies in societies has been an important focus of Marxist and critical theory that has also influenced the analysis of environmental policy formation. [166] Ownership and control of the media play an important role in the formation of public opinion on environmental issues. [167] [168] [169] Other meta-policy research relevant to the development of environmental policy focuses on institutional and systemic factors. For instance, the role of environmental institutions and their role, capacity and power within the broader systems of government is found to be an important factor in advancing or constraining environmental policy. [170] [171] [172] More broadly, the question of whether capitalism is compatible or not with long-term environmental protection has been a subject of debate. [165] [173] [174] [175] As, after the collapse of the Soviet Union and the introduction of capitalism in China, capitalism became a globally dominant system, this question has become even more important to the future development of environmental policy at the national and international levels. As many analysts of global environmental politics have pointed out, the institutions for developing effective environmental policy at that level are weak and rather ineffective, [176] [177] [165] [178] as demonstrated by accounts of continuing environmental deterioration. [179] [180] [181] [182] [183] Environmental policy evaluation[ edit ] Differences in approaches to environmental policy development and design, including the selection of policy instruments, linked to different historical, political-economic and socio-cultural contexts, and the inevitable role and influence of different cognitive and ideological frameworks in the analysis and design of policies, all make that evaluating environmental policies is also a complex and controversial matter. As many policy analysts have pointed out, judging the merits of policies goes beyond an assessment of the efficiency and cost-effectiveness of the policy instruments used. In the realm of public policy, policy evaluation is a topic that is seen as much more encompassing and complex. [184] [185] Apart from efficiency and cost-effectiveness, many other important aspects and criteria have been identified and discussed, including their knowledge (science) basis, their goals and objectives, ethical issues, distributional effects, and process and legitimacy. Although efforts have been made to put evaluation on its own (trans-) disciplinary footing [186] [187] [188] [189] as a systematic and independent stage in the policy process, either before the adoption of policies (ex-ante evaluation) or after their implementation (ex-post evaluation) [190] this remains fraught with problems. [191] [189] In practice, it remains a largely neglected aspect or stage of policymaking, in large part, because of the political nature and sensitivity of the evaluation of a government’s policies. [192] [183] The difficulties of policy evaluation also apply to environmental policies. Also there, policy evaluation is often approached in simple terms based on the extent to which the stated goals of a policy have been achieved or not (“success or failure”). [184] However, as many environmental policy analysts have pointed out, many other aspects of environmental policy are important. These include the goals and objectives of the policies (which may be deemed too vague, inadequate, poorly or wrongly targeted),[ [184] [193] their distributional effects (whether they contribute to or reduce environmental and social injustice), [194] [195] [196] the kind of instruments used (for instance, their ethical and political dimensions), [94] [95] the processes by which policies have been developed (public participation and deliberation) [197] [183] [198] , and the extent to which they are institutionally supported. [184] [199] Moreover, as many environmental thinkers and policy analysts have pointed out, addressing environmental problems effectively requires an integrated approach. [200] [201] [202] As the environment is an integrated whole or system, environmental policies need to take account of the interactions within that system and the effects of human actions and interventions not just on a problem in isolation, but also their (potential) effects of other problems. More often than not, fragmented policies and “solutions”, for instance, to combat pollution, lead to the displacement of environmental problems or the generation of new ones. [184] [203] [204] The interconnectedness of the environmental challenge, it has been said, requires an approach that is “ecological rational” and environmentally effective. [45] [205] This holistic way of thinking has been picked up and developed under a variety of labels, including Holistic Resource Management, [206] Integrated Environmental Management, [207] Ecosystems Management, [208] [209] [210] and the notion of Environmental Integration. [211] [212] [213] [214] Environmental integration, in broad terms, is “the integration of environmental considerations into all areas of human thinking, behaviour and practices that (potentially) affect the environment.” [215] This involves, among others, the development and adoption of an overarching view of the environment, an overarching policy to guide the “greening” of policies, and an institutional framework that gives “teeth” to environmental integration. [216] In academic and government circles (notably the EU), much of the focus has been on environmental policy integration (EPI), the process of integrating environmental objectives into non-environmental policy areas, such as energy, agriculture and transport, rather than leaving them to be pursued solely through purely environmental policy practices. This is often particularly challenging because of the need to reconcile global objectives and international rules with domestic needs and laws. [217] EPI is widely recognised as one of the key elements of sustainable development, and it was adopted as a formal requirement by the EU. [218] More recently, the notion of 'climate policy integration', also denoted as 'mainstreaming', has been applied to indicate the integration of climate considerations (both mitigation and adaptation) into the normal (often economically focused) activity of government. [219] Although, in the late 1980s and early 1990s, many governments began to adopt a more comprehensive approach to environmental issues, notably in the form of National Sustainable Development Strategies and “Green Planning”, [220] [221] [222] these efforts were largely abandoned during the 1990s and the rise to prominence of neoliberal thinking, policies and reforms. This development led to the return of the fragmented and reactive approach to environmental problems with an emphasis on climate change and the use of “market-based” instruments. [223] [224] [225] The field of Comparative Environment Policy and Politics aims to explain the differences in performance related to, among others, differences in political systems, institutions, policy styles and cultures. [226] [227] [228] However, the environmental performance of governments remains commonly based on achievements in a range of environmental problems and policy outputs, as measured by separate indicators like CO2 emissions, different forms of air pollution, water quality indicators, and biological diversity (individual species). [229] [230] [231] These assessments are often used as a basis for ranking the environmental performance of countries, with some characterised as leaders and others as laggards. [232] [233] [234] However, such rankings have been treated with scepticism, not only on methodological grounds but especially because they mean little in terms of the extent to which governments take environmental integration seriously. [235] [233] While it has been noted that, at different stages, some countries have been leaders in some areas of environmental integration, these efforts have not been sustained over time. [165] Ultimately, the environmental effectiveness of policies is measured by the extent to which they reduce or resolve environmental problems (ecological destruction and degradation, resource degradation and depletion, and adverse effects on humans by environmental modification, including by urban development and pollution). Whether environmental policies have addressed environmental problems more or less effectively remains a topic of debate. On the one hand, some take a very positive and optimistic view, arguing that, on many fronts, the environmental situation, especially as it affects humans, has improved. [236] [237] [238] On the other hand, many scientists and scientific reports paint a bleak picture of where the world is going, based on deteriorating environmental indicators linked to climate change, [239] declining biodiversity, [179] pollution trends (including of new forms of pollution such as the spread of plastic nanoparticles), [240] [241] and ongoing resource degradation and decline (such as water and agricultural land). [242] [243] [180] [181] [182] Improving environmental policy[ edit ] Reflecting the diversity of approaches to environmental policy development, influenced by contextual factors, policy perspectives, and political-ideological views, among others, there are also different views on how environmental policy could or should be improved. The three most common standpoints have been referred to as incrementalism (“tinkering”), democratisation, and systemic change. Incrementalism has been deemed to be the most common (standard) way governments change their policies with the stated aim of improving them. Propagated in particular by Charles Lindblom based on his view of American political reality, he argued that changing policies in small steps is not only the most common way policies are developed, but also the best way, as it avoids making big errors that could result from a “rational-comprehensive” approach. [244] [245] [246] [247] Also, over time, a series of small changes may add up and bring about significant and big change. Although incrementalism has been critiqued for its underlying assumptions and conservative implications (“tinkering”), [248] [249] [250] [251] and also for its failure to come to grips with environmental problems, [252] [249] [253] it is a very recognisable approach to policy “improvement” in many countries. As incrementalism does not question the political-economic status quo, its suggestions for policy improvement are foremost of a managerial or technological kind. Tinkering with policy and management tools, and technological innovation, are seen as the main and most desirable (“win-win”) ways to address environmental (and other) problems. This “technocentric” approach, which is seen as politically neutral, has been a preferred and dominant approach to “solving” environmental problems from the beginning of the environmental era, advocated by governments, businesses, and many environmentalists. [254] [255] [256] The managerial approach also involves training "environmental practitioners" and policy analysts. Given the growing need for trained environmental practitioners, graduate schools throughout the world offer specialized professional degrees in environmental policy studies. While there is not a standard curriculum , students typically take classes in policy analysis , environmental science , environmental law and politics , ecology , energy , and natural resource management . Graduates of these programs are employed by governments , international organizations , private sector , think tanks , advocacy organizations, and universities . Much of the research and innovation sponsored by governments, businesses and international organisations under the heading of “transition management” is aimed at the gradual (incremental) development of new “transformative” technologies, for instance, in areas like energy, transport and agriculture. [257] [258] An example is the European environmental research and innovation policy , which aims at defining and implementing a transformative agenda to greening the economy and society as a whole so as to achieve “truly” sustainable development. The EU strategies, actions and programmes promote more and better research and innovation for building a resource-efficient, climate-resilient society and thriving economy which are meant to be in sync with the natural environment. Research and innovation in Europe are financially supported by the programme Horizon 2020 , which is also open to participation worldwide. Yet, the “transition management” approach to sustainability has been critiqued for it’s a-political, technocratic and elitist nature. [259] [260] Also, Bucchi argues that the traditional technocentric approach no longer suffices as science has increasingly been commercialised and politicised and lost much of its image of neutrality that it enjoyed with the public at large. [261] In line with the policy process perspective, many environmental advocates and analysts support improving the opportunities for public participation and input in the policy process, as well as increasing transparency. The policy design literature aims to pull together insights gained from studies of the various stages of the policy cycle to design more effective policies, to better consider the tools, rules and assumptions on which they are based, the groups at which they are targeted, contextual factors, as well as the nature (complexity) of the problem. [262] [263] Enhancing public input and participation is argued to have the potential to improve all stages of the policy cycle, including problem definition, decision-making, policy implementation, and evaluation. UNFCCC research shows that climate-related projects and policies that involve women are more effective. Policies, projects and investments without meaningful participation by women are less effective and often increase existing gender inequalities. Women's found climate solutions that cross political or ethnic boundaries have been particularly important in regions where entire ecosystems are under threat, e.g. small island states, the Arctic and the Amazon and in areas where people's livelihoods depend on natural resources e.g. fishing, farming and forestry. [264] [265] [266] However, the degree and kind of opportunities provided for public input and deliberation are seen as a key factor, both for improving the effectiveness of policies and for enhancing their support basis and legitimacy. [267] [268] [269] Enhancing democracy, for instance, by adopting forms of “discursive designs” and other forms of “reflexive” deliberative democracy, [270] [271] [272] aims to create a level playing field on which citizens’ representatives have a more equal chance to partake in shaping policy. Relatively recently, “citizens’ assemblies” have been used in a range of countries to address controversial topics, including climate change policy. [273] [274] However, as these are temporary and advisory bodies, governments are not bound by their recommendations. [275] Over time, many governments have introduced laws to provide public access to government-held information, for instance, by the adoption of Freedom of Information legislation granting public access to information held by governments. [276] [277] Although a growing number of governments have adopted such legislation, a report by Privacy International notes that in many countries much work remains to be done on the implementation front and the creation of a culture, “leaving access largely unfulfilled.” [278] A third approach to improving environmental policy is based on the view that meaningful progress on resolving environmental problems requires fundamental or systemic change, in particular of the prevailing socio-cultural, political and economic systems. Three categories of factors are commonly identified: cognitive factors (the way(s) environmental problems have been interpreted (cognitive factors), linked to dominant belief and value systems); political factors (the nature of the prevailing political systems); and the nature of the prevailing economic systems. These three types of factors are not mutually exclusive, and analysts often combine them to provide more comprehensive explanations. [224] That the way environmental problems predominantly are interpreted is a fundamental obstacle to addressing the environmental challenge effectively, has been pointed out already from the earliest stages of the rise of environmental awareness and thinking. Many early environmental thinkers argued that environmental problems are interrelated, finding their roots in the interconnectedness of the environment itself and the failure of human societies to recognise that reality and to heed this in their behaviour and practices. [7] [279] [142] [280] [281] These thinkers point out the need to take a “holistic”, ecosystems or integrated approach to the management of the environment and the use of resources. [282] [283] [284] [285] Often, it is argued that such an approach was common to indigenous societies, but that this got pushed aside and lost with the rise of “modernity” and rational-analytic (scientific) thinking. In modern societies, nature has come to be seen, analysed and manipulated as a machine in the service of human ends. [286] [287] [288] [289] But as the way the environmental challenge is interpreted is closely linked to the dominant socio-cultural (value) system, the latter is also said to need fundamental change. There is a large body of literature on the role and importance of the dominant values in societies and the (possible) changes therein, among others linked to economic development, urbanisation and globalisation. On the one hand, analysts have identified the rise of individualism, materialism, consumerism, and the decline of community values in modern societies and cultures. [290] [291] [292] [293] [294] On the other hand, some analysts, notably based on Ronald Inglehart’s work, argue that, with rising standards of living, comes a shift in societies, facilitated by generational change, from material to “post-material” values, including self-actualisation, belonging, and aesthetics. [295] [296] However, it is debatable to what extent this shift represents a move towards environmental values becoming dominant and whether the level of support for the environment depends on a high standard of living. [297] [298] [299] Others, notably inspired by Riley Dunlap’s research, more directly explore whether the presently dominant paradigm is being replaced by what is referred to as the “New Environmental Paradigm”. [300] [301] As yet, however, the findings of this research are inconclusive, although there is evidence that environmental concern and support have grown globally. [302] [303] [304] Whether and how the dominant value systems and views on the environment can be purposefully changed by concerted social action aimed at assigning greater priority remains a matter of debate and uncertainty. On the one hand, the environmental movement has been touted as a “vanguard” in shifting the dominant paradigm. [305] [306] [307] On the other hand, the effectiveness of the environmental movement in bringing about fundamental value change can and has been drawn into doubt. One reason is that the environmental movement itself is very diverse in views on the kind of value change(s) required, ranging from technocentric to deep ecological stances. [254] [308] [309] To what extent green parties have been effective in changing dominant value patterns or are themselves subject to being co-opted by dominant values and interests is also subject to debate. [310] [311] [312] [313] To a large extent, as many analysts have pointed out, the ability to shape the dominant values and public views on the environment, depends on the relative (cognitive) power held and exercised by groups, notably through control over the media and other institutions such as education, universities, think tanks, and the social media. [314] [315] [316] [125] [317] [318] [319] The importance of the nature of political systems for the development of environmental (and other) policies has been the subject of much research, including in the field of Comparative Environmental Policy. [25] [320] [321] [322] [323] Analysts have pointed out a broad range of factors that stand in the way of environmental issues being adequately recognised and/or assigned political priority, including the role, privileged access, power and influence, and even dominance of (non-environmental) interest groups, bureaucratic thinking and interests, the lack of openness and transparency, (very) limited opportunities for public input and participation, and the short political horizon linked to electoral cycles. [324] [325] [326] [327] [328] [329] Many of these factors are not confined to liberal-democratic political systems but also play a role, perhaps even more so, in authoritarian political systems. [330] [331] [332] [333] [165] These political obstacles have generally led to a relative weakness in the power of government institutions (organisations and rules) advocating for environmental interests compared to non-environmental institutions and the circumscription of the power, role and influence of societal environmental groups, including green parties, if not their co-optation by the dominant powers and vested interests. [311] [310] [334] [335] This also affects the “environmental capacity” of political systems, severely limiting efforts to develop more comprehensive and integrated approaches to the environmental challenge. [172] [336] [337] [338] Other analysts emphasise the importance of economic systems, notably capitalism, as a fundamental obstacle to developing and adopting effective environmental policies. Some take the view that capitalism is fundamentally incompatible with long-term environmental protection, notably because of its inherent growth imperative. [339] [150] [340] [341] Others recognise this imperative as a problem but argue that it is possible to reform capitalism in a way that it does not require growth, or that enables “green growth” based on the recognition of environmental limits. [342] [343] [344] [345] Many have pointed out that socialist economic systems have had even worse environmental records than capitalist systems, implying that socialism is no better alternative for the environment apart from other reasons. [330] [346] [331] [347] [348] However, this view is contested by those who argue that socialism as an economic system does not necessarily require an authoritarian system and that there is scope for creating democratic socialist systems that assign greater priority to collective interests, including environmental protection. [349] [350] [351] [352] These cognitive, social, political and economic factors are often referred to as systemic, meaning that overcoming these obstacles requires systemic, fundamental or transformative change, notably of the systems that are the sources and drivers of environmental pressures and problems, including the political and economic systems, and sectors like agriculture, energy, and transport. Increasingly, the tweaking of environmental and other policies is seen as inadequate, and there is growing recognition of the need for “transformative change”. [353] [354] [355] However, the interrelatedness of these systems raises questions about whether and/or how such transformative change can be achieved, [165] which has led a growing number of environmental analysts, including scientists, to serious doubts and pessimism, [356] [357] [358] although others argue that it remains possible for societies to do so. [359] [360] [361] [362]
"can affect or is affected by the achievement of the organization's objectives" Business Management "without whose support the organization would cease to exist" Business Management Clarkson [16] "... persons or groups that have, or claim, ownership, rights, or interests in a corporation and its activities, past, present, or future." Business Management Grimble and Wellard [17] "...any group of people, organized or unorganized, who share a common interest or stake in a particular issue or system..." Natural resource management Gass et al. [18] "... any individual, group and institution who would potentially be affected, whether positively or negatively, by a specified event, process or change." Natural resource management Buanes et al [19] "... any group or individual who may directly or indirectly affect—or be affected—...planning to be at least potential stakeholders." Natural resource management Brugha and Varvasovszky [20] "... stakeholders (individuals, groups and organizations) who have an interest (stake) and the potential to influence the actions and aims of an organization, project or policy direction." Health policy "... persons, groups or institutions with interests in a project or programme." Development Therefore, it is dependent upon the circumstances of the stakeholders involved with natural resource as to which definition and subsequent theory is utilised. Billgrena and Holme [13] identified the aims of stakeholder analysis in natural resource management: Identify and categorise the stakeholders that may have influence Develop an understanding of why changes occur Establish who can make changes happen How to best manage natural resources This gives transparency and clarity to policy making allowing stakeholders to recognise conflicts of interest and facilitate resolutions. [13] [22] There are numerous stakeholder theories such as Mitchell et al. [23] however Grimble [22] created a framework of stages for a Stakeholder Analysis in natural resource management. Grimble [22] designed this framework to ensure that the analysis is specific to the essential aspects of natural resource management. Stages in Stakeholder analysis: [22] Clarify objectives of the analysis Place issues in a systems context Identify decision-makers and stakeholders Investigate stakeholder interests and agendas Investigate patterns of inter-action and dependence (e.g. conflicts and compatibilities, trade-offs and synergies) Application: Grimble and Wellard [17] established that Stakeholder analysis in natural resource management is most relevant where issued can be characterised as; Cross-cutting systems and stakeholder interests Multiple uses and users of the resource. Poverty and under-representation [17] [22] Case studies: In the case of the Bwindi Impenetrable National Park , a comprehensive stakeholder analysis would have been relevant and the Batwa people would have potentially been acknowledged as stakeholders preventing the loss of people's livelihoods and loss of life. [17] [22] Short video on Natural Resource Management in Wales by the Welsh Government In Wales , Natural Resources Wales , a Welsh Government sponsored body "pursues sustainable management of natural resources" and "applies the principles of sustainable management of natural resources" as stated in the Environment (Wales) Act 2016. [24] NRW is responsible for more than 40 different types of regulatory regime across a wide range of activities. Nepal, Indonesia and Koreas' community forestry are successful examples of how stakeholder analysis can be incorporated into the management of natural resources. This allowed the stakeholders to identify their needs and level of involvement with the forests. Criticisms: Natural resource management stakeholder analysis tends to include too many stakeholders which can create problems in of its self as suggested by Clarkson. "Stakeholder theory should not be used to weave a basket big enough to hold the world's misery." [25] Starik [26] proposed that nature needs to be represented as stakeholder. However this has been rejected by many scholars as it would be difficult to find appropriate representation and this representation could also be disputed by other stakeholders causing further issues. [13] Stakeholder analysis can be used exploited and abused in order to marginalise other stakeholders. [12] Identifying the relevant stakeholders for participatory processes is complex as certain stakeholder groups may have been excluded from previous decisions. [27] On-going conflicts and lack of trust between stakeholders can prevent compromise and resolutions. [27] Alternatives/ Complementary forms of analysis: Common pool resource Management of the resources[ edit ] Natural resource management issues are inherently complex and contentious. First, they involve the ecological cycles, hydrological cycles, climate, animals, plants and geography, etc. All these are dynamic and inter-related. A change in one of them may have far reaching and/or long term impacts which may even be irreversible. Second, in addition to the complexity of the natural systems, managers also have to consider various stakeholders and their interests, policies, politics, geographical boundaries and economic implications. It is impossible to fully satisfy all aspects at the same time. Therefore, between the scientific complexity and the diverse stakeholders, natural resource management is typically contentious. After the United Nations Conference for the Environment and Development (UNCED) held in Rio de Janeiro in 1992, [28] most nations subscribed to new principles for the integrated management of land, water, and forests. Although program names vary from nation to nation, all express similar aims. The various approaches applied to natural resource management include: Top-down (command and control) Ecosystem management Community-based natural resource management[ edit ] The community-based natural resource management (CBNRM) approach combines conservation objectives with the generation of economic benefits for rural communities. The three key assumptions being that: locals are better placed to conserve natural resources, people will conserve a resource only if benefits exceed the costs of conservation, and people will conserve a resource that is linked directly to their quality of life. [5] When a local people's quality of life is enhanced, their efforts and commitment to ensure the future well-being of the resource are also enhanced. [29] Regional and community based natural resource management is also based on the principle of subsidiarity . The United Nations advocates CBNRM in the Convention on Biodiversity and the Convention to Combat Desertification. Unless clearly defined, decentralised NRM can result in an ambiguous socio-legal environment with local communities racing to exploit natural resources while they can, such as the forest communities in central Kalimantan (Indonesia). [30] A problem of CBNRM is the difficulty of reconciling and harmonising the objectives of socioeconomic development, biodiversity protection and sustainable resource utilisation. [31] The concept and conflicting interests of CBNRM, [32] [33] show how the motives behind the participation are differentiated as either people-centred (active or participatory results that are truly empowering) [34] or planner-centred (nominal and results in passive recipients). Understanding power relations is crucial to the success of community based NRM. Locals may be reluctant to challenge government recommendations for fear of losing promised benefits. CBNRM is based particularly on advocacy by nongovernmental organizations working with local groups and communities, on the one hand, and national and transnational organizations, on the other, to build and extend new versions of environmental and social advocacy that link social justice and environmental management agendas [35] with both direct and indirect benefits observed including a share of revenues, employment, diversification of livelihoods and increased pride and identity. Ecological and societal successes and failures of CBNRM projects have been documented. [36] [37] CBNRM has raised new challenges, as concepts of community, territory, conservation, and indigenous are worked into politically varied plans and programs in disparate sites. Warner and Jones [38] address strategies for effectively managing conflict in CBNRM. The capacity of Indigenous communities, led by traditional custodians , to conserve natural resources has been acknowledged by the Australian Government with the Caring for Country [39] Program. Caring for our Country is an Australian Government initiative jointly administered by the Australian Government Department of Agriculture, Fisheries and Forestry and the Department of the Environment, Water, Heritage and the Arts. These Departments share responsibility for delivery of the Australian Government's environment and sustainable agriculture programs, which have traditionally been broadly referred to under the banner of 'natural resource management'. These programs have been delivered regionally, through 56 State government bodies, successfully allowing regional communities to decide the natural resource priorities for their regions. [40] More broadly, a research study based in Tanzania and the Pacific researched what motivates communities to adopt CBNRM's and found that aspects of the specific CBNRM program, of the community that has adopted the program, and of the broader social-ecological context together shape the why CBNRM's are adopted. [41] However, overall, program adoption seemed to mirror the relative advantage of CBNRM programs to local villagers and villager access to external technical assistance. [41] There have been socioeconomic critiques of CBNRM in Africa, [42] but ecological effectiveness of CBNRM measured by wildlife population densities has been shown repeatedly in Tanzania. [43] [44] Governance is seen as a key consideration for delivering community-based or regional natural resource management. In the State of NSW, the 13 catchment management authorities (CMAs) are overseen by the Natural Resources Commission (NRC), responsible for undertaking audits of the effectiveness of regional natural resource management programs. [45] Criticisms of Community-Based Natural Resource Management[ edit ] Though presenting a transformative approach to resource management that recognizes and involves local communities rather than displacing them, Community-Based Natural Resource Management strategies have faced scrutiny from both scholars and advocates for indigenous communities. Tania Murray, in her examination of CBNRM in Upland Southeast Asia, [46] discovered certain limitations associated with the strategy, primarily stemming from her observation of an idealistic perspective of the communities held by external entities implementing CBNRM programs. Murray's findings revealed that, in the Uplands, CBNRM as a legal strategy imposed constraints on the communities. One significant limitation was the necessity for communities to fulfill discriminatory and enforceable prerequisites in order to obtain legal entitlements to resources. Murray contends that such legal practices, grounded in specific distinguishing identities or practices, pose a risk of perpetuating and strengthening discriminatory norms in the region. [46] Furthermore, adopting a Marxist perspective centered on class struggle, some have criticized CBNRM as an empowerment tool, asserting that its focus on state-community alliances may limit its effectiveness, particularly for communities facing challenges from "vicious states," thereby restricting the empowerment potential of the programs. [46] Gender-based natural resource management[ edit ] Social capital and gender are factors that impact community-based natural resource management (CBNRM), including conservation strategies and collaborations between community members and staff. Through three months of participant observation in a fishing camp in San Evaristo, Mexico, Ben Siegelman learned that the fishermen build trust through jokes and fabrications. He emphasizes social capital as a process because it is built and accumulated through practice of intricate social norms. Siegelman notes that playful joking is connected to masculinity and often excludes women. He stresses that both gender and social capital are performed. Furthermore, in San Evaristo, the gendered network of fishermen is simultaneously a social network. Nearly all fishermen in San Evaristo are men and most families have lived there for generations. Men form intimate relationships by spending 14 hour work days together, while women spend time with the family managing domestic caretaking. Siegelman observes three categories of lies amongst the fishermen: exaggerations, deceptions, and jokes. For example a fisherman may exaggerate his success fishing at a particular spot to mislead friends, place his hand on the scale to turn a larger profit, or make a sexual joke to earn respect. As Siegelman puts it, "lies build trust." Siegelman saw that this division of labor was reproduced, at least in part, to do with the fact that the culture of lying and trust was a masculine activity unique to the fisherman. Similar to the ways in which the culture of lying excluded women from the social sphere of fishing, conservationists were also excluded from this social arrangement and, thus, were not able to obtain the trust needed to do their work of regulating fishing practices. As outsiders, conservationists, even male conservationists, were not able to fit the ideal of masculinity that was considered "trustable" by the fishermen and could convince them to implement or participate in conservation practices. In one instance, the researcher replied jokingly "in the sea" when a fisherman asked where the others were fishing that day. This vague response earned him trust. Women are excluded from this form of social capital because many of the jokes center around "masculine exploits". Siegelman finishes by asking: how can female conservationists act when they are excluded through social capital? What role should men play in this situation? [47] Adaptive Management[ edit ] The primary methodological approach adopted by catchment management authorities (CMAs) for regional natural resource management in Australia is adaptive management . [6] This approach includes recognition that adaption occurs through a process of 'plan-do-review-act'. It also recognises seven key components that should be considered for quality natural resource management practice: Determination of scale
Toggle the table of contents Wildlife conservation From Wikipedia, the free encyclopedia Practice of protecting wild plant and animal species and their habitats Ankeny Wildlife Refuge in Oregon. Wildlife conservation refers to the practice of protecting wild species and their habitats in order to maintain healthy wildlife species or populations and to restore, protect or enhance natural ecosystems . Major threats to wildlife include habitat destruction , degradation, fragmentation, overexploitation, poaching, pollution, climate change, and the illegal wildlife trade. The IUCN estimates that 42,100 species of the ones assessed are at risk for extinction. [1] Expanding to all existing species, a 2019 UN report on biodiversity put this estimate even higher at a million species. It is also being acknowledged that an increasing number of ecosystems on Earth containing endangered species are disappearing. To address these issues, there have been both national and international governmental efforts to preserve Earth's wildlife. Prominent conservation agreements include the 1973 Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) and the 1992 Convention on Biological Diversity (CBD). [2] [3] There are also numerous nongovernmental organizations (NGO's) dedicated to conservation such as the Nature Conservancy , World Wildlife Fund , the Wild Animal Health Fund and Conservation International . Threats to wildlife[ edit ] Habitat destruction[ edit ] Habitat destruction decreases the number of places  where wildlife can live in. Habitat fragmentation breaks up a continuous tract of habitat, often dividing large wildlife populations into several smaller ones. [4] Human-caused habitat loss and fragmentation are primary drivers of species declines and extinctions. Key examples of human-induced habitat loss include deforestation, agricultural expansion , and urbanization . Habitat destruction and fragmentation can increase the vulnerability of wildlife populations by reducing the space and resources available to them and by increasing the likelihood of conflict with humans. Moreover, destruction and fragmentation create smaller habitats. Smaller habitats support smaller populations, and smaller populations are more likely to go extinct. [5] The COVID-19 pandemic has caused a significant shift in human behavior, resulting in mandatory and voluntary limitations on movement. As a result, people have started utilizing green spaces more frequently, which were previously habitats for wildlife. Unfortunately, this increased human activity has caused destruction to the natural habitat of various species. [6] Deforestation[ edit ] Deforestation is the clearing and cutting down forests on purpose. Deforestation is a cause of human-induced habitat action destruction, by cutting down habitats of different species in the process of removing trees. Deforestation is often done for several reasons, often for either agricultural purposes or for logging , which is the obtainment of timber and wood for use in construction or fuel. [7] Deforestation causes many threats to wildlife as it not only causes habitat destruction for the many animals that survive in forests, as more than 80% of the world's species live in forests but also leads to further climate change. [8] Deforestation is a main concern in the tropical forests of the world. Tropical forests, like the Amazon , are home to the most biodiversity out of any other biome, making deforestation there an even more prevalent issue, especially in populated areas, as in these areas deforestation leads to habitat destruction and the endangerment of many species in one area. [9] Some policies have been enacted to attempt to stop deforestation in different parts of the world, like the Wilderness Act of 1964 which designated specific areas wilderness to be protected. [10] Overexploitation[ edit ] Overexploitation is the harvesting of animals and plants at a rate that's faster than the species' ability to recover. While often associated with Overfishing , overexploitation can apply to many groups including mammals, birds, amphibians, reptiles, and plants. [11] The danger of overexploitation is that if too many of a species offspring are taken, then the species may not recover. [12] For example, overfishing of top marine predatory fish like tuna and salmon over the past century has led to a decline in fish sizes as well as fish numbers. [4] Confiscated animal pelts from the illegal wildlife trade . Poaching[ edit ] Poaching for illegal wildlife trading is a major threat to certain species, particularly endangered ones whose status makes them economically valuable. [13] Such species include many large mammals like African elephants, tigers, and rhinoceros (traded for their tusks , skins, and horns respectively). [13] [14] Less well-known targets of poaching include the harvest of protected plants and animals for souvenirs, food, skins, pets, and more. [15] Poaching causes already small populations to decline even further as hunters tend to target threatened and endangered species because of their rarity and large profits. [15] Main article: Ocean acidification Pterapod shell dissolved in seawater adjusted to an ocean chemistry projected for the year 2100 As carbon dioxide levels increase concentration in the atmosphere, they increase in the ocean as well. Typically, the ocean will absorb carbon from the atmosphere, where it can be sequestered in the deep ocean and sea floor; this is a process called the biological pump . Increased carbon dioxide emissions and increased stratification (which slows the biological pump) decrease the ocean pH, making it more acidic. Calcifying organisms such as coral are especially susceptible to decreased pH, resulting in mass bleaching events, inevitably destroying a habitat for many of coral's diverse inhabitants.  Research (conducted through methods such as coral fossils and ancient ice core carbon analysis) suggests ocean acidification has occurred in the geological past (more likely at a slower pace), and correlate with past extinction events. [16] Main article: Culling Culling is the deliberate and selective killing of wildlife by governments for various purposes. An example of this is shark culling , in which "shark control" programs in Queensland and New South Wales (in Australia ) have killed thousands of sharks , as well as turtles , dolphins , whales , and other marine life. [17] [18] [19] The Queensland "shark control" program alone has killed about 50,000 sharks — it has also killed more than 84,000 marine animals. [20] [17] There are also examples of population culling in the United States, such as bison in Montana and swans, geese, and deer in New York and other places. [21] [22] Aerial view of the BP Deepwater Horizon oil spill in 2010. Pollution[ edit ] A wide range of pollutants negatively impact wildlife health. For some pollutants, simple exposure is enough to do damage (e.g. pesticides). For others, its through inhaling (e.g. air pollutants) or ingesting it (e.g. toxic metals). Pollutants affect different species in different ways so a pollutant that is bad for one might not affect another. Air pollutants: Most air pollutants come from burning fossil fuels and industrial emissions. These have direct and indirect effects on the health of wildlife and their ecosystems. For example, high levels of sulfur oxides (SOx) can damage plants and stunt their growth. [23] Sulfur oxides also contribute to acid rain, harming both terrestrial and aquatic ecosystems. Other air pollutants like smog , ground-level ozone , and particulate matter decrease air quality. Heavy metals: Heavy metals like arsenic , lead , and mercury naturally occur at low levels in the environment, but when ingested in high doses, can cause organ damage and cancer. [24] How toxic they are depends on the exact metal, how much was ingested, and the animal that ingested it. Human activities such as mining, smelting, burning fossil fuels, and various industrial processes have contributed to the rise in heavy metal levels in the environment. Toxic chemicals: There are many sources of toxic chemical pollution including industrial wastewater, oil spills, and pesticides. There's a wide range of toxic chemicals so there's also a wide range of negative health effects. For example, synthetic pesticides and certain industrial chemicals are persistent organic pollutants . These pollutants are long-lived and can cause cancer, reproductive disorders, immune system problems, and nervous system problems. [25] Main article: Climate change Humans are responsible for present-day climate change currently changing Earth's environmental conditions. It is related to some of the aforementioned threats to wildlife like habitat destruction and pollution. Rising temperatures, melting ice sheets, changes in precipitation patterns, severe droughts , more frequent heat waves , storm intensification, ocean acidification , and rising sea levels are some of the effects of climate change. [26] Phenomena like droughts, wildfires, heatwaves, intense storms, ocean acidification , and rising sea levels , directly lead to habitat destruction. For example, longer dry seasons, warmer springs, and dry soil has been observed to increase the length of wildfire season in forests, shrublands and grasslands. Increased severity and longevity of wildfires can completely wipe out entire ecosystems, causing them to take decades to fully recover. Wildfires are a prime example of the direct negative effect climate change has on wildlife and ecosystems. [27] Meanwhile, a warming climate, fluctuating precipitation, and changing weather patterns will impact species ranges. Overall, the effects of climate change increase stress on ecosystems, and species unable to cope with the rapidly changing conditions will go extinct. [28] While modern climate change is caused by humans, past climate change events occurred naturally and have led to extinctions. [29] Illegal Wildlife Trade[ edit ] The illegal wildlife trade is the illegal trading of plants and wildlife. This illegal trading is worth an estimate of 7-23 billion [30] and an annual trade of around 100 million plants and animals. [31] In 2021 it was found that this trade has caused a 60% decline in species abundance, and 80% for endangered species. [31] This trade can be devastating to both humans and animals. It has the capacity to spread zoonotic diseases to humans, as well as contribute to local extinction. The pathogens to humans may be spread through small animal vectors like ticks, or through ingestion of food and water. Extinction can be caused due to non-native species being introduced that become invasive. An example of how this may happen is through by-catch.These new species will outcompete the native species and take over, therefore causing the local or global extinction of a species. [32] Due to the fittest animals in the species being hunted or poached, the less fit organisms will mate, causing less fitness in the generations to come. In addition to species fitness being lowered and therefore endangering species, the illegal wildlife trade has ecological costs. Sex-ratio balances may be tipped or reproduction rates are slowed, which can be detrimental to vulnerable species. The recovery of these populations may take longer due to the reproduction rates being slower. [33] The wildlife trade also causes issues for natural resources that people use in their everyday lives. Ecotourism is how some people bring in money to their homes, and with depleting the wildlife, this may be a factor in taking away jobs. [33] Illegal wildlife trade has also become normalized through various social media outlets. There are TikTok accounts that have gone viral for their depiction of exotic pets, such as various monkey and bird species. These accounts show a cute and fun side of owning exotic pets, therefore indirectly encouraging illegal wildlife trade. On March 30, 2021, TikTik joined the Coalition to End Wildlife Trafficking Online. They, along with  other big social media companies work to protect species from illegal, harmful trade online. [34] [35] Research has shown that machine learning can filter through social media posts to identify indications of illegal wildlife trade. This filtration system is able to search for keywords, pictures, and phrases that indicate illegal wildlife trade, and report it. [36] Species conservation[ edit ] Leatherback sea turtle (Dermochelys turtile) It is estimated that, because of human activities, current species extinction rates are about 1000 times greater than the background extinction rate (the 'normal' extinction rate that occurs without additional influence). [37] According to the IUCN , out of all species assessed, over 42,100 are at risk of extinction and should be under conservation . [1] Of these, 25% are mammals, 14% are birds, and 40% are amphibians. [1] However, because not all species have been assessed, these numbers could be even higher. A 2019 UN report assessing global biodiversity extrapolated IUCN data to all species and estimated that 1 million species worldwide could face extinction. [38] [39] Conservation of a select species are often prioritized on several factors which include significant economic and ecological value, as well as desirability or attractiveness. [40] Yet, because resources are limited, sometimes it is not possible to give all species that need conservation due consideration. Leatherback sea turtle[ edit ] Main article: Leatherback sea turtle The leatherback sea turtle (Dermochelys coriacea) is the largest turtle in the world, is the only turtle without a hard shell, and is endangered. [41] It is found throughout the central Pacific and Atlantic Oceans but several of its populations are in decline across the globe (though not all). The leatherback sea turtle faces numerous threats including being caught as bycatch , harvest of its eggs, loss of nesting habitats, and marine pollution . [41] In the US where the leatherback is listed under the Endangered Species Act , measures to protect it include reducing bycatch captures through fishing gear modifications, monitoring and protecting its habitat (both nesting beaches and in the ocean), and reducing damage from marine pollution. [42] There is currently an international effort to protect the leatherback sea turtle. [43] Habitat conservation[ edit ] Red-cockaded woodpecker (Picoides borealis) Habitat conservation is the practice of protecting a habitat [44] in order to protect the species within it. [4] This is sometimes preferable to focusing on a single species especially if the species in question has very specific habitat requirements or lives in a habitat with many other endangered species. The latter is often true of species living in biodiversity hotspots , which are areas of the world with an exceptionally high concentration of endemic species (species found nowhere else in the world). [45] Many of these hotspots are in the tropics, mainly tropical forests like the Amazon. Habitat conservation is usually carried out by setting aside protected areas like national parks or nature reserves. Even when an area isn't made into a park or reserve, it can still be monitored and maintained. Main article: Red-cockaded woodpecker Red-cockaded Woodpecker (picoides borealis) The red-cockaded woodpecker (Picoides borealis) is an endangered bird in the southeastern US. [46] It only lives in longleaf pine savannas which are maintained by wildfires in mature pine forests. Today, it is a rare habitat (as fires have become rare and many pine forests have been cut down for agriculture) and is commonly found on land occupied by US military bases, where pine forests are kept for military training purposes and occasional bombings (also for training) set fires that maintain pine savannas. [4] Woodpeckers live in tree cavities they excavate in the trunk. In an effort to increase woodpecker numbers, artificial cavities (essentially birdhouses planted within tree trunks) were installed to give woodpeckers a place to live. An active effort is made by the US military and workers to maintain this rare habitat used by red-cockaded woodpeckers. Conservation genetics[ edit ] Florida panther (Puma concolor coryi) Conservation genetics studies genetic phenomena that impact the conservation of a species. Most conservation efforts focus on managing population size, but conserving genetic diversity is typically a high priority as well. High genetic diversity increases survival because it means greater capacity to adapt to future environmental changes. [5] Meanwhile, effects associated with low genetic diversity, such as inbreeding depression and loss of diversity from genetic drift , often decrease species survival by reducing the species' capacity to adapt or by increasing the frequency of genetic problems. Though not always the case, certain species are under threat because they have very low genetic diversity. As such, the best conservation action would be to restore their genetic diversity. Main article: Florida panther The Florida panther is a subspecies of cougar (specifically Puma concolor coryi) that resides in the state of Florida and is currently endangered. [47] Historically, the Florida panther's range covered the entire southeastern US. In the early 1990s, only a single population with 20-25 individuals were left. The population had very low genetic diversity, was highly inbred, and suffered from several genetic issues including kinked tails, cardiac defects, and low fertility. [5] In 1995, eight female Texas cougars were introduced to the Florida population. The goal was to increase genetic diversity by introducing genes from a different, unrelated puma population. By 2007, the Florida panther population had tripled and offspring between Florida and Texas individuals had higher fertility and less genetic problems. In 2015, the US Fish and Wildlife Service estimated there were 230 adult Florida panthers and in 2017, there were signs that the population's range was expanding within Florida. [47] Wildlife Monitoring[ edit ] Non-invasive monitoring of the dhole is crucial for knowledge about its conservation status. Monitoring of wildlife populations is an important part of conservation because it allows managers to gather information about the status of threatened species and to measure the effectiveness of management strategies. Monitoring can be local, regional, or range-wide, and can include one or many distinct populations. Metrics commonly gathered during monitoring include population numbers, geographic distribution, and genetic diversity, although many other metrics may be used. Monitoring methods can be categorized as either "direct" or "indirect". Direct methods rely on directly seeing or hearing the animals, whereas indirect methods rely on "signs" that indicate the animals are present. For terrestrial vertebrates, common direct monitoring methods include direct observation, mark-recapture , transects , and variable plot surveys. Indirect methods include track stations, fecal counts, food removal, open or closed burrow-opening counts, burrow counts, runaway counts, knockdown cards, snow tracks, or responses to audio calls. [48] For large, terrestrial vertebrates, a popular method is to use camera traps for population estimation along with mark-recapture techniques. This method has been used successfully with tigers, black bears and numerous other species. [49] [50] Trail cameras can be triggered remotely and automatically via sound, infrared sensors, etc. Computer vision -based animal individual re-identification methods have been developed to automate such sight-resight calculations. [51] [52] Mark-recapture methods are also used with genetic data from non-invasive hair or fecal samples. [53] Such information can be analyzed independently or in conjunction with photographic methods to get a more complete picture of population viability. When designing a wildlife monitoring strategy, it is important to minimize harm to the animal and implement the 3Rs principles (Replacement, Reduction, Refinement). [54] In wildlife research, this can be done through the use of non-invasive methods, sharing samples and data with other research groups, or optimizing traps to prevent injuries. [55] [56] Vaccine administration[ edit ] Distributing vaccinations to wildlife who are particularly vulnerable is useful in conservation to prevent or decelerate extreme population declination in a species from disease and also decrease the risk of a zoonotic spillover to humans. A pathogen that has never once been exposed to a specific species' evolutionary pathway can have detrimental impacts on the population. In most cases, these risks escalate in conjunction to other anthropogenic stressors, such as climate change or habitat loss, that ultimately lead a population to extinction without human intervention. [57] Methods of vaccination varies depending on both the extent and efficiency of limiting the transmission of disease, and can be applied orally , topically , intranasally (IN), or injected either subcutaneously (SC) or intramuscularly (IM). [58] [59] Conservation efforts regarding vaccinations often only serve the purpose of preventing disease related extinction. Rather than completely cleansing the population of the pathogen, infection rates are limited to a smaller percentage of the population. [60] Case study: Ethiopian Wolf Ethiopian wolf (Canis simensis citernii) The Ethiopian Wolf (Canis simensis), a canid native to Ethiopia , is an endangered species with less than 440 wolves remaining in the wild. [61] These wolves are primarily exposed to the rabies virus by domestic dogs and are facing extreme population declines, especially in the southern Ethiopia region of the Bale Mountains . [61] [62] To counter this, oral vaccinations are administered to these wolves within favorable bait that is widely distributed around their territories. The wolves consume the bait and with it ingest the vaccine, developing an immunity to rabies as antibodies are produced at significant levels. [62] Wolves within these packs who did not ingest the vaccine will be protected by herd immunity as fewer wolves are exposed to the virus. With continued periodic vaccinations, conservationists will be able to spend more resources on further proactive efforts to help prevent their extinction. [62] Government involvement[ edit ] In the US, the Endangered Species Act of 1973 was passed to protect US species deemed in danger of extinction. [63] The concern at the time was that the country was losing species that were scientifically, culturally, and educationally important. In the same year, the Convention on International Trade in Endangered Species of Fauna and Flora (CITES) was passed as part of an international agreement to prevent the global trade of endangered wildlife. [2] In 1980, the World Conservation Strategy was developed by the IUCN with help from the UN Environmental Programme, World Wildlife Fund, UN Food and Agricultural Organization, and UNESCO. [64] Its purpose was to promote the conservation of living resources important to humans. In 1992, the Convention on Biological Diversity (CBD) was agreed on at the UN Conference on Environment and Development (often called the Rio Earth Summit) as an international accord to protect the Earth's biological resources and diversity. [3] According to the National Wildlife Federation, wildlife conservation in the US gets a majority of its funding through appropriations from the federal budget, annual federal and state grants, and financial efforts from programs such as the Conservation Reserve Program , Wetlands Reserve Program and Wildlife Habitat Incentives Program . [65] [66] A substantial amount of funding comes from the sale of hunting/fishing licenses, game tags, stamps, and excise taxes from the purchase of hunting equipment and ammunition. [67] The Endangered Species Act is a continuously updated list that remains up-to-date on species that are endangered or threatened. Along with the update of the list, the Endangered Species Act also seeks to implement actions to protect the species within its list. [68] Furthermore, the Endangered Species Act also lists the species that the act has recovered. It is estimated that the act has prevented the extinction of about 291 species, like bald eagles and humpback whales , since its implementation through its different recovery plans and the protection that it provides for these threatened species. [69] Non-government involvement[ edit ] In the late 1980s, as the public became dissatisfied with government environmental conservation efforts, people began supporting private sector conservation efforts which included several non-governmental organizations (NGOs) . [70] Seeing this rise in support for NGOs, the U.S. Congress made amendments to the Foreign Assistance Act in 1979 and 1986 “earmarking U.S. Agency for International Development (USAID) funds for [biodiversity]”. [70] From 1990 till now, environmental conservation NGOs have become increasingly more focused on the political and economic impact of USAID funds dispersed for preserving the environment and its natural resources. [71] After the terrorist attacks on 9/11 and the start of former President Bush's War on Terror , maintaining and improving the quality of the environment and its natural resources became a “priority” to “prevent international tensions” according to the Legislation on Foreign Relations Through 2002 [71] and section 117 of the 1961 Foreign Assistance Act. [71] Non-governmental organizations[ edit ] Many NGOs exist to actively promote, or be involved with, wildlife conservation: The Nature Conservancy is a US charitable environmental organization that works to preserve the plants, animals, and natural communities that represent the diversity of life on Earth by protecting the lands and waters they need to survive. [72] World Wide Fund for Nature (WWF) is an international non-governmental organization working on the issues regarding the conservation, research and restoration of the environment, formerly named the World Wildlife Fund, which remains its official name in Canada and the United States. It is the world's largest independent conservation organization with over 5 million supporters worldwide, working in more than 90 countries, supporting around 1300[4] conservation and environmental projects around the world. It is a charity, with approximately 60% of its funding coming from voluntary donations by private individuals. 45% of the fund's income comes from the Netherlands, the United Kingdom and the United States. [73]
Toggle the table of contents Marine conservation From Wikipedia, the free encyclopedia Protection and preservation of saltwater  ecosystems Coral reefs have a great amount of biodiversity . Part of a series of overviews on e Marine conservation, also known as ocean conservation, is the protection and preservation of ecosystems in oceans and seas through planned management in order to prevent the over-exploitation of these marine resources . Marine conservation is informed by the study of marine plants and animal resources and ecosystem functions and is driven by response to the manifested negative effects  seen in the environment such as species loss , habitat degradation and changes in ecosystem functions [1] and focuses on limiting human-caused damage to marine ecosystems , restoring damaged marine ecosystems, and preserving vulnerable species and ecosystems of the marine life . Marine conservation is a relatively new discipline which has developed as a response to biological issues such as extinction and marine habitats change. Marine conservationists rely on a combination of scientific principles derived from marine biology , Ecology , oceanography , and fisheries science , as well as on human factors, such as demand for marine resources, maritime law , economics, and policy, in order to determine how to best protect and conserve marine species and ecosystems. Marine conservation may be described as a sub-discipline of conservation biology . Sylvia Earle : Marine Biologist, Explorer, & Author Steve Irwin : Naturalist, Conservationist, Zoologist, Herpetologist, & Television Personality “Although man’s record as a steward of the natural resources of the earth has been a discouraging one, there has long been a certain comfort in the belief that the sea, at least was inviolate, beyond man’s ability to change and to despoil. But this belief, unfortunately, has proved to be naïve”. If the oceans die, we all die. – Paul Watson [2] Human impacts on marine ecosystems[ edit ] Further information: Human impact on marine life Increasing human populations have resulted in increased human impact on ecosystems. Human activities has resulted in an increased extinction rate of species which has caused a major decrease in biological diversity of plants and animals in our environment. [3] These impacts include increased pressure from fisheries including reef degradation and overfishing as well as pressure from the tourism industry which has increased over the past few years. [4] The deterioration of coral reefs is mainly linked to human activities – 88% of reefs are threatened through various reasons as listed above, including excessive amounts of CO2 ( carbon dioxide ) emissions. Oceans absorb approximately 1/3 of the CO2 produced by humans, which has detrimental effects on the marine environment. The increasing levels of CO2 in oceans change the seawater chemistry by decreasing the pH , which is known as ocean acidification . The remains from the Exxon Valdez oil spill after the second treatment by oil spill workers in Alaska Oil spills also impact marine environments, contributing to marine pollution as a result of human activity. The effects of oil on marine fish have been studied following major spills in the United States. Shipping is a major vector for the introduction of exotic marine species, some of which can become overabundant and transform ecosystems. Collisions with ships can also be fatal for whales and can impact on the viability of whole populations, including the right whale population off the east coast of the United States. Main article: Coral reef Coral reefs are the epicenter of immense amounts of biodiversity and are a key player in the survival of entire ecosystems. They provide various marine animals with food, protection, and shelter which keep generations of species alive. [5] Furthermore, coral reefs are an integral part of sustaining human life through serving as a food source (i.e., fish and mollusks) as well as a marine space for ecotourism which provides economic benefits. [6] Also, humans are now conducting research regarding the use of corals as new potential sources for pharmaceuticals (i.e. steroids and anti-inflammatory drugs). [7] [8] Because of the human impact on coral reefs, these ecosystems are becoming increasingly degraded and in need of conservation. The biggest threats include overfishing , destructive fishing practices, sedimentation, and pollution from land-based sources. [9] This, in conjunction with increased carbon in oceans, coral bleaching, and diseases, means that there are no pristine reefs anywhere in the world. [10] Up to 88% of coral reefs in Southeast Asia are now threatened, with 50% of those reefs at either "high" or "very high" risk of disappearing, which directly affects the biodiversity and survival of species dependent on coral. [9] This is especially harmful to island nations such as Samoa , Indonesia , and the Philippines , because many people there depend on the coral reef ecosystems to feed their families and to make a living. However, many fishermen are unable to catch as many fish as they used to, so they are increasingly using cyanide and dynamite in fishing , which further degrades the coral reef ecosystem. [11] This perpetuation of bad habits simply leads to the further decline of coral reefs and therefore perpetuates the problem. One way of stopping this cycle is by educating the local community about why the conservation of marine spaces that include coral reefs is important. [12] Main article: Overfishing Overfishing is one of main causes of the decrease in the ocean's wildlife population over the past years. The Food and Agriculture Organization of the United Nation reported that the percentage of the world's fish stocks that are at biologically sustainable levels have decreased from 90% in 1974 to 65.8% in 2017. The overfishing of these large fisheries destroys the marine environment and threatens the livelihood of billions who depend on fish as protein or as a source of income for catching and selling. [13] According to the World Wildlife Fund , illegal, unreported, and unregulated fishing is a major factor in overfishing. Illegal fishing is estimated to account for up to 30% of the catch for some high value species, and the industry is estimated to be worth $36 billion per year. [14] Overabundance[ edit ] Overabundance can occur when the population of a certain species cannot be controlled naturally or by human intervention. The domination of one species can create an imbalance in an ecosystem, which can lead to the demise of other species and of the habitat. [15] Overabundance occurs predominately among invasive species . [16] Introduced species[ edit ] The international shipping trade has led to the establishment of many marine species beyond their native ranges. Some of these can have adverse consequences, such as the North pacific seastar which was introduced to Tasmania, Australia. Vectors for the translocation of organisms include hull biofouling , the dumping of ballast water and dumping of water from marine aquaria. A tank of ballast water is estimated to contain around 3,000 non-native species. [17] Once established, it is difficult to eradicate an exotic organism from an ecosystem. The San Francisco Bay is one of the places in the world that is the most impacted by foreign and invasive species. According to the Baykeeper organization, 97 percent of the organisms in the San Francisco Bay have been compromised by the 240 invasive species that have been brought into the ecosystem. [18] Invasive species in the bay such as the Asian clam have changed the food web of the ecosystem by depleting populations of native species such as plankton. [19] [20] The Asian clam clogs pipes and obstructs the flow of water in electrical generating facilities. Their presence in the San Francisco Bay has cost the United States an estimated one billion dollars in damages. [21] Extinct and endangered species[ edit ] Marine mammals[ edit ] A sleeping monk seal on the sandy beach with the ocean behind it Baleen whales were predominantly hunted from 1600 through the mid-1900s, and were nearing extinction when a global ban on commercial whaling was put into effect in 1986 by the IWC (International Whaling Convention). [15] [22] The Atlantic gray whale, last sighted in 1740, is now extinct due to European and Native American whaling. [23] [24] Since the 1960s the global population of monk seals has been rapidly declining. The Hawaiian and Mediterranean monk seals are considered to be one of the most endangered marine mammals on the planet, according to the NOAA . [23] The last sighting of the Caribbean monk seal was in 1952, and it has now been confirmed extinct by the NOAA. [25] [26] The vaquita porpoise, discovered in 1958, has become the most endangered marine species. Over half the population has disappeared since 2012, leaving 100 left in 2014. [27] The vaquita frequently drowns in fishing nets, which are used illegally in marine protected areas off the Gulf of Mexico. [28] Sea turtles[ edit ] In 2004, the Marine Turtle Specialist Group (MTSG), from the International Union for Conservation of Nature (IUCN), ran an assessment which determined that green turtles were globally endangered. Population decline in ocean basins is indicated through data collected by the MTSG that analyzes abundance and historical information on the species. This data examined the global population of green turtles at 32 nesting sites, and determined that over the last 100–150 years there has been a 48–65 percent decrease in the number of mature nesting females. [29] The Kemp's ridley sea turtle population fell in 1947 when 33,000 nests, which accounted for 80 percent of the population, were collected and sold by villagers in Racho Nuevo, Mexico. In the early 1960s only 5,000 individuals were left, and between 1978 and 1991, 200 Kemp's Ridley Turtles nested annually. In 2015, the World Wildlife Fund and National Geographic Magazine named the Kemp's ridley the most endangered sea turtle in the world, with 1000 females nesting annually. [30] Fish[ edit ] In 2014, the IUCN moved the Pacific bluefin tuna from "least concerned" to "vulnerable" on a scale that represents level of extinction risk. The Pacific bluefin tuna is targeted by the fishing industry mainly for its use in sushi. [31] A stock assessment released in 2013 by the International Scientific Committee for Tuna and Tuna-Like Species in the North Pacific Ocean (ISC) shows that the Pacific bluefin tuna population dropped by 96 percent in the Pacific Ocean. According to the ISC assessment, 90 percent of the Pacific bluefin tuna caught are juveniles that have not reproduced. [32] Between 2011 and 2014, the European eel , Japanese eel , and American eel were put on the IUCN red list of endangered species. [33] In 2015, the Environmental Agency concluded that the number of European eels has declined by 95 percent since 1990. An Environmental Agency officer, Andy Don, who has been researching eels for the past 20 years, said, "There is no doubt that there is a crisis. People have been reporting catching a kilo of glass eels this year when they would expect to catch 40 kilos. We have got to do something." [34] Marine plants[ edit ] Johnson's seagrass , a food source for the endangered green sea turtle, reproduces asexually , which limits its ability to populate and colonize habitats. This seagrass was formerly the only marine plant to be listed under the Endangered Species Act , and in 1998, it was granted protection as an endangered species. It is now known to be a clone of the widespread Halophila ovalis and protection was withdrawn. [35] [36] Data on this plant is limited, but it is known that since the 1970s there has been a 50 percent decrease in abundance. [37] There are many reasons behind the decline in the seagrass's proliferation, such as degradation of water quality, careless boating activities, anchoring. In addition to that hurricane activity caused by climate change , increase the risk of extinction.[ citation needed ] Main article: Marine protected area Strategies and techniques for marine conservation tend to combine theoretical disciplines, such as population biology, with practical conservation strategies, such as setting up protected areas, as with marine protected areas (MPAs) or Voluntary Marine Conservation Areas . These protected areas may be established for a variety of reasons and aim to limit the impact of human activity. These protected areas operate differently which includes areas that have seasonal closures and/or permanent closures as well as multiple levels of zoning that allow people to carryout different activities in separate areas; including, speed, no take and multi-use zones. [38] Other techniques include developing sustainable fisheries and restoring the populations of endangered species through artificial means. Another focus of conservationists is on curtailing human activities that are detrimental to either marine ecosystems or species through policy, techniques such as fishing quotas , like those set up by the Northwest Atlantic Fisheries Organization , or laws such as those listed below. Recognizing the economics involved in human use of marine ecosystems is key, as is education of the public about conservation issues. This includes educating tourists that come to an area who might not be familiar with certain regulations regarding the marine habitat. One example of this is a project called Green Fins based in Southeast Asia that uses the scuba diving industry to educate the public. This project, implemented by UNEP , encourages scuba diving operators to educate their students about the importance of marine conservation and encourage them to dive in an environmentally friendly manner that does not damage coral reefs or associated marine ecosystems. Scientists divide the process by a few parts, and there are various techniques in each part of it. In marking and capturing, the normal techniques include techniques for restraint in pinnipeds, chemical restraint and immobilization in pinnipeds, techniques for capture-release of cetaceans and techniques for restraint and handling. [39] Recently, some novel approaches include remote sensing techniques to model exposure of coastal-marine ecosystems to riverine flood plumes [40] and advanced iconography. [41] Techniques also include many social science-based methods. Many researchers have found the effectiveness of marine conservation through change caused by social events and encourage sustainable tourism development to raise the public awareness of it. Researchers suggest integrating customary management into marine conservation and emphasize that practical and conceptual differences exist between customary management and contemporary conservation which have often led to failed attempts to hybridize these systems. [42] Others have suggested to integrate marine conservation and tourism, the establishment of conservation areas can help reduce conflicts. Zoning the protected areas enables the grouping of compatible areas into specific zones and the separation of incompatible areas. [43] Common techniques to raise the general public's attention also include exposure to the concept of the carbon footprint and to educate people to make sustainable food choices and use fewer plastic products. [44] Technology and halfway technology[ edit ] Marine conservation technologies are used to protect endangered and threatened marine organisms and/or habitat. These technologies are innovative and revolutionary because they reduce by-catch, increase the survivorship and health of marine life and habitat, and benefit fishermen who depend on the resources for profit. Examples of technologies include marine protected areas (MPAs), turtle excluder devices (TEDs), autonomous recording unit , pop-up satellite archival tag , and radio-frequency identification (RFID). Commercial practicality plays an important role in the success of marine conservation because it is necessary to cater to the needs of fishermen while also protecting marine life. [45] Pop-up satellite archival tag (PSAT or PAT) plays a vital role in marine conservation by providing marine biologists with an opportunity to study animals in their natural environments. These are used to track movements of (usually large, migratory) marine animals. A PSAT is an archival tag (or data logger ) that is equipped with a means to transmit the collected data via satellite. Though the data are physically stored on the tag, its major advantage is that it does not have to be physically retrieved like an archival tag for the data to be available, making it a viable independent tool for animal behavior studies. These tags have been used to track movements of ocean sunfish , [46] marlin , blue sharks , bluefin tuna , swordfish and sea turtles . Location, depth, temperature, and body movement data are used to answer questions about migratory patterns, seasonal feeding movements, daily habits, and survival after catch and release. [47] [48] [49] Turtle excluder devices (TEDs) remove a major threat to turtles in their marine environment. Many sea turtles are accidentally captured, injured or killed by fishing. In response to this threat the National Oceanic and Atmospheric Administration (NOAA) worked with the shrimp trawling industry to create the TEDs. [50] By working with the industry they insured the commercial viability of the devices. A TED is a series of bars that is placed at the top or bottom of a trawl net, fitting the bars into the "neck" of the shrimp trawl and acting as a filter to ensure that only small animals may pass through. The shrimp will be caught but larger animals such as marine turtles that become caught by the trawler will be rejected by the filter function of the bars. [51] Similarly, halfway technologies work to increase the population of marine organisms. However, they do so without behavioral changes, and address the symptoms but not the cause of the declines. Examples of halfway technologies include hatcheries and fish ladders. [52] Laws and treaties[ edit ] International laws and treaties related to marine conservation include the 1966 Convention on Fishing and Conservation of Living Resources of the High Seas . United States laws related to marine conservation include the 1972 Marine Mammal Protection Act , as well as the 1972 Marine Protection, Research and Sanctuaries Act , which established the National Marine Sanctuaries program. In 2010, the Scottish Parliament enacted new legislation for the protection of marine life with the Marine (Scotland) Act 2010 . Its provisions include marine planning, marine licensing, marine conservation, seal conservation, and enforcement. Since 2006, United Nations introduce vulnerable marine ecosystem concept for the management of deep-sea fisheries in the areas beyond national jurisdiction. [53] This concept has been transposed by the European parliament for Atlantic European waters. [54] The United Nations Convention on the Law of the Sea [55] establishes guidelines for all uses of the oceans' resources and establishes a comprehensive regime of law and order in the world's oceans and seas. On December 10, 1982, the convention was made available for signature at Montego Bay, Jamaica . More than 150 nations representing all world regions, all legal and political systems, and the full range of socio-economic development participated. The Convention introduced new legal concepts, addressed new issues, and combined the conventional norms for the usage of the oceans into one document. The convention also established the framework for furthering the study of particular facets of maritime law. On November 16, 1994, the Convention came into effect in line with article 308. Today, it is the widely acknowledged system in charge of resolving all matters regarding the law of the sea. The Convention regulates all facets of ocean space, including delimitation, environmental control, marine scientific research, economic and commercial activity, transfer of technology, and the resolution of disputes pertaining to ocean issues. It has 320 articles and nine annexes. The UN Environment Programme (UNEP) works to safeguard oceans and seas and encourage the wise use of marine resources, particularly through its Regional Seas Program. [56] The sole international legal foundation for regional ocean and sea protection is the Regional Seas Conventions and Action Plans. The Global Programme of Action for the Protection of the Marine Environment from Land-based Activities was also established by UNEP. The relationship between terrestrial, freshwater, coastal, and marine ecosystems is directly addressed by it, which makes it the only international intergovernmental instrument to do so. Through the Intergovernmental Oceanographic Commission , the United Nations Educational, Scientific and Cultural Organization ( UNESCO ) organizes programs in marine research, observation systems, hazard mitigation, and better managing ocean and coastal ecosystems. The Organization is the originator of the Convention on the protection of the Underwater Cultural Heritage , the Global Ocean Observing System and UNESCO global geoparks . Around 1 800 quantifiable and monetary pledges totaling $108 billion have been obtained since 2017 for the UN Ocean Conference and the Our Ocean Conference. [57] [58] [59] [60] At the 1998 Oslo and Paris Convention for the Protection of the Marine Environment of the North-East Atlantic, environmental ministers from 15 Northeast Atlantic states and members of the European Commission agreed to identify marine species, habitats, and ecosystems that need protection and to "promote the establishment of a network of marine protected areas to ensure the sustainable use, protection, and conservation of marine biological diversity.” [61] [62] In June 2023, the High Seas Treaty was adopted by the UN to better protect marine environment and ecosystems in international waters. It needs ratification by at least 60 member states to enter into force. [63] [64] Global goals[ edit ] Marine conservation is included in the United Nations framework of Sustainable Development Goals (SDGs), most notably in SDG 14 ("Life below water"). The text of Target 14.5 is: "By 2020, conserve at least 10 per cent of coastal and marine areas, consistent with national and international law and based on the best available scientific information". [65] This target has one indicator: Indicator 14.5.1 is the "coverage of protected areas in relation to marine areas". The term " Marine Protected Areas " include marine reserves, fully protected marine areas, no-take zones, marine sanctuaries, ocean sanctuaries, marine parks, locally managed marine areas and other. Each area has a specific level of protection and a specific allowed range of activities. [66] Organizations[ edit ] The shore of the Pacific Ocean in San Francisco, California There are marine conservation non-governmental organizations throughout the world that focus on funding conservation efforts, educating the public and stakeholders, and lobbying for conservation law and policy. Examples of these include: Ocean Wise (Canada)
Toggle the table of contents Forest management From Wikipedia, the free encyclopedia Branch of forestry Forest management is a branch of forestry concerned with overall administrative, legal, economic, and social aspects, as well as scientific and technical aspects, such as silviculture , protection , and forest regulation . This includes management for timber, aesthetics , recreation , urban values, water , wildlife , inland and nearshore fisheries, wood products , plant genetic resources , and other forest resource values . [1] Management objectives can be for conservation, utilisation, or a mixture of the two. Techniques include timber extraction, planting and replanting of different species , building and maintenance of roads and pathways through forests, and preventing fire . Definition[ edit ] The forest is a natural system that can supply different products and services. Forests supply water, mitigate climate change , provide habitats for wildlife including many pollinators which are essential for sustainable food production, provide timber and fuelwood, serve as a source of non-wood forest products including food and medicine, and contribute to rural livelihoods. [2] The working of this system is influenced by the natural environment: climate, topography, soil, etc., and also by human activity. The actions of humans in forests constitute forest management. [3] In developed societies, this management tends to be elaborated and planned in order to achieve the objectives that are considered desirable.[ citation needed ] Some forests have been and are managed to obtain traditional forest products such as firewood, fiber for paper, and timber, with little thinking for other products and services. Nevertheless, as a result of the progression of environmental awareness, management of forests for multiple use is becoming more common. [4] Public input and awareness[ edit ] Deforestation and increased road-building in the Amazon Rainforest are a significant concern because of increased human encroachment upon wild areas, increased resource extraction and further threats to biodiversity . There has been increased public awareness of natural resource policy, including forest management.[ citation needed ] Public concern regarding forest management may have shifted from the extraction of timber for economic development, to maintaining the flow of the range of ecosystem services provided by forests, including provision of habitat for wildlife , protecting biodiversity , watershed management, and opportunities for recreation . Increased environmental awareness may contribute to an increased public mistrust of forest management professionals. [5] But it can also lead to greater understanding about what professionals do for forests for nature conservation and ecological services. The importance of taking care of the forests for ecological as well as economical sustainable reasons has been shown in the TV show Ax Men . Many tools like remote sensing, GIS and photogrammetry [6] [7] modelling have been developed to improve forest inventory and management planning. [8] Since 1953, the volume of standing trees in the United States has increased by 90% due to sustainable forest management. [9] Wildlife considerations[ edit ] The abundance and diversity of birds, mammals, amphibians and other wildlife are affected by strategies and types of forest management. [10] Forests are important because they provide these species with food, space and water. [11] Forest management is also important as it helps in conservation and utilization of the forest resources.[ citation needed ] Approximately 50 million hectares (or 24%) of European forest land is protected for biodiversity and landscape protection. Forests allocated for soil, water, and other ecosystem services encompass around 72 million hectares (32% of European forest area). [12] [13] [14] Over 90% of the world's forests regenerate organically, and more than half are covered by forest management plans or equivalents. [15] [16] Management intensity[ edit ] Forest management varies in intensity from a leave alone, natural situation to a highly intensive regime with silvicultural interventions. Forest Management is generally increased in intensity to achieve either economic criteria (increased timber yields, non-timber forest products , ecosystem services ) or ecological criteria (species recovery, fostering of rare species, carbon sequestration). [17] Proportion of forest area with long-term management plans, by region, 2020 [18] Most of the forests in Europe have management plans; on the other hand, management plans exist for less than 25 percent of forests in Africa and less than 20 percent in South America. The area of forest under management plans is increasing in all regions – globally, it has increased by 233 million ha since 2000, reaching 2.05 billion ha in 2020. [19] Forest certification is a globally recognized system for encouraging sustainable forest management and assuring that forest-based goods are derived from sustainably managed forests. [20] [21] [22] This is a voluntary procedure in which an impartial third-party organization evaluates the quality of forest management and output against a set of criteria established by a governmental or commercial certification agency. [23] [24]
Toggle the table of contents Water resources (Redirected from Water resources management ) Sources of water that are potentially useful This article is about all types of waters that are of potential use to humans. For a naturally occurring type of water resource that humans use a lot, see fresh water . Global values of water resources and human water use (excluding Antarctica ). Water resources 1961-90, water use around 2000. Computed by the global freshwater model WaterGAP . Water resources are natural resources of water that are potentially useful for humans, [1] for example as a source of drinking water supply or irrigation water. 97% of the water on Earth is salt water and only three percent is fresh water ; slightly over two-thirds of this is frozen in glaciers and polar ice caps . [2] The remaining unfrozen freshwater is found mainly as groundwater, with only a small fraction present above ground or in the air. [3] Natural sources of fresh water include surface water , under river flow, groundwater and frozen water . Artificial sources of fresh water can include treated wastewater ( wastewater reuse ) and desalinated seawater . Human uses of water resources include agricultural , industrial , household , recreational and environmental activities. Water resources are under threat from water scarcity , water pollution , water conflict and climate change . Fresh water is a renewable resource , yet the world's supply of groundwater is steadily decreasing, with depletion occurring most prominently in Asia, South America and North America, although it is still unclear how much natural renewal balances this usage, and whether ecosystems are threatened. [4] Natural sources of fresh water Natural sources of fresh water include surface water , under river flow, groundwater and frozen water . Surface water Lake Chungará and Parinacota volcano in northern Chile Surface water is water in a river, lake or fresh water wetland . Surface water is naturally replenished by precipitation and naturally lost through discharge to the oceans , evaporation , evapotranspiration and groundwater recharge . The only natural input to any surface water system is precipitation within its watershed . The total quantity of water in that system at any given time is also dependent on many other factors. These factors include storage capacity in lakes, wetlands and artificial reservoirs , the permeability of the soil beneath these storage bodies, the runoff characteristics of the land in the watershed, the timing of the precipitation and local evaporation rates. All of these factors also affect the proportions of water loss. Humans often increase storage capacity by constructing reservoirs and decrease it by draining wetlands. Humans often increase runoff quantities and velocities by paving areas and channelizing the stream flow. Natural surface water can be augmented by importing surface water from another watershed through a canal or pipeline . Brazil is estimated to have the largest supply of fresh water in the world, followed by Russia and Canada . [5] Panorama of a natural wetland ( Sinclair Wetlands , New Zealand) Water from glaciers Glacier runoff is considered to be surface water. The Himalayas, which are often called "The Roof of the World", contain some of the most extensive and rough high altitude areas on Earth as well as the greatest area of glaciers and permafrost outside of the poles. Ten of Asia's largest rivers flow from there, and more than a billion people's livelihoods depend on them. To complicate matters, temperatures there are rising more rapidly than the global average. In Nepal, the temperature has risen by 0.6 degrees Celsius over the last decade, whereas globally, the Earth has warmed approximately 0.7 degrees Celsius over the last hundred years. [6] Groundwater Relative groundwater travel times in the subsurface This section is an excerpt from Groundwater .[ edit ] Groundwater is the water present beneath Earth 's surface in rock and soil pore spaces and in the fractures of rock formations . About 30 percent of all readily available freshwater in the world is groundwater. [7] A unit of rock or an unconsolidated deposit is called an aquifer when it can yield a usable quantity of water. The depth at which soil pore spaces or fractures and voids in rock become completely saturated with water is called the water table . Groundwater is recharged from the surface; it may discharge from the surface naturally at springs and seeps , and can form oases or wetlands . Groundwater is also often withdrawn for agricultural , municipal , and industrial use by constructing and operating extraction wells . The study of the distribution and movement of groundwater is hydrogeology , also called groundwater hydrology . Typically, groundwater is thought of as water flowing through shallow aquifers , but, in the technical sense, it can also contain soil moisture , permafrost (frozen soil), immobile water in very low permeability bedrock , and deep geothermal or oil formation water. Groundwater is hypothesized to provide lubrication that can possibly influence the movement of faults . It is likely that much of Earth 's subsurface contains some water, which may be mixed with other fluids in some instances. Under river flow Throughout the course of a river, the total volume of water transported downstream will often be a combination of the visible free water flow together with a substantial contribution flowing through rocks and sediments that underlie the river and its floodplain called the hyporheic zone . For many rivers in large valleys, this unseen component of flow may greatly exceed the visible flow. The hyporheic zone often forms a dynamic interface between surface water and groundwater from aquifers, exchanging flow between rivers and aquifers that may be fully charged or depleted. This is especially significant in karst areas where pot-holes and underground rivers are common. Artificial sources of usable water Artificial sources of fresh water can include treated wastewater ( reclaimed water ), atmospheric water generators , [8] [9] [10] and desalinated seawater . However, the economic and environmental side effects of these technologies must also be taken into consideration. [11] Wastewater reuse This section is an excerpt from Reclaimed water .[ edit ] Water reclamation (also called wastewater reuse, water reuse or water recycling) is the process of converting municipal wastewater (sewage) or industrial wastewater into water that can be reused for a variety of purposes. Types of reuse include: urban reuse, agricultural reuse (irrigation), environmental reuse, industrial reuse, planned potable reuse, and de facto wastewater reuse (unplanned potable reuse). For example, reuse may include irrigation of gardens and agricultural fields or replenishing surface water and groundwater (i.e., groundwater recharge ). Reused water may also be directed toward fulfilling certain needs in residences (e.g. toilet flushing ), businesses, and industry, and could even be treated to reach drinking water standards. The injection of reclaimed water into the water supply distribution system is known as direct potable reuse. However, drinking reclaimed water is not a typical practice. [12] Treated municipal wastewater reuse for irrigation is a long-established practice, especially in arid countries. Reusing wastewater as part of sustainable water management allows water to remain as an alternative water source for human activities. This can reduce scarcity and alleviate pressures on groundwater and other natural water bodies. [13] There are several technologies used to treat wastewater for reuse. A combination of these technologies can meet strict treatment standards and make sure that the processed water is hygienically safe, meaning free from pathogens . The following are some of the typical technologies: Ozonation , ultrafiltration , aerobic treatment ( membrane bioreactor ), forward osmosis , reverse osmosis , and advanced oxidation , [14] or activated carbon . [15] Some water-demanding activities do not require high grade water. In this case, wastewater can be reused with little or no treatment. Desalinated water Wave-powered desalination Desalination is a process that takes away mineral components from saline water . More generally, desalination is the removal of salts and minerals from a target substance, [16] as in soil desalination , which is an issue for agriculture. Saltwater (especially sea water ) is desalinated to produce water suitable for human consumption or irrigation . The by-product of the desalination process is brine . [17] Desalination is used on many seagoing ships and submarines . Most of the modern interest in desalination is focused on cost-effective provision of fresh water for human use. Along with recycled wastewater , it is one of the few rainfall-independent water resources. [18] Due to its energy consumption, desalinating sea water is generally more costly than fresh water from surface water or groundwater , water recycling and water conservation . However, these alternatives are not always available and depletion of reserves is a critical problem worldwide. [19] [20] Desalination processes are using either thermal methods (in the case of distillation ) or membrane-based methods (e.g. in the case of reverse osmosis ) energy types. [21] [22] : 24 Research into other options Air-capture over oceans Schematic illustration of a proposed approach for capturing moisture above the ocean surface and transporting it to proximal land for improving water security [23] Map of water stress and spatial variability of water yield along the delineated near-offshore region of 200 km across the world [23] Researchers proposed "significantly increasing freshwater through the capture of humid air over oceans" to address present and, especially, future water scarcity/insecurity. [24] [23] Atmospheric water generators on land Further information: Atmospheric water generator A potentials-assessment study proposed hypothetical portable solar-powered atmospheric water harvesting devices which are under development, along with design criteria, finding they could help a billion people to access safe drinking water , albeit such off-the-grid generation may sometimes "undermine efforts to develop permanent piped infrastructure " among other problems. [25] [26] [27] Water uses Total renewable freshwater resources of the world, in mm/year (1 mm is equivalent to 1 L of water per m2) (long-term average for the years 1961–1990). Resolution is 0.5° longitude x 0.5° latitude (equivalent to 55 km x 55 km at the equator). Computed by the global freshwater model WaterGAP . The total quantity of water available at any given time is an important consideration. Some human water users have an intermittent need for water. For example, many farms require large quantities of water in the spring, and no water at all in the winter. To supply such a farm with water, a surface water system may require a large storage capacity to collect water throughout the year and release it in a short period of time. Other users have a continuous need for water, such as a power plant that requires water for cooling. To supply such a power plant with water, a surface water system only needs enough storage capacity to fill in when average stream flow is below the power plant's need. Nevertheless, over the long term the average rate of precipitation within a watershed is the upper bound for average consumption of natural surface water from that watershed. Agriculture and other irrigation This section is an excerpt from Irrigation .[ edit ] Irrigation of agricultural fields in Andalusia , Spain. Irrigation canal on the left. Irrigation (also referred to as watering) is the practice of applying controlled amounts of water to land to help grow crops , landscape plants , and lawns . Irrigation has been a key aspect of agriculture for over 5,000 years and has been developed by many cultures around the world. Irrigation helps to grow crops, maintain landscapes, and revegetate disturbed soils in dry areas and during times of below-average rainfall. In addition to these uses, irrigation is also employed to protect crops from frost , [28] suppress weed growth in grain fields, and prevent soil consolidation . It is also used to cool livestock , reduce dust , dispose of sewage , and support mining operations. Drainage , which involves the removal of surface and sub-surface water from a given location, is often studied in conjunction with irrigation. There are several methods of irrigation that differ in how water is supplied to plants. Surface irrigation , also known as gravity irrigation, is the oldest form of irrigation and has been in use for thousands of years. In sprinkler irrigation , water is piped to one or more central locations within the field and distributed by overhead high-pressure water devices. Micro-irrigation is a system that distributes water under low pressure through a piped network and applies it as a small discharge to each plant. Micro-irrigation uses less pressure and water flow than sprinkler irrigation. Drip irrigation delivers water directly to the root zone of plants. Subirrigation has been used in field crops in areas with high water tables for many years. It involves artificially raising the water table to moisten the soil below the root zone of plants. Irrigation water can come from groundwater (extracted from springs or by using wells ), from surface water (withdrawn from rivers , lakes or reservoirs ) or from non-conventional sources like treated wastewater , desalinated water , drainage water , or fog collection . Irrigation can be supplementary to rainfall , which is common in many parts of the world as rainfed agriculture , or it can be full irrigation, where crops rarely rely on any contribution from rainfall. Full irrigation is less common and only occurs in arid landscapes with very low rainfall or when crops are grown in semi-arid areas outside of rainy seasons. The environmental effects of irrigation relate to the changes in quantity and quality of soil and water as a result of irrigation and the subsequent effects on natural and social conditions in river basins and downstream of an irrigation scheme . The effects stem from the altered hydrological conditions caused by the installation and operation of the irrigation scheme. Amongst some of these problems is depletion of underground aquifers through overdrafting . Soil can be over-irrigated due to poor distribution uniformity or management wastes water, chemicals, and may lead to water pollution . Over-irrigation can cause deep drainage from rising water tables that can lead to problems of irrigation salinity requiring watertable control by some form of subsurface land drainage . See also: Industrial water treatment and Industrial wastewater treatment It is estimated that 22% of worldwide water is used in industry . [29] Major industrial users include hydroelectric dams, thermoelectric power plants , which use water for cooling , ore and oil refineries , which use water in chemical processes , and manufacturing plants, which use water as a solvent . Water withdrawal can be very high for certain industries, but consumption is generally much lower than that of agriculture. Water is used in renewable power generation. Hydroelectric power derives energy from the force of water flowing downhill, driving a turbine connected to a generator. This hydroelectricity is a low-cost, non-polluting, renewable energy source. Significantly, hydroelectric power can also be used for load following unlike most renewable energy sources which are intermittent . Ultimately, the energy in a hydroelectric power plant is supplied by the sun. Heat from the sun evaporates water, which condenses as rain in higher altitudes and flows downhill. Pumped-storage hydroelectric plants also exist, which use grid electricity to pump water uphill when demand is low, and use the stored water to produce electricity when demand is high. Thermoelectric power plants using cooling towers have high consumption, nearly equal to their withdrawal, as most of the withdrawn water is evaporated as part of the cooling process. The withdrawal, however, is lower than in once-through cooling systems. Water is also used in many large scale industrial processes, such as thermoelectric power production, oil refining, fertilizer production and other chemical plant use, and natural gas extraction from shale rock . Discharge of untreated water from industrial uses is pollution . Pollution includes discharged solutes  and increased water temperature ( thermal pollution ). Drinking water and domestic use (households) Main articles: Water supply , Drinking water , and Water footprint Drinking water It is estimated that 8% of worldwide water use is for domestic purposes. [29] These include drinking water , bathing , cooking , toilet flushing , cleaning, laundry and gardening . Basic domestic water requirements have been estimated by Peter Gleick at around 50 liters per person per day, excluding water for gardens. Drinking water is water that is of sufficiently high quality so that it can be consumed or used without risk of immediate or long term harm. Such water is commonly called potable water. In most developed countries, the water supplied to domestic, commerce and industry is all of drinking water standard even though only a very small proportion is actually consumed or used in food preparation. 844 million people still lacked even a basic drinking water service in 2017. [30] : 3 Of those, 159 million people worldwide drink water directly from surface water sources, such as lakes and streams. [30] : 3 One in eight people in the world do not have access to safe water. [31] [32] Environment Explicit environment water use is also a very small but growing percentage of total water use. Environmental water may include water stored in impoundments and released for environmental purposes (held environmental water), but more often is water retained in waterways through regulatory limits of abstraction. [33] Environmental water usage includes watering of natural or artificial wetlands, artificial lakes intended to create wildlife habitat, fish ladders , and water releases from reservoirs timed to help fish spawn, or to restore more natural flow regimes. [34] Environmental usage is non-consumptive but may reduce the availability of water for other users at specific times and places. For example, water release from a reservoir to help fish spawn may not be available to farms upstream, and water retained in a river to maintain waterway health would not be available to water abstractors downstream. Recreation Further information: Sea § Leisure Recreational water use is mostly tied to lakes, dams, rivers or oceans. If a water reservoir is kept fuller than it would otherwise be for recreation, then the water retained could be categorized as recreational usage. Examples are anglers, water skiers, nature enthusiasts and swimmers. Recreational usage is usually non-consumptive. However, recreational usage may reduce the availability of water for other users at specific times and places.  For example, water retained in a reservoir to allow boating in the late summer is not available to farmers during the spring planting season.  Water released for whitewater rafting may not be available for hydroelectric generation during the time of peak electrical demand. Challenges and threats Threats for the availability of water resources include: Water scarcity, water pollution, water conflict and climate change . Water scarcity This section is an excerpt from Water scarcity .[ edit ] Water scarcity (closely related to water stress or water crisis) is the lack of fresh water resources to meet the standard water demand. There are two type of water scarcity namely physical and economic water scarcity. [35] : 560 Physical water scarcity is where there is not enough water to meet all demands, including that needed for ecosystems to function. Arid areas for example Central Asia , West Asia , and North Africa often experience physical water scarcity. [36] Economic water scarcity on the other hand, is the result of lack of investment in infrastructure or technology to draw water from rivers, aquifers , or other water sources. It also results from weak human capacity to meet water demand. [35] : 560 Much of Sub-Saharan Africa experiences economic water scarcity. [37] : 11 Water pollution This section is an excerpt from Water pollution .[ edit ] Water pollution (or aquatic pollution) is the contamination of water bodies , usually as a result of human activities, so that it negatively affects its uses. [38] : 6 Water bodies include lakes , rivers , oceans , aquifers , reservoirs and groundwater . Water pollution results when contaminants mix with these water bodies. Contaminants can come from one of four main sources: sewage discharges, industrial activities, agricultural activities, and urban runoff including stormwater . [39] Water pollution is either surface water pollution or groundwater pollution . This form of pollution can lead to many problems, such as the degradation of aquatic ecosystems or spreading water-borne diseases when people use polluted water for drinking or irrigation . [40] Another problem is that water pollution reduces the ecosystem services (such as providing drinking water ) that the water resource would otherwise provide. Water conflict This section is an excerpt from Water conflict .[ edit ] Ethiopia's move to fill the dam 's reservoir could reduce Nile flows by as much as 25% and devastate Egyptian farmlands. [41] Water conflict typically refers to violence or disputes associated with access to, or control of, water resources, or the use of water or water systems as weapons or casualties of conflicts. The term water war is colloquially used in media for some disputes over water, and often is more limited to describing a conflict between countries, states, or groups over the rights to access water resources. [42] [43] The United Nations recognizes that water disputes result from opposing interests of water users, public or private. [44] A wide range of water conflicts appear throughout history, though they are rarely traditional wars waged over water alone. [45] Instead, water has long been a source of tension and one of the causes for conflicts. Water conflicts arise for several reasons, including territorial disputes, a fight for resources, and strategic advantage. [46] Climate change This section is an excerpt from Water security § Climate change .[ edit ] Impacts of climate change that are tied to water, affect people's water security on a daily basis. They include more frequent and intense heavy precipitation which affects the frequency, size and timing of floods. [47] Also droughts can alter the total amount of freshwater and cause a decline in groundwater storage, and reduction in groundwater recharge . [48] Reduction in water quality due to extreme events can also occur. [49] : 558 Faster melting of glaciers can also occur. [50] Water resource management Further information: Water resources law Water resource management is the activity of planning, developing, distributing and managing the optimum use of water resources. It is an aspect of water cycle management . The field of water resources management will have to continue to adapt to the current and future issues facing the allocation of water. With the growing uncertainties of global climate change and the long-term impacts of past management actions, this decision-making will be even more difficult. It is likely that ongoing climate change will lead to situations that have not been encountered. As a result, alternative management strategies, including participatory approaches and adaptive capacity are increasingly being used to strengthen water decision-making. Ideally, water resource management planning has regard to all the competing demands for water and seeks to allocate water on an equitable basis to satisfy all uses and demands. As with other resource management , this is rarely possible in practice so decision-makers must prioritise issues of sustainability, equity and factor optimisation (in that order!) to achieve acceptable outcomes. One of the biggest concerns for water-based resources in the future is the sustainability of the current and future water resource allocation. Sustainable Development Goal 6 has a target related to water resources management: "Target 6.5: By 2030, implement integrated water resources management at all levels, including through transboundary cooperation as appropriate." [51] [52] Sustainable water management At present, only about 0.08 percent of all the world's fresh water is accessible. And there is ever-increasing demand for drinking , manufacturing , leisure and agriculture . Due to the small percentage of water available, optimizing the fresh water we have left from natural resources has been a growing challenge around the world. Much effort in water resource management is directed at optimizing the use of water and in minimizing the environmental impact of water use on the natural environment. The observation of water as an integral part of the ecosystem is based on integrated water resources management , based on the 1992 Dublin Principles (see below). Sustainable water management requires a holistic approach based on the principles of Integrated Water Resource Management , originally articulated in 1992 at the Dublin (January) and Rio (July) conferences. The four Dublin Principles, promulgated in the Dublin Statement are: Fresh water is a finite and vulnerable resource, essential to sustain life, development and the environment; Water development and management should be based on a participatory approach, involving users, planners and policy-makers at all levels; Women play a central part in the provision, management and safeguarding of water; Water has an economic value in all its competing uses and should be recognized as an economic good. Implementation of these principles has guided reform of national water management law around the world since 1992. Further challenges to sustainable and equitable water resources management include the fact that many water bodies are shared across boundaries which may be international (see water conflict ) or intra-national (see Murray-Darling basin ). Integrated water resources management Integrated water resources management (IWRM) has been defined by the Global Water Partnership (GWP) as "a process which promotes the coordinated development and management of water, land and related resources, in order to maximize the resultant economic and social welfare in an equitable manner without compromising the sustainability of vital ecosystems ". [53] Some scholars say that IWRM is complementary to water security because water security is a goal or destination, whilst IWRM is the process necessary to achieve that goal. [54] IWRM is a paradigm that emerged at international conferences in the late 1900s and early 2000s, although participatory water management institutions have existed for centuries. [55] Discussions on a holistic way of managing water resources began already in the 1950s leading up to the 1977 United Nations Water Conference. [56] The development of IWRM was particularly recommended in the final statement of the ministers at the International Conference on Water and the Environment in 1992, known as the Dublin Statement . This concept aims to promote changes in practices which are considered fundamental to improved water resource management . IWRM was a topic of the second World Water Forum , which was attended by a more varied group of stakeholders than the preceding conferences and contributed to the creation of the GWP. [55] In the International Water Association definition, IWRM rests upon three principles that together act as the overall framework: [57] Social equity: ensuring equal access for all users (particularly marginalized and poorer user groups) to an adequate quantity and quality of water necessary to sustain human well-being . Economic efficiency: bringing the greatest benefit to the greatest number of users possible with the available financial and water resources. Ecological sustainability: requiring that aquatic ecosystems are acknowledged as users and that adequate allocation is made to sustain their natural functioning. In 2002, the development of IWRM was discussed at the World Summit on Sustainable Development held in Johannesburg, which aimed to encourage the implementation of IWRM at a global level. [58] The third World Water Forum recommended IWRM and discussed information sharing, stakeholder participation, and gender and class dynamics. [55] Operationally, IWRM approaches involve applying knowledge from various disciplines as well as the insights from diverse stakeholders to devise and implement efficient, equitable and sustainable solutions to water and development problems. As such, IWRM is a comprehensive, participatory planning and implementation tool for managing and developing water resources in a way that balances social and economic needs, and that ensures the protection of ecosystems for future generations. In addition, in light of contributing the achievement of Sustainable Development goals (SDGs) , [59] IWRM has been evolving into more sustainable approach as it considers the Nexus approach, which is a cross-sectoral water resource management. The Nexus approach is based on the recognition that "water, energy and food are closely linked through global and local water, carbon and energy cycles or chains." An IWRM approach aims at avoiding a fragmented approach of water resources management by considering the following aspects: Enabling environment, roles of Institutions, management Instruments. Some of the cross-cutting conditions that are also important to consider when implementing IWRM are: Political will and commitment, capacity development, adequate investment, financial stability and sustainable cost recovery, monitoring and evaluation. There is not one correct administrative model. The art of IWRM lies in selecting, adjusting and applying the right mix of these tools for a given situation. IWRM practices depend on context; at the operational level, the challenge is to translate the agreed principles into concrete action. Managing water in urban settings This section is an excerpt from Integrated urban water management .[ edit ] Integrated urban water management (IUWM) is the practice of managing freshwater , wastewater , and storm water as components of a basin-wide management plan. It builds on existing water supply and sanitation considerations within an urban settlement by incorporating urban water management within the scope of the entire river basin. [60] IUWM is commonly seen as a strategy for achieving the goals of Water Sensitive Urban Design . IUWM seeks to change the impact of urban development on the natural water cycle , based on the premise that by managing the urban water cycle as a whole; a more efficient use of resources can be achieved providing not only economic benefits but also improved social and environmental outcomes. One approach is to establish an inner, urban, water cycle loop through the implementation of reuse strategies.  Developing this urban water cycle loop requires an understanding both of the natural, pre-development, water balance and the post-development water balance.  Accounting for flows in the pre- and post-development systems is an important step toward limiting urban impacts on the natural water cycle. [61] IUWM within an urban water system can also be conducted by performance assessment of any new intervention strategies by developing a holistic approach which encompasses various system elements and criteria including sustainability type ones in which integration of water system components including water supply , waste water and storm water subsystems would be advantageous. [62] Simulation of metabolism type flows in urban water system can also be useful for analysing processes in urban water cycle of IUWM. [62] [63] Typical urban water cycle depicting drinking water purification and municipal sewage treatment systems By country Water resource management and governance is handled differently by different countries. For example, in the United States , the United States Geological Survey (USGS) and its partners monitor water resources, conduct research and inform the public about groundwater quality. [64] Water resources in specific countries are described below:
Toggle the table of contents Sustainable development From Wikipedia, the free encyclopedia Mode of human development Sustainable development requires six central capacities. [1] Sustainable development is an organizing principle that aims to meet human development goals while also enabling natural systems to provide necessary natural resources and ecosystem services to humans. [2] The desired result is a society where living conditions and resources meet human needs without undermining the planetary integrity and stability of the natural system. [3] [4] Sustainable development tries to find a balance between economic development , environmental protection , and social well-being . The Brundtland Report in 1987 defined sustainable development as "development that meets the needs of the present generation without compromising the ability of future generations to meet their own needs". [5] [6] The concept of sustainable development nowadays has a focus on economic development , social development and environmental protection for future generations. Sustainable development was first institutionalized with the Rio Process initiated at the 1992 Earth Summit in Rio de Janeiro . In 2015 the United Nations General Assembly (UNGA) adopted the Sustainable Development Goals (2015 to 2030) and explained how the goals are integrated and indivisible to achieve sustainable development at the global level. [7] The UNGA's 17 goals address the global challenges, including poverty , inequality, climate change , environmental degradation , peace, and justice. Sustainable development is interlinked with the normative concept of sustainability . UNESCO formulated a distinction between the two concepts as follows: "Sustainability is often thought of as a long-term goal (i.e. a more sustainable world), while sustainable development refers to the many processes and pathways to achieve it." [8] The concept of sustainable development has been criticized in various ways. While some see it as paradoxical (or as an oxymoron ) and regard development as inherently unsustainable, others are disappointed in the lack of progress that has been achieved so far. [9] [10] Part of the problem is that "development" itself is not consistently defined. [11] : 16 Definition[ edit ] In 1987, the United Nations World Commission on Environment and Development released the report Our Common Future, commonly called the Brundtland Report . [5] The report included a definition of "sustainable development" which is now widely used: [5] [12] Sustainable development is a development that meets the needs of the present without compromising the ability of future generations to meet their own needs. It contains two key concepts within it: The concept of 'needs', in particular, the essential needs of the world's poor, to which overriding priority should be given; and The idea of limitations imposed by the state of technology and social organization on the environment's ability to meet present and future needs. Sustainability[ edit ] This section is an excerpt from Sustainability .[ edit ] Several visual representations of sustainability and its three dimensions: the left image shows sustainability as three intersecting circles. In the top right it is a nested approach. In the bottom right it is three pillars. [13] The schematic with the nested ellipses emphasizes a hierarchy of the dimensions, putting environment as the foundation for the other two. Sustainability is a social goal for people to co-exist on Earth over a long time. Definitions of this term are disputed and have varied with literature, context, and time. [14] [13] Experts often describe sustainability as having three dimensions (or pillars): environmental, economic, and social, [13] and many publications emphasize the environmental dimension. [15] [16] In everyday use,[ specify ] sustainability often focuses on countering major environmental problems, including climate change , loss of biodiversity , loss of ecosystem services , land degradation , and air and water pollution . The idea of sustainability can guide decisions at the global, national, and individual levels (e.g. sustainable living ). [17] A related concept is sustainable development, and the terms are often used to mean the same thing. [18] UNESCO distinguishes the two like this: "Sustainability is often thought of as a long-term goal (i.e. a more sustainable world), while sustainable development refers to the many processes and pathways to achieve it." [19] Development of the concept[ edit ] See also: Sustainability Sustainable development has its roots in ideas regarding sustainable forest management , which were developed in Europe during the 17th and 18th centuries. [20] [21] [22] In response to a growing awareness of the depletion of timber resources in England, John Evelyn argued, in his 1662 essay Sylva , that "sowing and planting of trees had to be regarded as a national duty of every landowner, in order to stop the destructive over- exploitation of natural resources ." In 1713, Hans Carl von Carlowitz , a senior mining administrator in the service of Elector Frederick Augustus I of Saxony published Sylvicultura economics, a 400-page work on forestry. Building upon the ideas of Evelyn and French minister Jean-Baptiste Colbert , von Carlowitz developed the concept of managing forests for sustained yield . [20] His work influenced others, including Alexander von Humboldt and Georg Ludwig Hartig , eventually leading to the development of the science of forestry. This, in turn, influenced people like Gifford Pinchot , the first head of the US Forest Service , whose approach to forest management was driven by the idea of wise use of resources, and Aldo Leopold whose land ethic was influential in the development of the environmental movement in the 1960s. [20] [21] Following the publication of Rachel Carson 's Silent Spring in 1962, the developing environmental movement drew attention to the relationship between economic growth and environmental degradation . Kenneth E. Boulding , in his influential 1966 essay The Economics of the Coming Spaceship Earth , identified the need for the economic system to fit itself to the ecological system with its limited pools of resources. [21] Another milestone was the 1968 article by Garrett Hardin that popularized the term " tragedy of the commons ". [23] The direct linking of sustainability and development in a contemporary sense can be traced to the early 1970s. "Strategy of Progress", a 1972 book (in German) by Ernst Basler, explained how the long-acknowledged sustainability concept of preserving forests for future wood production can be directly transferred to the broader importance of preserving environmental resources to sustain the world for future generations. [24] That same year, the interrelationship of environment and development was formally demonstrated in a systems dynamic simulation model reported in the classic report on Limits to Growth . It was commissioned by the Club of Rome and written by a group of scientists led by Dennis and Donella Meadows of the Massachusetts Institute of Technology . Describing the desirable "state of global equilibrium", the authors wrote: "We are searching for a model output that represents a world system that is sustainable without sudden and uncontrolled collapse and capable of satisfying the basic material requirements of all of its people." [25] Also in 1972 was publication of the influential book, A Blueprint for Survival . [26] [27] In 1975, an MIT research group prepared ten days of hearings on "Growth and Its Implication for the Future" for the US Congress , the first hearings ever held on sustainable development. [28] In 1980, the International Union for Conservation of Nature published a world conservation strategy that included one of the first references to sustainable development as a global priority [29] and introduced the term "sustainable development". [30] : 4 Two years later, the United Nations World Charter for Nature raised five principles of conservation by which human conduct affecting nature is to be guided and judged. [31] Since the Brundtland Report , the concept of sustainable development has developed beyond the initial intergenerational framework to focus more on the goal of "socially inclusive and environmentally sustainable economic growth ". [30] : 5 In 1992, the UN Conference on Environment and Development published the Earth Charter , which outlines the building of a just, sustainable, and peaceful global society in the 21st century. The action plan Agenda 21 for sustainable development identified information, integration, and participation as key building blocks to help countries achieve development that recognizes these interdependent pillars. Furthermore, Agenda 21 emphasizes that broad public participation in decision-making is a fundamental prerequisite for achieving sustainable development. [32] The Rio Protocol was a huge leap forward: for the first time, the world agreed on a sustainability agenda. In fact, a global consensus was facilitated by neglecting concrete goals and operational details. The Sustainable Development Goals (SDGs) now have concrete targets (unlike the results from the Rio Process) but no methods for sanctions. [33] [11] : 137 Main article: Sustainability § Dimensions of sustainability Sustainable development, like sustainability , is regarded to have three dimensions: the environment, economy and society . The idea is that a good balance between the three dimensions should be achieved. Instead of calling them dimensions, other terms commonly used are pillars, domains, aspects, spheres. This section is an excerpt from Sustainability § Development of three dimensions .[ edit ] Sustainability Venn diagram , where sustainability is thought of as the area where the three dimensions overlap Scholars usually distinguish three different areas of sustainability. These are the environmental, the social, and the economic. Several terms are in use for this concept. Authors may speak of three pillars, dimensions, components, aspects, [34] perspectives, factors, or goals. All mean the same thing in this context. [13] The three dimensions paradigm has few theoretical foundations. It emerged without a single point of origin. [13] [35] Scholars rarely question the distinction itself. The idea of sustainability with three dimensions is a dominant interpretation in the literature. [13] Countries could develop systems for monitoring and evaluation of progress towards achieving sustainable development by adopting indicators that measure changes across economic, social and environmental dimensions. Further information: Weak and strong sustainability , Degrowth , and Eco-economic decoupling The concept of sustainable development has been and still is, subject to criticism, including the question of what is to be sustained in sustainable development. It has been argued that there is no such thing as sustainable use of a non-renewable resource , since any positive rate of exploitation will eventually lead to the exhaustion of earth's finite stock; [37] : 13 this perspective renders the Industrial Revolution as a whole unsustainable. [38] : 20f [39] : 61–67 [40] : 22f The sustainable development debate is based on the assumption that societies need to manage three types of capital (economic, social, and natural), which may be non-substitutable and whose consumption might be irreversible. [41] Natural capital can not necessarily be substituted by economic capital. [40] While it is possible that we can find ways to replace some natural resources, it is much less likely that they will ever be able to replace ecosystem services , such as the protection provided by the ozone layer, or the climate stabilizing function of the Amazonian forest. The concept of sustainable development has been criticized from different angles. While some see it as paradoxical (or an oxymoron ) and regard development as inherently unsustainable, others are disappointed in the lack of progress that has been achieved so far. [9] [10] Part of the problem is that "development" itself is not consistently defined. [11] : 16 [42] Such a viewpoint contradicts the mainstream academic community, which frequently concedes that the processes of capitalism are incompatible with the long-term sustainability of human life. The vagueness of the Brundtland definition of sustainable development has been criticized as follows: [11] : 17 The definition has "opened up the possibility of downplaying sustainability. Hence, governments spread the message that we can have it all at the same time, i.e. economic growth, prospering societies and a healthy environment. No new ethic is required. This so-called weak version of sustainability is popular among governments, and businesses, but profoundly wrong and not even weak , as there is no alternative to preserving the earth's ecological integrity." [43] : 2 Requirements[ edit ] Six interdependent capacities are deemed to be necessary for the successful pursuit of sustainable development. [1] These are the capacities to measure progress towards sustainable development; promote equity within and between generations; adapt to shocks and surprises; transform the system onto more sustainable development pathways; link knowledge with action for sustainability; and to devise governance arrangements that allow people to work together. Environmental characteristics of sustainable cities[ edit ] A sustainable city is an urban center that improves its environmental impact through urban planning and management. For the definition of an eco-city, imagine a city with parks and green spaces, solar-powered buildings, rooftop gardens, and more pedestrians and bicycles than cars. This is not a futuristic dream. Smart cities are actively moving towards greener urban ecosystems and better environmental management. [44] Further information: Human impact on the environment and Ecological footprint Deforestation of the Amazon rainforest . Deforestation and increased road-building in the Amazon rainforest are a concern because of increased human encroachment upon wilderness areas, increased resource extraction and further threats to biodiversity . Environmental sustainability concerns the natural environment and how it endures and remains diverse and productive. Since natural resources are derived from the environment, the state of air, water, and climate is of particular concern. Environmental sustainability requires society to design activities to meet human needs while preserving the life support systems of the planet. This, for example, entails using water sustainably, using renewable energy and sustainable material supplies (e.g. harvesting wood from forests at a rate that maintains the biomass and biodiversity). [45] An unsustainable situation occurs when natural capital (the total of nature's resources) is used up faster than it can be replenished. [46] : 58 Sustainability requires that human activity only uses nature's resources at a rate at which they can be replenished naturally. The concept of sustainable development is intertwined with the concept of carrying capacity . Theoretically, the long-term result of environmental degradation is the inability to sustain human life. [46] Important operational principles of sustainable development were published by Herman Daly in 1990: renewable resources should provide a sustainable yield (the rate of harvest should not exceed the rate of regeneration); for non-renewable resources there should be equivalent development of renewable substitutes; waste generation should not exceed the assimilative capacity of the environment. [47] Summary of different levels of consumption of natural resources. [46] : 58 Consumption of natural resources More than nature's ability to replenish Environmental degradation Equal to nature's ability to replenish Environmental equilibrium Less than nature's ability to replenish Environmental renewal Environmental problems associated with industrial agriculture and agribusiness are now being addressed through approaches such as sustainable agriculture , organic farming and more sustainable business practices . [48] The most cost-effective climate change mitigation options include afforestation , sustainable forest management , and reducing deforestation . [49] At the local level there are various movements working towards sustainable food systems which may include less meat consumption, local food production, slow food , sustainable gardening , and organic gardening . [50] The environmental effects of different dietary patterns depend on many factors, including the proportion of animal and plant foods consumed and the method of food production. [51] [52] Materials and waste[ edit ] Ecological footprint for different nations compared to their Human Development Index (2007) Further information: Cradle-to-cradle As global population and affluence have increased, so has the use of various materials increased in volume, diversity, and distance transported. Included here are raw materials, minerals, synthetic chemicals (including hazardous substances ), manufactured products, food, living organisms, and waste. [53] By 2050, humanity could consume an estimated 140 billion tons of minerals, ores, fossil fuels and biomass per year (three times its current amount) unless the economic growth rate is decoupled from the rate of natural resource consumption . Developed countries' citizens consume an average of 16 tons of those four key resources per capita per year, ranging up to 40 or more tons per person in some developed countries with resource consumption levels far beyond what is likely sustainable. By comparison, the average person in India today consumes four tons per year. [54] Sustainable use of materials has targeted the idea of dematerialization , converting the linear path of materials (extraction, use, disposal in landfill) to a circular material flow that reuses materials as much as possible, much like the cycling and reuse of waste in nature. [55] Dematerialization is being encouraged through the ideas of industrial ecology , eco design [56] and ecolabelling . This way of thinking is expressed in the concept of circular economy , which employs reuse , sharing , repair, refurbishment, remanufacturing and recycling to create a closed-loop system, minimizing the use of resource inputs and the creation of waste , pollution and carbon emissions. [57] Building electric vehicles has been one of the most popular ways in the field of sustainable development, the potential of using reusable energy and reducing waste offered a perspective in sustainable development. [58] The European Commission has adopted an ambitious Circular Economy Action Plan in 2020, which aims at making sustainable products the norm in the EU. [59] [60] Biodiversity and ecosystem services[ edit ] There is a connection between ecosystems and biodiversity. Ecosystems are made up of various living things interacting with one another and their surroundings. Along with this, biodiversity lays the groundwork for ecosystems to function well by defining the kinds of species that can coexist in an environment, as well as their functions and interactions with other species. [61] [62] In 2019, a summary for policymakers of the largest, most comprehensive study to date of biodiversity and ecosystem services was published by the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services . It recommended that human civilization will need a transformative change, including sustainable agriculture , reductions in consumption and waste, fishing quotas and collaborative water management. [63] [64] Biodiversity is not only crucial for the well-being of animals and wildlife but also plays a positive role in the lives of human beings in the way in which it aids development of human life. [65] Management of human consumption and impacts[ edit ] Further information: Consumption (economics) , Overconsumption , and Micro-sustainability Waste generation, measured in kilograms per person per day The environmental impact of a community or humankind as a whole depends both on population and impact per person, which in turn depends in complex ways on what resources are being used, whether or not those resources are renewable, and the scale of the human activity relative to the carrying capacity of the ecosystems involved. [66] Careful resource management can be applied at many scales, from economic sectors like agriculture, manufacturing and industry, to work organizations, the consumption patterns of households and individuals, and the resource demands of individual goods and services. [67] [68] The underlying driver of direct human impacts on the environment is human consumption. [69] This impact is reduced by not only consuming less but also making the full cycle of production, use, and disposal more sustainable. Consumption of goods and services can be analyzed and managed at all scales through the chain of consumption, starting with the effects of individual lifestyle choices and spending patterns, through to the resource demands of specific goods and services, the impacts of economic sectors, through national economies to the global economy. [70] Key resource categories relating to human needs are food , energy , raw materials and water. Improving on economic and social aspects[ edit ] Further information: Corporate sustainability and Sustainable business It has been suggested that because of rural poverty and overexploitation , environmental resources should be treated as important economic assets, called natural capital . [71] Economic development has traditionally required a growth in the gross domestic product. This model of unlimited personal and GDP growth may be over. Sustainable development may involve improvements in the quality of life for many but may necessitate a decrease in resource consumption . [72] "Growth" generally ignores the direct effect that the environment may have on social welfare, whereas "development" takes it into account. [73] As early as the 1970s, the concept of sustainability was used to describe an economy "in equilibrium with basic ecological support systems". [74] Scientists in many fields have highlighted The Limits to Growth , [75] [76] and economists have presented alternatives, for example a ' steady-state economy ', to address concerns over the impacts of expanding human development on the planet. [40] In 1987, the economist Edward Barbier published the study The Concept of Sustainable Economic Development, where he recognized that goals of environmental conservation and economic development are not conflicting and can be reinforcing each other. [77] A World Bank study from 1999 concluded that based on the theory of genuine savings (defined as "traditional net savings less the value of resource depletion and environmental degradation plus the value of investment in human capital "), policymakers have many possible interventions to increase sustainability, in macroeconomics or purely environmental. [78] Several studies have noted that efficient policies for renewable energy and pollution are compatible with increasing human welfare, eventually reaching a golden-rule[ clarification needed ] steady state. [79] [80] [81] [82] A meta review in 2002 looked at environmental and economic valuations and found a "lack of concrete understanding of what "sustainability policies" might entail in practice". [83] A study concluded in 2007 that knowledge, manufactured and human capital (health and education) has not compensated for the degradation of natural capital in many parts of the world. [84] It has been suggested that intergenerational equity can be incorporated into a sustainable development and decision making, as has become common in economic valuations of climate economics . [85] The World Business Council for Sustainable Development published a Vision 2050 document in 2021 to show "How business can lead the transformations the world needs". The vision states that "we envision a world in which 9+billion people can live well, within planetary boundaries , by 2050." [86] This report was highlighted by The Guardian as "the largest concerted corporate sustainability action plan to date – include reversing the damage done to ecosystems, addressing rising greenhouse gas emissions and ensuring societies move to sustainable agriculture." [87] Gender and leadership in sustainable development[ edit ] Gender and sustainable development have been examined, focusing on women's leadership potential and barriers to it. While leadership roles in sustainable development have become more androgynous over time, patriarchal structures and perceptions continue to constrain women from becoming leaders. [88] Some hidden issues are women's lack of self-confidence, impeding access to leadership roles, but men can potentially play a role as allies for women's leadership. [88] Barriers[ edit ] There are barriers that small and medium enterprises face when implementing sustainable development such as lack of expertise, lack of resources, and high initial capital cost of implementing sustainability measures. [89] Globally, the scale of collective action and lack of political will are barriers to achieving sustainable development. [42] [90] To overcome these challenges, governments must jointly form an agreement of social and political strength. Efforts to enact reforms or design and implement programs to decrease the harmful effects of human behaviors allow for progress toward present and future environmental sustainability goals. [91] The Paris Agreement exemplifies efforts of political will on a global level, a multinational agreement between 193 parties [92] intended to strengthen the global response to climate change by reducing emissions and working together to adjust to the consequent effects of climate change . [92] Experts continue to firmly suggest that governments should do more outside of The Paris Agreement , there persist a greater need for political will . [93] Another barrier towards sustainable development would be negative externalities that may potentially arise from implementing sustainable development technology. One example would be the development of lithium-ion batteries , a key element towards environmental sustainability and the reduction in reliance towards fossil fuels . However, currently with the technology and methodology available, Lithium production poses a negative environmental impact during its extraction from the earth as it uses a method very similar to fracking as well as during its processing to be used as a battery which is a chemically intensive process. [94] One suggested solution would be to weigh the possibility of recycling as this will cut down on the waste of old lithium as well as reducing the need for extracting new lithium from the ground, however, this sustainable development solution is barred from implementation by a high initial cost as studies have shown that recycling old technology for the purpose of extracting metals such as lithium and cobalt is typically more expensive than extracting them from the ground and processing them.[ citation needed ] The COVID-19 pandemic needs to be considered in the SDG process. Especially for developing countries exposed to social problems affected by COVID-19, the connection between post-epidemic recovery and SDG needs to be discussed and studied. [95] The COVID-19 pandemic has provided substantial roadblocks towards achieving Sustainable Development Goals (SDGs). While the long-term effects of COVID-19 on SDGs is limited, research has shown that SDG 1, SDG 4, and SDG 8 are the most likely to be adversely affected by the pandemic. One of the strategies proposed towards SDG in the light of the COVID-19 pandemic is green management, or the government strategy of utilizing resources such as water and energy with the intention to change resource consumption behavior. Other strategies include erecting sustainable food systems, labor market energization, inclusive education, and supporting research in the energy sector. [96] Society and culture[ edit ]
Toggle the table of contents Renewable energy From Wikipedia, the free encyclopedia Energy collected from renewable resources Examples of renewable energy options. Clockwise from top left: concentrated solar power with molten salt heat storage in Spain; wind energy in South Africa; Biomass plant in Scotland; The Three Gorges Dam on the Yangtze River in China Part of a series on Renewable energy, green energy, or low-carbon energy is energy from renewable resources that are naturally replenished on a human timescale . Renewable resources include sunlight , wind , the movement of water , and geothermal heat . [1] [2] [3] [4] [5] [6] [7] Although most renewable energy sources are sustainable , some are not. For example, some biomass sources are considered unsustainable at current rates of exploitation . [8] [9] Renewable energy is often used for electricity generation , heating and cooling . Renewable energy projects are typically large-scale, but they are also suited to rural and remote areas and developing countries , where energy is often crucial in human development . [10] [11] Renewable energy is often deployed together with further electrification , which has several benefits: electricity can move heat or objects efficiently, and is clean at the point of consumption. [12] [13] From 2011 to 2021, renewable energy grew from 20% to 28% of global electricity supply. Use of fossil energy shrank from 68% to 62%, and nuclear from 12% to 10%. The share of hydropower decreased from 16% to 15% while power from sun and wind increased from 2% to 10%. Biomass and geothermal energy grew from 2% to 3%. There are 3,146 gigawatts installed in 135 countries, while 156 countries have laws regulating the renewable energy sector. [14] [15] In 2021, China accounted for almost half of the global increase in renewable electricity. [16] Globally there are over 10 million jobs associated with the renewable energy industries, with solar photovoltaics being the largest renewable employer. [17] Renewable energy systems are rapidly becoming more efficient and cheaper and their share of total energy consumption is increasing, [18] with a large majority of worldwide newly installed electricity capacity being renewable. [19] In most countries, photovoltaic solar or onshore wind are the cheapest new-build electricity. [20] Many nations around the world already have renewable energy contributing more than 20% of their total energy supply, with some generating over half their electricity from renewables . [21] A few countries generate all their electricity using renewable energy. [22] National renewable energy markets are projected to continue to grow strongly in the 2020s and beyond. [23] According to the IEA, to achieve net zero emissions by 2050, 90% of global electricity generation will need to be produced from renewable sources. [24] Some studies say that a global transition to 100% renewable energy across all sectors – power, heat, transport and industry – is feasible and economically viable. [25] [26] [27] Renewable energy resources exist over wide geographical areas, in contrast to fossil fuels , which are concentrated in a limited number of countries. Deployment of renewable energy and energy efficiency technologies is resulting in significant energy security , climate change mitigation , and economic benefits. [28] However renewables are being hindered by hundreds of billions of dollars of fossil fuel subsidies . [29] In international public opinion surveys there is strong support for renewables such as solar power and wind power. [30] [31] In 2022 the International Energy Agency asked countries to solve policy, regulatory, permitting and financing obstacles to adding more renewables, to have a better chance of reaching net zero carbon emissions by 2050. [32] Definition[ edit ] Renewable energy is usually understood as energy harnessed from continuously occurring natural phenomena. The International Energy Agency defines it as "energy derived from natural processes that are replenished at a faster rate than they are consumed.” Solar power , wind power , hydroelectricity , geothermal energy, and biomass are widely agreed to be the main types of renewable energy. [35] Renewable energy often displaces conventional fuels in four areas: electricity generation , hot water / space heating , transportation , and rural (off-grid) energy services. [36] Although almost all forms of renewable energy cause much fewer carbon emissions than fossil fuels, the term is not synonymous with low-carbon energy . Some non-renewable sources of energy, such as nuclear power , generate almost no emissions, while some renewable energy sources can be very carbon-intensive, such as the burning of biomass if it is not offset by planting new plants. [37] Renewable energy is also distinct from sustainable energy , a more abstract concept that seeks to group energy sources based on their overall permanent impact on future generations of humans. For example, biomass is often associated with unsustainable deforestation . [38] Role in addressing climate change[ edit ] Deaths caused as a result of fossil fuel use (areas of rectangles in chart) greatly exceed those resulting from production of renewable energy (rectangles barely visible in chart). [39] As part of the global effort to address climate change , most of the world's countries have committed substantially reducing their greenhouse gas emissions . In practice, this means phasing out fossil fuels and replacing them with low-emissions energy sources. [37] At the 2023 United Nations Climate Change Conference , around three-quarters of the world's countries set a goal of tripling renewable energy capacity by 2030. [40] The European Union aims to generate 40% of its electricity from renewables by the same year. [41] Renewable energy is also more evenly distributed around the world than fossil fuels, which are concentrated in a limited number of countries. [42] It also brings health benefits by reducing air pollution caused by the burning of fossil fuels. The potential worldwide savings in health care costs have been estimated at trillions of dollars annually. [43] History[ edit ] New government spending, regulation and policies helped the renewables industry weather the global financial crisis better than many other sectors. [44] In 2022, renewables accounted for 30% of global electricity generation, up from 21% in 1985. [45] Global electricity power generation capacity 1419.0 GW (2023) [47] Levelized cost per megawatt hour Utility-scale photovoltaics: USD 38.343 (2019) [50] Primary technologies Komekurayama photovoltaic power station in Kofu , Japan Solar power produced around 1.3 terrawatt-hours (TWh) worldwide in 2022, [21] representing 4.6% of the world's electricity. Almost all of this growth has happened since 2010. [51] Solar energy can be harnessed anywhere that receives sunlight; however, the amount of solar energy that can be harnessed for electricity generation is influenced by weather conditions , geographic location and time of day. [52] There are two mainstream ways of harnessing solar energy: solar thermal , which converts solar energy into heat; and photovoltaics (PV), which converts it into electricity. [37] PV is far more widespread, accounting for around two thirds of the global solar energy capacity as of 2022. [53] It is also growing at a much faster rate, with 170 GW newly installed capacity in 2021, [54] compared to 25 GW of solar thermal. [53] Passive solar refers to a range of construction strategies and technologies that aim to optimize the distribution of solar heat in a building. Examples include solar chimneys , [37] orienting a building to the sun, using construction materials that can store heat , and designing spaces that naturally circulate air . [55] Main articles: Growth of photovoltaics , Solar power by country , and List of photovoltaic power stations Swanson's law –stating that solar module prices have dropped about 20% for each doubling of installed capacity—defines the " learning curve " of solar photovoltaics . [56] [57] A photovoltaic system , consisting of solar cells assembled into panels , converts light into electrical direct current via the photoelectric effect . [58] Almost all commercial PV cells consist of crystalline silicon , with a market share of 95%. Cadmium telluride thin-film solar cells account for the remainder. [59] PV has several advantages that make it by far the fastest-growing renewable energy technology. It is cheap, low-maintenance and scalable; adding to an existing PV installation as demanded arises is simple. Its main disadvantage is its poor performance in cloudy weather. [37] PV systems range from small, residential and commercial rooftop or building integrated installations, to large utility-scale photovoltaic power station . Building-integrated PV uses existing land and structures to generate power close to where it is consumed. [60] A household's solar panels, and batteries if they have them, can often either be used for just that household or if connected to an electrical grid can be aggregated with millions of others. [61] As of 2022, around 25 million households rely on rooftop solar power worldwide. [62] Australia has by far the most rooftop solar capacity per capita. [63] The first utility-scale solar power plant was built in 1982 in Hesperia, California by ARCO . [64] The plant was not profitable and was sold eight years later. [65] However, over the following decades, PV technology significantly improved its electricity generating efficiency , reduced the installation cost per watt as well as its energy payback time , and reached grid parity . [66] As a result, PV adoption has grown exponentially since 2010. [67] Global capacity increased from 230 GW at the end of 2015 to 890 GW in 2021. [68] PV grew fastest in China between 2016 and 2021, adding 560 GW, more than all advanced economies combined. [69] Four of the ten biggest solar power stations are in China, including the biggest, Golmud Solar Park in China. [70] Many utility-scale PV systems use tracking systems that follow the sun's daily path across the sky to generate more electricity than fixed-mounted systems. [71] This section is an excerpt from Solar thermal energy .[ edit ] Roof-mounted close-coupled thermosiphon solar water heater. Solar thermal energy (STE) is a form of energy and a technology for harnessing solar energy to generate thermal energy for use in industry , and in the residential and commercial sectors. Solar thermal collectors are classified by the United States Energy Information Administration as low-, medium-, or high-temperature collectors. Low-temperature collectors are generally unglazed and used to heat swimming pools or to heat ventilation air. Medium-temperature collectors are also usually flat plates but are used for heating water or air for residential and commercial use. High-temperature collectors concentrate sunlight using mirrors or lenses and are generally used for fulfilling heat requirements up to 300 deg C / 20 bar pressure in industries, and for electric power production. Two categories include Concentrated Solar Thermal (CST) for fulfilling heat requirements in industries, and Concentrated Solar Power (CSP) when the heat collected is used for electric power generation. CST and CSP are not replaceable in terms of application. Sunrise at the Fenton Wind Farm in Minnesota, United States Wind energy generation by region over time [72] Global electricity power generation capacity 1017.2 GW (2023) [73] Levelized cost per megawatt hour Land-based wind: USD 30.165 (2019) [75] Primary technology Windmill , windpump Humans have harnessed wind energy since at least 3500 BC. Until the 20th century, it was primarily used to power ships, windmills and water pumps. Today, the vast majority of wind power is used to generate electricity using wind turbines. [37] Modern utility-scale wind turbines range from around 600 kW to 9 MW of rated power. The power available from the wind is a function of the cube of the wind speed, so as wind speed increases, power output increases up to the maximum output for the particular turbine. [76] Areas where winds are stronger and more constant, such as offshore and high-altitude sites, are preferred locations for wind farms. Wind-generated electricity met nearly 4% of global electricity demand in 2015, with nearly 63 GW of new wind power capacity installed. Wind energy was the leading source of new capacity in Europe, the US and Canada, and the second largest in China. In Denmark, wind energy met more than 40% of its electricity demand while Ireland, Portugal and Spain each met nearly 20%. [77] Globally, the long-term technical potential of wind energy is believed to be five times total current global energy production, or 40 times current electricity demand, assuming all practical barriers needed were overcome. This would require wind turbines to be installed over large areas, particularly in areas of higher wind resources, such as offshore, and likely also industrial use of new types of VAWT turbines in addition to the horizontal axis units currently in use. As offshore wind speeds average ~90% greater than that of land, offshore resources can contribute substantially more energy than land-stationed turbines. [78] Global electricity power generation capacity 1,267.9 GW (2023) [79] Levelized cost per megawatt hour USD 65.581 (2019) [81] Pumped storage , mechanical power Since water is about 800 times denser than air , even a slow flowing stream of water, or moderate sea swell , can yield considerable amounts of energy. Water can generate electricity with a conversion efficiency of about 90%, which is the highest rate in renewable energy. [82] There are many forms of water energy: Historically, hydroelectric power came from constructing large hydroelectric dams and reservoirs, which are still popular in developing countries . [83] The largest of them are the Three Gorges Dam (2003) in China and the Itaipu Dam (1984) built by Brazil and Paraguay. Small hydro systems are hydroelectric power installations that typically produce up to 50 MW of power. They are often used on small rivers or as a low-impact development on larger rivers. China is the largest producer of hydroelectricity in the world and has more than 45,000 small hydro installations. [84] Run-of-the-river hydroelectricity plants derive energy from rivers without the creation of a large reservoir . The water is typically conveyed along the side of the river valley (using channels, pipes and/or tunnels) until it is high above the valley floor, whereupon it can be allowed to fall through a penstock to drive a turbine. A run-of-river plant may still produce a large amount of electricity, such as the Chief Joseph Dam on the Columbia River in the United States. [85] However many run-of-the-river hydro power plants are micro hydro or pico hydro plants. Hydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010.[ needs update ] Of the top 50 countries by percentage of electricity generated from renewables, 46 are primarily hydroelectric. [86] There are now seven hydroelectricity stations larger than 10 GW (10,000 MW ) worldwide, see table below. Rank 26°20′2″N 102°37′48″E﻿ / ﻿26.33389°N 102.63000°E﻿ / 26.33389; 102.63000﻿ (Three Gorges Dam) 10,200 Much hydropower is flexible, thus complementing wind and solar. [87] In 2021, the world renewable hydropower capacity was 1,360 GW. [69] Only a third of the world's estimated hydroelectric potential of 14,000 TWh/year has been developed. [88] [89] New hydropower projects face opposition from local communities due to their large impact, including relocation of communities and flooding of wildlife habitats and farming land. [90] High cost and lead times from permission process, including environmental and risk assessments, with lack of environmental and social acceptance are therefore the primary challenges for new developments. [91] It is popular to repower old dams thereby increasing their efficiency and capacity as well as quicker responsiveness on the grid. [92] Where circumstances permit existing dams such as the Russell Dam built in 1985 may be updated with "pump back" facilities for pumped-storage which is useful for peak loads or to support intermittent wind and solar power. Because dispatchable power is more valuable than VRE [93] [94] countries with large hydroelectric developments such as Canada and Norway are spending billions to expand their grids to trade with neighboring countries having limited hydro. [95] Sugarcane plantation to produce ethanol in Brazil A CHP power station using wood to supply 30,000 households in France Global electricity power generation capacity 150.3 GW (2023) [96] Levelized cost per megawatt hour USD 118.908 (2019) [98] Other energy applications Heating, cooking, transportation fuels Biomass is biological material derived from living, or recently living organisms. It commonly refers to plants or plant-derived materials. As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel in solid, liquid or gaseous form. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: thermal, chemical, and biochemical methods. Wood was the largest biomass energy source as of 2012; [99] examples include forest residues – such as dead trees, branches and tree stumps , yard clippings, wood chips and even municipal solid waste . Industrial biomass can be grown from numerous types of plants, including miscanthus , switchgrass , hemp , corn , poplar , willow , sorghum , sugarcane, bamboo , [100] and a variety of tree species, ranging from eucalyptus to oil palm ( palm oil ). Plant energy is produced by crops specifically grown for use as fuel that offer high biomass output per hectare with low input energy. [101] The grain can be used for liquid transportation fuels while the straw can be burned to produce heat or electricity. Plant biomass can also be degraded from cellulose to glucose through a series of chemical treatments, and the resulting sugar can then be used as a first-generation biofuel. Biomass can be converted to other usable forms of energy such as methane gas [102] or transportation fuels such as ethanol and biodiesel . Rotting garbage, and agricultural and human waste, all release methane gas – also called landfill gas or biogas . Crops, such as corn and sugarcane, can be fermented to produce the transportation fuel, ethanol. Biodiesel, another transportation fuel, can be produced from left-over food products such as vegetable oils and animal fats. [103] There is a great deal of research involving algal fuel or algae-derived biomass due to the fact that it is a non-food resource, grows around 20 times faster than other types of food crops, such as corn and soy, and can be grown almost anywhere. [104] [105] Once harvested, it can be fermented to produce biofuels such as ethanol, butanol , and methane, as well as biodiesel and hydrogen . The biomass used for electricity generation varies by region. Forest by-products, such as wood residues, are common in the United States. Agricultural waste is common in Mauritius (sugar cane residue) and Southeast Asia (rice husks). Biomass, biogas and biofuels are burned to produce heat/power and in doing so can harm the environment . Pollutants such as sulphurous oxides (SOx), nitrous oxides (NOx), and particulate matter (PM) are produced from the combustion of biomass. With regards to traditional use of biomass for heating and cooking , the World Health Organization estimates that 3.7 million prematurely died from outdoor air pollution in 2012 while indoor pollution from biomass burning effects over 3 billion people worldwide. [106] [107] Bioenergy global capacity in 2021 was 158 GW. Biofuels avoided 4.4% of global transport fuel demand in 2021. [69] Brazil produces bioethanol made from sugarcane available throughout the country. A typical gas station with dual fuel service is marked "A" for alcohol (ethanol) and "G" for gasoline. A bus fueled by biodiesel Biofuels include a wide range of fuels which are derived from biomass. The term covers solid , liquid , and gaseous fuels. [108] Liquid biofuels include bioalcohols, such as bioethanol, and oils, such as biodiesel. Gaseous biofuels include biogas , landfill gas and synthetic gas . Bioethanol is an alcohol made by fermenting the sugar components of plant materials and it is made mostly from sugar and starch crops. These include maize, sugarcane and, more recently, sweet sorghum . The latter crop is particularly suitable for growing in dryland conditions, and is being investigated by International Crops Research Institute for the Semi-Arid Tropics for its potential to provide fuel, along with food and animal feed, in arid parts of Asia and Africa. [109] With advanced technology being developed, cellulosic biomass, such as trees and grasses, are also used as feedstocks for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the United States and in Brazil . The energy costs for producing bio-ethanol are almost equal to, the energy yields from bio-ethanol. However, according to the European Environment Agency , biofuels do not address global warming concerns. [110] Biodiesel is made from vegetable oils , animal fats or recycled greases. It can be used as a fuel for vehicles in its pure form, or more commonly as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe. Biofuels provided 2.7% of the world's transport fuel in 2010. [111] [ needs update ] Policies in more than 80 countries support biofuels demand. [69] Since the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter. [112] Brazil's ethanol fuel program uses modern equipment and cheap sugarcane as feedstock, and the residual cane-waste ( bagasse ) is used to produce heat and power. [113] There are no longer light vehicles in Brazil running on pure gasoline. [114] Biojet is expected to be important for short-term reduction of carbon dioxide emissions from long-haul flights. [115] Steam rising from the Nesjavellir Geothermal Power Station in Iceland Geothermal plant at The Geysers , California, US Global electricity power generation capacity 14.9 GW (2023) [116] Levelized cost per megawatt hour USD 58.257 (2019) [119] Dry steam, flash steam, and binary cycle power stations Other energy applications Heating High temperature geothermal energy is from thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. Earth's geothermal energy originates from the original formation of the planet and from radioactive decay of minerals (in currently uncertain [120] but possibly roughly equal [121] proportions). The geothermal gradient , which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective geothermal originates from the Greek roots geo, meaning earth, and thermos, meaning heat. The heat that is used for geothermal energy can be from deep within the Earth, all the way down to Earth's core – 6,400 kilometres (4,000 mi) down. At the core, temperatures may reach over 5,000 °C (9,030 °F). Heat conducts from the core to the surrounding rock. Extremely high temperature and pressure cause some rock to melt, which is commonly known as magma. Magma convects upward since it is lighter than the solid rock. This magma then heats rock and water in the crust, sometimes up to 371 °C (700 °F). [122] Low temperature geothermal [123] refers to the use of the outer crust of the Earth as a thermal battery to facilitate renewable thermal energy for heating and cooling buildings, and other refrigeration and industrial uses. In this form of geothermal, a geothermal heat pump and ground-coupled heat exchanger are used together to move heat energy into the Earth (for cooling) and out of the Earth (for heating) on a varying seasonal basis. Low-temperature geothermal (generally referred to as "GHP"[ clarification needed ]) is an increasingly important renewable technology because it both reduces total annual energy loads associated with heating and cooling, and it also flattens the electric demand curve eliminating the extreme summer and winter peak electric supply requirements. Thus low temperature geothermal/GHP is becoming an increasing national[ clarification needed ] priority with multiple tax credit support [124] and focus as part of the ongoing movement toward net zero energy. [125] Geothermal power is cost effective, reliable, sustainable, and environmentally friendly, [126] but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are usually much lower per energy unit than those of fossil fuels. In 2017, the United States led the world in geothermal electricity production with 12.9 GW of installed capacity. [68] The largest group of geothermal power plants in the world is located at The Geysers , a geothermal field in California. [127] The Philippines follows the US as the second highest producer of geothermal power in the world, with 1.9 GW of capacity online. [68] Global geothermal capacity in 2021 was 15 GW. [69] Emerging technologies[ edit ] There are also other renewable energy technologies that are still under development, including concentrated solar power , cellulosic ethanol , hot-dry-rock geothermal power, and marine energy . [128] [129] These technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research, development and demonstration (RD&D) funding. [129] There are numerous organizations within the academic, federal,[ clarification needed ] and commercial sectors conducting large-scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields. [130] Multiple government supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners. [131] Enhanced geothermal system[ edit ] Main article: Enhanced geothermal systems Enhanced geothermal systems (EGS) are a new type of geothermal power technology that does not require natural convective hydrothermal resources. The vast majority of geothermal energy within drilling reach is in dry and non-porous rock. [132] EGS technologies "enhance" and/or create geothermal resources in this "hot dry rock (HDR)" through hydraulic fracturing . EGS and HDR technologies, such as hydrothermal geothermal, are expected to be baseload resources that produce power 24 hours a day like a fossil plant. Distinct from hydrothermal, HDR and EGS may be feasible anywhere in the world, depending on the economic limits of drill depth. Good locations are over deep granite covered by a thick (3–5 km or 1.9–3.1 mi) layer of insulating sediments which slow heat loss. [133] There are HDR and EGS systems currently being developed and tested in France, Australia, Japan, Germany, the U.S., and Switzerland. The largest EGS project in the world is a 25 megawatt demonstration plant currently being developed in the Cooper Basin, Australia. The Cooper Basin has the potential to generate 5,000–10,000 MW. Main article: Marine energy Marine energy (also sometimes referred to as ocean energy) is the energy carried by ocean waves , tides , salinity , and ocean temperature differences . The movement of water in the world's oceans creates a vast store of kinetic energy , or energy in motion. This energy can be harnessed to generate electricity to power homes, transport and industries. The term marine energy encompasses wave power – power from surface waves, marine current power - power from marine hydrokinetic streams (e.g., the Gulf Stream), and tidal power – obtained from the kinetic energy of large bodies of moving water. Reverse electrodialysis (RED) is a technology for generating electricity by mixing fresh river water and salty sea water in large power cells designed for this purpose; as of 2016, it is being tested at a small scale (50 kW). Offshore wind power is not a form of marine energy, as wind power is derived from the wind , even if the wind turbines are placed over water. The oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world. [134] [135] [ page needed ] Station Sihwa Lake Tidal Power Station South Korea Rance Tidal Power Station France Annapolis Royal Generating Station Canada Passive daytime radiative cooling can cool temperatures with zero energy consumption or pollution. [138] Passive daytime radiative cooling[ edit ] Main article: Passive daytime radiative cooling Passive daytime radiative cooling (PDRC) uses the coldness of outer space as a renewable energy source to achieve daytime cooling that can be used in many applications, [139] [140] [141] such as indoor space cooling , [142] [143] outdoor urban heat island mitigation, [144] [145] and solar cell efficiency . [146] [147] PDRC surfaces are designed to be high in solar reflectance to minimize heat gain and strong in longwave infrared (LWIR) thermal radiation heat transfer . [148] On a planetary scale, it has been proposed as a way to slow and reverse global warming . [138] [149] PDRC applications are deployed as sky-facing surfaces, similar to other renewable energy sources such as photovoltaic systems and solar thermal collectors . [147] PDRC became possible with the ability to suppress solar heating using photonic metamaterials , first published in a study by Raman et al. to the scientific community in 2014. [146] [150] PDRC applications for indoor space cooling is growing with an estimated "market size of ~$27 billion in 2025." [151] Earth infrared thermal radiation[ edit ] Earth emits roughly 1017 W of infrared thermal radiation that flows toward the cold outer space. Solar energy hits the surface and atmosphere of the earth and produces heat. Using various theorized devices like emissive energy harvester (EEH) or thermoradiative diode, this energy flow can be converted into electricity. In theory, this technology can be used during nighttime. [152] [153] Main article: Algae fuels Producing liquid fuels from oil-rich (fat-rich) varieties of algae is an ongoing research topic. Various microalgae grown in open or closed systems are being tried including some systems that can be set up in brownfield and desert lands. [154] Water vapor[ edit ] Collection of static electricity charges from water droplets on metal surfaces is an experimental technology that would be especially useful in low-income countries with relative air humidity over 60%. [155] Nuclear energy[ edit ] Breeder reactors could, in principle, extract almost all of the energy contained in uranium or thorium , decreasing fuel requirements by a factor of 100 compared to widely used once-through light water reactors , which extract less than 1% of the energy in the actinide metal (uranium or thorium) mined from the earth. [156] The high fuel-efficiency of breeder reactors could greatly reduce concerns about fuel supply, energy used in mining, and storage of radioactive waste . With seawater uranium extraction (currently too expensive to be economical), there is enough fuel for breeder reactors to satisfy the world's energy needs for 5 billion years at 1983's total energy consumption rate, thus making nuclear energy effectively a renewable energy. [157] [158] In addition to seawater the average crustal granite rocks contain significant quantities of uranium and thorium that with breeder reactors can supply abundant energy for the remaining lifespan of the sun on the main sequence of stellar evolution. [159] Main article: Artificial photosynthesis Artificial photosynthesis uses techniques including nanotechnology to store solar electromagnetic energy in chemical bonds by splitting water to produce hydrogen and then using carbon dioxide to make methanol. [160] Researchers in this field strived to design molecular mimics of photosynthesis that use a wider region of the solar spectrum, employ catalytic systems made from abundant, inexpensive materials that are robust, readily repaired, non-toxic, stable in a variety of environmental conditions and perform more efficiently allowing a greater proportion of photon energy to end up in the storage compounds, i.e., carbohydrates (rather than building and sustaining living cells). [161] However, prominent research faces hurdles, Sun Catalytix a MIT spin-off stopped scaling up their prototype fuel-cell in 2012 because it offers few savings over other ways to make hydrogen from sunlight. [162] Consumption by sector[ edit ] One of the efforts to decarbonize transportation is the increased use of electric vehicles (EVs). [163] Despite that and the use of biofuels , such as biojet , less than 4% of transport energy is from renewables. [164] Occasionally hydrogen fuel cells are used for heavy transport. [165] Meanwhile, in the future electrofuels may also play a greater role in decarbonizing hard-to-abate sectors like aviation and maritime shipping. [166] Solar water heating makes an important contribution to renewable heat in many countries, most notably in China, which now has 70% of the global total (180 GWth). Most of these systems are installed on multi-family apartment buildings [167] and meet a portion of the hot water needs of an estimated 50–60 million households in China. Worldwide, total installed solar water heating systems meet a portion of the water heating needs of over 70 million households. Heat pumps provide both heating and cooling, and also flatten the electric demand curve and are thus an increasing priority. [123] Renewable thermal energy is also growing rapidly. [168] About 10% of heating and cooling energy is from renewables. [169] Integration into the energy system and sector coupling[ edit ] Main article: Variable renewable energy Estimated power demand over a week in May 2012 and May 2020, Germany, showing the need for dispatchable generation rather than baseload generation in the grid[ clarification needed ] Renewable energy production from some sources such as wind and solar is more variable and more geographically spread than technology based on fossil fuels and nuclear. While integrating it into the wider energy system is feasible, it does lead to some additional challenges such as increased production volatility and decreased system inertia. [170] Implementation of energy storage, using a wide variety of renewable energy technologies, and implementing a smart grid in which energy is automatically used at the moment it is produced can reduce risks and costs of renewable energy implementation. [170] [171] : 15–16 Sector coupling of the power generation sector with other sectors may increase flexibility: for example the transport sector can be coupled by charging electric vehicles and sending electricity from vehicle to grid . [172] Similarly the industry sector can be coupled by hydrogen produced by electrolysis, [173] and the buildings sector by thermal energy storage for space heating and cooling. [174] Electrical energy storage[ edit ] Main articles: Energy storage and Grid energy storage Electrical energy storage is a collection of methods used to store electrical energy. Electrical energy is stored during times when production (especially from intermittent sources such as wind power , tidal power , solar power ) exceeds consumption, and returned to the grid when production falls below consumption. Pumped-storage hydroelectricity accounts for more than 85% of all grid power storage . [175] Batteries are increasingly being deployed for storage [176] and grid ancillary services [177] and for domestic storage. [178] Green hydrogen is a more economical means of long-term renewable energy storage, in terms of capital expenditures compared to pumped hydroelectric or batteries. [179] [180] Main article: Renewable energy commercialization Most new renewables are solar, followed by wind then hydro then bioenergy. [181] Investment in renewables, especially solar, tends to be more effective in creating jobs than coal, gas or oil. [182] [183] Worldwide, renewables employ about 12 million people as of 2020, with solar PV being the technology employing the most at almost 4 million. [184] However, as of February 2024, the world's supply of workforce for solar energy is lagging greatly behind demand as universities worldwide still produce more workforce for fossil fuels than for renewable energy industries. [185] Cost comparison[ edit ] The International Renewable Energy Agency (IRENA) stated that ~86% (187 GW) of renewable capacity added in 2022 had lower costs than electricity generated from fossil fuels. [186] IRENA also stated that capacity added since 2000 reduced electricity bills in 2022 by at least $520 billion, and that in non-OECD countries, the lifetime savings of 2022 capacity additions will reduce costs by up to $580 billion. [186] Av. auction pricesUS¢/kWh [190] Cost development2010–2019 [189] * = 2018. All other values for 2019. Growth of renewables[ edit ] Investment and sources Investment: Companies, governments and households have committed increasing amounts to decarbonization, including renewable energy (solar, wind), electric vehicles and associated charging infrastructure, energy storage, energy-efficient heating systems, carbon capture and storage, and hydrogen. [191] [192] Clean energy investment has benefited from post-pandemic economic recovery, a global energy crisis involving high fossil fuel prices, and growing policy support across various nations. [193] The countries most reliant on fossil fuels for electricity vary widely on how great a portion of that electricity is generated from renewables, leaving wide variation in renewables' growth potential. [194] Costs Levelized cost: With increasingly widespread implementation of renewable energy sources, costs have declined, most notably for energy generated by solar panels. [195] [196] Levelized cost of energy (LCOE) is a measure of the average net present cost of electricity generation for a generating plant over its lifetime. Past costs of producing renewable energy declined significantly, [197] with 62% of total renewable power generation added in 2020 having lower costs than the cheapest new fossil fuel option. [198] "Learning curves": Trend of costs and deployment over time, with steeper lines showing greater cost reductions as deployment progresses. [199] With increased deployment, renewables benefit from learning curves and economies of scale . [200] The results of a recent review of the literature concluded that as greenhouse gas (GHG) emitters begin to be held liable for damages resulting from GHG emissions resulting in climate change, a high value for liability mitigation would provide powerful incentives for deployment of renewable energy technologies. [201] In the decade of 2010–2019, worldwide investment in renewable energy capacity excluding large hydropower amounted to US$2.7 trillion, of which the top countries China contributed US$818 billion, the United States contributed US$392.3 billion, Japan contributed US$210.9 billion, Germany contributed US$183.4 billion, and the United Kingdom contributed US$126.5 billion. [202] This was an increase of over three and possibly four times the equivalent amount invested in the decade of 2000–2009 (no data is available for 2000–2003). [202] As of 2022, an estimated 28% of the world's electricity was generated by renewables. This is up from 19% in 1990. [203] See also: Energy transition In 2023, electricity generation from wind and solar sources was projected to exceed 30% by 2030. [204] A December 2022 report by the IEA forecasts that over 2022-2027, renewables are seen growing by almost 2 400 GW in its main forecast, equal to the entire installed power capacity of China in 2021. This is an 85% acceleration from the previous five years, and almost 30% higher than what the IEA forecast in its 2021 report, making its largest ever upward revision. Renewables are set to account for over 90% of global electricity capacity expansion over the forecast period. [69] To achieve net zero emissions by 2050, IEA believes that 90% of global electricity generation will need to be produced from renewable sources. [24] In June 2022 IEA Executive Director Fatih Birol said that countries should invest more in renewables to "ease the pressure on consumers from high fossil fuel prices, make our energy systems more secure, and get the world on track to reach our climate goals.” [205] China's five year plan to 2025 includes increasing direct heating by renewables such as geothermal and solar thermal. [206] REPowerEU , the EU plan to escape dependence on fossil Russian gas , is expected to call for much more green hydrogen . [207] After a transitional period, [208] renewable energy production is expected to make up most of the world's energy production. In 2018, the risk management firm, DNV GL , forecasts that the world's primary energy mix will be split equally between fossil and non-fossil sources by 2050. [209] Demand[ edit ] In July 2014, WWF and the World Resources Institute convened a discussion among a number of major US companies who had declared their intention to increase their use of renewable energy. These discussions identified a number of "principles" which companies seeking greater access to renewable energy considered important market deliverables. These principles included choice (between suppliers and between products), cost competitiveness, longer term fixed price supplies, access to third-party financing vehicles, and collaboration. [210] UK statistics released in September 2020 noted that "the proportion of demand met from renewables varies from a low of 3.4 per cent (for transport, mainly from biofuels) to highs of over 20 per cent for 'other final users', which is largely the service and commercial sectors that consume relatively large quantities of electricity, and industry". [211] In some locations, individual households can opt to purchase renewable energy through a consumer green energy program . Shop selling PV panels in Ouagadougou , Burkina Faso Solar cookers use sunlight as energy source for outdoor cooking. Renewable energy in developing countries is an increasingly used alternative to fossil fuel energy, as these countries scale up their energy supplies and address energy poverty . Renewable energy technology was once seen as unaffordable for developing countries. [212] However, since 2015, investment in non-hydro renewable energy has been higher in developing countries than in developed countries, and comprised 54% of global renewable energy investment in 2019. [213] The International Energy Agency forecasts that renewable energy will provide the majority of energy supply growth through 2030 in Africa and Central and South America, and 42% of supply growth in China. [214] Most developing countries have abundant renewable energy resources, including solar energy , wind power , geothermal energy , and biomass , as well as the ability to manufacture the relatively labor-intensive systems that harness these. By developing such energy sources developing countries can reduce their dependence on oil and natural gas, creating energy portfolios that are less vulnerable to price rises. In many circumstances, these investments can be less expensive than fossil fuel energy systems. [215] Policy[ edit ] Share of electricity production from renewables, 2022 [45] Policies to support renewable energy have been vital in their expansion. Where Europe dominated in establishing energy policy in the early 2000s, most countries around the world now have some form of energy policy. [219] Policy trends[ edit ] The International Renewable Energy Agency (IRENA) is an intergovernmental organization for promoting the adoption of renewable energy worldwide. It aims to provide concrete policy advice and facilitate capacity building and technology transfer. IRENA was formed in 2009, with 75 countries signing the charter of IRENA. [220] As of April 2019, IRENA has 160 member states. [221] The then United Nations Secretary-General Ban Ki-moon has said that renewable energy can lift the poorest nations to new levels of prosperity, [222] and in September 2011 he launched the UN Sustainable Energy for All initiative to improve energy access, efficiency and the deployment of renewable energy. [223] The 2015 Paris Agreement on climate change motivated many countries to develop or improve renewable energy policies. [23] In 2017, a total of 121 countries adopted some form of renewable energy policy. [219] National targets that year existed in 176 countries. [23] In addition, there is also a wide range of policies at the state/provincial, and local levels. [111] Some public utilities help plan or install residential energy upgrades . Many national, state and local governments have created green banks . A green bank is a quasi-public financial institution that uses public capital to leverage private investment in clean energy technologies. [224] Green banks use a variety of financial tools to bridge market gaps that hinder the deployment of clean energy. Climate neutrality by the year 2050 is the main goal of the European Green Deal . [225] For the European Union to reach their target of climate neutrality, one goal is to decarbonise its energy system by aiming to achieve "net-zero greenhouse gas emissions by 2050." [226] Full renewable energy[ edit ] These paragraphs are an excerpt from 100% renewable energy .[ edit ] 100% renewable energy is the goal of the use renewable resources for all energy. 100% renewable energy for electricity, heating, cooling and transport is motivated by climate change , pollution and other environmental issues, as well as economic and energy security concerns. Shifting the total global primary energy supply to renewable sources requires a transition of the energy system , since most of today's energy is derived from non-renewable fossil fuels . Research into this topic is fairly new, with very few studies published before 2009, but has gained increasing attention in recent years. The majority of studies show that a global transition to 100% renewable energy across all sectors – power, heat, transport and industry – is feasible and economically viable. [227] [228] [229] [230] [ need quotation to verify ] A cross-sectoral, holistic approach is seen as an important feature of 100% renewable energy systems and is based on the assumption "that the best solutions can be found only if one focuses on the synergies between the sectors" of the energy system such as electricity, heat, transport or industry. [231] The main barriers to the widespread implementation of large-scale renewable energy and low-carbon energy strategies are seen to be primarily social and political rather than technological or economic. [232] According to the 2013 Post Carbon Pathways report, which reviewed many international studies, the key roadblocks are: climate change denial , the fossil fuels lobby , political inaction, unsustainable energy consumption , outdated energy infrastructure , and financial constraints. [233] Finance[ edit ] The International Renewable Energy Agency 's (IRENA) 2023 report on renewable energy finance highlights steady investment growth since 2018: USD 348 billion in 2020 (a 5.6% increase from 2019), USD 430 billion in 2021 (24% up from 2020), and USD 499 billion in 2022 (16% higher). This trend is driven by increasing recognition of renewable energy's role in mitigating climate change and enhancing energy security , along with investor interest in alternatives to fossil fuels. Policies such as feed-in tariffs in China and Vietnam have significantly increased renewable adoption. Furthermore, from 2013 to 2022, installation costs for solar photovoltaic (PV), onshore wind, and offshore wind fell by 69%, 33%, and 45%, respectively, making renewables more cost-effective. [234] [235] Solar[ edit ] From 2020 to 2022, solar technology investments almost doubled from USD 162 billion to USD 308 billion, driven by the sector's increasing maturity and cost reductions, particularly in solar photovoltaic (PV), which accounted for 90% of total investments. China and the United States were the main recipients, collectively making up about half of all solar investments since 2013. Despite reductions in Japan and India due to policy changes and COVID-19 , growth in China, the United States, and a significant increase from Vietnam's feed-in tariff program offset these declines. Globally, the solar sector added 714 gigawatts (GW) of solar PV and concentrated solar power (CSP) capacity between 2013 and 2021, with a notable rise in large-scale solar heating installations in 2021, especially in China, Europe, Turkey, and Mexico. [235] Wind[ edit ] Investments in wind technologies reached USD 161 billion in 2020, with onshore wind dominating at 80% of total investments from 2013 to 2022. Offshore wind investments nearly doubled to USD 41 billion between 2019 and 2020, primarily due to policy incentives in China and expansion in Europe. Global wind capacity increased by 557 GW between 2013 and 2021, with capacity additions increasing by an average of 19% each year. [235] Other renewable energy[ edit ] Between 2013 and 2022, the renewable energy sector underwent a significant realignment of investment priorities. Investment in solar and wind energy technologies markedly increased. In contrast, other renewable technologies such as hydropower (including pumped storage hydropower ), biomass , biofuels , geothermal , and marine energy experienced a substantial decrease in financial investment. Notably, from 2017 to 2022, investment in these alternative renewable technologies declined by 45%, falling from USD 35 billion to USD 17 billion. [235] Further information: Climate change mitigation § Overviews, strategies and comparisons of measures Most respondents to a climate survey conducted in 2021-2022 by the European Investment Bank say countries should back renewable energy to fight climate change. [236] The same survey a year later shows that renewable energy is considered an investment priority in the European Union, China and the United States [237] Renewable electricity generation by wind and solar is variable . This results in reduced capacity factor and may require keeping some gas-fired power plants or other dispatchable generation on standby [238] [239] [240] until there is enough energy storage, demand response , grid improvement, and/or base load power from non-intermittent sources like hydropower , nuclear power or bioenergy. The market for renewable energy technologies has continued to grow. Climate change concerns and increasing in green jobs , coupled with high oil prices, peak oil , oil wars, oil spills , promotion of electric vehicles and renewable electricity, nuclear disasters and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization . [30] [ better source needed ] The International Energy Agency has stated that deployment of renewable technologies usually increases the diversity of electricity sources and, through local generation, contributes to the flexibility of the system and its resistance to central shocks. [241] Public support[ edit ] Acceptance of wind and solar facilities in one's community is stronger among U.S. Democrats (blue), while acceptance of nuclear power plants is stronger among U.S. Republicans (red). [242] Solar power plants may compete with arable land , [243] [244] while on-shore wind farms face opposition due to aesthetic concerns and noise, which is impacting both humans and wildlife. [245] [246] [247] [ need quotation to verify ]In the United States, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive. According to a town councilor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area. [248] These concerns, when directed against renewable energy, are sometimes described as "not in my back yard" attitude ( NIMBY ). A 2011 UK Government document states that "projects are generally more likely to succeed if they have broad public support and the consent of local communities. This means giving communities both a say and a stake". [249] In countries such as Germany and Denmark many renewable projects are owned by communities, particularly through cooperative structures, and contribute significantly to overall levels of renewable energy deployment. [250] [251] Nuclear power proposed as renewable energy[ edit ] This section is an excerpt from Nuclear power proposed as renewable energy .[ edit ] Whether nuclear power should be considered a form of renewable energy is an ongoing subject of debate. Statutory definitions of renewable energy usually exclude many present nuclear energy technologies, with the notable exception of the state of Utah . [252] Dictionary-sourced definitions of renewable energy technologies often omit or explicitly exclude mention of nuclear energy sources, with an exception made for the natural nuclear decay heat generated within the Earth . [253] [254] The most common fuel used in conventional nuclear fission power stations , uranium-235 is "non-renewable" according to the Energy Information Administration , the organization however is silent on the recycled MOX fuel . [254] The National Renewable Energy Laboratory does not mention nuclear power in its "energy basics" definition. [255] In 1987, the Brundtland Commission (WCED) classified fission reactors that produce more fissile nuclear fuel than they consume ( breeder reactors , and if developed, fusion power ) among conventional renewable energy sources , such as solar power and hydropower . [256] The monitoring and storage of radioactive waste products is also required upon the use of other renewable energy sources, such as geothermal energy. [257] A concept of a super grid The geopolitical impact of the growing use of renewable energy is a subject of ongoing debate and research. [258] In the long term, observers expect fossil fuel-exporting countries to see a decline in their global influence, [259] while nations abundant in renewable resources, and the minerals required for renewables technology, could gain strength. [260] [261] Most analysts consider the biggest countries at risk of losing economic and geopolitical influence to be Nigeria , Russia , Saudi Arabia and Venezuela . [262] Transitions to renewable energy have many geopolitical implications such as the potential of revenue losses leading to political instability in insufficiently prepared fossil-fuel-exporting economies, albeit it is unclear whether the transition will increase or reduce conflict overall. In particular, a study hypothesizes that a "configuration emerges in which fossil fuel importers are better off decarbonizing, competitive fossil fuel exporters are better off flooding markets and uncompetitive fossil fuel producers—rather than benefitting from 'free-riding'—suffer from their exposure to stranded assets and lack of investment in decarbonization technologies". [263] [264] A study found that transition from fossil fuels to renewable energy systems reduces risks from mining, trade and political dependence because renewable energy systems don't need fuel – they depend on trade only for the acquisition of materials and components during construction. [265] Nations rich in solar and wind energy could become major energy exporters. [266] Trade in hydrogen could fundamentally redraw the geography of the global energy trade, and international governance and investments that seek to scale up the hydrogen economy could reduce "the risk of market fragmentation, carbon lock-in, and intensified geo-economic rivalry". [267] [266] [268] Electricity will overtake other energy carriers by 2050, accounting for almost 50% of total energy consumption (up from 22% in 2015). Given the limitations of using solely electricity, clean hydrogen has significant potential in a number of industries. [269] [270] Hydrogen has the potential to be long-term stored in the electricity and heating industries. [271] In 2019, oil and gas companies were listed by Forbes with sales of US$4.8 trillion, about 5% of the global GDP . [272] Net importers such as China and the EU would gain advantages from a transition to low-carbon technologies driven by technological development, energy efficiency or climate change policy, while Russia, the USA or Canada could see their fossil fuel industries nearly shut down. [273] On the other hand, countries with large areas such as Australia, Russia, China, the US, Canada and Brazil and also Africa and the Middle East have a potential for huge installations of renewable energy. The production of renewable energy technologies requires rare-earth elements with new supply chains. [274] In October 2021, European Commissioner for Climate Action Frans Timmermans suggested "the best answer" to the 2021 global energy crisis is "to reduce our reliance on fossil fuels." [275] He said those blaming the European Green Deal were doing so "for perhaps ideological reasons or sometimes economic reasons in protecting their vested interests." [275] Some critics blamed the European Union Emissions Trading System (EU ETS) and closure of nuclear plants for contributing to the energy crisis. [276] [277] [278] European Commission President Ursula von der Leyen said that Europe is "too reliant" on natural gas and too dependent on natural gas imports . According to Von der Leyen, "The answer has to do with diversifying our suppliers ... and, crucially, with speeding up the transition to clean energy." [279] Metal and mineral extraction[ edit ] The renewable energy transition requires increased extraction of certain metals and minerals . [280] Solar power panels require large amounts of aluminum. [281] This impacts the environment and can lead to environmental conflict . [282] The International Energy Agency does not recognise shortages of resources but states that supply could struggle to keep pace with the world's climate ambitions. Electric vehicles (EV) and battery storage are expected to cause the most demand. Wind farms and solar PV are less consuming. The extension of electrical grids requires large amounts of copper and aluminium . The IEA recommends to scale up recycling. By 2040, quantities of copper , lithium , cobalt, and nickel from spent batteries could reduce combined primary supply requirements for these minerals by around 10%. [280] The demand for lithium by 2040 is expected to grow by the factor of 42. Graphite and nickel exploration is predicted to grow about 20-fold. For each of the most relevant minerals and metals, a significant share of resources are concentrated in only one country: copper in Chile , nickel in Indonesia , rare earths in China , cobalt in the Democratic Republic of the Congo (DRC) , and lithium in Australia . China dominates processing of them all. [280] A controversial approach is deep sea mining . Minerals can be collected from new sources like polymetallic nodules lying on the seabed , [283] but this could damage biodiversity. [284] Health and environmental impact[ edit ] Moving to modern renewable energy has very large health benefits due to reducing air pollution from fossil fuels. [285] [286] [287] [288] [289] [290] Renewable sources other than biomass such as wind power , photovoltaics , and hydroelectricity have the advantage of being able to conserve water, lower pollution [291] and reduce CO2 emissions. Solar panels change the albedo of the surface, so if used on a very large scale (such as covering 20% of the Sahara Desert), could change global weather patterns. [292] Conservation areas, recycling and rare-earth elements[ edit ] See also: Environmental footprint of electric cars Installations used to produce wind, solar and hydropower are an increasing threat to key conservation areas, with facilities built in areas set aside for nature conservation and other environmentally sensitive areas. They are often much larger than fossil fuel power plants, needing areas of land up to 10 times greater than coal or gas to produce equivalent energy amounts. [293] More than 2000 renewable energy facilities are built, and more are under construction, in areas of environmental importance and threaten the habitats of plant and animal species across the globe. The authors' team emphasized that their work should not be interpreted as anti-renewables because renewable energy is crucial for reducing carbon emissions. The key is ensuring that renewable energy facilities are built in places where they do not damage biodiversity. [294] The transition to renewable energy depends on non-renewable resources, such as mined metals. [243] Manufacturing of photovoltaic panels, wind turbines and batteries requires significant amounts of rare-earth elements [295] which has significant social and environmental impact if mined in forests and protected areas. [296] Due to co-occurrence of rare-earth and radioactive elements ( thorium , uranium and radium ), rare-earth mining results in production of low-level radioactive waste . [297] In 2020 scientists published a world map of areas that contain renewable energy materials as well as estimations of their overlaps with "Key Biodiversity Areas", "Remaining Wilderness" and " Protected Areas ". The authors assessed that careful strategic planning is needed. [298] [299] [300] Solar panels are recycled to reduce electronic waste and create a source for materials that would otherwise need to be mined, [301] but such business is still small and work is ongoing to improve and scale-up the process. [302] [303] [304] History[ edit ] Prior to the development of coal in the mid 19th century, nearly all energy used was renewable. The oldest known use of renewable energy, in the form of traditional biomass to fuel fires , dates from more than a million years ago. The use of biomass for fire did not become commonplace until many hundreds of thousands of years later. [305] Probably the second oldest usage of renewable energy is harnessing the wind in order to drive ships over water. This practice can be traced back some 7000 years, to ships in the Persian Gulf and on the Nile. [306] From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times. [307] Moving into the time of recorded history, the primary sources of traditional renewable energy were human labor , animal power , water power , wind, in grain crushing windmills , and firewood , a traditional biomass. In 1885, Werner Siemens , commenting on the discovery of the photovoltaic effect in the solid state, wrote: In conclusion, I would say that however great the scientific importance of this discovery may be, its practical value will be no less obvious when we reflect that the supply of solar energy is both without limit and without cost, and that it will continue to pour down upon us for countless ages after all the coal deposits of the earth have been exhausted and forgotten. [308] Max Weber mentioned the end of fossil fuel in the concluding paragraphs of his Die protestantische Ethik und der Geist des Kapitalismus (The Protestant Ethic and the Spirit of Capitalism), published in 1905. [309] Development of solar engines continued until the outbreak of World War I. The importance of solar energy was recognized in a 1911 Scientific American article: "in the far distant future, natural fuels having been exhausted [solar power] will remain as the only means of existence of the human race". [310] The theory of peak oil was published in 1956. [311] In the 1970s environmentalists promoted the development of renewable energy both as a replacement for the eventual depletion of oil , as well as for an escape from dependence on oil, and the first electricity-generating wind turbines appeared. Solar had long been used for heating and cooling, but solar panels were too costly to build solar farms until 1980. [312] New government spending, regulation and policies helped the industry weather the 2009 economic crisis better than many other sectors. [44]
Toggle the table of contents Environmental education From Wikipedia, the free encyclopedia Branch of pedagogy Moroccan students watching birds at Nador lagoon during the activities organized by SEO/BirdLife during the World Wetlands Day in Morocco e Environmental education (EE) refers to organized efforts to teach how natural environments function, and particularly, how human beings can manage behavior and ecosystems to live sustainably . It is a multi-disciplinary field integrating disciplines such as biology, chemistry, physics, ecology, earth science, atmospheric science, mathematics, and geography. The United Nations Educational, Scientific and Cultural Organization (UNESCO) states that EE is vital in imparting an inherent respect for nature among society and in enhancing public environmental awareness. UNESCO emphasises the role of EE in safeguarding future global developments of societal quality of life (QOL), through the protection of the environment, eradication of poverty , minimization of inequalities and insurance of sustainable development . [1] The term often implies education within the school system, from primary to post-secondary. However, it sometimes includes all efforts to educate the public and other audiences, including print materials, websites, media campaigns, etc.. There are also ways that environmental education is taught outside the traditional classroom. Aquariums, zoos, parks, and nature centers all have ways of teaching the public about the environment. UNESCO and environmental awareness and education[ edit ] UNESCO 's involvement in environmental awareness and education goes back to the very beginnings of the Organization, with the creation in 1948 of the IUCN (International Union for the Conservation of Nature, now the World Conservation Union), the first major non-governmental organization (NGO) mandated to help preserve the natural environment . UNESCO was also closely involved in convening the United Nations Conference on the Human Environment in Stockholm, Sweden in 1972, which led to the setting up of the United Nations Environment Programme (UNEP). Subsequently, for two decades, UNESCO and UNEP led the International Environmental Education Programme (1975-1995), which set out a vision for, and gave practical guidance on how to mobilize education for environmental awareness. In 1976, UNESCO launched an environmental education newsletter 'Connect' as the official organ of the UNESCO-UNEP International Environmental Education Programme (IEEP). It served as a clearinghouse to exchange information on Environmental Education (EE) in general and to promote the aims and activities of the IEEP in particular, as well as being a network for institutions and individuals interested and active in environment education until 2007. [2] The long-standing cooperation between UNESCO and UNEP on environmental education (and later ESD [a] ) also led to the co-organization of four major international conferences on environmental education since 1977: the First Intergovernmental Conference on Environmental Education in Tbilisi, Georgia (October 1977); the Conference "International Strategy for Action in the Field of Environmental Education and Training for the 1990s" in Moscow, Russian Federation (August 1987); the third International Conference "Environment and Society: Education and Public Awareness for Sustainability" at Thessaloniki, Greece (December 1997); and the Fourth International Conference on Environmental Education towards a Sustainable Future in Ahmedabad, India (November 2007). These meetings highlighted the pivotal role education plays in sustainable development . It was at the Tbilisi conference in 1977 that the essential role of 'education in environmental matters' (as stated in the recommendations of the 1972 Stockholm Conference) [3] was fully explored. Organized by UNESCO in cooperation with UNEP, this was the world's first intergovernmental conference on environmental education. In the subsequent Tbilisi Declaration, environment was interpreted in its 'totality—natural and built, technological and social (economic, political, cultural-historical, ethical, aesthetic)' (point 3). [4] The goals formulated for environmental education went far beyond ecology in the curriculum and included development of a 'clear awareness of, and concern about, economic, social, political, and ecological interdependence in urban and rural areas' (point 2) [5] which became one of the major bases of ESD. [2] 1. Engaging with citizens of all demographics to; 2. Think critically, ethically, and creatively when evaluating environmental issues; 3. Make educated judgments about those environmental issues; 4. Develop skills and a commitment to act independently and collectively to sustain and enhance the environment; and, 5. To enhance their appreciation of the environment; resulting in positive environmental behavioural change. [6] [7] Attributes[ edit ] There are a few central qualities involved in environmental education that are useful contributions to the individual. Environmental education: Enhances real-world problem solving. [8] Strengthens physical activity and diet quality. [9] Improves communication/leadership when working in groups. [10] [11] Careers[ edit ] There are various different career paths one could delve into within environmental education. Many of these careers require discovering and planning how to resolve environmental issues occurring in today's world. The specific responsibilities associated with each career will depend in part on their physical location, taking into account what environmental issue is most prevalent in the area. [12] A general outlook of some careers in this field are: Federal Government Park Ranger - Responsible for protecting the national parks , historical sites, and national seashores across the United States including the wildlife and ecosystems within them. There are many qualifications in order for one to become a park ranger and some include: obtaining a bachelor's degree and a passing grade in the PEB. [13] Some focuses within this field include: enforcing park rules, giving tours to groups for educational purposes, and protecting parks from forest fires. [14] Outdoor Education Teacher- Teach students by using outdoor field and classroom work. Some invite guest speakers who are experts in their field to help teach how the basic principles of science are implemented in the real world. [15] Some requirements for this career include becoming CPR certified and having a bachelor's degree in either environmental science or a field related to it. [16] It can be a problematic field as there is no concurrence on the central concepts that are taught as well as teachers do not agree on what constitutes an important environmental issue. [17] Environmental Scientist - Use of field work to research contamination in nature when writing plans in creating projects for environmental research. Environmental Scientists research topics such as air pollution, water quality, and wildlife. They also study how human health is affected by changes in the environment. Some requirements for this career are a bachelor's degree with a double major in environmental science and either biology, physics or chemistry. [18] [19] Environmental Engineer - Involves the combination of biology/chemistry with engineering to generate ways to ensure the health of the planet. Scientific research is analyzed and projects are designed as a result of that research in order to come up with solutions to issues of the environment like air pollution. A bachelor's degree in civil engineering or general engineering is required as well as some experience in this field. [12] Related fields[ edit ] Environmental education has crossover with multiple other disciplines. These fields of education complement environmental education yet have unique philosophies. Citizen Science (CS) aims to address both scientific and environmental outcomes through enlisting the public in the collection of data, through relatively simple protocols, generally from local habitats over long periods of time. [20] Education for Sustainable Development (ESD) aims to reorient education to empower individuals to make informed decisions for environmental integrity, social justice, and economic viability for both present and future generations, whilst respecting cultural diversities. [21] Climate Change Education (CCE) aims in enhancing the public's understanding of climate change , its consequences, and its problems, and to prepare current and future generations to limit the magnitude of climate change and to respond to its challenges. [22] Specifically, CCE needs to help learners develop knowledge, skills and values and action to engage and learn about the causes, impact and management of climate change. [23] Science Education (SE) focuses primarily on teaching knowledge and skills, to develop innovative thought in society. [6] Outdoor Education (OE) relies on the assumption that learning experiences outdoors in 'nature' foster an appreciation of nature, resulting in pro-environmental awareness and action. Outdoor education means learning "in" and "for" the outdoors. [24] Experiential education (ExE) is a process through which a learner constructs knowledge, skill, and value from direct experiences" Experiential education can be viewed as both a process and method to deliver the ideas and skills associated with environmental education. [25] Garden-based learning (GBL) is an instructional strategy that utilizes the garden as a teaching tool. It encompasses programs, activities and projects in which the garden is the foundation for integrated learning, in and across disciplines, through active, engaging, real-world experiences that have personal meaning for children, youth, adults and communities in an informal outside learning setting. Inquiry-based Science (IBS) is an active open style of teaching in which students follow scientific steps in a similar manner as scientists to study some problem. [26] Often used in biological and environmental settings. While each of these educational fields has their own objectives, there are points where they overlap with the intentions and philosophy of environmental education. History[ edit ] The roots of environmental education can be traced back as early as the 18th century when Jean-Jacques Rousseau stressed the importance of an education that focuses on the environment in Emile: or, On Education . Several decades later, Louis Agassiz , a Swiss-born naturalist, echoed Rousseau's philosophy as he encouraged students to "Study nature, not books." [27] These two influential scholars helped lay the foundation for a concrete environmental education program, known as nature study , which took place in the late 19th and early 20th century. The nature study movement used fables and moral lessons to help students develop an appreciation of nature and embrace the natural world. [28] Anna Botsford Comstock, the head of the Department of Nature Study at Cornell University, was a prominent figure in the nature study movement. She wrote the Handbook for Nature Study in 1911 which used nature to educate children on cultural values. [28] Comstock and the other leaders of the movement, such as Liberty Hyde Bailey, helped Nature Study garner tremendous amounts of support from community leaders, teachers, and scientists to change the science curriculum for children across the United States. A new type of environmental education, Conservation Education, emerged in the US as a result of the Great Depression and Dust Bowl during the 1920s and 1930s. Conservation Education dealt with the natural world in a drastically different way from Nature Study because it focused on rigorous scientific training rather than natural history. [28] Conservation Education was a major scientific management and planning tool that helped solve social, economic, and environmental problems during this time period. The modern environmental education movement, which gained significant momentum in the late 1960s and early 1970s, stems from Nature Study and Conservation Education. During this time period, many events—such as the Cold War , the Civil rights movement and the Vietnam War —placed many Americans at odds with one another and the U.S. government. However, as more people began to fear the fallout from radiation, the chemical pesticides mentioned in Rachel Carson's Silent Spring , and the significant amounts of air pollution and waste, the public's concern for their health and the health of their natural environment led to a unifying phenomenon known as environmentalism . Environmental education was born of the realization that solving complex local and global problems cannot be accomplished by politicians and experts alone, but requires "the support and active participation of an informed public in their various roles as consumers, voters, employers, and business and community leaders." [29] In 1960  the National Rural Studies Association (now known as the National Association for Environmental Education ) was established in the UK to promote environmental education and support teachers in incorporating sustainability into their curricula. One of the first articles about environmental education as a new movement appeared in the Phi Delta Kappan in 1969, authored by James A. Swan . [30] A definition of "Environmental Education" first appeared in The Journal of Environmental Education in 1969, written by William B. Stapp . [31] Stapp later went on to become the first Director of Environmental Education for UNESCO , and then the Global Rivers International Network . Ultimately, the first Earth Day on April 22, 1970 – a national teach-in about environmental problems – paved the way for the modern environmental education movement. Later that same year, President Nixon passed the National Environmental Education Act, which was intended to incorporate environmental education into K-12 schools. [32] Then, in 1971, the National Association for Environmental Education (now known as the North American Association for Environmental Education ) was created to improve environmental literacy by providing resources to teachers and promoting environmental education programs. Internationally, environmental education gained recognition when the UN Conference on the Human Environment held in Stockholm, Sweden, in 1972, declared environmental education must be used as a tool to address global environmental problems. The United Nations Education Scientific and Cultural Organization ( UNESCO ) and United Nations Environment Program ( UNEP ) created three major declarations that have guided the course of environmental education. In 2002, the United Nations Decade of Education for Sustainable Development 2005-2014 (UNDESD) was formed as a way to reconsider, excite, and change approaches to acting positively on global challenges. The Commission on Education and Communication (CEC) helped support the work of the UNDESD by composing a backbone structure for education for sustainability, which contained five major components. The components are "Imagining a better future", "Critical thinking and reflection", "Participation in decision making" and "Partnerships, and Systemic thinking". [33] On June 9–14, 2013, the seventh World Environmental Education Congress was held in Marrakesh, Morocco. The overall theme of the conference was "Environmental education and issues in cities and rural areas: seeking greater harmony", and incorporated 11 different areas of concern. The World Environmental Education Congress had 2,400 members, representing over 150 countries. This meeting was the first time ever that it had been held in an Arab country, and was put together by two different organizations, the Mohamed VI Foundation for Environmental Protection and the World Environmental Education Congress Permanent Secretariat in Italy. Topics addressed at the congress include stressing the importance of environmental education and its role to empower, establishing partnerships to promote environmental education, how to mainstream environmental and sustainability, and even how to make universities "greener". [34] Stockholm Declaration[ edit ] June 5–16, 1972 - The Declaration of the United Nations Conference on the Human Environment . The document was made up of 7 proclamations and 26 principles "to inspire and guide the peoples of the world in the preservation and enhancement of the human environment." Belgrade Charter[ edit ] October 13–22, 1975 - The Belgrade Charter [35] was the outcome of the International Workshop on Environmental Education held in Belgrade, Jugoslavia (now Serbia). The Belgrade Charter was built upon the Stockholm Declaration and adds goals, objectives, and guiding principles of environmental education programs. It defines an audience for environmental education, which includes the general public. Tbilisi Declaration[ edit ] October 14–26, 1977 - The Tbilisi Declaration "noted the unanimous accord in the important role of environmental education in the preservation and improvement of the world's environment, as well as in the sound and balanced development of the world's communities." The Tbilisi Declaration updated and clarified The Stockholm Declaration and The Belgrade Charter by including new goals, objectives, characteristics, and guiding principles of environmental education. Later that decade, in 1977, the Intergovernmental Conference on Environmental Education in Tbilisi, Georgian SSR , Soviet Union emphasized the role of Environmental Education in preserving and improving the global environment and sought to provide the framework and guidelines for environmental education. The Conference laid out the role, objectives, and characteristics of environmental education, and provided several goals and principles for environmental education. About[ edit ] This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (April 2021) ( Learn how and when to remove this template message ) Environmental education has been considered an additional or elective subject in much of traditional K-12 curriculum . At the elementary school level, environmental education can take the form of science enrichment curriculum, natural history field trips, community service projects, and participation in outdoor science schools. EE policies assist schools and organizations in developing and improving environmental education programs that provide citizens with an in-depth understanding of the environment. School related EE policies focus on three main components: curricula, green facilities, and training. Schools can integrate environmental education into their curricula with sufficient funding from EE policies. This approach – known as using the "environment as an integrating context" for learning – uses the local environment as a framework for teaching state and district education standards. In addition to funding environmental curricula in the classroom, environmental education policies allot the financial resources for hands-on, outdoor learning. These activities and lessons help address and mitigate " nature deficit disorder ", as well as encourage healthier lifestyles. Green schools, or green facility promotion, are another main component of environmental education policies. Greening school facilities cost, on average, a little less than 2 percent more than creating a traditional school, but payback from these energy efficient buildings occur within only a few years. [36] Environmental education policies help reduce the relatively small burden of the initial start-up costs for green schools. Green school policies also provide grants for modernization, renovation, or repair of older school facilities. Additionally, healthy food options are also a central aspect of green schools. These policies specifically focus on bringing freshly prepared food, made from high-quality, locally grown ingredients into schools. In secondary school , environmental curriculum can be a focused subject within the sciences or is a part of student interest groups or clubs. At the undergraduate and graduate level, it can be considered its own field within education, environmental studies, environmental science and policy, ecology, or human/cultural ecology programs. Environmental education is not restricted to in-class lesson plans. Children can learn about the environment in many ways. Experiential lessons in the school yard, field trips to national parks, after-school green clubs, and school-wide sustainability projects help make the environment an easily accessible topic. Furthermore, celebration of Earth Day or participation in EE week (run through the National Environmental Education Foundation) can help further environmental education. Effective programs promote a holistic approach and lead by example, using sustainable practices in the school to encourage students and parents to bring environmental education into their home. The final aspect of environmental education policies involves training individuals to thrive in a sustainable society. In addition to building a strong relationship with nature, citizens must have the skills and knowledge to succeed in a 21st-century workforce. Thus, environmental education policies fund both teacher training and worker training initiatives. Teachers train to effectively teach and incorporate environmental studies. On the other hand, the current workforce must be trained or re-trained so they can adapt to the new green economy . Environmental education policies that fund training programs are critical to educating citizens to prosper in a sustainable society. In the United States[ edit ] Main article: Environmental education in the United States Following the 1970s, non-governmental organizations that focused on environmental education continued to form and grow, the number of teachers implementing environmental education in their classrooms increased, and the movement gained stronger political backing. A critical move forward came when the United States Congress passed the National Environmental Education Act of 1990, which placed the Office of Environmental Education in the U.S. Environmental Protection Agency (EPA) and allowed the agency to create environmental education initiatives at the federal level. [37] EPA defines environmental education as "a process that allows individuals to explore environmental issues, engage in problem solving, and take action to improve the environment. As a result, individuals develop a deeper understanding of environmental issues and have the skills to make informed and responsible decisions." EPA has listed the components of what should be gained from EE: Awareness and sensitivity to the environment and environmental challenges Knowledge and understanding of the environment and environmental challenges Attitudes of concern for the environment and motivation to improve or maintain environmental quality Skills to identify and help resolve environmental challenges Participation in activities that lead to the resolution of environmental challenges. [38] Through the EPA Environmental Education (EE) Grant Program, public schools, communities agencies, and NGO's are eligible to receive federal funding for local educational projects that reflect the EPA's priorities: air quality, water quality, chemical safety, and public participation among the communities. [39] In the United States some of the antecedents of environmental education were the Nature Study movement, conservation education and school camping. Nature studies integrated academic approach with outdoor exploration. Conservation education raised awareness about the misuse of natural resources and the need for their preservation. George Perkins Marsh discoursed on humanity's integral part of the natural world. Governmental agencies such as the U.S. Forest Service and the EPA supported conservation efforts. Conservation ideals still guide environmental education today. School camping was exposure to the environment and use of resources outside of the classroom for educational purposes. The legacies of these antecedents are still present in the evolving arena of environmental education. [40] Obstacles[ edit ] A study of Ontario teachers explored obstacles to environmental education. [41] Through an internet-based survey questionnaire, 300 K-12 teachers from Ontario, Canada responded. Based on the results of the survey, the most significant challenges identified by the sample of Ontario teachers include over-crowded curriculum, lack of resources, low priority of environmental education in schools, limited access to the outdoors, student apathy to environmental issues, and the controversial nature of sociopolitical action. [41] An influential article by Stevenson outlines conflicting goals of environmental education and traditional schooling. [42] According to Stevenson, the recent critical and action orientation of environmental education creates a challenging task for schools. Contemporary environmental education strives to transform values that underlie decision making from ones that aid environmental (and human) degradation to those that support a sustainable planet. [43] [44] This contrasts with the traditional purpose of schools of conserving the existing social order by reproducing the norms and values that currently dominate environmental decision making. [45] Confronting this contradiction is a major challenge to environmental education teachers. Additionally, the dominant narrative that all environmental educators have an agenda can present difficulties in expanding reach. It is said that an environmental educator is one "who uses information and educational processes to help people analyze the merits of the many and varied points of view usually present on a given environmental issues." [46] Greater efforts must be taken to train educators on the importance of staying within the profession's substantive structure, and in informing the general public on the profession's intention to empower fully informed decision making. Another obstacle facing the implementation of environmental education lies the quality of education itself. Charles Sayan, the executive director of the Ocean Conservation Society, represents alternate views and critiques on environmental education in his new book The Failure of Environmental Education (And How We Can Fix It). In a Yale Environment 360 interview, Sayan discusses his book and outlines several flaws within environmental education, particularly its failed efforts to "reach its potential in fighting climate change, biodiversity loss , and environmental degradation ". [47] He believes that environmental education is not "keeping pace with environmental degradation" and encourages structural reform by increasing student engagement as well as improving relevance of information. [48] These same critiques are discussed in Stewart Hudson's BioScience paper, "Challenges for Environmental Education: Issues and Ideas for the 21st Century". [49] Another study describes obstacles in environmental education also rooted in the capability of the school leaders. [50] They are the epicentre of education. However, implementing ESD in schools is difficult as school leaders faced too many challenges for a single plan to work. In 2017, a study found that high school science textbooks and government resources on climate change from United States, EU, Canada and Australia did focus their recommendations for CO2 emission reductions on lower-impact actions instead of promoting the most effective emission-reduction strategies. [51] Movement[ edit ] A movement that has progressed since the relatively recent founding  of environmental education in industrial societies has transported the participant from nature appreciation and awareness to education for an ecologically sustainable future. This trend may be viewed as a microcosm of how many environmental education programs seek to first engage participants through developing a sense of nature appreciation which then translates into actions that affect conservation and sustainability. Programs range from New York to California, including Life Lab at University of California, Santa Cruz, as well as Cornell University in Environmental Education in the Global South[ edit ] This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (April 2021) ( Learn how and when to remove this template message ) Environmentalism has also begun to make waves in the development of the global South, as the " First World " takes on the responsibility of helping developing countries to combat environmental issues produced and prolonged by conditions of poverty. Unique to environmental education in the Global South is its particular focus on sustainable development. This goal has been a part of international agenda since the 1900s, with the United Nations Educational Scientific and Cultural Organizations (UNESCO) and the Earth Council Alliance (ECA) at the forefront of pursuing sustainable development in the south. The 1977 Tbilisi intergovernmental conference played a key role in the development of  outcome of the conference was the Tbilisi Declaration, a unanimous accord which "constitutes the framework, principles, and guidelines for environmental education at all levels—local, national, regional, and international—and for all age groups both inside and outside the formal school system" recommended as a criterion for implementing environmental education. The Declaration was established with the intention of increasing environmental stewardship, awareness and behavior, which paved the way for the rise of modern environmental education. After the 1992 Rio Earth Summit, over 80 National Councils for Sustainable Development in developing countries were created between 1992–1998 to aid in compliance of international sustainability goals and encourage "creative solutions". In 1993, the Earth Council Alliance released the Treaty on environmental education for sustainable societies and global responsibility, sparking discourse on environmental education. The Treaty, in 65 statements, outlines the role of environmental education in facilitating sustainable development through all aspects of democratized participation and provides a methodology for the Treaty's signatories. It has been instrumentally utilized in expanding the field towards the global South, wherein the discourse of "environmental education for sustainable development" recognizes a need to include human population dynamics in EE and emphasizes "aspects related to contemporary economic realities and by placing greater emphasis on concerns for planetary solidarity". Even as a necessary tool for the proliferation of environmental stewardship, environmental education implemented in the South varies and addresses environmental issues in relation to their impact different communities and specific community needs. Whereas in the developed global North where the environmentalist sentiments are centered around conservation without taking into consideration "the needs of people living within communities", the global South must push forth a conservation agenda that parallels with social, economic, and political development. The role of environmental education in the South is centered around potential economic growth in development projects, as explicitly stated by the UNESCO, to apply environmental education for sustainable development through a "creative and effective use of human potential and all forms of capital to ensure rapid and more equitable economic growth, with minimal impact on the environment". Moving into the 21st century, EE was furthered by United Nations as a part of the 2000 Millennium Development Goals to improve the planet by 2015. The MDGs included global efforts to end extreme poverty, work towards gender equality, access to education, and sustainable development to name a few. Although the MDGs produced great outcomes, its objectives were not met, and MDGs were soon replaced by Sustainable Development Goals . A "universal call to action to end poverty, protect the planet and ensure that all people enjoy peace and prosperity", SDGs became the new face of global priorities. [52] These new goals incorporated objectives from MDGs yet incorporated a necessary environmental framework to "address key systemic barriers to sustainable development such as inequality, unsustainable consumption patterns, weak institutional capacity and environmental degradation that the MDGs neglected". [53] Trends[ edit ] One of the current trends within environmental education seeks to move from an approach of ideology and activism to one that allows students to make informed decisions and take action based on experience as well as data. Within this process, environmental curricula have progressively been integrated into governmental education standards. A study found that standardized curriculum can be a significant impediment to environmental education implementation. [54] Some environmental educators find this movement distressing and move away from the original political and activist approach to environmental education while others find this approach more valid and accessible. [55] Regardless, many educational institutions are encouraging students to take an active role in environmental education and stewardship at their institutions. They know that "to be successful, greening initiatives require both grassroots support from the student body and top down support from high-level campus administrators." [56] Italy announced in 2019 that environmental education (including topics of sustainability and climate change) will be integrated into other subject matter and will be a mandatory part of the curriculum in public schools. [57] In the United States, Title IV, Part A of the Every Student Succeeds Act states that environmental education is eligible for grant funding. The program gives a "well-rounded" education as well as access to student health and safety programs. Title IV, Part B states that environmental literacy programs are also eligible for funding through the 21st Century Community Learning Centers Program. The funds that are available for both parts are block granted to the states using the Title I formula. In the FY2018 budget, Titles IVA and IVB were both given $1.1 billion and $1.2 billion. For title IVA, this is a $700 million raise from the 2017 budget which makes the 2018–2019 school year the most availability to environmental education ever. [58] Renewable Energy Education[ edit ] Renewable energy education (REE) is a relatively new field of education. The overall objectives of REE pertain to giving a working knowledge and understanding of concepts, facts, principles and technologies for gathering the renewable sources of energy. Based on these objectives, the role of a renewable energy education programs should be informative, investigative, educative, and imaginative. REE should be taught with the world's population in mind as the world will run out of non-renewable resources within the next century. Renewable energy education is also being brought to political leaders as a means of getting more sustainable development to occur around the globe. This is happening in the hopes that it will uproot millions of people out of poverty and into a better quality of life in many countries. Renewable energy education is also about bringing awareness of climate change to the general public as well as an understanding of the current renewable energy technologies. An understanding of the new technologies is imperative to get them stream-lined and accepted by the vast majority of the public. [59]
From Wikipedia, the free encyclopedia Water overflow submerging usually-dry land For other uses, see Flood (disambiguation) . "Inundation" redirects here. For other uses, see Inundation (disambiguation) . Urban flooding in a street in Morpeth , England A flood is an overflow of water ( or rarely other fluids ) that submerges land that is usually dry. [1] In the sense of "flowing water", the word may also be applied to the inflow of the tide . Floods are an area of study of the discipline hydrology and are of significant concern in agriculture , civil engineering and public health . Human changes to the environment often increase the intensity and frequency of flooding, for example land use changes such as deforestation and removal of wetlands , changes in waterway course or flood controls such as with levees , and larger environmental issues such as climate change and sea level rise . In particular climate change's increased rainfall and extreme weather events increases the severity of other causes for flooding, resulting in more intense floods and increased flood risk. [2] [3] Flooding may occur as an overflow of water from water bodies, such as a river , lake , or ocean, in which the water overtops or breaks levees , resulting in some of that water escaping its usual boundaries, [4] or it may occur due to an accumulation of rainwater on saturated ground in an areal flood. While the size of a lake or other body of water will vary with seasonal changes in precipitation and snow melt, these changes in size are unlikely to be considered significant unless they flood property or drown domestic animals . Floods can also occur in rivers when the flow rate exceeds the capacity of the river channel , particularly at bends or meanders in the waterway . Floods often cause damage to homes and businesses if they are in the natural flood plains of rivers.  While riverine flood damage can be eliminated by moving away from rivers and other bodies of water, people have traditionally lived and worked by rivers because the land is usually flat and fertile and because rivers provide easy travel and access to commerce and industry. Flooding can lead to secondary consequences in addition to damage to property, such as long-term displacement of residents and creating increased spread of waterborne diseases and vector-bourne disesases transmitted by mosquitos. [5] Types View of flooded New Orleans in the aftermath of Hurricane Katrina . Flooding of a creek due to heavy monsoonal rain and high tide in Darwin , Northern Territory , Australia . Flood in Jeddah , covering the King Abdullah Street in Saudi Arabia . Areal In spring time, the floods are quite typical in Ostrobothnia , a flat-lying area in Finland . A flood-surrounded house in Ilmajoki , South Ostrobothnia . Floods can happen on flat or low-lying areas when water is supplied by rainfall or snowmelt more rapidly than it can either infiltrate or run off . The excess accumulates in place, sometimes to hazardous depths. Surface soil can become saturated, which effectively stops infiltration, where the water table is shallow, such as a floodplain , or from intense rain from one or a series of storms . Infiltration also is slow to negligible through frozen ground, rock, concrete , paving, or roofs. Areal flooding begins in flat areas like floodplains and in local depressions not connected to a stream channel, because the velocity of overland flow depends on the surface slope. Endorheic basins may experience areal flooding during periods when precipitation exceeds evaporation. [6] River flooding Floods occur in all types of river and stream channels, from the smallest ephemeral streams in humid zones to normally-dry channels in arid climates to the world's largest rivers. When overland flow occurs on tilled fields, it can result in a muddy flood where sediments are picked up by run off and carried as suspended matter or bed load . Localized flooding may be caused or exacerbated by drainage obstructions such as landslides , ice , debris , or beaver dams. Slow-rising floods most commonly occur in large rivers with large catchment areas . The increase in flow may be the result of sustained rainfall, rapid snow melt, monsoons , or tropical cyclones . However, large rivers may have rapid flooding events in areas with dry climates, since they may have large basins but small river channels, and rainfall can be very intense in smaller areas of those basins. Flash flood in Ein Avdat, Negev, Israel Rapid flooding events, including flash floods , more often occur on smaller rivers, rivers with steep valleys, rivers that flow for much of their length over impermeable terrain, or normally-dry channels. The cause may be localized convective precipitation (intense thunderstorms ) or sudden release from an upstream impoundment created behind a dam , landslide, or glacier . In one instance, a flash flood killed eight people enjoying the water on a Sunday afternoon at a popular waterfall in a narrow canyon.[ citation needed ] Without any observed rainfall, the flow rate increased from about 50 to 1,500 cubic feet per second (1.4 to 42 m3/s) in just one minute. [7] Two larger floods occurred at the same site within a week, but no one was at the waterfall on those days. The deadly flood resulted from a thunderstorm over part of the drainage basin, where steep, bare rock slopes are common and the thin soil was already saturated. Flash floods are the most common flood type in normally-dry channels in arid zones, known as arroyos in the southwest United States and many other names elsewhere. In that setting, the first flood water to arrive is depleted as it wets the sandy stream bed. The leading edge of the flood thus advances more slowly than later and higher flows. As a result, the rising limb of the hydrograph becomes ever quicker as the flood moves downstream, until the flow rate is so great that the depletion by wetting soil becomes insignificant. Coastal flooding Main article: Coastal flooding Coastal areas may be flooded by storm surges combining with high tides and large wave events at sea, resulting in waves over-topping flood defenses or in severe cases by tsunami or tropical cyclones. A storm surge , from either a tropical cyclone or an extratropical cyclone , falls within this category. A storm surge is "an additional rise of water generated by a storm, over and above the predicted astronomical tides". [8] Due to the effects of climate change (e.g. sea level rise and an increase in extreme weather events) and an increase in the population living in coastal areas, the damage caused by coastal flood events has intensified and more people are being affected. [9] Flooding in estuaries is commonly caused by a combination of storm surges caused by winds and low barometric pressure and large waves meeting high upstream river flows. Urban flooding This section is an excerpt from Urban flooding .[ edit ] Urban flooding is the inundation of land or property in a built environment , particularly in more densely populated areas, caused by rainfall overwhelming the capacity of drainage systems, such as storm sewers . Urban flooding is a condition that is characterized by its repetitive and systemic impacts on communities, that can happen regardless of whether or not affected communities are located within designated floodplains or near any body of water. [10] It is triggered for example by an overflow of rivers and lakes, flash flooding or snowmelt . During the flood, stormwater or water released from damaged water mains may accumulate on property and in public rights-of-way, seep through building walls and floors, or backup into buildings through sewer pipes, toilets and sinks. In urban areas, flood effects can be exacerbated by existing paved streets and roads, which increase the speed of flowing water. Impervious surfaces prevent rainfall from infiltrating into the ground, thereby causing a higher surface run-off that may be in excess of local drainage capacity. [11] Catastrophic Catastrophic riverine flooding can result from major infrastructure failures, often the collapse of a dam . It can also be caused by drainage channel modification from a landslide , earthquake or volcanic eruption . Examples include outburst floods and lahars . Tsunamis can cause catastrophic coastal flooding , most commonly resulting from undersea earthquakes. Causes See also: Coastal flooding § Causes Floods are caused by many factors or a combination of any of these generally prolonged heavy rainfall (locally concentrated or throughout a catchment area), highly accelerated snowmelt , severe winds over water, unusual high tides, tsunamis , or failure of dams, levees , retention ponds , or other structures that retained the water. Flooding can be exacerbated by increased amounts of impervious surface or by other natural hazards such as wildfires, which reduce the supply of vegetation that can absorb rainfall. During times of rain, some of the water is retained in ponds or soil, some is absorbed by grass and vegetation, some evaporates, and the rest travels over the land as surface runoff . Floods occur when ponds, lakes, riverbeds, soil, and vegetation cannot absorb all the water. This has been exacerbated by human activities such as draining wetlands that naturally store large amounts of water and building paved surfaces that do not absorb any water. [12] Water then runs off the land in quantities that cannot be carried within stream channels or retained in natural ponds, lakes, and human-made reservoirs . About 30 percent of all precipitation becomes runoff [13] and that amount might be increased by water from melting snow. River flooding is often caused by heavy rain, sometimes increased by melting snow. A flood that rises rapidly, with little or no warning, is called a flash flood . Flash floods usually result from intense rainfall over a relatively small area, or if the area was already saturated from previous precipitation. Periodic floods occur on many rivers, forming a surrounding region known as the flood plain . Even when rainfall is relatively light, the shorelines of lakes and bays can be flooded by severe winds—such as during hurricanes —that blow water into the shore areas. Upslope factors The amount, location, and timing of water reaching a drainage channel from natural precipitation and controlled or uncontrolled reservoir releases determines the flow at downstream locations.  Some precipitation evaporates, some slowly percolates through soil, some may be temporarily sequestered as snow or ice, and some may produce rapid runoff from surfaces including rock, pavement, roofs, and saturated or frozen ground.  The fraction of incident precipitation promptly reaching a drainage channel has been observed from nil for light rain on dry, level ground to as high as 170 percent for warm rain on accumulated snow. [14] Most precipitation records are based on a measured depth of water received within a fixed time interval. Frequency of a precipitation threshold of interest may be determined from the number of measurements exceeding that threshold value within the total time period for which observations are available.  Individual data points are converted to intensity by dividing each measured depth by the period of time between observations. This intensity will be less than the actual peak intensity if the duration of the rainfall event was less than the fixed time interval for which measurements are reported.  Convective precipitation events (thunderstorms) tend to produce shorter duration storm events than orographic precipitation. Duration, intensity, and frequency of rainfall events are important to flood prediction.  Short duration precipitation is more significant to flooding within small drainage basins. [15] The most important upslope factor in determining flood magnitude is the land area of the watershed upstream of the area of interest.  Rainfall intensity is the second most important factor for watersheds of less than approximately 30 square miles or 80 square kilometres.  The main channel slope is the second most important factor for larger watersheds.  Channel slope and rainfall intensity become the third most important factors for small and large watersheds, respectively. [16] Time of Concentration is the time required for runoff from the most distant point of the upstream drainage area to reach the point of the drainage channel controlling flooding of the area of interest. The time of concentration defines the critical duration of peak rainfall for the area of interest. [17] The critical duration of intense rainfall might be only a few minutes for roof and parking lot drainage structures, while cumulative rainfall over several days would be critical for river basins. Downslope factors Water flowing downhill ultimately encounters downstream conditions slowing movement. The final limitation in coastal flooding lands is often the ocean or some coastal flooding bars which form natural lakes . In flooding low lands, elevation changes such as tidal fluctuations are significant determinants of coastal and estuarine flooding. Less predictable events like tsunamis and storm surges may also cause elevation changes in large bodies of water. Elevation of flowing water is controlled by the geometry of the flow channel and, especially, by depth of channel, speed of flow and amount of sediments in it [16] Flow channel restrictions like bridges and canyons tend to control water elevation above the restriction. The actual control point for any given reach of the drainage may change with changing water elevation, so a closer point may control for lower water levels until a more distant point controls at higher water levels. Effective flood channel geometry may be changed by growth of vegetation, accumulation of ice or debris, or construction of bridges, buildings, or levees within the flood channel. Coincidence Extreme flood events often result from coincidence such as unusually intense, warm rainfall melting heavy snow pack, producing channel obstructions from floating ice, and releasing small impoundments like beaver dams. [18] Coincident events may cause extensive flooding to be more frequent than anticipated from simplistic statistical prediction models considering only precipitation runoff flowing within unobstructed drainage channels. [19] Debris modification of channel geometry is common when heavy flows move uprooted woody vegetation and flood-damaged structures and vehicles, including boats and railway equipment. Recent field measurements during the 2010–11 Queensland floods showed that any criterion solely based upon the flow velocity, water depth or specific momentum cannot account for the hazards caused by velocity and water depth fluctuations. [20] These considerations ignore further the risks associated with large debris entrained by the flow motion. [21] Some researchers have mentioned the storage effect in urban areas with transportation corridors created by cut and fill . Culverted fills may be converted to impoundments if the culverts become blocked by debris, and flow may be diverted along streets.  Several studies have looked into the flow patterns and redistribution in streets during storm events and the implication on flood modelling. [22] Climate change High tide flooding is increasing due to sea level rise, land subsidence, and the loss of natural barriers. [23] Long-term sea level rise occurs in addition to intermittent tidal flooding. NOAA predicts different levels of sea level rise for coastlines within a single country. [24] This section is an excerpt from Effects of climate change § Floods .[ edit ] Due to an increase in heavy rainfall events, floods are likely to become more severe when they do occur. [25] : 1155 The interactions between rainfall and flooding are complex. There are some regions in which flooding is expected to become rarer. This depends on several factors. These include changes in rain and snowmelt, but also soil moisture . [25] : 1156 Climate change leaves soils drier in some areas, so they may absorb rainfall more quickly. This leads to less flooding. Dry soils can also become harder. In this case heavy rainfall runs off  into rivers and lakes. This increases risks of flooding. [25] : 1155 Intentional flooding The intentional flooding of land that would otherwise remain dry may take place for military, agricultural, or river-management purposes. This is a form of hydraulic engineering . Agricultural flooding may occur in preparing paddy fields for the growing of semi-aquatic rice in many countries. Chinese Kuomintang soldiers during the 1938 Yellow River flood Flooding for river management may occur in the form of diverting flood waters in a river at flood stage upstream from areas that are considered more valuable than the areas that are sacrificed in this way. This may be done ad hoc, as in the 2011 intentional breach of levees by the United States Army Corps of Engineers in Missouri, [26] or permanently, as in the so-called overlaten (literally "let-overs"), an intentionally lowered segment in Dutch riparian levees, like the Beerse Overlaat in the left levee of the Meuse between the villages of Gassel and Linden, North Brabant . Military inundation creates an obstacle in the field that is intended to impede the movement of the enemy. [27] This may be done both for offensive and defensive purposes. Furthermore, in so far as the methods used are a form of hydraulic engineering, it may be useful to differentiate between controlled inundations (as in most historic inundations in the Netherlands under the Dutch Republic and its successor states in that area [28] [29] and exemplified in the two Hollandic Water Lines , the Stelling van Amsterdam , the Frisian Water Line , the IJssel Line , the Peel-Raam Line , and the Grebbe line in that country) and uncontrolled ones (as in the second Siege of Leiden [30] during the first part of the Eighty Years' War , the flooding of the Yser plain during the First World War , [31] and the Inundation of Walcheren , and the Inundation of the Wieringermeer during the Second World War ). To count as controlled, a military inundation has to take the interests of the civilian population into account, by allowing them a timely evacuation , by making the inundation reversible, and by making an attempt to minimize the adverse ecological impact of the inundation. That impact may also be adverse in a hydrogeological sense if the inundation lasts a long time. [32] The Itaipu dam caused concern that in times of conflict could be used as a weapon to flood Buenos Aires. [33] [34] [35] [36] Negative impacts Flooded walnut orchards in Butte County after several atmospheric rivers hit California in early 2023 Floods can also be a huge destructive power. When water flows, it has the ability to demolish all kinds of buildings and objects, such as  bridges, structures, houses, trees, and cars.  Economical, social and natural environmental damages are common factors that are impacted by flooding events and the impacts that flooding has on these areas can be catastrophic. [37] There have been numerous flood incidents around the world which have caused devastating damage to infrastructure, the natural environment and human life. [37] Flood risks can be defined as the risk that floods pose to individuals, property and the natural landscape based on specific hazards and vulnerability. The extent of flood risks can impact the types of mitigation strategies required and implemented. [38] Floods can have devastating impacts to human societies. Flooding events worldwide are increasing in frequency and severity, leading to increasing costs to societies. [37] A large amount of the world's population lives in close proximity to major coastlines , [39] while many major cities and agricultural areas are located near floodplains . [40] There is significant risk for increased coastal and fluvial flooding due to changing climatic conditions. [41] Economic impacts The primary effects of flooding include loss of life and damage to buildings and other structures, including bridges, sewerage systems, roadways, and canals. The economic impacts caused by flooding can be severe. [40] Every year flooding causes countries billions of dollars worth of damage that threatens the livelihood of individuals. [42] As a result, there is also significant socio-economic threats to vulnerable populations around the world from flooding. [42] For example, in Bangladesh in 2007, a flood was responsible for the destruction of more than one million houses. And yearly in the United States, floods cause over $7 billion in damage. [43] Mud was deposited in this house by flooding in the 2018 Kerala floods in India. Flooding not only creates water damage, but can also deposit large amounts of sediment. Flood waters typically inundate farm land, making the land unworkable and preventing crops from being planted or harvested, which can lead to shortages of food both for humans and farm animals. Entire harvests for a country can be lost in extreme flood circumstances. Some tree species may not survive prolonged flooding of their root systems. [44] Flooding in areas where people live also has significant economic implications for affected neighborhoods. In the United States , industry experts estimate that wet basements can lower property values by 10–25 percent and are cited among the top reasons for not purchasing a home. [45] According to the U.S. Federal Emergency Management Agency (FEMA), almost 40 percent of small businesses never reopen their doors following a flooding disaster. [46] In the United States, insurance is available against flood damage to both homes and businesses. [47] Economic hardship due to a temporary decline in tourism, rebuilding costs, or food shortages leading to price increases is a common after-effect of severe flooding. The impact on those affected may cause psychological damage to those affected, in particular where deaths, serious injuries and loss of property occur. Health impacts Coastal flooding in a community in Florida, United States. Flooding after 1991 Bangladesh cyclone , which killed around 140,000 people. Fatalities connected directly to floods are usually caused by drowning ; the waters in a flood are very deep and have strong currents . [48] Deaths do not just occur from drowning, deaths are connected with dehydration , heat stroke , heart attack and any other illness that needs medical supplies that cannot be delivered. [48] Injuries can lead to an excessive amount of morbidity when a flood occurs. Injuries are not isolated to just those who were directly in the flood, rescue teams and even people delivering supplies can sustain an injury. Injuries can occur anytime during the flood process; before, during and after. [48] During floods accidents occur with falling debris or any of the many fast moving objects in the water. After the flood rescue attempts are where large numbers injuries can occur. [48] Communicable diseases are increased due to many pathogens and bacteria that are being transported by the water .There are many waterborne diseases such as cholera , hepatitis A , hepatitis E and diarrheal diseases , to mention a few. Gastrointestinal disease and diarrheal diseases are very common due to a lack of clean water during a flood. Most of clean water supplies are contaminated when flooding occurs. Hepatitis A and E are common because of the lack of sanitation in the water and in living quarters depending on where the flood is and how prepared the community is for a flood. [48] When floods hit, people lose nearly all their crops, livestock, and food reserves and face starvation. [49] Floods also frequently damage power transmission and sometimes power generation , which then has knock-on effects caused by the loss of power. This includes loss of drinking water treatment and water supply, which may result in loss of drinking water or severe water contamination. It may also cause the loss of sewage disposal facilities. Lack of clean water combined with human sewage in the flood waters raises the risk of waterborne diseases , which can include typhoid , giardia , cryptosporidium , cholera and many other diseases depending upon the location of the flood. Damage to roads and transport infrastructure may make it difficult to mobilize aid to those affected or to provide emergency health treatment. Flooding can cause chronically wet houses, leading to the growth of indoor mold and resulting in adverse health effects, particularly respiratory symptoms. [50] Respiratory diseases are a common after the disaster has occurred. This depends on the amount of water damage and mold that grows after an incident. Research suggests that there will be an increase of 30–50% in adverse respiratory health outcomes caused by dampness and mold exposure for those living in coastal and wetland areas. Fungal contamination in homes is associated with increased allergic rhinitis and asthma. [51] Vector borne diseases increase as well due to the increase in still water after the floods have settled. The diseases that are vector borne are malaria , dengue , West Nile , and yellow fever . [48] Floods have a huge impact on victims' psychosocial integrity . People suffer from a wide variety of losses and stress . One of the most treated illness in long-term health problems are depression caused by the flood and all the tragedy that flows with one. [48] Loss of life Below is a list of the deadliest floods worldwide, showing events with death tolls at or above 100,000 individuals. Death toll
Toggle the table of contents Earthquake From Wikipedia, the free encyclopedia Sudden movement of the Earth's crust For other uses, see Earthquake (disambiguation) . This article may be confusing or unclear to readers. In particular, tone switches from too scientific to encyclopedic between sections. Please help clarify the article . There might be a discussion about this on the talk page . (October 2022) ( Earthquake epicenters occur mostly along tectonic plate boundaries, especially on the Pacific Ring of Fire . Global plate tectonic movement e An earthquake – also called a quake, tremor, or temblor – is the shaking of the Earth 's surface resulting from a sudden release of energy in the lithosphere that creates seismic waves . Earthquakes can range in intensity , from those so weak they cannot be felt, to those violent enough to propel objects and people into the air, damage critical infrastructure, and wreak destruction across entire cities. The seismic activity of an area is the frequency, type, and size of earthquakes experienced over a particular time. The seismicity at a particular location in the Earth is the average rate of seismic energy release per unit volume. In its most general sense, the word earthquake is used to describe any seismic event that generates seismic waves. Earthquakes can occur naturally or be induced by human activities, such as mining , fracking , and nuclear tests . The initial point of rupture is called the hypocenter or focus, while the ground level directly above it is the epicenter . Earthquakes are primarily caused by geological faults , but also by volcanic activity , landslides, and other seismic events. The frequency, type, and size of earthquakes in an area define its seismic activity, reflecting the average rate of seismic energy release. Significant historical earthquakes include the 1556 Shaanxi earthquake in China, with over 830,000 fatalities, and the 1960 Valdivia earthquake in Chile, the largest ever recorded at 9.5 magnitude. Earthquakes result in various effects, such as ground shaking and soil liquefaction , leading to significant damage and loss of life. When the epicenter of a large earthquake is located offshore, the seabed may be displaced sufficiently to cause a tsunami . Earthquakes can trigger landslides . Earthquakes' occurrence is influenced by tectonic movements along faults, including normal, reverse (thrust), and strike-slip faults, with energy release and rupture dynamics governed by the elastic-rebound theory . Efforts to manage earthquake risks involve prediction, forecasting, and preparedness, including seismic retrofitting and earthquake engineering to design structures that withstand shaking. The cultural impact of earthquakes spans myths, religious beliefs, and modern media, reflecting their profound influence on human societies. Similar seismic phenomena, known as marsquakes and moonquakes , have been observed on other celestial bodies, indicating the universality of such events beyond Earth. Terminology An earthquake is the shaking of the surface of Earth resulting from a sudden release of energy in the lithosphere that creates seismic waves . Earthquakes may also be referred to as quakes, tremors, or temblors. The word tremor is also used for non-earthquake seismic rumbling . In its most general sense, an earthquake is any seismic event—whether natural or caused by humans—that generates seismic waves. Earthquakes are caused mostly by the rupture of geological faults but also by other events such as volcanic activity, landslides, mine blasts, fracking and nuclear tests . An earthquake's point of initial rupture is called its hypocenter or focus. The epicenter is the point at ground level directly above the hypocenter. The seismic activity of an area is the frequency, type, and size of earthquakes experienced over a particular time. The seismicity at a particular location in the Earth is the average rate of seismic energy release per unit volume. Major examples Main article: Lists of earthquakes Earthquakes (M6.0+) since 1900 through 2017 Earthquakes of magnitude 8.0 and greater from 1900 to 2018. The apparent 3D volumes of the bubbles are linearly proportional to their respective fatalities. [1] One of the most devastating earthquakes in recorded history was the 1556 Shaanxi earthquake , which occurred on 23 January 1556 in Shaanxi , China. More than 830,000 people died. [2] Most houses in the area were yaodongs —dwellings carved out of loess hillsides—and many victims were killed when these structures collapsed. The 1976 Tangshan earthquake , which killed between 240,000 and 655,000 people, was the deadliest of the 20th century. [3] The 1960 Chilean earthquake is the largest earthquake that has been measured on a seismograph, reaching 9.5 magnitude on 22 May 1960. [4] [5] Its epicenter was near Cañete, Chile. The energy released was approximately twice that of the next most powerful earthquake, the Good Friday earthquake (27 March 1964), which was centered in Prince William Sound , Alaska. [6] [7] The ten largest recorded earthquakes have all been megathrust earthquakes ; however, of these ten, only the 2004 Indian Ocean earthquake is simultaneously one of the deadliest earthquakes in history. Earthquakes that caused the greatest loss of life, while powerful, were deadly because of their proximity to either heavily populated areas or the ocean, where earthquakes often create tsunamis that can devastate communities thousands of kilometers away. Regions most at risk for great loss of life include those where earthquakes are relatively rare but powerful, and poor regions with lax, unenforced, or nonexistent seismic building codes. Occurrence Three types of faults: A. Strike-slip B. Normal C. Reverse Tectonic earthquakes occur anywhere on the earth where there is sufficient stored elastic strain energy to drive fracture propagation along a fault plane . The sides of a fault move past each other smoothly and aseismically only if there are no irregularities or asperities along the fault surface that increases the frictional resistance. Most fault surfaces do have such asperities, which leads to a form of stick-slip behavior . Once the fault has locked, continued relative motion between the plates leads to increasing stress and, therefore, stored strain energy in the volume around the fault surface. This continues until the stress has risen sufficiently to break through the asperity, suddenly allowing sliding over the locked portion of the fault, releasing the stored energy . [8] This energy is released as a combination of radiated elastic strain seismic waves , [9] frictional heating of the fault surface, and cracking of the rock, thus causing an earthquake. This process of gradual build-up of strain and stress punctuated by occasional sudden earthquake failure is referred to as the elastic-rebound theory . It is estimated that only 10 percent or less of an earthquake's total energy is radiated as seismic energy. Most of the earthquake's energy is used to power the earthquake fracture growth or is converted into heat generated by friction. Therefore, earthquakes lower the Earth's available elastic potential energy and raise its temperature, though these changes are negligible compared to the conductive and convective flow of heat out from the Earth's deep interior. [10] Fault types Main article: Fault (geology) There are three main types of fault, all of which may cause an interplate earthquake : normal, reverse (thrust), and strike-slip. Normal and reverse faulting are examples of dip-slip, where the displacement along the fault is in the direction of dip and where movement on them involves a vertical component. Many earthquakes are caused by movement on faults that have components of both dip-slip and strike-slip; this is known as oblique slip. The topmost, brittle part of the Earth's crust, and the cool slabs of the tectonic plates that are descending into the hot mantle, are the only parts of our planet that can store elastic energy and release it in fault ruptures. Rocks hotter than about 300 °C (572 °F) flow in response to stress; they do not rupture in earthquakes. [11] [12] The maximum observed lengths of ruptures and mapped faults (which may break in a single rupture) are approximately 1,000 km (620 mi). Examples are the earthquakes in Alaska (1957) , Chile (1960) , and Sumatra (2004) , all in subduction zones. The longest earthquake ruptures on strike-slip faults, like the San Andreas Fault ( 1857 , 1906 ), the North Anatolian Fault in Turkey ( 1939 ), and the Denali Fault in Alaska ( 2002 ), are about half to one third as long as the lengths along subducting plate margins, and those along normal faults are even shorter. Normal faults Normal faults occur mainly in areas where the crust is being extended such as a divergent boundary . Earthquakes associated with normal faults are generally less than magnitude 7. Maximum magnitudes along many normal faults are even more limited because many of them are located along spreading centers, as in Iceland, where the thickness of the brittle layer is only about six kilometres (3.7 mi). [13] [14] Reverse faults Reverse faults occur in areas where the crust is being shortened such as at a convergent boundary. Reverse faults, particularly those along convergent plate boundaries , are associated with the most powerful earthquakes, megathrust earthquakes , including almost all of those of magnitude 8 or more. Megathrust earthquakes are responsible for about 90% of the total seismic moment released worldwide. [15] Strike-slip faults Strike-slip faults are steep structures where the two sides of the fault slip horizontally past each other; transform boundaries are a particular type of strike-slip fault. Strike-slip faults, particularly continental transforms , can produce major earthquakes up to about magnitude 8. Strike-slip faults tend to be oriented near vertically, resulting in an approximate width of 10 km (6.2 mi) within the brittle crust. [16] Thus, earthquakes with magnitudes much larger than 8 are not possible. Aerial photo of the San Andreas Fault in the Carrizo Plain , northwest of Los Angeles In addition, there exists a hierarchy of stress levels in the three fault types. Thrust faults are generated by the highest, strike-slip by intermediate, and normal faults by the lowest stress levels. [17] This can easily be understood by considering the direction of the greatest principal stress, the direction of the force that "pushes" the rock mass during the faulting. In the case of normal faults, the rock mass is pushed down in a vertical direction, thus the pushing force (greatest principal stress) equals the weight of the rock mass itself. In the case of thrusting, the rock mass "escapes" in the direction of the least principal stress, namely upward, lifting the rock mass, and thus, the overburden equals the least principal stress. Strike-slip faulting is intermediate between the other two types described above. This difference in stress regime in the three faulting environments can contribute to differences in stress drop during faulting, which contributes to differences in the radiated energy, regardless of fault dimensions. Energy released For every unit increase in magnitude, there is a roughly thirtyfold increase in the energy released. For instance, an earthquake of magnitude 6.0 releases approximately 32 times more energy than a 5.0 magnitude earthquake and a 7.0 magnitude earthquake releases 1,000 times more energy than a 5.0 magnitude earthquake. An 8.6 magnitude earthquake releases the same amount of energy as 10,000 atomic bombs of the size used in World War II . [18] This is so because the energy released in an earthquake, and thus its magnitude, is proportional to the area of the fault that ruptures [19] and the stress drop. Therefore, the longer the length and the wider the width of the faulted area, the larger the resulting magnitude. The most important parameter controlling the maximum earthquake magnitude on a fault, however, is not the maximum available length, but the available width because the latter varies by a factor of 20. Along converging plate margins, the dip angle of the rupture plane is very shallow, typically about 10 degrees. [20] Thus, the width of the plane within the top brittle crust of the Earth can become 50–100 km (31–62 mi) ( Japan, 2011 ; Alaska, 1964 ), making the most powerful earthquakes possible. Focus Collapsed Gran Hotel building in the San Salvador metropolis, after the shallow 1986 San Salvador earthquake The majority of tectonic earthquakes originate in the Ring of Fire at depths not exceeding tens of kilometers. Earthquakes occurring at a depth of less than 70 km (43 mi) are classified as "shallow-focus" earthquakes, while those with a focal depth between 70 and 300 km (43 and 186 mi) are commonly termed "mid-focus" or "intermediate-depth" earthquakes. In subduction zones, where older and colder oceanic crust descends beneath another tectonic plate, deep-focus earthquakes may occur at much greater depths (ranging from 300 to 700 km (190 to 430 mi)). [21] These seismically active areas of subduction are known as Wadati–Benioff zones . Deep-focus earthquakes occur at a depth where the subducted lithosphere should no longer be brittle, due to the high temperature and pressure. A possible mechanism for the generation of deep-focus earthquakes is faulting caused by olivine undergoing a phase transition into a spinel structure. [22] Volcanic activity Main article: Volcano tectonic earthquake Earthquakes often occur in volcanic regions and are caused there, both by tectonic faults and the movement of magma in volcanoes . Such earthquakes can serve as an early warning of volcanic eruptions, as during the 1980 eruption of Mount St. Helens . [23] Earthquake swarms can serve as markers for the location of the flowing magma throughout the volcanoes. These swarms can be recorded by seismometers and tiltmeters (a device that measures ground slope) and used as sensors to predict imminent or upcoming eruptions. [24] Rupture dynamics A tectonic earthquake begins as an area of initial slip on the fault surface that forms the focus. Once the rupture has been initiated, it begins to propagate away from the focus, spreading out along the fault surface. Lateral propagation will continue until either the rupture reaches a barrier, such as the end of a fault segment, or a region on the fault where there is insufficient stress to allow continued rupture. For larger earthquakes, the depth extent of rupture will be constrained downwards by the brittle-ductile transition zone and upwards by the ground surface. The mechanics of this process are poorly understood because it is difficult either to recreate such rapid movements in a laboratory or to record seismic waves close to a nucleation zone due to strong ground motion. [25] In most cases, the rupture speed approaches, but does not exceed, the shear wave (S-wave) velocity of the surrounding rock. There are a few exceptions to this: Supershear earthquakes The 2023 Turkey–Syria earthquakes ruptured along segments of the East Anatolian Fault at supershear speeds; more than 50,000 people died in both countries. [26] Supershear earthquake ruptures are known to have propagated at speeds greater than the S-wave velocity. These have so far all been observed during large strike-slip events. The unusually wide zone of damage caused by the 2001 Kunlun earthquake has been attributed to the effects of the sonic boom developed in such earthquakes. Slow earthquakes Slow earthquake ruptures travel at unusually low velocities. A particularly dangerous form of slow earthquake is the tsunami earthquake , observed where the relatively low felt intensities, caused by the slow propagation speed of some great earthquakes, fail to alert the population of the neighboring coast, as in the 1896 Sanriku earthquake . [25] Co-seismic overpressuring and effect of pore pressure During an earthquake, high temperatures can develop at the fault plane, increasing pore pressure and consequently vaporization of the groundwater already contained within the rock. [27] [28] [29] In the coseismic phase, such an increase can significantly affect slip evolution and speed, in the post-seismic phase it can control the Aftershock sequence because, after the main event, pore pressure increase slowly propagates into the surrounding fracture network. [30] [29] From the point of view of the Mohr-Coulomb strength theory , an increase in fluid pressure reduces the normal stress acting on the fault plane that holds it in place, and fluids can exert a lubricating effect. As thermal overpressurization may provide positive feedback between slip and strength fall at the fault plane, a common opinion is that it may enhance the faulting process instability. After the mainshock, the pressure gradient between the fault plane and the neighboring rock causes a fluid flow that increases pore pressure in the surrounding fracture networks; such an increase may trigger new faulting processes by reactivating adjacent faults, giving rise to aftershocks. [30] [29] Analogously, artificial pore pressure increase, by fluid injection in Earth's crust, may induce seismicity . Tidal forces Tides may trigger some seismicity . Clusters Most earthquakes form part of a sequence, related to each other in terms of location and time. [31] Most earthquake clusters consist of small tremors that cause little to no damage, but there is a theory that earthquakes can recur in a regular pattern. [32] Earthquake clustering has been observed, for example, in Parkfield, California where a long-term research study is being conducted around the Parkfield earthquake cluster. [33] Aftershocks Magnitude of the Central Italy earthquakes of August and October 2016 and January 2017 and the aftershocks (which continued to occur after the period shown here) An aftershock is an earthquake that occurs after a previous earthquake, the mainshock. Rapid changes of stress between rocks, and the stress from the original earthquake are the main causes of these aftershocks, [34] along with the crust around the ruptured fault plane as it adjusts to the effects of the mainshock. [31] An aftershock is in the same region as the main shock but always of a smaller magnitude, however, they can still be powerful enough to cause even more damage to buildings that were already previously damaged from the mainshock. [34] If an aftershock is larger than the mainshock, the aftershock is redesignated as the mainshock and the original main shock is redesignated as a foreshock . Aftershocks are formed as the crust around the displaced fault plane adjusts to the effects of the mainshock. [31] Swarms Main article: Earthquake swarm Earthquake swarms are sequences of earthquakes striking in a specific area within a short period. They are different from earthquakes followed by a series of aftershocks by the fact that no single earthquake in the sequence is the main shock, so none has a notably higher magnitude than another. An example of an earthquake swarm is the 2004 activity at Yellowstone National Park . [35] In August 2012, a swarm of earthquakes shook Southern California 's Imperial Valley , showing the most recorded activity in the area since the 1970s. [36] Sometimes a series of earthquakes occur in what has been called an earthquake storm, where the earthquakes strike a fault in clusters, each triggered by the shaking or stress redistribution of the previous earthquakes. Similar to aftershocks but on adjacent segments of fault, these storms occur over the course of years, with some of the later earthquakes as damaging as the early ones. Such a pattern was observed in the sequence of about a dozen earthquakes that struck the North Anatolian Fault in Turkey in the 20th century and has been inferred for older anomalous clusters of large earthquakes in the Middle East. [37] [38] Frequency The Messina earthquake and tsunami took almost 100,000 lives on December 28, 1908, in Sicily and Calabria . [39] It is estimated that around 500,000 earthquakes occur each year, detectable with current instrumentation. About 100,000 of these can be felt. [4] [5] Minor earthquakes occur very frequently around the world in places like California and Alaska in the U.S., as well as in El Salvador, Mexico, Guatemala, Chile, Peru, Indonesia, the Philippines, Iran, Pakistan, the Azores in Portugal, Turkey, New Zealand, Greece, Italy, India, Nepal, and Japan. [40] Larger earthquakes occur less frequently, the relationship being exponential ; for example, roughly ten times as many earthquakes larger than magnitude 4 occur than earthquakes larger than magnitude 5. [41] In the (low seismicity) United Kingdom, for example, it has been calculated that the average recurrences are: an earthquake of 3.7–4.6 every year, an earthquake of 4.7–5.5 every 10 years, and an earthquake of 5.6 or larger every 100 years. [42] This is an example of the Gutenberg–Richter law . The number of seismic stations has increased from about 350 in 1931 to many thousands today. As a result, many more earthquakes are reported than in the past, but this is because of the vast improvement in instrumentation, rather than an increase in the number of earthquakes. The United States Geological Survey (USGS) estimates that, since 1900, there have been an average of 18 major earthquakes (magnitude 7.0–7.9) and one great earthquake (magnitude 8.0 or greater) per year, and that this average has been relatively stable. [43] In recent years, the number of major earthquakes per year has decreased, though this is probably a statistical fluctuation rather than a systematic trend. [44] More detailed statistics on the size and frequency of earthquakes is available from the United States Geological Survey. [45] A recent increase in the number of major earthquakes has been noted, which could be explained by a cyclical pattern of periods of intense tectonic activity, interspersed with longer periods of low intensity. However, accurate recordings of earthquakes only began in the early 1900s, so it is too early to categorically state that this is the case. [46] Most of the world's earthquakes (90%, and 81% of the largest) take place in the 40,000-kilometre-long (25,000 mi), horseshoe-shaped zone called the circum-Pacific seismic belt, known as the Pacific Ring of Fire , which for the most part bounds the Pacific Plate . [47] [48] Massive earthquakes tend to occur along other plate boundaries too, such as along the Himalayan Mountains . [49] With the rapid growth of mega-cities such as Mexico City, Tokyo, and Tehran in areas of high seismic risk , some seismologists are warning that a single earthquake may claim the lives of up to three million people. [50] Induced seismicity Main article: Induced seismicity While most earthquakes are caused by the movement of the Earth's tectonic plates , human activity can also produce earthquakes. Activities both above ground and below may change the stresses and strains on the crust, including building reservoirs, extracting resources such as coal or oil, and injecting fluids underground for waste disposal or fracking . [51] Most of these earthquakes have small magnitudes. The 5.7 magnitude 2011 Oklahoma earthquake is thought to have been caused by disposing wastewater from oil production into injection wells , [52] and studies point to the state's oil industry as the cause of other earthquakes in the past century. [53] A Columbia University paper suggested that the 8.0 magnitude 2008 Sichuan earthquake was induced by loading from the Zipingpu Dam , [54] though the link has not been conclusively proved. [55] Measurement and location Main articles: Seismic magnitude scales and Seismology The instrumental scales used to describe the size of an earthquake began with the Richter magnitude scale in the 1930s. It is a relatively simple measurement of an event's amplitude, and its use has become minimal in the 21st century. Seismic waves travel through the Earth's interior and can be recorded by seismometers at great distances. The surface wave magnitude was developed in the 1950s as a means to measure remote earthquakes and to improve the accuracy for larger events. The moment magnitude scale not only measures the amplitude of the shock but also takes into account the seismic moment (total rupture area, average slip of the fault, and rigidity of the rock). The Japan Meteorological Agency seismic intensity scale , the Medvedev–Sponheuer–Karnik scale , and the Mercalli intensity scale are based on the observed effects and are related to the intensity of shaking. Intensity and magnitude The shaking of the earth is a common phenomenon that has been experienced by humans from the earliest of times. Before the development of strong-motion accelerometers, the intensity of a seismic event was estimated based on the observed effects. Magnitude and intensity are not directly related and calculated using different methods. The magnitude of an earthquake is a single value that describes the size of the earthquake at its source. Intensity is the measure of shaking at different locations around the earthquake. Intensity values vary from place to place, depending on the distance from the earthquake and the underlying rock or soil makeup. [56] The first scale for measuring earthquake magnitudes was developed by Charles Francis Richter in 1935. Subsequent scales (see seismic magnitude scales ) have retained a key feature, where each unit represents a ten-fold difference in the amplitude of the ground shaking and a 32-fold difference in energy. Subsequent scales are also adjusted to have approximately the same numeric value within the limits of the scale. [57] Although the mass media commonly reports earthquake magnitudes as "Richter magnitude" or "Richter scale", standard practice by most seismological authorities is to express an earthquake's strength on the moment magnitude scale, which is based on the actual energy released by an earthquake, the static seismic moment. [58] [59] Seismic waves Every earthquake produces different types of seismic waves, which travel through rock with different velocities: Longitudinal P-waves (shock- or pressure waves) Transverse S-waves (both body waves) Surface waves – ( Rayleigh and Love waves) Speed of seismic waves Propagation velocity of the seismic waves through solid rock ranges from approx. 3 km/s (1.9 mi/s) up to 13 km/s (8.1 mi/s), depending on the density and elasticity of the medium. In the Earth's interior, the shock- or P-waves travel much faster than the S-waves (approx. relation 1.7:1). The differences in travel time from the epicenter to the observatory are a measure of the distance and can be used to image both sources of earthquakes and structures within the Earth. Also, the depth of the hypocenter can be computed roughly. P-wave speed Upper crust soils and unconsolidated sediments: 2–3 km (1.2–1.9 mi) per second Upper crust solid rock: 3–6 km (1.9–3.7 mi) per second Lower crust: 6–7 km (3.7–4.3 mi) per second Deep mantle: 13 km (8.1 mi) per second. S-waves speed Light sediments: 2–3 km (1.2–1.9 mi) per second Earths crust:4–5 km (2.5–3.1 mi) per second Deep mantle: 7 km (4.3 mi) per second Seismic wave arrival As a consequence, the first waves of a distant earthquake arrive at an observatory via the Earth's mantle. On average, the kilometer distance to the earthquake is the number of seconds between the P- and S-wave times 8. [60] Slight deviations are caused by inhomogeneities of subsurface structure. By such analysis of seismograms, the Earth's core was located in 1913 by Beno Gutenberg . S-waves and later arriving surface waves do most of the damage compared to P-waves. P-waves squeeze and expand the material in the same direction they are traveling, whereas S-waves shake the ground up and down and back and forth. [61] Location and reporting Main article: Earthquake location Earthquakes are not only categorized by their magnitude but also by the place where they occur. The world is divided into 754 Flinn–Engdahl regions (F-E regions), which are based on political and geographical boundaries as well as seismic activity. More active zones are divided into smaller F-E regions whereas less active zones belong to larger F-E regions. Standard reporting of earthquakes includes its magnitude , date and time of occurrence, geographic coordinates of its epicenter , depth of the epicenter, geographical region, distances to population centers, location uncertainty, several parameters that are included in USGS earthquake reports (number of stations reporting, number of observations, etc.), and a unique event ID. [62] Although relatively slow seismic waves have traditionally been used to detect earthquakes, scientists realized in 2016 that gravitational measurement could provide instantaneous detection of earthquakes, and confirmed this by analyzing gravitational records associated with the 2011 Tohoku-Oki ("Fukushima") earthquake. [63] [64] Effects 1755 copper engraving depicting Lisbon in ruins and in flames after the 1755 Lisbon earthquake , which killed an estimated 60,000 people. A tsunami overwhelms the ships in the harbor. The effects of earthquakes include, but are not limited to, the following: Shaking and ground rupture Damaged buildings in Port-au-Prince , Haiti , January 2010 Shaking and ground rupture are the main effects created by earthquakes, principally resulting in more or less severe damage to buildings and other rigid structures. The severity of the local effects depends on the complex combination of the earthquake magnitude , the distance from the epicenter , and the local geological and geomorphological conditions, which may amplify or reduce wave propagation . [65] The ground-shaking is measured by ground acceleration . Specific local geological, geomorphological, and geostructural features can induce high levels of shaking on the ground surface even from low-intensity earthquakes. This effect is called site or local amplification. It is principally due to the transfer of the seismic motion from hard deep soils to soft superficial soils and the effects of seismic energy focalization owing to the typical geometrical setting of such deposits. Ground rupture is a visible breaking and displacement of the Earth's surface along the trace of the fault, which may be of the order of several meters in the case of major earthquakes. Ground rupture is a major risk for large engineering structures such as dams , bridges, and nuclear power stations and requires careful mapping of existing faults to identify any that are likely to break the ground surface within the life of the structure. [66] Soil liquefaction Main article: Soil liquefaction Soil liquefaction occurs when, because of the shaking, water-saturated granular material (such as sand) temporarily loses its strength and transforms from a solid to a liquid. Soil liquefaction may cause rigid structures, like buildings and bridges, to tilt or sink into the liquefied deposits. For example, in the 1964 Alaska earthquake , soil liquefaction caused many buildings to sink into the ground, eventually collapsing upon themselves. [67] Human impacts Ruins of the Għajn Ħadid Tower , which collapsed during the 1856 Heraklion earthquake Physical damage from an earthquake will vary depending on the intensity of shaking in a given area and the type of population.  Undeserved and developing communities frequently experience more severe impacts (and longer lasting) from a seismic event compared to well-developed communities. [68] Impacts may include: Injuries and loss of life Damage to critical infrastructure (short and long-term) Roads, bridges, and public transportation networks Water, power, sewer and gas interruption Communication systems General property damage Collapse or destabilization (potentially leading to future collapse) of buildings With these impacts and others, the aftermath may bring disease, lack of basic necessities, mental consequences such as panic attacks, and depression to survivors, [69] and higher insurance premiums. Recovery times will vary based on the level of damage along with the socioeconomic status of the impacted community. Landslides Further information: Landslide Earthquakes can produce slope instability leading to landslides, a major geological hazard. Landslide danger may persist while emergency personnel is attempting rescue work. [70] Fires Fires of the 1906 San Francisco earthquake Earthquakes can cause fires by damaging electrical power or gas lines. In the event of water mains rupturing and a loss of pressure, it may also become difficult to stop the spread of a fire once it has started. For example, more deaths in the 1906 San Francisco earthquake were caused by fire than by the earthquake itself. [71] Tsunami Main article: Tsunami Tsunamis are long-wavelength, long-period sea waves produced by the sudden or abrupt movement of large volumes of water—including when an earthquake occurs at sea . In the open ocean, the distance between wave crests can surpass 100 kilometres (62 mi), and the wave periods can vary from five minutes to one hour. Such tsunamis travel 600–800 kilometers per hour (373–497 miles per hour), depending on water depth. Large waves produced by an earthquake or a submarine landslide can overrun nearby coastal areas in a matter of minutes. Tsunamis can also travel thousands of kilometers across open ocean and wreak destruction on far shores hours after the earthquake that generated them. [72] Ordinarily, subduction earthquakes under magnitude 7.5 do not cause tsunamis, although some instances of this have been recorded. Most destructive tsunamis are caused by earthquakes of magnitude 7.5 or more. [72] Floods Further information: Flood Floods may be secondary effects of earthquakes if dams are damaged. Earthquakes may cause landslips to dam rivers, which collapse and cause floods. [73] The terrain below the Sarez Lake in Tajikistan is in danger of catastrophic flooding if the landslide dam formed by the earthquake, known as the Usoi Dam , were to fail during a future earthquake. Impact projections suggest the flood could affect roughly five million people. [74] Management Main article: Earthquake prediction Earthquake prediction is a branch of the science of seismology concerned with the specification of the time, location, and magnitude of future earthquakes within stated limits. [75] Many methods have been developed for predicting the time and place in which earthquakes will occur. Despite considerable research efforts by seismologists , scientifically reproducible predictions cannot yet be made to a specific day or month. [76] Forecasting Main article: Earthquake forecasting While forecasting is usually considered to be a type of prediction , earthquake forecasting is often differentiated from earthquake prediction . Earthquake forecasting is concerned with the probabilistic assessment of general earthquake hazards, including the frequency and magnitude of damaging earthquakes in a given area over years or decades. [77] For well-understood faults the probability that a segment may rupture during the next few decades can be estimated. [78] [79] Earthquake warning systems have been developed that can provide regional notification of an earthquake in progress, but before the ground surface has begun to move, potentially allowing people within the system's range to seek shelter before the earthquake's impact is felt. Preparedness Main article: Earthquake preparedness The objective of earthquake engineering is to foresee the impact of earthquakes on buildings, bridges, tunnels, roadways, and other structures, and to design such structures to minimize the risk of damage. Existing structures can be modified by seismic retrofitting to improve their resistance to earthquakes. Earthquake insurance can provide building owners with financial protection against losses resulting from earthquakes. Emergency management strategies can be employed by a government or organization to mitigate risks and prepare for consequences. Artificial intelligence may help to assess buildings and plan precautionary operations: the Igor expert system is part of a mobile laboratory that supports the procedures leading to the seismic assessment of masonry buildings and the planning of retrofitting operations on them. It has been successfully applied to assess buildings in Lisbon , Rhodes , Naples . [80] Individuals can also take preparedness steps like securing water heaters and heavy items that could injure someone, locating shutoffs for utilities, and being educated about what to do when the shaking starts. For areas near large bodies of water, earthquake preparedness encompasses the possibility of a tsunami caused by a large earthquake. In culture Historical views An image from a 1557 book depicting an earthquake in Italy in the 4th century BCE From the lifetime of the Greek philosopher Anaxagoras in the 5th century BCE to the 14th century CE, earthquakes were usually attributed to "air (vapors) in the cavities of the Earth." [81] Thales of Miletus (625–547 BCE) was the only documented person who believed that earthquakes were caused by tension between the earth and water. [81] Other theories existed, including the Greek philosopher Anaxamines' (585–526 BCE) beliefs that short incline episodes of dryness and wetness caused seismic activity. The Greek philosopher Democritus (460–371 BCE) blamed water in general for earthquakes. [81] Pliny the Elder called earthquakes "underground thunderstorms". [81] Mythology and religion In Norse mythology , earthquakes were explained as the violent struggle of the god Loki . When Loki, god of mischief and strife, murdered Baldr , god of beauty and light, he was punished by being bound in a cave with a poisonous serpent placed above his head dripping venom. Loki's wife Sigyn stood by him with a bowl to catch the poison, but whenever she had to empty the bowl the poison dripped on Loki's face, forcing him to jerk his head away and thrash against his bonds, which caused the earth to tremble. [82] In Greek mythology , Poseidon was the cause and god of earthquakes. When he was in a bad mood, he struck the ground with a trident , causing earthquakes and other calamities. He also used earthquakes to punish and inflict fear upon people as revenge. [83] In Japanese mythology , Namazu (鯰) is a giant catfish who causes earthquakes. Namazu lives in the mud beneath the earth and is guarded by the god Kashima who restrains the fish with a stone. When Kashima lets his guard fall, Namazu thrashes about, causing violent earthquakes. [84] In popular culture In modern popular culture, the portrayal of earthquakes is shaped by the memory of great cities laid waste, such as Kobe in 1995 or San Francisco in 1906 . [85] Fictional earthquakes tend to strike suddenly and without warning. [85] For this reason, stories about earthquakes generally begin with the disaster and focus on its immediate aftermath, as in Short Walk to Daylight (1972), The Ragged Edge (1968) or Aftershock: Earthquake in New York (1999). [85] A notable example is Heinrich von Kleist's classic novella, The Earthquake in Chile , which describes the destruction of Santiago in 1647. Haruki Murakami 's short fiction collection After the Quake depicts the consequences of the Kobe earthquake of 1995. The most popular single earthquake in fiction is the hypothetical "Big One" expected of California's San Andreas Fault someday, as depicted in the novels Richter 10 (1996), Goodbye California (1977), 2012 (2009) and San Andreas (2015) among other works. [85] Jacob M. Appel's widely anthologized short story, A Comparative Seismology, features a con artist who convinces an elderly woman that an apocalyptic earthquake is imminent. [86] Contemporary depictions of earthquakes in film are variable in the manner in which they reflect human psychological reactions to the actual trauma that can be caused to directly afflicted families and their loved ones. [87] Disaster mental health response research emphasizes the need to be aware of the different roles of loss of family and key community members, loss of home and familiar surroundings, and loss of essential supplies and services to maintain survival. [88] [89] Particularly for children, the clear availability of caregiving adults who can protect, nourish, and clothe them in the aftermath of the earthquake, and to help them make sense of what has befallen them has been shown even more important to their emotional and physical health than the simple giving of provisions. [90] As was observed after other disasters involving destruction and loss of life and their media depictions, recently observed in the 2010 Haiti earthquake , it is also important not to pathologize the reactions to loss and displacement or disruption of governmental administration and services, but rather to validate these reactions, to support constructive problem-solving and reflection as to how one might improve the conditions of those affected. [91] Outside of earth Phenomena similar to earthquakes have been observed in other planets (e.g., marsquakes on Mars) and on the Moon (see moonquakes ). See also
Toggle the table of contents Earth science From Wikipedia, the free encyclopedia Fields of natural science related to Earth The rocky side of a mountain creek in Costa Rica Earth science or geoscience includes all fields of natural science related to the planet Earth . [1] This is a branch of science dealing with the physical, chemical, and biological complex constitutions and synergistic linkages of Earth's four spheres: the biosphere , hydrosphere / cryosphere , atmosphere , and geosphere (or lithosphere ). Earth science can be considered to be a branch of planetary science but with a much older history. There are reductionist and holistic approaches to Earth sciences. It is also the study of Earth and its neighbors in space. Some Earth scientists use their knowledge of the planet to locate and develop energy and mineral resources. Others study the impact of human activity on Earth's environment, and design methods to protect the planet. Some use their knowledge about Earth processes such as volcanoes , earthquakes , and hurricanes to help protect people from these dangerous events. Typically, Earth scientists use tools from geology , chronology , physics , chemistry , geography , biology , and mathematics to build a quantitative understanding of how Earth works and evolves. For example, meteorologists study the weather and watch for dangerous storms. Hydrologists examine water and warn of floods. Seismologists study earthquakes and try to understand where they will strike. Geologists study rocks and help to locate useful minerals. Earth scientists often work in the field—perhaps climbing mountains, exploring the seabed, crawling through caves, or wading in swamps. They measure and collect samples (such as rocks or river water), then record their findings on charts and maps. Layers of sedimentary rock in Makhtesh Ramon Geology is broadly the study of Earth's structure, substance, and processes. Geology is largely the study of the lithosphere , or Earth's surface, including the crust and rocks . It includes the physical characteristics and processes that occur in the lithosphere as well as how they are affected by geothermal energy . It incorporates aspects of chemistry, physics, and biology as elements of geology interact. Historical geology is the application of geology to interpret Earth history and how it has changed over time. Geochemistry studies the chemical components and processes of the Earth. Geophysics studies the physical properties of the Earth. Paleontology studies fossilized biological material in the lithosphere. Planetary geology studies geoscience as it pertains to extraterrestrial bodies. Geomorphology studies the origin of landscapes. Structural geology studies the deformation of rocks to produce mountains and lowlands. Resource geology studies how energy resources can be obtained from minerals. Environmental geology studies how pollution and contaminants affect soil and rock. [2] Mineralogy is the study of minerals and includes the study of mineral formation, crystal structure , hazards associated with minerals, and the physical and chemical properties of minerals. [3] Petrology is the study of rocks, including the formation and composition of rocks. Petrography is a branch of petrology that studies the typology and classification of rocks. [4] Main article: Structure of Earth Plate tectonics , mountain ranges , volcanoes , and earthquakes are geological phenomena that can be explained in terms of physical and chemical processes in the Earth's crust. [6] Beneath the Earth's crust lies the mantle which is heated by the radioactive decay of heavy elements . The mantle is not quite solid and consists of magma which is in a state of semi-perpetual convection . This convection process causes the lithospheric plates to move, albeit slowly. The resulting process is known as plate tectonics. [7] [8] [9] [10] Areas of the crust where new crust is created are called divergent boundaries, those where it is brought back into the Earth are convergent boundaries and those where plates slide past each other, but no new lithospheric material is created or destroyed, are referred to as transform (or conservative) boundaries [8] [10] [11] Earthquakes result from the movement of the lithospheric plates, and they often occur near convergent boundaries where parts of the crust are forced into the earth as part of subduction. [12] Plate tectonics might be thought of as the process by which the Earth is resurfaced. As the result of seafloor spreading , new crust and lithosphere is created by the flow of magma from the mantle to the near surface, through fissures, where it cools and solidifies Through subduction , oceanic crust and lithosphere vehemently returns to the convecting mantle. [8] [10] [13] Volcanoes result primarily from the melting of subducted crust material. Crust material that is forced into the asthenosphere melts, and some portion of the melted material becomes light enough to rise to the surface—giving birth to volcanoes. [8] [12] Main article: Atmospheric science The magnetosphere shields the surface of Earth from the charged particles of the solar wind .(image not to scale.) Atmospheric science initially developed in the late-19th century as a means to forecast the weather through meteorology , the study of weather. Atmospheric chemistry was developed in the 20th century to measure air pollution and expanded in the 1970s in response to acid rain . Climatology studies the climate and climate change . [14] The troposphere , stratosphere , mesosphere , thermosphere , and exosphere are the five layers which make up Earth's atmosphere. 75% of the mass in the atmosphere is located within the troposphere, the lowest layer. In all, the atmosphere is made up of about 78.0% nitrogen , 20.9% oxygen , and 0.92% argon , and small amounts of other gases including CO2 and water vapor. [15] Water vapor and CO2 cause the Earth's atmosphere to catch and hold the Sun's energy through the greenhouse effect . [16] This makes Earth's surface warm enough for liquid water and life. In addition to trapping heat, the atmosphere also protects living organisms by shielding the Earth's surface from cosmic rays . [17] The magnetic field —created by the internal motions of the core—produces the magnetosphere which protects Earth's atmosphere from the solar wind . [18] As the Earth is 4.5 billion years old, [19] [20] it would have lost its atmosphere by now if there were no protective magnetosphere. Earth's magnetic field[ edit ] This section is an excerpt from Earth's magnetic field .[ edit ] Computer simulation of Earth 's field in a period of normal polarity between reversals. [21] The lines represent magnetic field lines, blue when the field points towards the center and yellow when away. The rotation axis of Earth is centered and vertical. The dense clusters of lines are within Earth's core. [22] Earth's magnetic field , also known as the geomagnetic field, is the magnetic field that extends from Earth's interior out into space, where it interacts with the solar wind , a stream of charged particles emanating from the Sun . The magnetic field is generated by electric currents due to the motion of convection currents of a mixture of molten iron and nickel in Earth's outer core : these convection currents are caused by heat escaping from the core, a natural process called a geodynamo . The magnitude of Earth's magnetic field  at its surface ranges from 25 to 65 μT (0.25 to 0.65 G). [23] As an approximation, it is represented by a field of a magnetic dipole currently tilted at an angle of about 11° with respect to Earth's rotational axis, as if there were an enormous bar magnet placed at that angle through the center of Earth. The North geomagnetic pole actually represents the South pole of Earth's magnetic field, and conversely the South geomagnetic pole corresponds to the north pole of Earth's magnetic field (because opposite magnetic poles attract and the north end of a magnet, like a compass needle, points toward Earth's South magnetic field, Ellesmere Island , Nunavut , Canada). While the North and South magnetic poles are usually located near the geographic poles, they slowly and continuously move over geological time scales, but sufficiently slowly for ordinary compasses to remain useful for navigation. However, at irregular intervals averaging several hundred thousand years, Earth's field reverses and the North and South Magnetic Poles respectively, abruptly switch places. These reversals of the geomagnetic poles leave a record in rocks that are of value to paleomagnetists in calculating geomagnetic fields in the past. Such information in turn is helpful in studying the motions of continents and ocean floors in  above the ionosphere that is defined by the extent of Earth's magnetic field in space. It extends several tens of thousands of kilometres into space , protecting Earth from the charged particles of the solar wind and cosmic rays that would otherwise strip away the upper atmosphere, including the ozone layer that protects Earth from harmful ultraviolet radiation . Main article: Hydrology Movement of water through the water cycle Hydrology is the study of the hydrosphere and the movement of water on Earth . It emphasizes the study of how humans use and interact with freshwater supplies. Study of water's movement is closely related to geomorphology and other branches of Earth science. Applied hydrology involves engineering to maintain aquatic environments and distribute water supplies. Subdisciplines of hydrology include oceanography , hydrogeology , ecohydrology , and glaciology . Oceanography is the study of oceans. [24] Hydrogeology is the study of groundwater . It includes the mapping of groundwater supplies and the analysis of groundwater contaminants. Applied hydrogeology seeks to prevent contamination of groundwater and mineral springs and make it available as drinking water . The earliest exploitation of groundwater resources dates back to 3000 BC, and hydrogeology as a science was developed by hydrologists beginning in the 17th century. [25] Ecohydrology is the study of ecological systems in the hydrosphere . It can be divided into the physical study of aquatic ecosystems and the biological study of aquatic organisms. Ecohydrology includes the effects that organisms and aquatic ecosystems have on one another as well as how these ecoystems are affected by humans. [26] Glaciology is the study of the cryosphere, including glaciers and coverage of the Earth by ice and snow. Concerns of glaciology include access to glacial freshwater, mitigation of glacial hazards, obtaining resources that exist beneath frozen land, and addressing the effects of climate change on the cryosphere. [27] Main article: Ecology Ecology is the study of the biosphere . This includes the study of nature and of how living things interact with the Earth and one another and the consequences of that. It considers how living things use resources such as oxygen , water , and nutrients from the Earth to sustain themselves. It also considers how humans and other living creatures cause changes to nature. [28] Main article: Physical geography Physical geography is the study of Earth's systems and how they interact with one another as part of a single self-contained system. It incorporates astronomy, mathematical geography, meteorology, climatology, geology, geomorphology, biology, biogeography, pedology, and soils geography. Physical geography is distinct from human geography , which studies the human populations on Earth, though it does include human effects on the environment. [29]
Toggle the table of contents Volcanic ash From Wikipedia, the free encyclopedia Natural material created during volcanic eruptions "Ash cloud" redirects here. For general topic, see Ash . For other uses, see Ash (disambiguation) . Ash cloud from the 2008 eruption of Chaitén volcano , Chile , stretching across Patagonia from the Pacific to the Atlantic Ocean Ash plume rising from Eyjafjallajökull on April 17, 2010 Volcanic ash deposits on a parked McDonnell Douglas DC-10-30 during the 1991 eruption of Mount Pinatubo . While falling ash behaves in a similar manner to snow , the sheer weight of deposits can cause serious damage to buildings and vehicles. In this case, volcanic ash deposits shifted the airliner's center of gravity, leading to it resting on its tail. Ash plume from Mt Cleveland , a stratovolcano in the Aleutian Islands Volcanic ash consists of fragments of rock, mineral crystals , and volcanic glass , produced during volcanic eruptions and measuring less than 2 mm (0.079 inches) in diameter. [1] The term volcanic ash is also often loosely used to refer to all explosive eruption products (correctly referred to as tephra ), including particles larger than 2 mm. Volcanic ash is formed during explosive volcanic eruptions when dissolved gases in magma expand and escape violently into the atmosphere. The force of the gases shatters the magma and propels it into the atmosphere where it solidifies into fragments of volcanic rock and glass. Ash is also produced when magma comes into contact with water during phreatomagmatic eruptions , causing the water to explosively flash to steam leading to shattering of magma. Once in the air, ash is transported by wind up to thousands of kilometres away. Due to its wide dispersal, ash can have a number of impacts on society, including animal and human health, disruption to aviation, disruption to critical infrastructure (e.g., electric power supply systems, telecommunications, water and waste-water networks, transportation), primary industries (e.g., agriculture), buildings and structures. Formation[ edit ] 454 million-year-old volcanic ash between layers of limestone in the catacombs of Peter the Great's Naval Fortress in Estonia near Laagri . This is a remnant of one of the oldest large eruptions preserved. The diameter of the black camera lens cover is 58 mm (2.3 in). Volcanic ash is formed during explosive volcanic eruptions and phreatomagmatic eruptions, [2] and may also be formed during transport in pyroclastic density currents. [3] Explosive eruptions occur when magma decompresses as it rises, allowing dissolved volatiles (dominantly water and carbon dioxide ) to exsolve into gas bubbles. [4] As more bubbles nucleate a foam is produced, which decreases the density of the magma, accelerating it up the conduit. Fragmentation occurs when bubbles occupy ~70–80 vol% of the erupting mixture. [5] When fragmentation occurs, violently expanding bubbles tear the magma apart into fragments which are ejected into the atmosphere where they solidify into ash particles. Fragmentation is a very efficient process of ash formation and is capable of generating very fine ash even without the addition of water. [6] Volcanic ash is also produced during phreatomagmatic eruptions. During these eruptions fragmentation occurs when magma comes into contact with bodies of water (such as the sea, lakes and marshes) groundwater, snow or ice. As the magma, which is significantly hotter than the boiling point of water, comes into contact with water an insulating vapor film forms ( Leidenfrost effect ). [7] Eventually this vapor film will collapse leading to direct coupling of the cold water and hot magma. This increases the heat transfer which leads to the rapid expansion of water and fragmentation of the magma into small particles which are subsequently ejected from the volcanic vent. Fragmentation causes an increase in contact area between magma and water creating a feedback mechanism, [7] leading to further fragmentation and production of fine ash particles. Pyroclastic density currents can also produce ash particles. These are typically produced by lava dome collapse or collapse of the eruption column . [8] Within pyroclastic density currents particle abrasion occurs as particles violently collide, resulting in a reduction in grain size and production of fine grained ash particles. In addition, ash can be produced during secondary fragmentation of pumice fragments, due to the conservation of heat within the flow. [9] These processes produce large quantities of very fine grained ash which is removed from pyroclastic density currents in co-ignimbrite ash plumes. Physical and chemical characteristics of volcanic ash are primarily controlled by the style of volcanic eruption. [10] Volcanoes display a range of eruption styles which are controlled by magma chemistry, crystal content, temperature and dissolved gases of the erupting magma and can be classified using the volcanic explosivity index (VEI) . Effusive eruptions (VEI 1) of basaltic composition produce <105 m3 of ejecta, whereas extremely explosive eruptions (VEI 5+) of rhyolitic and dacitic composition can inject large quantities (>109 m3) of ejecta into the atmosphere. [11] Chemical[ edit ] The types of minerals present in volcanic ash are dependent on the chemistry of the magma from which it erupted. Considering that the most abundant elements found in silicate magma are silicon and oxygen , the various types of magma (and therefore ash) produced during volcanic eruptions are most commonly explained in terms of their silica content. Low energy eruptions of basalt produce a characteristically dark coloured ash containing ~45–55% silica that is generally rich in iron (Fe) and magnesium (Mg). The most explosive rhyolite eruptions produce a felsic ash that is high in silica (>69%) while other types of ash with an intermediate composition (e.g., andesite or dacite ) have a silica content between 55 and 69%. The principal gases released during volcanic activity are water , carbon dioxide , hydrogen , sulfur dioxide , hydrogen sulfide , carbon monoxide and hydrogen chloride . [12] The sulfur and halogen gases and metals are removed from the atmosphere by processes of chemical reaction, dry and wet deposition, and by adsorption onto the surface of volcanic ash. It has long been recognised that a range of sulfate and halide (primarily chloride and fluoride ) compounds are readily mobilised from fresh volcanic ash. [13] [14] It is considered most likely that these salts are formed as a consequence of rapid acid dissolution of ash particles within eruption plumes , which is thought to supply the cations involved in the deposition of sulfate and halide salts . [15] While some 55 ionic species have been reported in fresh ash leachates , [12] the most abundant species usually found are the cations Na+ , K+ , Ca2+ and Mg2+ and the anions Cl− , F− and SO42− . [12] [14] Molar ratios between ions present in leachates suggest that in many cases these elements are present as simple salts such as NaCl and CaSO4 . [12] [16] [17] [18] In a sequential leaching experiment on ash from the 1980 eruption of Mount St. Helens , chloride salts were found to be the most readily soluble, followed by sulfate salts [16] Fluoride compounds are in general only sparingly soluble (e.g., CaF2 , MgF2 ), with the exception of fluoride salts of alkali metals and compounds such as calcium hexafluorosilicate (CaSiF6). [19] The pH of fresh ash leachates is highly variable, depending on the presence of an acidic gas condensate (primarily as a consequence of the gases SO2 , HCl and HF in the eruption plume) on the ash surface. The crystalline-solid structure of the salts act more as an insulator than a conductor . [20] [21] [22] [23] However, once the salts are dissolved into a solution by a source of moisture (e.g., fog, mist, light rain, etc.), the ash may become corrosive and electrically conductive. A recent study has shown that the electrical conductivity of volcanic ash increases with (1) increasing moisture content, (2) increasing soluble salt content, and (3) increasing compaction (bulk density). [23] The ability of volcanic ash to conduct electric current has significant implications for electric power supply systems. A scanning electron micrograph image  of a particle of volcanic ash from Mount St. Helens Volcanic ash particles erupted during magmatic eruptions are made up of various fractions of vitric (glassy, non-crystalline), crystalline or lithic (non-magmatic) particles. Ash produced during low viscosity magmatic eruptions (e.g., Hawaiian and Strombolian basaltic eruptions) produce a range of different pyroclasts dependent on the eruptive process. For example, ash collected from Hawaiian lava fountains consists of sideromelane (light brown basaltic glass) pyroclasts which contain microlites (small quench crystals, not to be confused with the rare mineral microlite ) and phenocrysts . Slightly more viscous eruptions of basalt (e.g., Strombolian) form a variety of pyroclasts from irregular sideromelane droplets to blocky tachylite (black to dark brown microcrystalline pyroclasts). In contrast, most high-silica ash (e.g. rhyolite) consists of pulverised products of pumice (vitric shards), individual phenocrysts (crystal fraction) and some lithic fragments ( xenoliths ). [24] Ash generated during phreatic eruptions primarily consists of hydrothermally altered lithic and mineral fragments, commonly in a clay matrix. Particle surfaces are often coated with aggregates of zeolite crystals or clay and only relict textures remain to identify pyroclast types. [24] Morphology[ edit ] Light microscope image of ash from the 1980 eruption of Mount St. Helens, Washington The morphology (shape) of volcanic ash is controlled by a plethora of different eruption and kinematic processes. [24] [25] Eruptions of low-viscosity magmas (e.g., basalt) typically form droplet shaped particles. This droplet shape is, in part, controlled by surface tension , acceleration of the droplets after they leave the vent, and air friction. Shapes range from perfect spheres to a variety of twisted, elongate droplets with smooth, fluidal surfaces. [25] The morphology of ash from eruptions of high-viscosity magmas (e.g., rhyolite, dacite, and some andesites) is mostly dependent on the shape of vesicles in the rising magma before disintegration. Vesicles are formed by the expansion of magmatic gas before the magma has solidified. Ash particles can have varying degrees of vesicularity and vesicular particles can have extremely high surface area to volume ratios. [24] Concavities, troughs, and tubes observed on grain surfaces are the result of broken vesicle walls. [25] Vitric ash particles from high-viscosity magma eruptions are typically angular, vesicular pumiceous fragments or thin vesicle-wall fragments while lithic fragments in volcanic ash are typically equant, or angular to subrounded. Lithic morphology in ash is generally controlled by the mechanical properties of the wall rock broken up by spalling or explosive expansion of gases in the magma as it reaches the surface. The morphology of ash particles from phreatomagmatic eruptions is controlled by stresses within the chilled magma which result in fragmentation of the glass to form small blocky or pyramidal glass ash particles. [24] Vesicle shape and density play only a minor role in the determination of grain shape in phreatomagmatic eruptions. In this sort of eruption, the rising magma is quickly cooled on contact with ground or surface water. Stresses within the "quenched" magma cause fragmentation into five dominant pyroclast shape-types: (1) blocky and equant; (2) vesicular and irregular with smooth surfaces; (3) moss-like and convoluted; (4) spherical or drop-like; and (5) plate-like. Density[ edit ] The density of individual particles varies with different eruptions. The density of volcanic ash varies between 700 and 1200 kg/m3 for pumice, 2350–2450 kg/m3 for glass shards, 2700–3300 kg/m3 for crystals, and 2600–3200 kg/m3 for lithic particles. [26] Since coarser and denser particles are deposited close to source, fine glass and pumice shards are relatively enriched in ash fall deposits at distal locations. [27] The high density and hardness (~5 on the Mohs Hardness Scale ) together with a high degree of angularity, make some types of volcanic ash (particularly those with a high silica content) very abrasive. Grain size[ edit ] Volcanic ash grain size distributions from four volcanic eruptions Volcanic ash consists of particles (pyroclasts) with diameters less than 2 mm (particles larger than 2 mm are classified as lapilli), [1] and can be as fine as 1 μm. [10] The overall grain size distribution of ash can vary greatly with different magma compositions. Few attempts have been made to correlate the grain size characteristics of a deposit with those of the event which produced it, though some predictions can be made. Rhyolitic magmas generally produce finer grained material compared to basaltic magmas, due to the higher viscosity and therefore explosivity. The proportions of fine ash are higher for silicic explosive eruptions, probably because vesicle size in the pre-eruptive magma is smaller than those in mafic magmas. [1] There is good evidence that pyroclastic flows produce high proportions of fine ash by communition and it is likely that this process also occurs inside volcanic conduits and would be most efficient when the magma fragmentation surface is well below the summit crater. [1] Dispersal[ edit ] Ash plume rising from Mount Redoubt after an eruption on April 21, 1990 Ash particles are incorporated into eruption columns as they are ejected from the vent at high velocity. The initial momentum from the eruption propels the column upwards. As air is drawn into the column, the bulk density decreases and it starts to rise buoyantly into the atmosphere. [8] At a point where the bulk density of the column is the same as the surrounding atmosphere, the column will cease rising and start moving laterally. Lateral dispersion is controlled by prevailing winds and the ash may be deposited hundreds to thousands of kilometres from the volcano, depending on eruption column height, particle size of the ash and climatic conditions (especially wind direction and strength and humidity). [28] Ash plume and ash fallout at Mount Pagan , May 1994 Ash fallout occurs immediately after the eruption and is controlled by particle density. Initially, coarse particles fall out close to source. This is followed by fallout of accretionary lapilli , which is the result of particle agglomeration within the column. [29] Ash fallout is less concentrated during the final stages as the column moves downwind. This results in an ash fall deposit which generally decreases in thickness and grain size exponentially with increasing distance from the volcano. [30] Fine ash particles may remain in the atmosphere for days to weeks and be dispersed by high-altitude winds. These particles can impact on the aviation industry (refer to impacts section) and, combined with gas particles, can affect global climate. Volcanic ash plumes can form above pyroclastic density currents. These are called co-ignimbrite plumes. As pyroclastic density currents travel away from the volcano, smaller particles are removed from the flow by elutriation and form a less dense zone overlying the main flow. This zone then entrains the surrounding air and a buoyant co-ignimbrite plume is formed. These plumes tend to have higher concentrations of fine ash particles compared to magmatic eruption plumes due to the abrasion within the pyroclastic density current. [1] e Population growth has caused the progressive encroachment of urban development into higher risk areas, closer to volcanic centres, increasing the human exposure to volcanic ash fall events. [31] Direct health effects of volcanic ash on humans are usually short-term and mild for persons in normal health, though prolonged exposure potentially poses some risk of silicosis in unprotected workers. [32] Of greater concern is the impact of volcanic ash on the infrastructure critical to supporting modern societies, particularly in urban areas, where high population densities create high demand for services. [33] [31] Several recent eruptions have illustrated the vulnerability of urban areas that received only a few millimetres or centimetres of volcanic ash. [34] [35] [36] [37] [38] This has been sufficient to cause disruption of transportation, [39] electricity , [40] water , [41] [42] sewage and storm water systems. [43] Costs have been incurred from business disruption, replacement of damaged parts and insured losses. Ash fall impacts on critical infrastructure can also cause multiple knock-on effects, which may disrupt many different sectors and services. [44] Volcanic ash fall is physically, socially, and economically disruptive. [45] Volcanic ash can affect both proximal areas and areas many hundreds of kilometres from the source, [46] and causes disruptions and losses in a wide variety of different infrastructure sectors. Impacts are dependent on: ash fall thickness; the grain size and chemistry of the ash; whether the ash is wet or dry; the duration of the ash fall; and any preparedness , management and prevention (mitigation) measures employed to reduce effects from the ash fall. Different sectors of infrastructure and society are affected in different ways and are vulnerable to a range of impacts or consequences. These are discussed in the following sections. [31] Human and animal health[ edit ] Ash particles of less than 10 µm diameter suspended in the air are known to be inhalable, and people exposed to ash falls have experienced respiratory discomfort, breathing difficulty, eye and skin irritation, and nose and throat symptoms. [47] Most of these effects are short-term and are not considered to pose a significant health risk to those without pre-existing respiratory conditions . [32] The health effects of volcanic ash depend on the grain size, mineralogical composition and chemical coatings on the surface of the ash particles. [32] Additional factors related to potential respiratory symptoms are the frequency and duration of exposure, the concentration of ash in the air and the respirable ash fraction; the proportion of ash with less than 10 µm diameter, known as PM10 . The social context may also be important. Chronic health effects from volcanic ash fall are possible, as exposure to free crystalline silica is known to cause silicosis . Minerals associated with this include quartz , cristobalite and tridymite , which may all be present in volcanic ash. These minerals are described as ‘free’ silica as the SiO2 is not attached to another element to create a new mineral. However, magmas containing less than 58% SiO2 are thought to be unlikely to contain crystalline silica. [32] The exposure levels to free crystalline silica in the ash are commonly used to characterise the risk of silicosis in occupational studies (for people who work in mining, construction and other industries,) because it is classified as a human carcinogen by the International Agency for Research on Cancer . Guideline values have been created for exposure, but with unclear rationale; UK guidelines for particulates in air (PM10) are 50 µg/m3 and USA guidelines for exposure to crystalline silica are 50 µg/m3. [32] It is thought that the guidelines on exposure levels could be exceeded for short periods of time without significant health effects on the general population. [47] There have been no documented cases of silicosis developed from exposure to volcanic ash. However, long-term studies necessary to evaluate these effects are lacking. [32] Ingesting ash[ edit ] For surface water sources such as lakes and reservoirs, the volume available for dilution of ionic species leached from ash is generally large. The most abundant components of ash leachates (Ca, Na, Mg, K, Cl, F and SO4) occur naturally at significant concentrations in most surface waters and therefore are not affected greatly by inputs from volcanic ashfall, and are also of low concern in drinking water, with the exception of fluorine . The elements iron , manganese and aluminium are commonly enriched over background levels by volcanic ashfall. These elements may impart a metallic taste to water, and may produce red, brown or black staining of whiteware, but are not considered a health risk. Volcanic ashfalls are not known to have caused problems in water supplies for toxic trace elements such as mercury (Hg) and lead (Pb) which occur at very low levels in ash leachates. [42] Ingesting ash may be harmful to livestock , causing abrasion of the teeth, and in cases of high fluorine content, fluorine poisoning (toxic at levels of >100 µg/g) for grazing animals. [48] It is known from the 1783 eruption of Laki in Iceland that fluorine poisoning occurred in humans and livestock as a result of the chemistry of the ash and gas, which contained high levels of hydrogen fluoride . Following the 1995/96 Mount Ruapehu eruptions in New Zealand, two thousand ewes and lambs died after being affected by fluorosis while grazing on land with only 1–3 mm of ash fall. [48] Symptoms of fluorosis among cattle exposed to ash include brown-yellow to green-black mottles in the teeth, and hypersensibility to pressure in the legs and back. [49] Ash ingestion may also cause gastrointestinal blockages. [37] Sheep that ingested ash from the 1991 Mount Hudson volcanic eruption in Chile, suffered from diarrhoea and weakness. Other effects on livestock[ edit ] Ash accumulating in the back wool of sheep may add significant weight, leading to fatigue and sheep that can not stand up. Rainfall may result in a significant burden as it adds weight to ash. [50] Pieces of wool may fall away and any remaining wool on sheep may be worthless as poor nutrition associated with volcanic eruptions impacts the quality of the fibre. [50] As the usual pastures and plants become covered in volcanic ash during eruption some livestock may resort to eat whatever is available including toxic plants. [51] There are reports of goats and sheep in Chile and Argentina having natural abortions in connection to volcanic eruptions. [52] Electricity[ edit ] Electrical insulator flashover caused by volcanic ash contamination Volcanic ash can disrupt electric power supply systems at all levels of power generation, transformation, transmission, and distribution. There are four main impacts arising from ash-contamination of apparatus used in the power delivery process: [53] Wet deposits of ash on high voltage insulators can initiate a leakage current (small amount of current flow across the insulator surface) which, if sufficient current is achieved, can cause ‘flashover’ (the unintended electrical discharge around or over the surface of an insulating material). If the resulting short-circuit current is high enough to trip the circuit breaker then disruption of service will occur. Ash-induced flashover across transformer insulation (bushings) can burn, etch or crack the insulation irreparably and can result in the disruption of the power supply. [54] Volcanic ash can erode, pit, and scour metallic apparatus, particularly moving parts such as water and wind turbines and cooling fans on transformers or thermal power plants. [55] The high bulk density of some ash deposits can cause line breakage and damage to steel towers and wooden poles due to ash loading. This is most hazardous when the ash and/or the lines and structures are wet (e.g., by rainfall) and there has been ≥10  mm of ashfall. Fine-grained ash (e.g., <0.5  mm diameter) adheres to lines and structures most readily. Volcanic ash may also load overhanging vegetation, causing it to fall onto lines. Snow and ice accumulation on lines and overhanging vegetation further increases the risk of breakage and or collapse of lines and other hardware. [56] Controlled outages of vulnerable connection points (e.g., substations ) or circuits until ash fall has subsided or for de-energised cleaning of equipment. [57] Drinking water supplies[ edit ] Water turbine from the Agoyan hydroelectric plant eroded by volcanic ash laden water Groundwater-fed systems are resilient to impacts from ashfall, although airborne ash can interfere with the operation of well-head pumps. Electricity outages caused by ashfall can also disrupt electrically powered pumps if there is no backup generation. [58] The physical impacts of ashfall can affect the operation of water treatment plants. Ash can block intake structures, cause severe abrasion damage to pump impellers and overload pump motors. [58] Ash can enter filtration systems such as open sand filters both by direct fallout and via intake waters. In most cases, increased maintenance will be required to manage the effects of an ashfall, but there will not be service interruptions. [59] The final step of drinking water treatment is disinfection to ensure that final drinking water is free from infectious microorganisms. As suspended particles (turbidity) can provide a growth substrate for microorganisms and can protect them from disinfection treatment, it is extremely important that the water treatment process achieves a good level of removal of suspended particles. Chlorination may have to be increased to ensure adequate disinfection. [60] Many households, and some small communities, rely on rainwater for their drinking water supplies. Roof-fed systems are highly vulnerable to contamination by ashfall, as they have a large surface area relative to the storage tank volume. In these cases, leaching of chemical contaminants from the ashfall can become a health risk and drinking of water is not recommended. Prior to an ashfall, downpipes should be disconnected so that water in the tank is protected. A further problem is that the surface coating of fresh volcanic ash can be acidic. Unlike most surface waters, rainwater generally has a very low alkalinity (acid-neutralising capacity) and thus ashfall may acidify tank waters. This may lead to problems with plumbosolvency , whereby the water is more aggressive towards materials that it comes into contact with. This can be a particular problem if there are lead-head nails or lead flashing used on the roof, and for copper pipes and other metallic plumbing fittings. [61] During ashfall events, large demands are commonly placed on water resources for cleanup and shortages can result. Shortages compromise key services such as firefighting and can lead to a lack of water for hygiene, sanitation and drinking. Municipal authorities need to monitor and manage this water demand carefully, and may need to advise the public to utilise cleanup methods that do not use water (e.g., cleaning with brooms rather than hoses). [62] Wastewater treatment[ edit ] Wastewater networks may sustain damage similar to water supply networks. It is very difficult to exclude ash from the sewerage system. Systems with combined storm water/sewer lines are most at risk. Ash will enter sewer lines where there is inflow/infiltration by stormwater through illegal connections (e.g., from roof downpipes), cross connections, around manhole covers or through holes and cracks in sewer pipes. [63] [64] Ash-laden sewage entering a treatment plant is likely to cause failure of mechanical prescreening equipment such as step screens or rotating screens. Ash that penetrates further into the system will settle and reduce the capacity of biological reactors as well as increasing the volume of sludge and changing its composition. [64] Main article: Volcanic ash and aviation safety The principal damage sustained by aircraft flying into a volcanic ash cloud is abrasion to forward-facing surfaces, such as the windshield and leading edges of the wings, and accumulation of ash into surface openings, including engines. [65] Abrasion of windshields and landing lights will reduce visibility forcing pilots to rely on their instruments. However, some instruments may provide incorrect readings as sensors (e.g., pitot tubes ) can become blocked with ash. Ingestion of ash into engines causes abrasion damage to compressor fan blades. The ash erodes sharp blades in the compressor, reducing its efficiency. The ash melts in the combustion chamber to form molten glass. The ash then solidifies on turbine blades, blocking air flow and causing the engine to stall. [66] The composition of most ash is such that its melting temperature is within the operating temperature (>1000 °C) of modern large jet engines . [67] The degree of impact depends upon the concentration of ash in the plume, the length of time the aircraft spends within the plume and the actions taken by the pilots. Critically, melting of ash, particularly volcanic glass, can result in accumulation of resolidified ash on turbine nozzle guide vanes, resulting in compressor stall and complete loss of engine thrust. [68] The standard procedure of the engine control system when it detects a possible stall is to increase power which would exacerbate the problem. It is recommended that pilots reduce engine power and quickly exit the cloud by performing a descending 180° turn. [68] Volcanic gases, which are present within ash clouds, can also cause damage to engines and acrylic windshields, and can persist in the stratosphere as an almost invisible aerosol for prolonged periods of time. [69] Occurrence[ edit ] There are many instances of damage to jet aircraft as a result of an ash encounter. On 24 June 1982, a British Airways Boeing 747-236B ( Flight 9 ) flew through the ash cloud from the eruption of Mount Galunggung , Indonesia resulting in the failure of all four engines. The plane descended 24,000 feet (7,300 m) in 16 minutes before the engines restarted, allowing the aircraft to make an emergency landing. On 15 December 1989, a KLM Boeing 747-400 ( Flight 867 ) also lost power to all four engines after flying into an ash cloud from Mount Redoubt , Alaska . After dropping 14,700 feet (4,500 m) in four minutes, the engines were started just 1–2 minutes before impact. Total damage was US$80 million and it took 3 months' work to repair the plane. [67] In the 1990s, a further US$100 million of damage was sustained by commercial aircraft (some in the air, others on the ground) as a consequence of the 1991 eruption of Mount Pinatubo in the Philippines . [67] In April 2010, airspace all over Europe was affected, with many flights cancelled -which was unprecedented-due to the presence of volcanic ash in the upper atmosphere from the eruption of the Icelandic volcano Eyjafjallajökull . [70] On 15 April 2010, the Finnish Air Force halted training flights when damage was found from volcanic dust ingestion by the engines of one of its Boeing F-18 Hornet fighters. [71] In June 2011, there were similar closures of airspace in Chile, Argentina, Brazil, Australia and New Zealand, following the eruption of Puyehue-Cordón Caulle , Chile . [72] Coverage of the nine VAAC around the world The AVOID instrument mounted on the fuselage of an AIRBUS A340 test aircraft Volcanic ash clouds are very difficult to detect from aircraft as no onboard cockpit instruments exist to detect them. However, a new system called Airborne Volcanic Object Infrared Detector (AVOID) has recently been developed by Dr Fred Prata [73] while working at CSIRO Australia [74] and the Norwegian Institute for Air Research , which will allow pilots to detect ash plumes up to 60 km (37 mi) ahead and fly safely around them. [75] The system uses two fast-sampling infrared cameras, mounted on a forward-facing surface, that are tuned to detect volcanic ash. This system can detect ash concentrations of <1 mg/m3 to > 50 mg/m3, giving pilots approximately 7–10 minutes warning. [75] The camera was tested [76] [77] by the easyJet airline company, [78] AIRBUS and Nicarnica Aviation (co-founded by Dr Fred Prata).  The results showed the system could work to distances of ~60 km and up to 10,000 ft [79] but not any higher without some significant modifications. In addition, ground and satellite based imagery, radar , and lidar can be used to detect ash clouds. This information is passed between meteorological agencies, volcanic observatories and airline companies through Volcanic Ash Advisory Centers (VAAC) . There is one VAAC for each of the nine regions of the world. VAACs can issue advisories describing the current and future extent of the ash cloud. [80] Airport systems[ edit ] Volcanic ash not only affects in-flight operations but can affect ground-based airport operations as well. Small accumulations of ash can reduce visibility, produce slippery runways and taxiways, infiltrate communication and electrical systems, interrupt ground services, damage buildings and parked aircraft. [81] Ash accumulation of more than a few millimeters requires removal before airports can resume full operations. Ash does not disappear (unlike snowfalls) and must be disposed of in a manner that prevents it from being remobilised by wind and aircraft. [82] Land transport[ edit ] Ash may disrupt transportation systems over large areas for hours to days, including roads and vehicles, railways and ports and shipping. Falling ash will reduce the visibility which can make driving difficult and dangerous. [26] In addition, fast travelling cars will stir up ash, generating billowing clouds which perpetuate ongoing visibility hazards. Ash accumulations will decrease traction, especially when wet, and cover road markings. [26] Fine-grained ash can infiltrate openings in cars and abrade most surfaces, especially between moving parts. Air and oil filters will become blocked requiring frequent replacement. Rail transport is less vulnerable, with disruptions mainly caused by reduction in visibility. [26] Marine transport can also be impacted by volcanic ash. Ash fall will block air and oil filters and abrade any moving parts if ingested into engines. Navigation will be impacted by a reduction in visibility during ash fall. Vesiculated ash ( pumice and scoria ) will float on the water surface in ‘pumice rafts’ which can clog water intakes quickly, leading to over heating of machinery. [26] Communications[ edit ] Telecommunication and broadcast networks can be affected by volcanic ash in the following ways: attenuation and reduction of signal strength; damage to equipment; and overloading of network through user demand. Signal attenuation due to volcanic ash is not well documented; however, there have been reports of disrupted communications following the 1969 Surtsey eruption and 1991 Mount Pinatubo eruption. Research by the New Zealand -based Auckland Engineering Lifelines Group determined theoretically that impacts on telecommunications signals from ash would be limited to low frequency services such as satellite communication . [37] Signal interference may also be caused by lightning, as this is frequently generated within volcanic eruption plumes. [83] Telecommunication equipment may become damaged due to direct ash fall. Most modern equipment requires constant cooling from air conditioning units . These are susceptible to blockage by ash which reduces their cooling efficiency. [84] Heavy ash falls may cause telecommunication lines, masts, cables, aerials, antennae dishes and towers to collapse due to ash loading.  Moist ash may also cause accelerated corrosion of metal components. [37] Reports from recent eruptions suggest that the largest disruption to communication networks is overloading due to high user demand. [26] This is common of many natural disasters. [85] Computers[ edit ] Computers may be impacted by volcanic ash, with their functionality and usability decreasing during ashfall, but it is unlikely they will completely fail. [86] The most vulnerable components are the mechanical components, such as cooling fans , cd drives , keyboard , mice and touch pads . These components can become jammed with fine grained ash causing them to cease working; however, most can be restored to working order by cleaning with compressed air. Moist ash may cause electrical short circuits within desktop computers; however, will not affect laptop computers. [86] Buildings and structures[ edit ] Damage to buildings and structures can range from complete or partial roof collapse to less catastrophic damage of exterior and internal materials. Impacts depend on the thickness of ash, whether it is wet or dry, the roof and building design and how much ash gets inside a building. The specific weight of ash can vary significantly and rain can increase this by 50–100%. [10] Problems associated with ash loading are similar to that of snow; however, ash is more severe as 1) the load from ash is generally much greater, 2) ash does not melt and 3) ash can clog and damage gutters, especially after rain fall. Impacts for ash loading depend on building design and construction, including roof slope, construction materials, roof span and support system, and age and maintenance of the building. [10] Generally flat roofs are more susceptible to damage and collapse than steeply pitched roofs. Roofs made of smooth materials (sheet metal or glass) are more likely to shed ash than roofs made with rough materials (thatch, asphalt or wood shingles). Roof collapse can lead to widespread injuries and deaths and property damage. For example, the collapse of roofs from ash during the 15 June 1991 Mount Pinatubo eruption killed about 300 people. [87] Environment and agriculture[ edit ] Volcanic ash can have a detrimental impact on the environment which can be difficult to predict due to the large variety of environmental conditions that exist within the ash fall zone. Natural waterways can be impacted in the same way as urban water supply networks. Ash will increase water turbidity which can reduce the amount of light reaching lower depths, which can inhibit growth of submerged aquatic plants and consequently affect species which are dependent on them such as fish and shellfish . [88] High turbidity can also affect the ability of fish gills to absorb dissolved oxygen . [89] Acidification will also occur, which will reduce the pH of the water and impact the fauna and flora living in the environment. Fluoride contamination will occur if the ash contains high concentrations of fluoride. [90] Ash accumulation will also affect pasture, plants and trees which are part of the horticulture and agriculture industries. Thin ash falls (<20 mm) may put livestock off eating, and can inhibit transpiration and photosynthesis and alter growth. There may be an increase in pasture production due to a mulching effect and slight fertilizing effect, such as occurred following the 1980 Mount St. Helens and 1995/96 Mt Ruapehu eruptions. [91] [92] Heavier falls will completely bury pastures and soil leading to death of pasture and sterilization of the soil due to oxygen deprivation. Plant survival is dependent on ash thickness, ash chemistry, compaction of ash, amount of rainfall, duration of burial and the length of plant stalks at the time of ash fall. [10] Defoliated and fallen trees on Windy Ridge, Mount St. Helens , 22 years after the 1980 eruption Young forests (trees <2 years old) are most at risk from ash falls and are likely to be destroyed by ash deposits >100 mm. [93] Ash fall is unlikely to kill mature trees, but ash loading may break large branches during heavy ash falls (>500 mm). Defoliation of trees may also occur, especially if there is a coarse ash component within the ash fall. [10] Land rehabilitation after ash fall may be possible depending on the ash deposit thickness. Rehabilitation treatment may include: direct seeding of deposit; mixing of deposit with buried soil; scraping of ash deposit from land surface; and application of new topsoil over the ash deposit. [37] Interdependence[ edit ] Interdependency of volcanic ashfall impacts from the Eyjafjallajökull 2010 eruptions Critical infrastructure and infrastructure services are vital to the functionality of modern society, to provide: medical care, policing, emergency services , and lifelines such as water, wastewater, and power and transportation links. Often critical facilities themselves are dependent on such lifelines for operability, which makes them vulnerable to both direct impacts from a hazard event and indirect effects from lifeline disruption. [94] The impacts on lifelines may also be inter-dependent . The vulnerability of each lifeline may depend on: the type of hazard, the spatial density of its critical linkages, the dependency on critical linkages, susceptibility to damage and speed of service restoration, state of repair or age, and institutional characteristics or ownership. [33] The 2010 eruption of Eyjafjallajokull in Iceland highlighted the impacts of volcanic ash fall in modern society and our dependence on the functionality of infrastructure services. During this event, the airline industry suffered business interruption losses of €1.5–2.5 billion from the closure of European airspace for six days in April 2010 and subsequent closures into May 2010. [95] Ash fall from this event is also known to have caused local crop losses in agricultural industries, losses in the tourism industry, destruction of roads and bridges in Iceland (in combination with glacial melt water), and costs associated with emergency response and clean-up. However, across Europe there were further losses associated with travel disruption, the insurance industry, the postal service, and imports and exports across Europe and worldwide. These consequences demonstrate the interdependency and diversity of impacts from a single event. [38] Preparedness, mitigation and management[ edit ] Two management methods during the 2014 eruptions of Kelud : sweeping (top) and spraying with water (bottom) Preparedness for ashfalls should involve sealing buildings, protecting infrastructure and homes, and storing sufficient supplies of food and water to last until the ash fall is over and clean-up can begin. Dust masks can be worn to reduce inhalation of ash and mitigate against any respiratory health affects. [47] Goggles can be worn to protect against eye irritation. At home, staying informed about volcanic activity, and having contingency plans in place for alternative shelter locations, constitutes good preparedness for an ash fall event. This can prevent some impacts associated with ash fall, reduce the effects, and increase the human capacity to cope with such events. A few items such as a flashlight, plastic sheeting to protect electronic equipment from ash ingress, and battery operated radios, are extremely useful during ash fall events. [10] Communication plans should be made beforehand to inform of mitigation actions being undertaken. Spare parts and back-up systems should be in place prior to ash fall events to reduce service disruption and return functionality as quickly as possible. Good preparedness also includes the identification of ash disposal sites, before ash fall occurs, to avoid further movement of ash and to aid clean-up. [96] Some effective techniques for the management of ash have been developed including cleaning methods and cleaning apparatus, and actions to mitigate or limit damage. The latter include covering of openings such as: air and water intakes, aircraft engines and windows during ash fall events. Roads may be closed to allow clean-up of ash falls, or speed restrictions may be put in place, in order to prevent motorists from developing motor problems and becoming stranded following an ash fall. [97] To prevent further effects on underground water systems or waste water networks, drains and culverts should be unblocked and ash prevented from entering the system. [96] Ash can be moistened (but not saturated) by sprinkling with water, to prevent remobilisation of ash and to aid clean-up. [97] Prioritisation of clean-up operations for critical facilities and coordination of clean-up efforts also constitute good management practice. [96] [97] [98] It is recommended to evacuate livestock in areas where ashfall may reach 5 cm or more. [99] Volcanic ash soils[ edit ] Volcanic ash's primary use is that of a soil enricher. Once the minerals in ash are washed into the soil by rain or other natural processes, it mixes with the soil and forms an andisol layer. This layer is highly rich in nutrients and is very good for agricultural use; the presence of lush forests on volcanic islands is often as a result of trees growing and flourishing in the phosphorus and nitrogen -rich andisol. [100] Volcanic ash can also be used as a replacement for sand. [101] Bentonite – Rock type or absorbent swelling clay Deposition (aerosol physics) – Process by which aerosol particles collect onto solid surfaces Energetically modified cement – Class of cements, mechanically processed to transform reactivity NOTAM – Aviation notice of potential flight hazards Roman concrete – Building material used in ancient Rome
Toggle the table of contents Landslide From Wikipedia, the free encyclopedia Natural hazard involving ground movement This article is about the geological phenomenon. For other uses, see Landslide (disambiguation) . A landslide near Cusco, Peru , in 2018 A NASA model has been developed to look at how potential landslide activity is changing around the world. Animation of a landslide in San Mateo County, California Landslides, also known as landslips, [1] [2] [3] are several forms of mass wasting that may include a wide range of ground movements, such as rockfalls , mudflows , shallow or deep-seated slope failures and debris flows . [4] Landslides occur in a variety of environments, characterized by either steep or gentle slope gradients, from mountain ranges to coastal cliffs or even underwater, [5] in which case they are called submarine landslides . Gravity is the primary driving force for a landslide to occur, but there are other factors affecting slope stability that produce specific conditions that make a slope prone to failure. In many cases, the landslide is triggered by a specific event (such as a heavy rainfall , an earthquake , a slope cut to build a road, and many others), although this is not always identifiable. Landslides are frequently made worse by human development (such as urban sprawl ) and resource exploitation (such as mining and deforestation ). Land degradation frequently leads to less stabilization of soil by vegetation . [6] Additionally, global warming caused by climate change and other human impact on the environment , can increase the frequency of natural events (such as extreme weather ) which trigger landslides. [7] Landslide mitigation describes the policy and practices for reducing the risk of human impacts of landslides, reducing the risk of natural disaster . The Mameyes Landslide , in the Mameyes neighborhood of barrio Portugués Urbano in Ponce , Puerto Rico , was caused by extensive accumulation of rains and, according to some sources, lightning. It buried more than 100 homes. The landslide at Surte in Sweden, 1950. It was a quick clay slide that killed one person. Landslides occur when the slope (or a portion of it) undergoes some processes that change its condition from stable to unstable. This is essentially due to a decrease in the shear strength of the slope material, an increase in the shear stress borne by the material, or a combination of the two. A change in the stability of a slope can be caused by a number of factors, acting together or alone. Natural causes of landslides include: increase in water content (loss of suction) or saturation by rain water infiltration, snow melting, or glaciers melting; [8] rising of groundwater or increase of pore water pressure (e.g. due to aquifer recharge in rainy seasons, or by rain water infiltration); [9] increase of hydrostatic pressure in cracks and fractures; [9] [10] loss or absence of vertical vegetative structure, soil nutrients , and soil structure (e.g. after a wildfire); [11] erosion of the top of a slope by rivers or sea waves ; [12] physical and chemical weathering (e.g. by repeated freezing and thawing, heating and cooling, salt leaking in the groundwater or mineral dissolution); [13] [14] [15] ground shaking caused by earthquakes , which can destabilize the slope directly (e.g., by inducing soil liquefaction ) or weaken the material and cause cracks that will eventually produce a landslide; [10] [16] [17] changes in pore fluid composition; [19] changes in temperature (seasonal or induced by climate change). [20] [21] Landslides are aggravated by human activities, such as: in shallow soils , the removal of deep- rooted vegetation that binds colluvium to bedrock ; agricultural or forestry activities ( logging ), and urbanization , which change the amount of water infiltrating the soil. temporal variation in land use and land cover (LULC): it includes the human abandonment of farming areas, e.g. due to the economic and social transformations which occurred in Europe after the Second World War. Land degradation and extreme rainfall can increase the frequency of erosion and landslide phenomena. [6] Hungr-Leroueil-Picarelli classification[ edit ] In traditional usage, the term landslide has at one time or another been used to cover almost all forms of mass movement of rocks and regolith at the Earth's surface. In 1978, geologist David Varnes noted this imprecise usage and proposed a new, much tighter scheme for the classification of mass movements and subsidence processes. [24] This scheme was later modified by Cruden and Varnes in 1996, [25] and refined by Hutchinson (1988), [26] Hungr et al. (2001), [27] and finally by Hungr, Leroueil and Picarelli (2014). [4] The classification resulting from the latest update is provided below. Type of movement Solifluction Note: the words in italics are placeholders. Use only one. Under this classification, six types of movement are recognized. Each type can be seen both in rock and in soil. A fall is a movement of isolated blocks or chunks of soil in free-fall. The term topple refers to blocks coming away by rotation from a vertical face. A slide is the movement of a body of material that generally remains intact while moving over one or several inclined surfaces or thin layers of material (also called shear zones) in which large deformations are concentrated. Slides are also sub-classified by the form of the surface(s) or shear zone(s) on which movement happens. The planes may be broadly parallel to the surface ("planar slides") or spoon-shaped ("rotational slides"). Slides can occur catastrophically, but movement on the surface can also be gradual and progressive. Spreads are a form of subsidence, in which a layer of material cracks, opens up, and expands laterally. Flows are the movement of fluidised material, which can be both dry or rich in water (such as in mud flows). Flows can move imperceptibly for years, or accelerate rapidly and cause disasters. Slope deformations are slow, distributed movements that can affect entire mountain slopes or portions of it. Some landslides are complex in the sense that they feature different movement types in different portions of the moving body, or they evolve from one movement type to another over time. For example, a landslide can initiate as a rock fall or topple and then, as the blocks disintegrate upon the impact, transform into a debris slide or flow. An avalanching effect can also be present, in which the moving mass entrains additional material along its path. Flows[ edit ] Slope material that becomes saturated with water may produce a debris flow or mud flow . However, also dry debris can exhibit flow-like movement. [28] Flowing debris or mud may pick up trees, houses and cars, and block bridges and rivers causing flooding along its path. This phenomenon is particularly hazardous in alpine areas, where narrow gorges and steep valleys are conducive of faster flows. Debris and mud flows may initiate on the slopes or result from the fluidization of landslide material as it gains speed or incorporates further debris and water along its path. River blockages as the flow reaches a main stream can generate temporary dams. As the impoundments fail, a domino effect may be created, with a remarkable growth in the volume of the flowing mass, and in its destructive power. The Costa della Gaveta earthflow in Potenza , Italy. Even though it moves at a rate of just a few millimeters per year [13] and is hardly visible, this landslide causes progressive damage to the national road, the national highway, a flyover, and several houses that are built on it. A rock slide in Guerrero , Mexico An earthflow is the downslope movement of mostly fine-grained material. Earthflows can move at speeds within a very wide range, from as low as 1 mm/yr [13] [14] to many km/h. Though these are a lot like mudflows , overall they are more slow-moving and are covered with solid material carried along by the flow from within. Clay, fine sand and silt, and fine-grained, pyroclastic material are all susceptible to earthflows. These flows are usually controlled by the pore water pressures within the mass, which should be high enough to produce a low shearing resistance. On the slopes, some earthflow may be recognized by their elongated shape, with one or more lobes at their toes. As these lobes spread out, drainage of the mass increases and the margins dry out, lowering the overall velocity of the flow. This process also causes the flow to thicken. Earthflows occur more often during periods of high precipitation, which saturates the ground and builds up water pressures. However, earthflows that keep advancing also during dry seasons are not uncommon. Fissures may develop during the movement of clayey materials, which facilitate the intrusion of water into the moving mass and produce faster responses to precipitation. [29] A rock avalanche, sometimes referred to as sturzstrom , is a large and fast-moving landslide of the flow type. It is rarer than other types of landslides but it is often very destructive. It exhibits typically a long runout, flowing very far over a low-angle, flat, or even slightly uphill terrain. The mechanisms favoring the long runout can be different, but they typically result in the weakening of the sliding mass as the speed increases. [30] [31] [32] The causes of this weakening are not completely understood. Especially for the largest landslides, it may involve the very quick heating of the shear zone due to friction, which may even cause the water that is present to vaporize and build up a large pressure, producing a sort of hovercraft effect. [33] In some cases, the very high temperature may even cause some of the minerals to melt. [34] During the movement, the rock in the shear zone may also be finely ground, producing a nanometer-size mineral powder that may act as a lubricant, reducing the resistance to motion and promoting larger speeds and longer runouts. [35] The weakening mechanisms in large rock avalanches are similar to those occurring in seismic faults. [32] Slides[ edit ] Slides can occur in any rock or soil material and are characterized by the movement of a mass over a planar or curvilinear surface or shear zone. A debris slide is a type of slide characterized by the chaotic movement of material mixed with water and/or ice. It is usually triggered by the saturation of thickly vegetated slopes which results in an incoherent mixture of broken timber, smaller vegetation and other debris. [29] Debris flows and avalanches differ from debris slides because their movement is fluid-like and generally much more rapid. This is usually a result of lower shear resistances and steeper slopes. Typically, debris slides start with the detachment of large rock fragments high on the slopes, which break apart as they descend. Clay and silt slides are usually slow but can experience episodic acceleration in response to heavy rainfall or rapid snowmelt. They are often seen on gentle slopes and move over planar surfaces, such as over the underlying bedrock. Failure surfaces can also form within the clay or silt layer itself, and they usually have concave shapes, resulting in rotational slides Shallow and deep-seated landslides[ edit ] Hotel Panorama at Lake Garda . Part of a hill of Devonian shale was removed to make the road, forming a dip-slope. The upper block detached along a bedding plane and is sliding down the hill, forming a jumbled pile of rock at the toe of the slide. Slope failure mechanisms often contain large uncertainties and could be significantly affected by heterogeneity of soil properties. [36] A landslide in which the sliding surface is located within the soil mantle or weathered bedrock (typically to a depth from few decimeters to some meters) is called a shallow landslide. Debris slides and debris flows are usually shallow. Shallow landslides can often happen in areas that have slopes with high permeable soils on top of low permeable soils. The low permeable soil traps the water in the shallower soil generating high water pressures. As the top soil is filled with water, it can become unstable and slide downslope. Deep-seated landslide on a mountain in Sehara, Kihō , Japan caused by torrential rain of Tropical Storm Talas Landslide of soil and regolith in Pakistan Deep-seated landslides are those in which the sliding surface is mostly deeply located, for instance well below the maximum rooting depth of trees. They usually involve deep regolith , weathered rock, and/or bedrock and include large slope failures associated with translational, rotational, or complex movements. [37] They tend to form along a plane of weakness such as a fault or bedding plane . They can be visually identified by concave scarps at the top and steep areas at the toe. [38] Deep-seated landslides also shape landscapes over geological timescales and produce sediment that strongly alters the course of fluvial streams . [39] Related phenomena[ edit ] An avalanche , similar in mechanism to a landslide, involves a large amount of ice, snow and rock falling quickly down the side of a mountain. A pyroclastic flow is caused by a collapsing cloud of hot ash , gas and rocks from a volcanic explosion that moves rapidly down an erupting volcano . Extreme precipitation and flow can cause gully formation in flatter environments not susceptible to landslides. See also: Tsunami § Tsunami generated by landslides Landslides that occur undersea, or have impact into water e.g. significant rockfall or volcanic collapse into the sea, [40] can generate tsunamis . Massive landslides can also generate megatsunamis , which are usually hundreds of meters high. In 1958, one such tsunami occurred in Lituya Bay in Alaska. [41] [42] Landslide prediction mapping[ edit ] See also: Slope stability analysis Landslide hazard analysis and mapping can provide useful information for catastrophic loss reduction, and assist in the development of guidelines for sustainable land-use planning . The analysis is used to identify the factors that are related to landslides, estimate the relative contribution of factors causing slope failures, establish a relation between the factors and landslides, and to predict the landslide hazard in the future based on such a relationship. [43] The factors that have been used for landslide hazard analysis can usually be grouped into geomorphology , geology , land use/land cover, and hydrogeology . Since many factors are considered for landslide hazard mapping, GIS is an appropriate tool because it has functions of collection, storage, manipulation, display, and analysis of large amounts of spatially referenced data which can be handled fast and effectively. [44] Cardenas reported evidence on the exhaustive use of GIS in conjunction of uncertainty modelling tools for landslide mapping. [45] [46] Remote sensing techniques are also highly employed for landslide hazard assessment and analysis. Before and after aerial photographs and satellite imagery are used to gather landslide characteristics, like distribution and classification, and factors like slope, lithology , and land use/land cover to be used to help predict future events. [47] Before and after imagery also helps to reveal how the landscape changed after an event, what may have triggered the landslide, and shows the process of regeneration and recovery. [48] Using satellite imagery in combination with GIS and on-the-ground studies, it is possible to generate maps of likely occurrences of future landslides. [49] Such maps should show the locations of previous events as well as clearly indicate the probable locations of future events. In general, to predict landslides, one must assume that their occurrence is determined by certain geologic factors, and that future landslides will occur under the same conditions as past events. [50] Therefore, it is necessary to establish a relationship between the geomorphologic conditions in which the past events took place and the expected future conditions. [51] Natural disasters are a dramatic example of people living in conflict with the environment. Early predictions and warnings are essential for the reduction of property damage and loss of life. Because landslides occur frequently and can represent some of the most destructive forces on earth, it is imperative to have a good understanding as to what causes them and how people can either help prevent them from occurring or simply avoid them when they do occur. Sustainable land management and development is also an essential key to reducing the negative impacts felt by landslides. A Wireline extensometer monitoring slope displacement and transmitting data remotely via radio or Wi-Fi. In situ or strategically deployed extensometers may be used to provide early warning of a potential landslide. [52] GIS offers a superior method for landslide analysis because it allows one to capture, store, manipulate, analyze, and display large amounts of data quickly and effectively. Because so many variables are involved, it is important to be able to overlay the many layers of data to develop a full and accurate portrayal of what is taking place on the Earth's surface. Researchers need to know which variables are the most important factors that trigger landslides in any given location. Using GIS, extremely detailed maps can be generated to show past events and likely future events which have the potential to save lives, property, and money. Since the ‘90s, GIS have been also successfully used in conjunction to decision support systems , to show on a map real-time risk evaluations based on monitoring data gathered in the area of the Val Pola disaster (Italy). [53] Global landslide risks
Toggle the table of contents Volcanology From Wikipedia, the free encyclopedia Study of volcanoes, lava, magma and associated phenomena Not to be confused with Volcanism . A volcanologist sampling lava using a rock hammer and a bucket of water Eruption of Stromboli (Isole Eolie/Italia), ca. 100m (300ft) vertically. Exposure of several seconds. The dashed trajectories are the result of lava pieces with a bright hot side and a cool dark side rotating in mid-air. Volcanology (also spelled vulcanology) is the study of volcanoes , lava , magma and related geological , geophysical and geochemical phenomena ( volcanism ).   The term volcanology is derived from the Latin word vulcan .  Vulcan was the ancient Roman god of fire. A volcanologist is a geologist who studies the eruptive activity and formation of volcanoes and their current and historic eruptions. Volcanologists frequently visit volcanoes, especially active ones, to observe volcanic eruptions , collect eruptive products including tephra (such as ash or pumice ), rock and lava samples. One major focus of enquiry is the prediction of eruptions; there is currently no accurate way to do this, but predicting or forecasting eruptions, like predicting earthquakes, could save many lives. Modern volcanology[ edit ] Volcanologist examining tephra horizons in south-central Iceland . A diagram of a destructive plate margin , where subduction fuels volcanic activity at the subduction zones of tectonic plate boundaries. In 1841, the first volcanological observatory, the Vesuvius Observatory , was founded in the Kingdom of the Two Sicilies . [1] Volcanology advances have required more than just structured observation, and the science relies upon the understanding and integration of knowledge in many fields including geology , tectonics , physics , chemistry and mathematics , with many advances only being able to occur after the advance had occurred in another field of science. For example the study of radioactivity only commenced in 1896, [2] and its application to the theory of plate tectonics and radiometric dating took about 50 years after this. Many other developments in fluid dynamics , experimental physics and chemistry, techniques of mathematical modelling , instrumentation and in other sciences have been applied to volcanology since 1841. Techniques[ edit ] Seismic observations are made using seismographs deployed near volcanic areas, watching out for increased seismicity during volcanic events, in particular looking for long period harmonic tremors, which signal magma movement through volcanic conduits. [3] Surface deformation monitoring includes the use of geodetic techniques such as leveling, tilt, strain, angle and distance measurements through tiltmeters, total stations and EDMs. This also includes GNSS observations and InSAR. [4] Surface deformation indicates magma upwelling: increased magma supply produces bulges in the volcanic center's surface. Gas emissions may be monitored with equipment including portable ultra-violet spectrometers (COSPEC, now superseded by the miniDOAS), which analyzes the presence of volcanic gases such as sulfur dioxide ; or by infra-red spectroscopy (FTIR). Increased gas emissions, and more particularly changes in gas compositions, may signal an impending volcanic eruption. [3] Temperature changes are monitored using thermometers and observing changes in thermal properties of volcanic lakes and vents, which may indicate  upcoming activity. [5] Satellites are widely used to monitor volcanoes, as they allow a large area to be monitored easily.  They can measure the spread of an ash plume, such as the one from Eyjafjallajökull 's 2010 eruption, [6] as well as SO2 emissions. [7] InSAR and thermal imaging can monitor large, scarcely populated areas where it would be too expensive to maintain instruments on the ground. Other geophysical techniques (electrical, gravity and magnetic observations) include monitoring fluctuations and sudden change in resistivity, gravity anomalies or magnetic anomaly patterns that may indicate volcano-induced faulting and magma upwelling. [5] Stratigraphic analyses includes analyzing tephra and lava deposits and dating these to give volcano eruption patterns, [8] with estimated cycles of intense activity and size of eruptions. [3] Compositional analysis has been very successful in the grouping of volcanoes by type, [9] : 274 origin of magma, [9] : 274 including matching of volcanoes to a mantle plume of a particular hotspot , mantle plume melting depths, [10] the history of recycled subducted crust, [9] : 302–3 matching of tephra deposits to each other and to volcanoes of origin, [11] and the understanding the formation and evolution of magma reservoirs, [9] : 296–303 an approach which has now been validated by real time sampling. [12] Forecasting[ edit ] Some of the techniques mentioned above, combined with modelling, have proved useful and successful in the forecasting of some eruptions, [13] : 1–2 such as the evacuation of the locality around Mount Pinatubo in 1991 that may have saved 20,000 lives. [14] Short-term forecasts tend to use seismic or multiple monitoring data with long term forecasting involving the study of the previous history of local volcanism. [13] : 1 However, volcanology forecasting does not just involve predicting the next initial onset time of an eruption, as it might also address the size of a future eruption, and evolution of an eruption once it has begun. [13] : 1–2 History[ edit ] Volcanology has an extensive history. The earliest known recording of a volcanic eruption may be on a wall painting dated to about 7,000 BCE found at the Neolithic site at Çatal Höyük in Anatolia , Turkey . [15] : 203 This painting has been interpreted as a depiction of an erupting volcano, with a cluster of houses below shows a twin peaked volcano in eruption, with a town at its base (though archaeologists now question this interpretation). [16] The volcano may be either Hasan Dağ , or its smaller neighbour, Melendiz Dağ. [17] Greco-Roman philosophy[ edit ] Eruption of Vesuvius in 1822. The eruption of CE 79 would have appeared very similar. The classical world of Greece and the early Roman Empire explained volcanoes as sites of various gods. Greeks considered that Hephaestus , the god of fire, sat below the volcano Etna , forging the weapons of Zeus . The Greek word used to describe volcanoes was etna, or hiera, after Heracles , the son of Zeus. The Roman poet Virgil , in interpreting the Greek mythos, held that the giant Enceladus was buried beneath Etna by the goddess Athena as punishment for rebellion against the gods; the mountain's rumblings were his tormented cries, the flames his breath and the tremors his railing against the bars of his prison. Enceladus' brother Mimas was buried beneath Vesuvius by Hephaestus, and the blood of other defeated giants welled up in the Phlegrean Fields surrounding Vesuvius. [18] The Greek philosopher Empedocles (c. 490-430 BCE) saw the world divided into four elemental forces, of Earth, Air, Fire and Water. Volcanoes, Empedocles maintained, were the manifestation of Elemental Fire.  Plato contended that channels of hot and cold waters flow in inexhaustible quantities through subterranean rivers. In the depths of the earth snakes a vast river of fire, the Pyriphlegethon, which feeds all the world's volcanoes. Aristotle considered underground fire as the result of "the...friction of the wind when it plunges into narrow passages." Wind played a key role in volcano explanations until the 16th century after Anaxagoras , in the fifth century BC, had proposed eruptions were caused by a great wind. [19] Lucretius , a Roman philosopher, claimed Etna was completely hollow and the fires of the underground driven by a fierce wind circulating near sea level. Ovid believed that the flame was fed from "fatty foods" and eruptions stopped when the food ran out. Vitruvius contended that sulfur, alum and bitumen fed the deep fires. Observations by Pliny the Elder noted the presence of earthquakes preceded an eruption; he died in the eruption of Vesuvius in 79 CE while investigating it at Stabiae . His nephew, Pliny the Younger , gave detailed descriptions of the eruption in which his uncle died, attributing his death to the effects of toxic gases. Such eruptions have been named Plinian in honour of the two authors. Renaissance observations[ edit ] After the first eruption of Mount St. Helens on May 18, five more explosive eruptions occurred in 1980, including this event on July 22. This eruption sent pumice and ash 6 to 11 miles (10-18 kilometers) into the air, and was visible in Seattle , Washington, 100 miles (160 kilometers) to the north. The view here is from the south. Nuées ardentes were described from the Azores in 1580. Georgius Agricola argued the rays of the sun, as later proposed by Descartes had nothing to do with volcanoes. Agricola believed vapor under pressure caused eruptions of 'mointain oil' and basalt. The Jesuit Athanasius Kircher (1602–1680) witnessed eruptions of Mount Etna and Stromboli, then visited the crater of Vesuvius and published his view of an Earth with a central fire connected to numerous others caused by the burning of sulfur, bitumen and coal. He published his view of this in Mundus Subterraneus with volcanoes acting as a type of safety valve. [20] Johannes Kepler considered volcanoes as conduits for the tears and excrement of the Earth, voiding bitumen, tar and sulfur. [21] [ better source needed ] Descartes, pronouncing that God had created the Earth in an instant, declared he had done so in three layers; the fiery depths, [19] a layer of water, and the air. Volcanoes, he said, were formed where the rays of the sun pierced the earth. Science wrestled with the ideas of the combustion of pyrite with water, that rock was solidified bitumen, and with notions of rock being formed from water ( Neptunism ). Of the volcanoes then known, all were near the water, hence the action of the sea upon the land was used to explain volcanism . Interaction with religion and mythology[ edit ] Pele's hair caught on a radio antenna mounted on the south rim of Puʻu ʻŌʻō , Hawaiʻi , July 22, 2005 Tribal legends of volcanoes abound from the Pacific Ring of Fire and the Americas, usually invoking the forces of the supernatural or the divine to explain the violent outbursts of volcanoes. [22] Taranaki and Tongariro , according to Māori mythology, were lovers who fell in love with Pihanga , and a spiteful jealous fight ensued. Some Māori will not to this day live on the direct line between Tongariro and Taranaki for fear of the dispute flaring up again. [23] In the Hawaiian religion , Pele ( /ˈpeɪleɪ/ Pel-a; [ˈpɛlɛ] ) is the goddess of volcanoes and a popular figure in Hawaiian mythology . [24] Pele was used for various scientific terms as for Pele's hair , Pele's tears , and Limu o Pele (Pele's seaweed). A volcano on the Jovian moon Io is also named Pele . [25] Saint Agatha is patron saint of Catania , close to mount Etna, and an important highly venerated (till today [26] ) example of virgin martyrs of Christian antiquity. [27] In 253 CE, one year after her violent death, the stilling of an eruption of Mt. Etna was attributed to her intercession. Catania was however nearly completely destroyed by the eruption of Mt. Etna in 1169, and over 15,000 of its inhabitants died. Nevertheless, the saint was invoked again for the 1669 Etna eruption and, for an outbreak that was endangering the town of Nicolosi in 1886. [28] The way the saint is invoked and dealt with in Italian folk religion , in a quid pro quo manner, or bargaining approach which is sometimes used in prayerful interactions with saints, has been related (in the tradition of James Frazer ) to earlier pagan beliefs and practices. [29] In 1660 the eruption of Vesuvius rained twinned pyroxene crystals and ash upon the nearby villages. The crystals resembled the crucifix and this was interpreted as the work of Saint Januarius . In Naples , the relics of St Januarius are paraded through town at every major eruption of Vesuvius. The register of these processions and the 1779 and 1794 diary of Father Antonio Piaggio allowed British diplomat and amateur naturalist Sir William Hamilton to provide a detailed chronology and description of Vesuvius' eruptions. [30]
Toggle the table of contents Hydrogeology From Wikipedia, the free encyclopedia Study of the distribution and movement of groundwater This article is about subterranean hydrology. For the rest of the hydrologic cycle, see hydrology . Learn how and when to remove this template message ) Boy drinks from a tap at a NEWAH WASH water project [8] in Puware Shikhar, Udayapur District , Nepal . Checking wells Karst spring ( Cuneo , Piemonte , Italy ) [9] Hydrogeology (hydro- meaning water, and -geology meaning the study of the Earth ) is the area of geology that deals with the distribution and movement of groundwater in the soil and rocks of the Earth's crust (commonly in aquifers ). The terms groundwater hydrology, geohydrology, and hydrogeology are often used interchangeably. Hydrogeology is the study of the laws governing the movement of subterranean water, the mechanical, chemical, and thermal interaction of this water with the porous solid, and the transport of energy, chemical constituents, and particulate matter by flow (Domenico and Schwartz, 1998). Groundwater engineering, another name for hydrogeology, is a branch of engineering which is concerned with groundwater movement and design of wells, pumps, and drains. [1] The main concerns in groundwater engineering include groundwater contamination, conservation of supplies, and water quality. [2] Wells are constructed for use in developing nations, as well as for use in developed nations in places which are not connected to a city water system. Wells must be designed and maintained to uphold the integrity of the aquifer, and to prevent contaminants from reaching the groundwater. Controversy arises in the use of groundwater when its usage impacts surface water systems, or when human activity threatens the integrity of the local aquifer system. Introduction[ edit ] Hydrogeology is an interdisciplinary subject; it can be difficult to account fully for the chemical , physical , biological and even legal interactions between soil , water , nature and society . The study of the interaction between groundwater movement and geology can be quite complex. Groundwater does not always follow the surface topography ; groundwater follows pressure gradients (flow from high pressure to low), often through fractures and conduits in circuitous paths. Taking into account the interplay of the different facets of a multi-component system often requires knowledge in several diverse fields at both the experimental and theoretical levels. The following is a more traditional introduction to the methods and nomenclature of saturated subsurface hydrology. Hydrogeology in relation to other fields[ edit ] Painting by Ivan Aivazovsky (1841) Hydrogeology, as stated above, is a branch of the earth sciences dealing with the flow of water through aquifers and other shallow porous media (typically less than 450 m below the land surface). The very shallow flow of water in the subsurface (the upper 3 m) is pertinent to the fields of soil science , agriculture and civil engineering , as well as to hydrogeology. The general flow of fluids (water, hydrocarbons , geothermal fluids, etc.) in deeper formations is also a concern of  geologists, geophysicists and petroleum geologists .  Groundwater is a slow-moving, viscous fluid (with a Reynolds number less than unity); many of the empirically derived laws of groundwater flow can be alternately derived in fluid mechanics from the special case of Stokes flow (viscosity and pressure terms, but no inertial term). A piezometer is a device used to measure the hydraulic head of groundwater . The mathematical relationships used to describe the flow of water through porous media are Darcy's law , the diffusion and Laplace equations, which have applications in many diverse fields. Steady groundwater flow (Laplace equation) has been simulated using electrical , elastic and heat conduction analogies. Transient groundwater flow is analogous to the diffusion of heat in a solid, therefore some solutions to hydrological problems have been adapted from heat transfer literature. Traditionally, the movement of groundwater has been studied separately from surface water, climatology , and even the chemical and microbiological aspects of hydrogeology (the processes are uncoupled). As the field of hydrogeology matures, the strong interactions between groundwater, surface water , water chemistry , soil moisture and even climate are becoming more clear. California and Washington both require special certification of hydrogeologists to offer professional services to the public. Twenty-nine states require professional licensing for geologists to offer their services to the public, which often includes work within the domains of developing, managing, and/or remediating groundwater resources. [3] For example: aquifer drawdown or overdrafting and the pumping of fossil water may be a contributing factor to sea-level rise. [4] One of the main tasks a hydrogeologist typically performs is the prediction of future behavior of an aquifer system, based on analysis of past and present observations. Some hypothetical, but characteristic questions asked would be: Can the aquifer support another subdivision ? Will the river dry up if the farmer doubles his irrigation ? Did the chemicals from the dry cleaning facility travel through the aquifer to my well and make me sick? Will the plume of effluent leaving my neighbor's septic system flow to my drinking water well ? Most of these questions can be addressed through simulation of the hydrologic system (using numerical models or analytic equations). Accurate simulation of the aquifer system requires knowledge of the aquifer properties and boundary conditions. Therefore, a common task of the hydrogeologist is determining aquifer properties using aquifer tests . In order to further characterize aquifers and aquitards some primary and derived physical properties are introduced below. Aquifers are broadly classified as being either confined or unconfined ( water table aquifers), and either saturated or unsaturated; the type of aquifer affects what properties control the flow of water in that medium (e.g., the release of water from storage for confined aquifers is related to the storativity , while it is related to the specific yield for unconfined aquifers). Main article: Aquifer An aquifer is a collection of water underneath the surface, large enough to be useful in a spring or a well. Aquifers can be unconfined, where the top of the aquifer is defined by the water table , or confined, where the aquifer exists underneath a confining bed. [5] There are three aspects that control the nature of aquifers: stratigraphy , lithology , and geological formations and deposits. The stratigraphy relates the age and geometry of the many formations that compose the aquifer. The lithology refers to the physical components of an aquifer, such as the mineral composition and grain size. The structural features are the elements that arise due to deformations after deposition, such as fractures and folds. Understanding these aspects is paramount to understanding of how an aquifer is formed and how professionals can utilize it for groundwater engineering. [6] Main article: Hydraulic head Differences in hydraulic head (h) cause water to move from one place to another; water flows from locations of high h to locations of low h. Hydraulic head is composed of pressure head (ψ) and elevation head (z). The head gradient is the change in hydraulic head per length of flowpath, and appears in Darcy's law as being proportional to the discharge. Hydraulic head is a directly measurable property that can take on any value (because of the arbitrary datum involved in the z term); ψ can be measured with a pressure transducer (this value can be negative, e.g., suction, but is positive in saturated aquifers), and z can be measured relative to a surveyed datum (typically the top of the well casing). Commonly, in wells tapping unconfined aquifers the water level in a well is used as a proxy for hydraulic head, assuming there is no vertical gradient of pressure. Often only changes in hydraulic head through time are needed, so the constant elevation head term can be left out (Δh = Δψ). A record of hydraulic head through time at a well is a hydrograph or, the changes in hydraulic head recorded during the pumping of a well in a test are called drawdown . [Left] High porosity, well sorted [Right] Low porosity, poorly sorted Porosity (n) is a directly measurable aquifer property; it is a fraction between 0 and 1 indicating the amount of pore space between unconsolidated soil particles or within a fractured rock. Typically, the majority of groundwater (and anything dissolved in it) moves through the porosity available to flow (sometimes called effective porosity ). Permeability is an expression of the connectedness of the pores. For instance, an unfractured rock unit may have a high porosity (it has many holes between its constituent grains), but a low permeability (none of the pores are connected). An example of this phenomenon is pumice , which, when in its unfractured state, can make a poor aquifer. Porosity does not directly affect the distribution of hydraulic head in an aquifer, but it has a very strong effect on the migration of dissolved contaminants, since it affects groundwater flow velocities through an inversely proportional relationship. Darcy's law is commonly applied to study the movement of water, or other fluids through porous media, and constitutes the basis for many hydrogeological analyses. Main article: water content Water content (θ) is also a directly measurable property; it is the fraction of the total rock which is filled with liquid water. This is also a fraction between 0 and 1, but it must also be less than or equal to the total porosity. The water content is very important in vadose zone hydrology, where the hydraulic conductivity is a strongly nonlinear function of water content; this complicates the solution of the unsaturated groundwater flow equation. Main article: Hydraulic conductivity Hydraulic conductivity (K) and transmissivity (T) are indirect aquifer properties (they cannot be measured directly). T is the K integrated over the vertical thickness (b) of the aquifer (T=Kb when K is constant over the entire thickness). These properties are measures of an aquifer 's ability to transmit water . Intrinsic permeability (κ) is a secondary medium property which does not depend on the viscosity and density of the fluid (K and T are specific to water); it is used more in the petroleum industry. Specific storage and specific yield[ edit ] Main article: Specific storage Illustration of seasonal fluctuations in the water table . Specific storage (Ss) and its depth-integrated equivalent, storativity (S=Ssb), are indirect aquifer properties (they cannot be measured directly); they indicate the amount of groundwater released from storage due to a unit depressurization of a confined aquifer. They are fractions between 0 and 1. Specific yield (Sy) is also a ratio between 0 and 1 (Sy ≤ porosity) and indicates the amount of water released due to drainage from lowering the water table in an unconfined aquifer. The value for specific yield is less than the value for porosity because some water will remain in the medium even after drainage due to intermolecular forces. Often the porosity or effective porosity is used as an upper bound to the specific yield. Typically Sy is orders of magnitude larger than Ss. Fault zone hydrogeology[ edit ] Main article: Fault zone hydrogeology Fault zone hydrogeology is the study of how brittlely deformed rocks alter fluid flows in different lithological settings , such as clastic , igneous and carbonate rocks . Fluid movements, that can be quantified as permeability , can be facilitated or impeded due to the existence of a fault zone . [7] This is because different mechanism and deformed rocks can alter the porosity and hence the permeability within fault zone. Fluids involved generally are groundwater (fresh and marine waters) and hydrocarbons (Oil and Gas). [8] As fault zone is a zone of weakness that helps to increase the weathered zone thickness and hence the help in ground water recharge. [9] Along with faults , fractures and foliations also facilitate the groundwater mainly in hard rock terrains. [9] Contaminant transport properties[ edit ] Often we are interested in how the moving groundwater will transport dissolved contaminants around (the sub-field of contaminant hydrogeology). The contaminants which are man-made (e.g., petroleum products , nitrate , Chromium or radionuclides ) or naturally occurring (e.g., arsenic , salinity ), can be transported through three main mechanisms, advection (transport along the main direction of flow at seepage velocity), diffusion (migration of the contaminant from high to low concentration areas), and dispersion (due to microscale heterogeneities present in the porous medium and non-uniform velocity distribution relative to seepage velocity). [10] Besides needing to understand where the groundwater is flowing, based on the other hydrologic properties discussed above, there are additional aquifer properties which affect how dissolved contaminants move with groundwater. Transport and fate of contaminants in groundwater Hydrodynamic dispersion[ edit ] Hydrodynamic dispersivity (αL, αT) is an empirical factor which quantifies how much contaminants stray away from the path of the groundwater which is carrying it. Some of the contaminants will be "behind" or "ahead" the mean groundwater, giving rise to a longitudinal dispersivity (αL), and some will be "to the sides of" the pure advective groundwater flow, leading to a transverse dispersivity (αT). Dispersion in groundwater arises because each water "particle", passing beyond a soil particle, must choose where to go, whether left or right or up or down, so that the water "particles" (and their solute) are gradually spread in all directions around the mean path. This is the "microscopic" mechanism, on the scale of soil particles. More important, over long distances, can be the macroscopic inhomogeneities of the aquifer, which can have regions of larger or smaller permeability, so that some water can find a preferential path in one direction, some other in a different direction, so that the contaminant can be spread in a completely irregular way, like in a (three-dimensional) delta of a river. Dispersivity is actually a factor which represents our lack of information about the system we are simulating. There are many small details about the aquifer which are effectively averaged when using a macroscopic approach (e.g., tiny beds of gravel and clay in sand aquifers); these manifest themselves as an apparent dispersivity. Because of this, α is often claimed to be dependent on the length scale of the problem — the dispersivity found for transport through 1 m3 of aquifer is different from that for transport through 1 cm3 of the same aquifer material. [11] Molecular diffusion[ edit ] Diffusion is a fundamental physical phenomenon, which Albert Einstein characterized as Brownian motion , that describes the random thermal movement of molecules and small particles in gases and liquids. It is an important phenomenon for small distances (it is essential for the achievement of thermodynamic equilibria), but, as the time necessary to cover a distance by diffusion is proportional to the square of the distance itself, it is ineffective for spreading a solute over macroscopic distances. The diffusion coefficient, D, is typically quite small, and its effect can often be considered negligible (unless groundwater flow velocities are extremely low, as they are in clay aquitards). It is important not to confuse diffusion with dispersion, as the former is a physical phenomenon and the latter is an empirical factor which is cast into a similar form as diffusion, because we already know how to solve that problem. Retardation by adsorption[ edit ] The retardation factor is another very important feature that make the motion of the contaminant to deviate from the average groundwater motion. It is analogous to the retardation factor of chromatography . Unlike diffusion and dispersion, which simply spread the contaminant, the retardation factor changes its global average velocity, so that it can be much slower than that of water. This is due to a chemico-physical effect: the adsorption to the soil, which holds the contaminant back and does not allow it to progress until the quantity corresponding to the chemical adsorption equilibrium has been adsorbed. This effect is particularly important for less soluble contaminants, which thus can move even hundreds or thousands times slower than water. The effect of this phenomenon is that only more soluble species can cover long distances. The retardation factor depends on the chemical nature of both the contaminant and the aquifer. History and development[ edit ] Henry Darcy, whose work set the foundation of quantitative hydrogeology Henry Darcy: 19th century[ edit ] Henry Darcy was a French scientist who made advances in flow of fluids through porous materials. He conducted experiments which studied the movement of fluids through sand columns. These experiments led to the determination of Darcy's law , which describes fluid flow through a medium with high levels of porosity. Darcy's work is considered to be the beginning of quantitative hydrogeology. [12] Oscar Edward Meinzer: 20th century[ edit ] Oscar Edward Meinzer was an American scientist who is often called the "father of modern groundwater hydrology". He standardized key terms in the field as well as determined principles regarding occurrence, movement, and discharge. He proved that the flow of water obeys Darcy's law. He also proposed the use of geophysical methods and recorders on wells, as well as suggested pumping tests to gather quantitative information on the properties of aquifers. Meinzer also highlighted the importance of studying the geochemistry of water, as well as the impact of high salinity levels in aquifers. [13] Main article: Darcy's law Darcy's law is a constitutive equation , empirically derived by Henry Darcy in 1856, which states that the amount of groundwater discharging through a given portion of aquifer is proportional to the cross-sectional area of flow, the hydraulic gradient , and the hydraulic conductivity . Groundwater flow equation[ edit ] Main article: Groundwater flow equation Geometry of a partially penetrating well drainage system in an anisotropic layered aquifer The groundwater flow equation, in its most general form, describes the movement of groundwater in a porous medium (aquifers and aquitards). It is known in mathematics as the diffusion equation , and has many analogs in other fields. Many solutions for groundwater flow problems were borrowed or adapted from existing heat transfer solutions. It is often derived from a physical basis using Darcy's law and a conservation of mass for a small control volume. The equation is often used to predict flow to wells , which have radial symmetry, so the flow equation is commonly solved in polar or cylindrical coordinates . The Theis equation is one of the most commonly used and fundamental solutions to the groundwater flow equation; it can be used to predict the transient evolution of head due to the effects of pumping one or a number of pumping wells. The Thiem equation is a solution to the steady state groundwater flow equation (Laplace's Equation) for flow to a well. Unless there are large sources of water nearby (a river or lake), true steady-state is rarely achieved in reality. Both above equations are used in aquifer tests (pump tests). The Hooghoudt equation is a groundwater flow equation applied to subsurface drainage by pipes, tile drains or ditches. [14] An alternative subsurface drainage method is drainage by wells for which groundwater flow equations are also available. [15] Calculation of groundwater flow[ edit ] Relative groundwater travel times. To use the groundwater flow equation to estimate the distribution of hydraulic heads, or the direction and rate of groundwater flow, this partial differential equation (PDE) must be solved. The most common means of analytically solving the diffusion equation in the hydrogeology literature are: Laplace , Hankel and Fourier transforms (to reduce the number of dimensions of the PDE), similarity transform (also called the Boltzmann transform) is commonly how the Theis solution is derived, separation of variables , which is more useful for non-Cartesian coordinates, and Green's functions , which is another common method for deriving the Theis solution — from the fundamental solution to the diffusion equation in free space. No matter which method we use to solve the groundwater flow equation , we need both initial conditions (heads at time (t) = 0) and boundary conditions (representing either the physical boundaries of the domain, or an approximation of the domain beyond that point). Often the initial conditions are supplied to a transient simulation, by a corresponding steady-state simulation (where the time derivative in the groundwater flow equation is set equal to 0). There are two broad categories of how the (PDE) would be solved; either analytical methods, numerical methods, or something possibly in between. Typically, analytic methods solve the groundwater flow equation under a simplified set of conditions exactly, while numerical methods solve it under more general conditions to an approximation. Analytic methods[ edit ] Analytic methods typically use the structure of mathematics to arrive at a simple, elegant solution, but the required derivation for all but the simplest domain geometries can be quite complex (involving non-standard coordinates , conformal mapping , etc.). Analytic solutions typically are also simply an equation that can give a quick answer based on a few basic parameters. The Theis equation is a very simple (yet still very useful) analytic solution to the groundwater flow equation , typically used to analyze the results of an aquifer test or slug test . Numerical methods[ edit ] The topic of numerical methods is quite large, obviously being of use to most fields of engineering and science in general. Numerical methods have been around much longer than computers have (In the 1920s Richardson developed some of the finite difference schemes still in use today, but they were calculated by hand, using paper and pencil, by human "calculators"), but they have become very important through the availability of fast and cheap personal computers . A quick survey of the main numerical methods used in hydrogeology, and some of the most basic principles are shown below and further discussed in the Groundwater model article. There are two broad categories of numerical methods: gridded or discretized methods and non-gridded or mesh-free methods. In the common finite difference method and finite element method (FEM) the domain is completely gridded ("cut" into a grid or mesh of small elements). The analytic element method (AEM) and the boundary integral equation method (BIEM — sometimes also called BEM, or Boundary Element Method) are only discretized at boundaries or along flow elements (line sinks, area sources, etc.), the majority of the domain is mesh-free. General properties of gridded methods[ edit ] Gridded Methods like finite difference and finite element methods solve the groundwater flow equation by breaking the problem area (domain) into many small elements (squares, rectangles, triangles, blocks, tetrahedra , etc.) and solving the flow equation for each element (all material properties are assumed constant or possibly linearly variable within an element), then linking together all the elements using conservation of mass across the boundaries between the elements (similar to the divergence theorem ). This results in a system which overall approximates the groundwater flow equation, but exactly matches the boundary conditions (the head or flux is specified in the elements which intersect the boundaries). Finite differences are a way of representing continuous differential operators using discrete intervals (Δx and Δt), and the finite difference methods are based on these (they are derived from a Taylor series ). For example, the first-order time derivative is often approximated using the following forward finite difference, where the subscripts indicate a discrete time location, h t . {\displaystyle {\frac {\partial h}{\partial t}}=h'(t_{i})\approx {\frac {h_{i}-h_{i-1}}{\Delta t}}.} The forward finite difference approximation is unconditionally stable, but leads to an implicit set of equations (that must be solved using matrix methods, e.g. LU or Cholesky decomposition ). The similar backwards difference is only conditionally stable, but it is explicit and can be used to "march" forward in the time direction, solving one grid node at a time (or possibly in parallel , since one node depends only on its immediate neighbors). Rather than the finite difference method, sometimes the Galerkin FEM approximation is used in space (this is different from the type of FEM often used in structural engineering ) with finite differences still used in time. Application of finite difference models[ edit ] MODFLOW is a well-known example of a general finite difference groundwater flow model. It is developed by the US Geological Survey as a modular and extensible simulation tool for modeling groundwater flow. It is free software developed, documented and distributed by the USGS. Many commercial products have grown up around it, providing graphical user interfaces to its input file based interface, and typically incorporating pre- and post-processing of user data. Many other models have been developed to work with MODFLOW input and output, making linked models which simulate several hydrologic processes possible (flow and transport models, surface water and groundwater models and chemical reaction models), because of the simple, well documented nature of MODFLOW. Application of finite element models[ edit ] Finite Element programs are more flexible in design (triangular elements vs. the block elements most finite difference models use) and there are some programs available (SUTRA, a 2D or 3D density-dependent flow model by the USGS; Hydrus , a commercial unsaturated flow model; FEFLOW , a commercial modelling environment for subsurface flow, solute and heat transport processes; OpenGeoSys, a scientific open-source project for thermo-hydro-mechanical-chemical (THMC) processes in porous and fractured media; [16] [17] COMSOL Multiphysics (a commercial general modelling environment), FEATool Multiphysics an easy to use MATLAB simulation toolbox, and Integrated Water Flow Model (IWFM), but they are still not as popular in with practicing hydrogeologists as MODFLOW is.  Finite element models are more popular in university and laboratory environments, where specialized models solve non-standard forms of the flow equation ( unsaturated flow, density dependent flow, coupled heat and groundwater flow, etc.) Application of finite volume models[ edit ] The finite volume method is a method for representing and evaluating partial differential equations as algebraic equations. [18] [19] [ full citation needed ] Similar to the finite difference method, values are calculated at discrete places on a meshed geometry. "Finite volume" refers to the small volume surrounding each node point on a mesh. In the finite volume method, volume integrals in a partial differential equation that contain a divergence term are converted to surface integrals, using the divergence theorem. These terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods are conservative. Another advantage of the finite volume method is that it is easily formulated to allow for unstructured meshes. The method is used in many computational fluid dynamics packages. PORFLOW software package is a comprehensive mathematical model for simulation of Ground Water Flow and Nuclear Waste Management developed by Analytic & Computational Research, Inc., ACRi. The FEHM software package is available free from Los Alamos National Laboratory .  This versatile porous flow simulator includes capabilities to model multiphase, thermal, stress, and multicomponent reactive chemistry.  Current work using this code includes simulation of methane hydrate formation, CO2 sequestration , oil shale extraction , migration of both nuclear and chemical contaminants, environmental isotope migration in the unsaturated zone, and karst formation. Other methods[ edit ] These include mesh-free methods like the Analytic Element Method (AEM) and the Boundary Element Method (BEM), which are closer to analytic solutions, but they do approximate the groundwater flow equation in some way.  The BEM and AEM exactly solve the groundwater flow equation (perfect mass balance), while approximating the boundary conditions.  These methods are more exact and can be much more elegant solutions (like analytic methods are), but have not seen as widespread use outside academic and research groups yet. Water wells[ edit ] A water well is a mechanism for bringing groundwater to the surface by drilling or digging and bringing it up to the surface with a pump or by hand using buckets or similar devices. The first historical instance of water wells was in the 52nd century BC in modern-day Austria . [20] Today, wells are used all over the world, from developing nations to suburbs in the United States. There are three main types of wells, shallow, deep, and artesian. Shallow wells tap into unconfined aquifers, and are, generally, shallow, less than 15 meters deep. Shallow wells have a small diameter, usually less than 15 centimeters. [21] Deep wells access confined aquifers, and are always drilled by machine. All deep wells bring water to the surface using mechanical pumps. In artesian wells, water flows naturally without the use of a pump or some other mechanical device. This is due to the top of the well being located below the water table. [22] Water well design and construction[ edit ] A water well in Kerala , India . One of the most important aspects of groundwater engineering and hydrogeology is water well design and construction. Proper well design and construction are important to maintain the health of the groundwater and the people which will use the well. Factors which must be considered in well design are: A reliable aquifer, providing a continuous water supply The quality of the accessible groundwater How to monitor the well Operating costs of the well Expected yield of the well Any prior drilling into the aquifer [23] There are five main areas to be considered when planning and constructing a new water well, along with the factors above. They are: Aquifer Suitability Well Testing" [24] Aquifer suitability starts with determining possible locations for the well using " USGS reports, well logs, and cross sections" of the aquifer. This information should be used to determine aquifer properties such as depth, thickness, transmissivity, and well yield. In this stage, the quality of the water in the aquifer should also be determined, and screening should occur to check for contaminants. [24] After factors such as depth and well yield are determined, the well design and drilling approach must be established. Drilling method is selected based on "soil conditions, well depth, design, and costs." [24] At this stage, cost estimates are prepared, and plans are adjusted to meet budgetary needs. Important parts of a well include the well seals, casings or liners, drive shoes, well screen assemblies, and a sand or gravel pack (optional). Each of these components ensures that the well only draws from one aquifer, and no leakage occurs at any stage of the process. [24] There are several methods of drilling which can be used when constructing a water well. They include: "Cable tool, Air rotary, Mud rotary, and Flooded reverse circulation dual rotary" drilling techniques. [24] Cable tool drilling is inexpensive and can be used for all types of wells, but the alignment must be constantly checked and it has a slow advance rate. It is not an effective drilling technique for consolidated formations, but does provide a small drilling footprint. Air rotary drilling is cost effective and works well for consolidated formations. It has a fast advance rate, but is not adequate for large diameter wells. Mud rotary drilling is especially cost effective for deep wells. It maintains good alignment, but requires a larger footprint. It has a very fast advance rate. Flooded reverse circulation dual rotary drilling is more expensive, but good for large well designs. It is versatile and maintains alignment. It has a fast advance rate. [24] Well screens ensure that only water makes it to the surface, and sediments remain beneath the Earth's surface. Screens are placed along the shaft of the well to filter out sediment as water is pumped towards the surface. Screen design can be impacted by the nature of the soil, and natural pack designs can be used to maximize efficiency. [24] After construction of the well, testing must be done to assess productivity, efficiency and yield of the well, as well as determine the impacts of the well on the aquifer. Several different tests should be completed on the well in order to test all relevant qualities of the well. [24] Issues in groundwater engineering and hydrogeology[ edit ] Contamination[ edit ] Groundwater contamination happens when other fluids seep into the aquifer and mix with existing groundwater. Pesticides, fertilizers, and gasoline are common contaminants of aquifers. Underground storage tanks for chemicals such as gasoline are especially concerning sources of groundwater contamination. As these tanks corrode, they can leak, and their contents can contaminate nearby groundwater. For buildings which are not connected to a wastewater treatment system, septic tanks can be used to dispose of waste at a safe rate. If septic tanks are not built or maintained properly, they can leak bacteria, viruses and other chemicals into the surrounding groundwater. Landfills are another potential source of groundwater contamination. As trash is buried, harmful chemicals can migrate from the garbage and into the surrounding groundwater if the protective base layer is cracked or otherwise damaged. Other chemicals, such as road salts and chemicals used on lawns and farms, can runoff into local reservoirs, and eventually into aquifers. As water goes through the water cycle, contaminants in the atmosphere can contaminate the water. This water can also make its way into groundwater. [25] Fracking[ edit ] Contamination of groundwater due to fracking has long been debated. Since chemicals commonly used in hydraulic fracturing are not tested by government agencies responsible for determining the effects of fracking on groundwater, laboratories at the United States Environmental Protection Agency , or EPA, have a hard time determining if chemicals used in fracking are present in nearby aquifers. [26] In 2016, the EPA released a report which states that drinking water can be contaminated by fracking. This was a reversal of their previous policies after a $29 million study into the effects of fracking on local drinking water. [27] California[ edit ] California sees some of the largest controversies in groundwater usage due to the dry conditions California faces, high population, and intensive agriculture. Conflicts generally occur over pumping groundwater and shipping it out of the area, unfair use of water by a commercial company, and contamination of groundwater by development projects. In Siskiyou County in northern California, the California Superior Court ruled poor groundwater regulations have allowed pumping to diminish the flows in the Scott River and disturbed the natural habitat of salmon. In Owens Valley in central California, groundwater was pumped for use in fish farms, which resulted in the death of local meadows and other ecosystems. This resulted in a lawsuit and settlement against the fish companies. Development in southern California is threatening local aquifers, contaminating groundwater through construction and normal human activity. For example, a solar project in San Bernardino County would allegedly threaten the ecosystem of bird and wildlife species because of its use of up to 1.3 million cubic meters of groundwater, which could impact Harper Lake . [28] In September 2014, California passed the Sustainable Groundwater Management Act , which requires users to manage groundwater appropriately, as it is connected to surface water systems. [28] Colorado[ edit ] Due to its arid climate, the state of Colorado gets most of its water from underground. Because of this, there have been issues regarding groundwater engineering practices. As many as 65,000 people were affected when high levels of PFCs were found in the Widefield Aquifer. Groundwater use in Colorado dates back to before the 20th century. Nineteen of Colorado's 63 counties depend mostly on groundwater for supplies and domestic uses. The Colorado Geological Survey has three significant reports on groundwater in the Denver Basin. The first report Geology of Upper Cretaceous, Paleocene and Eocene Strata in the Southwestern Denver Basin, The second report Bedrock Geology, Structure, and Isopach Maps of the Upper Cretaceous to Paleogene Strata between Greeley and Colorado Springs, The third publication Cross Sections of the Freshwater Bearing Strata of the Denver Basin between Greeley and Colorado Springs. [29] [30] New trends in groundwater engineering/hydrogeology[ edit ] Since the first wells were made thousands of years ago, groundwater systems have been changed by human activity. Fifty years ago, the sustainability of these systems on a larger scale began to come into consideration, becoming one of the main focuses of groundwater engineering. New ideas and research are advancing groundwater engineering into the 21st century, while still considering groundwater conservation. [31] Topographical mapping[ edit ] New advancements have arisen in topographical mapping to improve sustainability. Topographic mapping has been updated to include radar, which can penetrate the ground to help pinpoint areas of concern. In addition, large computations can use gathered data from maps to further the knowledge of groundwater aquifers in recent years. This has made highly complex and individualized water cycle models possible, which has helped to make groundwater sustainability more applicable to specific situations. [31] The role of technology[ edit ] Technological improvements have advanced topographical mapping, and have also improved the quality of lithosphere, hydrosphere, biosphere, and atmosphere simulations.  These simulations are useful on their own; however, when used together, they help to give an even more accurate prediction of the future sustainability of an area, and what changes can be made to ensure stability in the area. This would not be possible without the advancement of technology. As technology continues to progress, the simulations will increase in accuracy and allow for more complex studies and projects in groundwater engineering. [31] Growing populations[ edit ] As populations continue to grow, areas which were using groundwater at a sustainable rate are now beginning to face sustainability issues for the future. Populations of the size currently seen in large cities were not taken into consideration when the long term sustainability of aquifers. These large population sizes are beginning to stress groundwater supply. This has led to the need for new policies in some urban areas. These are known as proactive land-use management, where cities can move proactively to conserve groundwater. In Brazil, overpopulation caused municipally provided water to run low. Due to the shortage of water, people began to drill wells within the range normally served by the municipal water system. This was a solution for people in high socioeconomic standing, but left much of the underprivileged population without access to water. Because of this, a new municipal policy was created which drilled wells to assist those who could not afford to drill wells of their own. Because the city is in charge of drilling the new wells, they can better plan for the future sustainability of the groundwater in the region, by carefully placing the wells and taking growing populations into consideration. [32] Dependency on groundwater in the United States[ edit ] In the United States , 51% of the drinking water comes from groundwater supplies. Around 99% of the rural population depends on groundwater. In addition, 64% of the total groundwater of the country is used for irrigation, and some of it is used for industrial processes and recharge for lakes and rivers. In 2010, 22 percent of freshwater used in US came from groundwater and the other 78 percent came from surface water. Groundwater is important for some states that don't have access to fresh water. most of the fresh groundwater 65 percent is used for irrigation and the 21 percent is used for public purposes drinking mostly. [33] [34] Environmental engineering is a broad category hydrogeology fits into; Flownet is an analysis tool for steady-state flow; Groundwater energy balance : groundwater flow equations based on the energy balance; Fault zone hydrogeology : field specifically analyzing hydrogeology in fault zones Hydrogeophysics : field integrating hydrogeology with geophysics
The mineral zircon is often used in radiometric dating . Geologists also use methods to determine the absolute age of rock samples and geological events. These dates are useful on their own and may also be used in conjunction with relative dating methods or to calibrate relative methods. [21] At the beginning of the 20th century, advancement in geological science was facilitated by the ability to obtain accurate absolute dates to geological events using radioactive isotopes and other methods. This changed the understanding of geological time. Previously, geologists could only use fossils and stratigraphic correlation to date sections of rock relative to one another. With isotopic dates, it became possible to assign absolute ages to rock units, and these absolute dates could be applied to fossil sequences in which there was datable material, converting the old relative ages into new absolute ages. For many geological applications, isotope ratios of radioactive elements are measured in minerals that give the amount of time that has passed since a rock passed through its particular closure temperature , the point at which different radiometric isotopes stop diffusing into and out of the crystal lattice . [22] [23] These are used in geochronologic and thermochronologic studies. Common methods include uranium–lead dating , potassium–argon dating , argon–argon dating and uranium–thorium dating . These methods are used for a variety of applications. Dating of lava and volcanic ash layers found within a stratigraphic sequence can provide absolute age data for sedimentary rock units that do not contain radioactive isotopes and calibrate relative dating techniques. These methods can also be used to determine ages of pluton emplacement. Thermochemical techniques can be used to determine temperature profiles within the crust, the uplift of mountain ranges, and paleo-topography. Fractionation of the lanthanide series elements is used to compute ages since rocks were removed from the mantle. Other methods are used for more recent events. Optically stimulated luminescence and cosmogenic radionuclide dating are used to date surfaces and/or erosion rates. Dendrochronology can also be used for the dating of landscapes. Radiocarbon dating is used for geologically young materials containing organic carbon . Geological development of an area[ edit ] An originally horizontal sequence of sedimentary rocks (in shades of tan) are affected by igneous activity. Deep below the surface is a magma chamber and large associated igneous bodies. The magma chamber feeds the volcano , and sends offshoots of magma that will later crystallize into dikes and sills. Magma also advances upwards to form intrusive igneous bodies . The diagram illustrates both a cinder cone volcano, which releases ash, and a composite volcano , which releases both lava and ash. An illustration of the three types of faults. A. Strike-slip faults occur when rock units slide past one another. B. Normal faults occur when rocks are undergoing horizontal extension. C. Reverse (or thrust) faults occur when rocks are undergoing horizontal shortening. The San Andreas Fault in California The geology of an area changes through time as rock units are deposited and inserted, and deformational processes alter their shapes and locations. Rock units are first emplaced either by deposition onto the surface or intrusion into the overlying rock . Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows blanket the surface. Igneous intrusions such as batholiths , laccoliths , dikes , and sills , push upwards into the overlying rock, and crystallize as they intrude. After the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed . Deformation typically occurs as a result of horizontal shortening, horizontal extension , or side-to-side ( strike-slip ) motion. These structural regimes broadly relate to convergent boundaries , divergent boundaries , and transform boundaries, respectively, between tectonic plates. When rock units are placed under horizontal compression , they shorten and become thicker. Because rock units, other than muds, do not significantly change in volume , this is accomplished in two primary ways: through faulting and folding . In the shallow crust, where brittle deformation can occur, thrust faults form, which causes the deeper rock to move on top of the shallower rock. Because deeper rock is often older, as noted by the principle of superposition , this can result in older rocks moving on top of younger ones. Movement along faults can result in folding, either because the faults are not planar or because rock layers are dragged along, forming drag folds as slip occurs along the fault. Deeper in the Earth, rocks behave plastically and fold instead of faulting. These folds can either be those where the material in the center of the fold buckles upwards, creating " antiforms ", or where it buckles downwards, creating " synforms ". If the tops of the rock units within the folds remain pointing upwards, they are called anticlines and synclines , respectively. If some of the units in the fold are facing downward, the structure is called an overturned anticline or syncline, and if all of the rock units are overturned or the correct up-direction is unknown, they are simply called by the most general terms, antiforms, and synforms. A diagram of folds, indicating an anticline and a syncline Even higher pressures and temperatures during horizontal shortening can cause both folding and metamorphism of the rocks. This metamorphism causes changes in the mineral composition of the rocks; creates a foliation , or planar surface, that is related to mineral growth under stress. This can remove signs of the original textures of the rocks, such as bedding in sedimentary rocks, flow features of lavas , and crystal patterns in crystalline rocks . Extension causes the rock units as a whole to become longer and thinner. This is primarily accomplished through normal faulting and through the ductile stretching and thinning. Normal faults drop rock units that are higher below those that are lower. This typically results in younger units ending up below older units. Stretching of units can result in their thinning. In fact, at one location within the Maria Fold and Thrust Belt , the entire sedimentary sequence of the Grand Canyon appears over a length of less than a meter. Rocks at the depth to be ductilely stretched are often also metamorphosed. These stretched rocks can also pinch into lenses, known as boudins , after the French word for "sausage" because of their visual similarity. Where rock units slide past one another, strike-slip faults develop in shallow regions, and become shear zones at deeper depths where the rocks deform ductilely. Geological cross section of Kittatinny Mountain . This cross-section shows metamorphic rocks, overlain by younger sediments deposited after the metamorphic event. These rock units were later folded and faulted during the uplift of the mountain. The addition of new rock units, both depositionally and intrusively, often occurs during deformation. Faulting and other deformational processes result in the creation of topographic gradients, causing material on the rock unit that is increasing in elevation to be eroded by hillslopes and channels. These sediments are deposited on the rock unit that is going down. Continual motion along the fault maintains the topographic gradient in spite of the movement of sediment and continues to create accommodation space for the material to deposit. Deformational events are often also associated with volcanism and igneous activity. Volcanic ashes and lavas accumulate on the surface, and igneous intrusions enter from below. Dikes , long, planar igneous intrusions, enter along cracks, and therefore often form in large numbers in areas that are being actively deformed. This can result in the emplacement of dike swarms , such as those that are observable across the Canadian shield, or rings of dikes around the lava tube of a volcano. All of these processes do not necessarily occur in a single environment and do not necessarily occur in a single order. The Hawaiian Islands , for example, consist almost entirely of layered basaltic lava flows. The sedimentary sequences of the mid-continental United States and the Grand Canyon in the southwestern United States contain almost-undeformed stacks of sedimentary rocks that have remained in place since Cambrian time. Other areas are much more geologically complex. In the southwestern United States, sedimentary, volcanic, and intrusive rocks have been metamorphosed, faulted, foliated, and folded. Even older rocks, such as the Acasta gneiss of the Slave craton in northwestern Canada , the oldest known rock in the world have been metamorphosed to the point where their origin is indiscernible without laboratory analysis. In addition, these processes can occur in stages. In many places, the Grand Canyon in the southwestern United States being a very visible example, the lower rock units were metamorphosed and deformed, and then deformation ended and the upper, undeformed units were deposited. Although any amount of rock emplacement and rock deformation can occur, and they can occur any number of times, these concepts provide a guide to understanding the geological history of an area. Investigative methods[ edit ] A standard Brunton Pocket Transit , commonly used by geologists for mapping and surveying Geologists use a number of fields, laboratory, and numerical modeling methods to decipher Earth history and to understand the processes that occur on and inside the Earth. In typical geological investigations, geologists use primary information related to petrology (the study of rocks), stratigraphy (the study of sedimentary layers), and structural geology (the study of positions of rock units and their deformation). In many cases, geologists also study modern soils, rivers , landscapes , and glaciers ; investigate past and current life and biogeochemical pathways, and use geophysical methods to investigate the subsurface. Sub-specialities of geology may distinguish endogenous and exogenous geology. [24] Field methods[ edit ] A typical USGS field mapping camp in the 1950s
Toggle the table of contents Atmospheric science From Wikipedia, the free encyclopedia Study of the atmosphere, its processes, and its interactions with other systems e Atmospheric science is the study of the Earth's atmosphere and its various inner-working physical processes. Meteorology includes atmospheric chemistry and atmospheric physics with a major focus on weather forecasting . Climatology is the study of atmospheric changes (both long and short-term) that define average climates and their change over time climate variability . Aeronomy is the study of the upper layers of the atmosphere, where dissociation and ionization are important. Atmospheric science has been extended to the field of planetary science and the study of the atmospheres of the planets and natural satellites of the Solar System . Experimental instruments used in atmospheric science include satellites , rocketsondes , radiosondes , weather balloons , radars , and lasers . The term aerology (from Greek ἀήρ, aēr, " air "; and -λογία, -logia ) is sometimes used as an alternative term for the study of Earth's atmosphere; [1] in other definitions, aerology is restricted to the free atmosphere , the region above the planetary boundary layer . [2] Main article: Atmospheric chemistry Composition diagram showing the evolution/cycles of various elements in Earth's atmosphere. Atmospheric chemistry is a branch of atmospheric science in which the chemistry of the Earth's atmosphere and that of other planets is studied. It is a multidisciplinary field of research and draws on environmental chemistry, physics, meteorology, computer modeling, oceanography, geology and volcanology and other disciplines. Research is increasingly connected with other areas of study such as climatology. The composition and chemistry of the atmosphere is of importance for several reasons, but primarily because of the interactions between the atmosphere and living organisms. The composition of the Earth's atmosphere has been changed by human activity and some of these changes are harmful to human health, crops and ecosystems. Examples of problems which have been addressed by atmospheric chemistry include acid rain, photochemical smog and global warming. Atmospheric chemistry seeks to understand the causes of these problems, and by obtaining a theoretical understanding of them, allow possible solutions to be tested and the effects of changes in government policy evaluated. See also: Synoptic scale meteorology Atmospheric dynamics is the study of motion systems of meteorological importance, integrating observations at multiple locations and times and theories. Common topics studied include diverse phenomena such as thunderstorms , tornadoes , gravity waves , tropical cyclones , extratropical cyclones , jet streams , and global-scale circulations. The goal of dynamical studies is to explain the observed circulations on the basis of fundamental principles from physics . The objectives of such studies incorporate improving weather forecasting , developing methods for predicting seasonal and interannual climate fluctuations, and understanding the implications of human-induced perturbations (e.g., increased carbon dioxide concentrations or depletion of the ozone layer) on the global climate. [4] Main article: Atmospheric physics Atmospheric physics is the application of physics to the study of the atmosphere. Atmospheric physicists attempt to model Earth's atmosphere and the atmospheres of the other planets using fluid flow equations, chemical models, radiation balancing, and energy transfer processes in the atmosphere and underlying oceans and land. In order to model weather systems, atmospheric physicists employ elements of scattering theory, wave propagation models, cloud physics , statistical mechanics and spatial statistics , each of which incorporate high levels of mathematics and physics. Atmospheric physics has close links to meteorology and climatology and also covers the design and construction of instruments for studying the atmosphere and the interpretation of the data they provide, including remote sensing instruments. In the United Kingdom, atmospheric studies are underpinned by the Meteorological Office. Divisions of the U.S. National Oceanic and Atmospheric Administration (NOAA) oversee research projects and weather modeling involving atmospheric physics. The U.S. National Astronomy and Ionosphere Center also carries out studies of the high atmosphere.
Toggle the table of contents Environmental geology From Wikipedia, the free encyclopedia Science of the practical application of geology in environmental problems. Oil well in Tsaidam Environmental geology, like hydrogeology , is an applied science concerned with the practical application of the principles of geology in the solving of environmental problems created by man. It is a multidisciplinary field that is closely related to engineering geology and, to a lesser extent, to environmental geography . [1] Each of these fields involves the study of the interaction of humans with the geologic environment, including the biosphere , the lithosphere , the hydrosphere , and to some extent the atmosphere . In other words, environmental geology is the application of geological information to solve conflicts, minimizing possible adverse environmental degradation , or maximizing possible advantageous conditions resulting from the use of natural and modified environment. With an increasing world population and industrialization , the natural environment and resources are under high strain which puts them at the forefront of world issues. Environmental geology is on the rise with these issues as solutions are found by utilizing it. [2] Environmental geology in relation to other fields[ edit ] Hydrogeology[ edit ] Hydrogeology is the area of geology that deals with the distribution and movement of groundwater in the soil and rocks of the Earth's crust . Environmental geology is applied in this field as environmental problems are created in groundwater pollution due to mining, agriculture, and other human activities. Pollution is the impairment of groundwater by heat, bacteria, or chemicals. The greatest contributors to groundwater pollution are surface sources such as fertilizers, leaking sewers, polluted streams, and mining/mineral wastes. [3] Environmental geology approaches the groundwater pollution problem by creating objectives when monitoring. These objectives include: determining the nature, extent, and degree of contamination, determining the propagation mechanism and hydrological parameters, so that the appropriate countermeasures can be taken, detecting and warning of movement into critical areas, assessing the effectiveness of the immediate countermeasures undertaken to offset the effects of contamination, recording of data for long term evaluation and compliance with standards, and initiating research monitoring to validate and verify the models and assumptions upon which the immediate countermeasures are based. Soil Science[ edit ] Soil science is the study of soil as a natural resource on the surface of the Earth. Environmental geology is applied in this field as soil scientists raise concerns on soil preservation and arable land with the world increasing population, increasing per capita food consumption, and land degradation . These environmental problems are attacked and reduced with environmental geology by using soil surveys. [4] These surveys assess the properties of soils and are of use in geologic mapping, rural and urban land planning, especially in terms of agriculture and forestry. Soil surveys are essential parts of land use planning and mapping as they provide insight on agricultural land usage. Soil surveys provide information on optimum cropping systems and soil management so less land degradation is done and agriculture provides its optimum yield for the increasing per capita food consumption. [1] Soil survey investigations include: Focuses[ edit ] Environmental geology includes managing geological and hydrogeological resources such as fossil fuels , minerals , water (surface and groundwater ), and land use , studying the earth's surface through the disciplines of geomorphology , and edaphology , defining and mitigating exposure of natural hazards on humans, managing industrial and domestic waste disposal and minimizing or eliminating effects of pollution , and performing associated activities, often involving litigation. Environmental geology is often applied to some well known environmental issues including population growth, mining, diminishing resources, and global land use. [5] Mining[ edit ] Since the Stone Age, when humans began mining for flint , they have been dependent on this practice, and the dependency on minerals continues to increase as society evolves. One of the downsides of mining is that it is restricted to areas where minerals are present and economically viable. Mining duration is also restrained as mineral resources are finite, so when a deposit is exhausted, mining in that location comes to an end. [1] Although modern mining and mineral activities utilize many ways to reduce negative environmental impacts, accidental releases can occur and the appropriate mitigation and prevention practices were not common in historical practices. Potentially harmful metals, other deposit constituents, and mineral processing chemicals or byproducts can contaminate the surrounding environment due to these situations. [6] Some common environmental impacts of mining are rock displacements that allow fine dust particles to seep into surface waters, the defacement of the local landscape, and the large amounts of waste with some being chemically reactive. [7] Ultimately, the impact that mining has on the environment is determined by many factors such as the size of the operation and the type of mining. Environmental geology has reduced the negative environmental impacts of mining as it has been used in litigation toward mining. In some countries like Brazil [8] and Australia [9] for example, it is decreed by law that sites must undergo rehabilitation after a mining operation has ceased. Prior to any mining, an assessment is also necessary to analyze the potential environmental impacts. Another measure taken is that an environmental management program must be produced to show how the mine will operate. Land planning is an important aspect in deciding whether a site is suitable for mining but some environmental degradation is inevitable. Environmental Geology continues to lower the amount of negative effects that mining has on the natural environment. [1] Recycling[ edit ] Nonrenewable resources are only one type of resource with the other two being potentially renewable and perpetual . Nonrenewable resources, such as fossil fuels and metals , are finite, and therefore cannot be replenished during human lifetime, but are being depleted at a high rate. Due to their importance in many economies, this creates an issue as the world keeps developing the technologies used to exploit these resources. Some important roles of these nonrenewable resources are to heat homes, fuel cars, and build infrastructure. Environmental geology has been used to approach this issue with the sustainable development of recycling and reusing . [10] Recycling is the process of collecting recyclable consumer and industrial materials and products and then sorting them so they can be processed into raw materials with the intention of then using the raw materials to create new products. [11] Recycling and reusing can be done on an individual scale as well as an industrial scale. These practices maximize the usage of resources as much as possible all while minimizing waste. They also manage the industrial and domestic waste disposal as they reduce the amount of waste discharged into the global environment. [12] Reusing and recycling include: composting : the biological decomposition of organic garden and food waste in order to use it as soil conditioner, upcycling : increasing the value and quality in materials and products through the recycling process, freecycling: giving or getting free items from others before buying new ones industrial ecology : dismantling of massive artifacts to become input for new processes Environmental geology's approach to the decline of nonrenewable resources along with high amounts of waste polluting the Earth has been to reduce wasteful usage and recycle when possible. Land use[ edit ] This is an example of a simple land use map. This shows the land usage of West Newton in southeast Minnesota along the Mississippi River as of April 2021. Planning out the usage of land is important to reduce the risk of natural hazards on humans and their infrastructure, but mostly to reduce negative human impact on the natural environment. The land, water, air, materials, and energy use are all critically impacted by human settlement and resource production. [13] New sites must be found for mining, waste disposal, and industrial sites as these are all parts of an industrial society. Suitable sites are often difficult to find and get approval for as they must be shown to have barriers so contaminants are prevented from entering the environment. Site investigation in land use planning often includes at least two phases, an orientating investigation and a detailed investigation. The information in an orientating investigation is obtained through maps and other archived data. The information in a detailed investigation is obtained through a reconnaissance survey in the field and by reviewing the historic land use. The orientating investigation includes: topography , land use and vegetation, settlements, roads and railways, climate : precipitation, temperature, evapotranspiration, direction and the velocity of the wind, as well as the frequency of strong winds, hydrological and hydrogeological conditions: streams, lakes and ponds, springs, wells, use and quality of surface and groundwater, runoff, water balance, aquifer/aquiclude properties and stratigraphy, groundwater table, groundwater recharge and discharge, geology: soil, geological structures, stratigraphy and lithology, ecological aspects: e.g., nature reserves, protected geotopes, water protection areas. The detailed investigation includes: [2] geology: thickness and lateral extent of strata and geological units, lithology, homogeneity and heterogeneity, bedding conditions and tectonic structures, fractures, impact of weathering, groundwater : water table, water content, direction and rate of groundwater flow, hydraulic conductivity, value of aquifer, geochemical site characterization: chemical composition of soil, rocks and groundwater, estimation of contamination retention, geotechnical stability: The geological barrier must be capable of adsorbing strain from the weight of a landfill, slag heap, or industrial building. geogenic events: active faults, karst, earthquakes, subsidence, landslides, anthropogenic activities: mining damage, buildings, quarries, gravel pits, etc., and changes in soil and groundwater quality Environmental geology includes both the monitorization and planning of land use. Land use maps are made to represent current land use along with possible future uses. Land maps like the one shown can be used to reduce human settlement in areas with potential natural hazards such as floods, geological instability, wildfires, etc. [13] In the land map shown it can be seen that there is a margin of trees and vegetation between the settlements and Mississippi River to reduce the risk of flood damage as the Mississippi Rivers water levels change.
Toggle the table of contents Paleoclimatology From Wikipedia, the free encyclopedia Part of a series on e Study of changes in ancient climate Paleoclimatology ( British spelling , palaeoclimatology) is the scientific study of climates predating the invention of meteorological instruments , when no direct measurement data were available. [1] As instrumental records only span a tiny part of Earth's history , the reconstruction of ancient climate is important to understand natural variation and the evolution of the current climate. Paleoclimatology uses a variety of proxy methods from Earth and life sciences to obtain data previously preserved within rocks , sediments , boreholes , ice sheets , tree rings , corals , shells , and microfossils . Combined with techniques to date the proxies, the paleoclimate records are used to determine the past states of Earth's atmosphere . The scientific field of paleoclimatology came to maturity in the 20th century. Notable periods studied by paleoclimatologists include the frequent glaciations that Earth has undergone, rapid cooling events like the Younger Dryas , and the rapid warming during the Paleocene–Eocene Thermal Maximum . Studies of past changes in the environment and biodiversity often reflect on the current situation, specifically the impact of climate on mass extinctions and biotic recovery and current global warming . [2] [3] Main articles: History of climate change science and Historical climatology Notions of a changing climate most likely evolved in ancient Egypt , Mesopotamia , the Indus Valley and China , where prolonged periods of droughts and floods were experienced. [4] In the seventeenth century, Robert Hooke postulated that fossils of giant turtles found in Dorset could only be explained by a once warmer climate, which he thought could be explained by a shift in Earth's axis. [4] Fossils were, at that time, often explained as a consequence of a biblical flood. [5] Systematic observations of sunspots started by amateur astronomer Heinrich Schwabe in the early 19th century, starting a discussion of the Sun's influence on Earth's climate. [4] The scientific study of paleoclimatology began to take shape in the early 19th century, when discoveries about glaciations and natural changes in Earth's past climate helped to understand the greenhouse effect . It was only in the 20th century that paleoclimatology became a unified scientific field. Before, different aspects of Earth's climate history were studied by a variety of disciplines. [5] At the end of the 20th century, the empirical research into Earth's ancient climates started to be combined with computer models of increasing complexity. A new objective also developed in this period: finding ancient analog climates that could provide information about current climate change . [5] Reconstructing ancient climates[ edit ] Main article: Proxy (climate) Paleoclimatologists employ a wide variety of techniques to deduce ancient climates. The techniques used depend on which variable has to be reconstructed (this could be temperature , precipitation , or something else) and how long ago the climate of interest occurred. For instance, the deep marine record, the source of most isotopic data, exists only on oceanic plates, which are eventually subducted ; the oldest remaining material is 200 million years old. Older sediments are also more prone to corruption by diagenesis . Resolution and confidence in the data decrease over time. Proxies for climate[ edit ] Ice[ edit ] Mountain glaciers and the polar ice caps / ice sheets provide much data in paleoclimatology. Ice-coring projects in the ice caps of Greenland and Antarctica have yielded data going back several hundred thousand years, over 800,000 years in the case of the EPICA project. Air trapped within fallen snow becomes encased in tiny bubbles as the snow is compressed into ice in the glacier under the weight of later years' snow. The trapped air has proven a tremendously valuable source for direct measurement of the composition of air from the time the ice was formed. Layering can be observed because of seasonal pauses in ice accumulation and can be used to establish chronology, associating specific depths of the core with ranges of time. Changes in the layering thickness can be used to determine changes in precipitation or temperature. Oxygen-18 quantity changes ( δ18O ) in ice layers represent changes in average ocean surface temperature. Water molecules containing the heavier O-18 evaporate at a higher temperature than water molecules containing the normal Oxygen-16 isotope. The ratio of O-18 to O-16 will be higher as temperature increases but it also depends on factors such as water salinity and the volume of water locked up in ice sheets. Various cycles in isotope ratios have been detected. Pollen has been observed in the ice cores and can be used to understand which plants were present as the layer formed. Pollen is produced in abundance and its distribution is typically well understood. A pollen count for a specific layer can be produced by observing the total amount of pollen categorized by type (shape) in a controlled sample of that layer. Changes in plant frequency over time can be plotted through statistical analysis of pollen counts in the core. Knowing which plants were present leads to an understanding of precipitation and temperature, and types of fauna present. Palynology includes the study of pollen for these purposes. Volcanic ash is contained in some layers, and can be used to establish the time of the layer's formation. Volcanic events distribute ash with a unique set of properties (shape and color of particles, chemical signature). Establishing the ash's source will give a time period to associate with the layer of ice. A multinational consortium, the European Project for Ice Coring in Antarctica (EPICA), has drilled an ice core in Dome C on the East Antarctic ice sheet and retrieved ice from roughly 800,000 years ago. [6] The international ice core community has, under the auspices of International Partnerships in Ice Core Sciences (IPICS), defined a priority project to obtain the oldest possible ice core record from Antarctica, an ice core record reaching back to or towards 1.5 million years ago. [7] Main article: Dendroclimatology Climatic information can be obtained through an understanding of changes in tree growth. Generally, trees respond to changes in climatic variables by speeding up or slowing down growth, which in turn is generally reflected by a greater or lesser thickness in growth rings. Different species however, respond to changes in climatic variables in different ways. A tree-ring record is established by compiling information from many living trees in a specific area. Older intact wood that has escaped decay can extend the time covered by the record by matching the ring depth changes to contemporary specimens. By using that method, some areas have tree-ring records dating back a few thousand years. Older wood not connected to a contemporary record can be dated generally with radiocarbon techniques. A tree-ring record can be used to produce information regarding precipitation, temperature, hydrology, and fire corresponding to a particular area. Sedimentary content[ edit ] On a longer time scale, geologists must refer to the sedimentary record for data. Sediments, sometimes lithified to form rock, may contain remnants of preserved vegetation, animals, plankton, or pollen , which may be characteristic of certain climatic zones. Biomarker molecules such as the alkenones may yield information about their temperature of formation. Chemical signatures, particularly Mg/Ca ratio of calcite in Foraminifera tests, can be used to reconstruct past temperature. Isotopic ratios can provide further information. Specifically, the δ18O record responds to changes in temperature and ice volume, and the δ13C record reflects a range of factors, which are often difficult to disentangle. Sea floor core sample labelled to identify the exact spot on the sea floor where the sample was taken. Sediments from nearby locations can show significant differences in chemical and biological composition. Sedimentary facies On a longer time scale, the rock record may show signs of sea level rise and fall, and features such as "fossilised" sand dunes can be identified. Scientists can get a grasp of long-term climate by studying sedimentary rock going back billions of years. The division of Earth history into separate periods is largely based on visible changes in sedimentary rock layers that demarcate major changes in conditions. Often, they include major shifts in climate. Sclerochronology[ edit ] Corals (see also sclerochronology ) Coral "rings" are similar to tree rings except that they respond to different things, such as the water temperature, freshwater influx, pH changes, and wave action. From there, certain equipment can be used to derive the sea surface temperature and water salinity from the past few centuries. The δ18O of coralline red algae provides a useful proxy of the combined sea surface temperature and sea surface salinity at high latitudes and the tropics, where many traditional techniques are limited. [8] [9] Landscapes and landforms[ edit ] Within climatic geomorphology , one approach is to study relict landforms to infer ancient climates. [10] Being often concerned about past climates climatic geomorphology is considered sometimes to be a theme of historical geology . [11] Climatic geomorphology is of limited use to study recent ( Quaternary , Holocene ) large climate changes since there are seldom discernible in the geomorphological record. [12] Timing of proxies[ edit ] The field of geochronology has scientists working on determining how old certain proxies are. For recent proxy archives of tree rings and corals the individual year rings can be counted, and an exact year can be determined. Radiometric dating uses the properties of radioactive elements in proxies. In older material, more of the radioactive material will have decayed and the proportion of different elements will be different from newer proxies. One example of radiometric dating is radiocarbon dating . In the air, cosmic rays constantly convert nitrogen into a specific radioactive carbon isotope, 14C . When plants then use this carbon to grow, this isotope is not replenished anymore and starts decaying. The proportion of 'normal' carbon and Carbon-14 gives information of how long the plant material has not been in contact with the atmosphere. [13] Notable climate events in Earth history[ edit ]
Toggle the table of contents Physical oceanography From Wikipedia, the free encyclopedia Study of physical conditions and processes within the ocean World ocean bathymetry . Physical oceanography is the study of physical conditions and physical processes within the ocean , especially the motions and physical properties of ocean waters. Physical oceanography is one of several sub-domains into which oceanography is divided. Others include biological , chemical and geological oceanography. Physical oceanography may be subdivided into descriptive and dynamical physical oceanography. [1] Descriptive physical oceanography seeks to research the ocean through observations and complex numerical models, which describe the fluid motions as precisely as possible. Dynamical physical oceanography  focuses primarily upon the processes that govern the motion of fluids with emphasis upon theoretical research and numerical models. These are part of the large field of Geophysical Fluid Dynamics (GFD) that is shared together with meteorology . GFD is a sub field of Fluid dynamics describing flows occurring on spatial and temporal scales that are greatly influenced by the Coriolis force . Space and time scales of physical oceanographic processes. [2] Perspective view of the sea floor of the Atlantic Ocean and the Caribbean Sea. The purple sea floor at the center of the view is the Puerto Rico Trench . Roughly 97% of the planet's water is in its oceans, and the oceans are the source of the vast majority of water vapor that condenses in the atmosphere and falls as rain or snow on the continents. [3] [4] The tremendous heat capacity of the oceans moderates the planet's climate , and its absorption of various gases affects the composition of the atmosphere . [4] The ocean's influence extends even to the composition of volcanic rocks through seafloor metamorphism , as well as to that of volcanic gases and magmas created at subduction zones . [4] From sea level, the oceans are far deeper than the continents are tall; examination of the Earth's hypsographic curve shows that the average elevation of Earth's landmasses is only 840 metres (2,760 ft), while the ocean's average depth is 3,800 metres (12,500 ft). Though this apparent discrepancy is great, for both land and sea, the respective extremes such as mountains and trenches are rare. [3] Area, volume plus mean and maximum depths of oceans (excluding adjacent seas) Body Temperature, salinity and density[ edit ] This section needs expansion. You can help by adding to it . (June 2008) WOA surface density. Because the vast majority of the world ocean's volume is deep water, the mean temperature of seawater is low; roughly 75% of the ocean's volume has a temperature from 0° – 5 °C (Pinet 1996). The same percentage falls in a salinity range between 34 and 35 ppt (3.4–3.5%) (Pinet 1996). There is still quite a bit of variation, however. Surface temperatures can range from below freezing near the poles to 35 °C in restricted tropical seas, while salinity can vary from 10 to 41 ppt (1.0–4.1%). [5] The vertical structure of the temperature can be divided into three basic layers, a surface mixed layer , where gradients are low, a thermocline where gradients are high, and a poorly stratified abyss. In terms of temperature, the ocean's layers are highly latitude -dependent; the thermocline is pronounced in the tropics, but nonexistent in polar waters (Marshak 2001). The halocline usually lies near the surface, where evaporation raises salinity in the tropics, or meltwater dilutes it in polar regions. [5] These variations of salinity and temperature with depth change the density of the seawater, creating the pycnocline . [3] Main article: Ocean current Density-driven thermohaline circulation Energy for the ocean circulation (and for the atmospheric circulation) comes from solar radiation and gravitational energy from the sun and moon. [6] The amount of sunlight absorbed at the surface varies strongly with latitude, being greater at the equator than at the poles, and this engenders fluid motion in both the atmosphere and ocean that acts to redistribute heat from the equator towards the poles, thereby reducing the temperature gradients that would exist in the absence of fluid motion. Perhaps three quarters of this heat is carried in the atmosphere; the rest is carried in the ocean. The atmosphere is heated from below, which leads to convection, the largest expression of which is the Hadley circulation . By contrast the ocean is heated from above, which tends to suppress convection. Instead ocean deep water is formed in polar regions where cold salty waters sink in fairly restricted areas. This is the beginning of the thermohaline circulation . Oceanic currents are largely driven by the surface wind stress; hence the large-scale atmospheric circulation is important to understanding the ocean circulation. The Hadley circulation leads to Easterly winds in the tropics and Westerlies in mid-latitudes. This leads to slow equatorward flow throughout most of a subtropical ocean basin (the Sverdrup balance ). The return flow occurs in an intense, narrow, poleward western boundary current . Like the atmosphere, the ocean is far wider than it is deep, and hence horizontal motion is in general much faster than vertical motion. In the southern hemisphere there is a continuous belt of ocean, and hence the mid-latitude westerlies force the strong Antarctic Circumpolar Current . In the northern hemisphere the land masses prevent this and the ocean circulation is broken into smaller gyres in the Atlantic and Pacific basins. Coriolis effect[ edit ] The Coriolis effect results in a deflection of fluid flows (to the right in the Northern Hemisphere and left in the Southern Hemisphere). This has profound effects on the flow of the oceans. In particular it means the flow goes around high and low pressure systems, permitting them to persist for long periods of time. As a result, tiny variations in pressure can produce measurable currents. A slope of one part in one million in sea surface height, for example, will result in a current of 10 cm/s at mid-latitudes. The fact that the Coriolis effect is largest at the poles and weak at the equator results in sharp, relatively steady western boundary currents which are absent on eastern boundaries. Also see secondary circulation effects. Ekman transport[ edit ] Ekman transport results in the net transport of surface water 90 degrees to the right of the wind in the Northern Hemisphere, and 90 degrees to the left of the wind in the Southern Hemisphere. As the wind blows across the surface of the ocean, it "grabs" onto a thin layer of the surface water. In turn, that thin sheet of water transfers motion energy to the thin layer of water under it, and so on. However, because of the Coriolis Effect, the direction of travel of the layers of water slowly move farther and farther to the right as they get deeper in the Northern Hemisphere, and to the left in the Southern Hemisphere. In most cases, the very bottom layer of water affected by the wind is at a depth of 100 m – 150 m and is traveling about 180 degrees, completely opposite of the direction that the wind is blowing. Overall, the net transport of water would be 90 degrees from the original direction of the wind. Langmuir circulation[ edit ] Langmuir circulation results in the occurrence of thin, visible stripes, called windrows on the surface of the ocean parallel to the direction that the wind is blowing. If the wind is blowing with more than 3 m s−1, it can create parallel windrows alternating upwelling and downwelling about 5–300 m apart. These windrows are created by adjacent ovular water cells (extending to about 6 m (20 ft) deep) alternating rotating clockwise and counterclockwise. In the convergence zones debris, foam and seaweed accumulates, while at the divergence zones plankton are caught and carried to the surface. If there are many plankton in the divergence zone fish are often attracted to feed on them. Ocean–atmosphere interface[ edit ] Hurricane Isabel east of the Bahamas on 15 September 2003 At the ocean-atmosphere interface, the ocean and atmosphere exchange fluxes of heat, moisture and momentum. Heat The important heat terms at the surface are the sensible heat flux , the latent heat flux, the incoming solar radiation and the balance of long-wave ( infrared ) radiation . In general, the tropical oceans will tend to show a net gain of heat, and the polar oceans a net loss, the result of a net transfer of energy polewards in the oceans. The oceans' large heat capacity moderates the climate of areas adjacent to the oceans, leading to a maritime climate at such locations. This can be a result of heat storage in summer and release in winter; or of transport of heat from warmer locations: a particularly notable example of this is Western Europe , which is heated at least in part by the north atlantic drift . Momentum Surface winds tend to be of order meters per second; ocean currents of order centimeters per second. Hence from the point of view of the atmosphere, the ocean can be considered effectively stationary; from the point of view of the ocean, the atmosphere imposes a significant wind stress on its surface, and this forces large-scale currents in the ocean. Through the wind stress, the wind generates ocean surface waves ; the longer waves have a phase velocity tending towards the wind speed . Momentum of the surface winds is transferred into the energy flux by the ocean surface waves. The increased roughness of the ocean surface, by the presence of the waves, changes the wind near the surface. Moisture The ocean can gain moisture from rainfall , or lose it through evaporation . Evaporative loss leaves the ocean saltier; the Mediterranean and Persian Gulf for example have strong evaporative loss; the resulting plume of dense salty water may be traced through the Straits of Gibraltar into the Atlantic Ocean . At one time, it was believed that evaporation / precipitation was a major driver of ocean currents; it is now known to be only a very minor factor.
Toggle the table of contents Climate change (Redirected from Global warming ) Current rise in Earth's average temperature and its effects "Global warming" redirects here. For other uses, see Climate change (disambiguation) and Global warming (disambiguation) . This article is about contemporary climate change. For historical climate trends, see Climate variability and change . Changes in surface air temperature over the past 50 years. [1] The Arctic has warmed the most, and temperatures on land have generally increased more than sea surface temperatures . Earth's average surface air temperature has increased almost 1.5 °C (about 2.5 °F) since the Industrial Revolution . Natural forces cause some variability, but the 20-year average shows the progressive influence of human activity. [2] Examples of some effects of climate change : Wildfire intensified by heat and drought, bleaching of corals occurring more often due to marine heatwaves , and worsening droughts compromising water supplies. Many climate change impacts have been felt in recent years, with 2023 the warmest on record at +1.48 °C (2.66 °F). [18] Additional warming will increase these impacts and can trigger tipping points , such as melting all of the Greenland ice sheet . [19] Under the 2015 Paris Agreement , nations collectively agreed to keep warming "well under 2 °C". However, with pledges made under the Agreement, global warming would still reach about 2.7 °C (4.9 °F) by the end of the century. [20] Limiting warming to 1.5 °C will require halving emissions by 2030 and achieving net-zero emissions by 2050. [21] Fossil fuel use can be phased out by conserving energy and switching to energy sources that do not produce significant carbon pollution. These energy sources include wind , solar , hydro , and nuclear power . [22] [23] Cleanly generated electricity can replace fossil fuels for powering transportation , heating buildings , and running industrial processes. [24] Carbon can also be removed from the atmosphere , for instance by increasing forest cover and farming with methods that capture carbon in soil . [25] [26] Terminology Before the 1980s it was unclear whether the warming effect of increased greenhouse gases was stronger than the cooling effect of airborne particulates in air pollution . Scientists used the term inadvertent climate modification to refer to human impacts on the climate at this time. [27] In the 1980s, the terms global warming and climate change became more common, often being used interchangeably. [28] Scientifically, global warming refers only to increased surface warming, while climate change describes both global warming and its effects on Earth's climate system , such as precipitation changes. [27] Climate change can also be used more broadly to include changes to the climate that have happened throughout Earth's history. [29] Global warming—used as early as 1975 [30] —became the more popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate . [31] Since the 2000s, climate change has increased usage. [32] Various scientists, politicians and media now use the terms climate crisis or climate emergency to talk about climate change, and global heating instead of global warming. [33] Global temperature rise Global surface temperature reconstruction over the last 2000 years using proxy data from tree rings, corals, and ice cores in blue. [34] Directly observed data is in red. [35] Temperature records prior to global warming Main articles: Climate variability and change ; Temperature record of the last 2,000 years ; and Paleoclimatology Prior to human evolution the record includes hotter temperatures and occasional abrupt changes , such as the Paleocene–Eocene Thermal Maximum 55.5 million years ago. [36] Over the last few million years Human beings evolved in a climate that cycled through ice ages , with global average temperature ranging between current levels and 5–6 °C colder than today. [37] [38] Historical patterns of warming and cooling, like the Medieval Warm Period and the Little Ice Age , did not occur at the same time across different regions. Temperatures may have reached as high as those of the late 20th century in a limited set of regions. [39] Climate information for that period comes from climate proxies , such as trees and ice cores . [40] Warming since the Industrial Revolution Main article: Instrumental temperature record In recent decades, new high temperature records have substantially outpaced new low temperature records on a growing portion of Earth's surface. [41] There has been an increase in ocean heat content during recent decades as the oceans absorb over 90% of the heat from global warming . [42] Around 1850 thermometer records began to provide global coverage. [43] Between the 18th century and 1970 there was little net warming, as the warming impact of greenhouse gas emissions was offset by cooling from sulfur dioxide emissions. Sulfur dioxide causes acid rain , but it also produces sulfate aerosols in the atmosphere, which reflect sunlight and cause so-called global dimming . After 1970, the increasing accumulation of greenhouse gases and controls on sulfur pollution led to a marked increase in temperature. [44] [45] [46] Multiple independent datasets all show worldwide increases in surface temperature, [47] at a rate of around 0.2 °C per decade. [48] The 2013-2022 decade warmed to an average 1.15 °C [1.00–1.25 °C] compared to the pre-industrial baseline (1850–1900). [49] Not every single year was warmer than the last: internal climate variability processes can make any year 0.2 °C warmer or colder than the average. [50] From 1998 to 2013, negative phases of two such processes, Pacific Decadal Oscillation (PDO) [51] and Atlantic Multidecadal Oscillation (AMO) [52] caused a so-called " global warming hiatus ". [53] After the hiatus, the opposite occurred, with years like 2023 exhibiting temperatures well above even the recent average. [54] This is why the temperature change is defined in terms of a 20-year average, which minimises the noise of hot and cold years and decadal climate patterns, and detects the long-term signal. [55] : 5 [56] A wide range of other observations reinforce the evidence of warming. [57] [58] The upper atmosphere is cooling, because greenhouse gases are trapping heat near the Earth's surface, and so less heat is radiating into space. [59] Warming reduces average snow cover and forces the retreat of glaciers . At the same time, warming also causes greater evaporation from the oceans , leading to more atmospheric humidity , more and heavier precipitation . [60] Plants are flowering earlier in spring, and thousands of animal species have been permanently moving to cooler areas. [61] Differences by region Different regions of the world warm at different rates . The pattern is independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, the average surface temperature over land regions has increased almost twice as fast as the global average surface temperature. [62] This is because oceans lose more heat by evaporation and oceans can store a lot of heat . [63] The thermal energy in the global climate system has grown with only brief pauses since at least 1970, and over 90% of this extra energy has been stored in the ocean . [64] [65] The rest has heated the atmosphere , melted ice, and warmed the continents. [66] The Northern Hemisphere and the North Pole have warmed much faster than the South Pole and Southern Hemisphere . The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice . As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they start absorbing more heat . [67] Local black carbon deposits on snow and ice also contribute to Arctic warming. [68] Arctic surface temperatures are increasing between three and four times faster than in the rest of the world. [69] [70] [71] Melting of ice sheets near the poles weakens both the Atlantic and the Antarctic limb of thermohaline circulation , which further changes the distribution of heat and precipitation around the globe. [72] [73] [74] [75] Future global temperatures Further information: Carbon budget and Earth's energy budget CMIP6 multi-model projections of global surface temperature changes for the year 2090 relative to the 1850–1900 average. The current trajectory for warming by the end of the century is roughly halfway between these two extremes. [20] [76] [77] The World Meteorological Organization estimates a 66% chance of global temperatures exceeding 1.5 °C warming from the preindustrial baseline for at least one year between 2023 and 2027. [78] [79] Because the IPCC uses a 20-year average to define global temperature changes, a single year exceeding 1.5 °C does not break the limit. The IPCC expects the 20-year average global temperature to exceed +1.5 °C in the early 2030s. [80] The IPCC Sixth Assessment Report (2023) included projections that by 2100 global warming is very likely to reach 1.0-1.8 °C under a scenario with very low emissions of greenhouse gases , 2.1-3.5 °C under an intermediate emissions scenario ,  or 3.3-5.7 °C under a very high emissions scenario . [81] In the intermediate and high emission scenarios, the warming will continue past 2100. [82] [83] The remaining carbon budget for staying beneath certain temperature increases is determined by modelling the carbon cycle and climate sensitivity to greenhouse gases. [84] According to the IPCC, global warming can be kept below 1.5 °C with a two-thirds chance if emissions after 2018 do not exceed 420 or 570 gigatonnes of CO2. This corresponds to 10 to 13 years of current emissions.  There are high uncertainties about the budget. For instance, it may be 100 gigatonnes of CO2 equivalent smaller due to CO2 and methane release from permafrost and wetlands . [85] However, it is clear that fossil fuel resources need to be proactively kept in the ground to prevent substantial warming. Otherwise, their shortages would not occur until the emissions have already locked in significant long-term impacts. [86] Causes of recent global temperature rise Main article: Causes of climate change Drivers of climate change from 1850–1900 to 2010–2019. There was no significant contribution from internal variability or solar and volcanic drivers. The climate system experiences various cycles on its own which can last for years, decades or even centuries. For example, El Niño events cause short-term spikes in surface temperature while La Niña events cause short term cooling. [87] Their relative frequency can affect global temperature trends on a decadal timescale. [88] Other changes are caused by an imbalance of energy from external forcings . [89] Examples of these include changes in the concentrations of greenhouse gases , solar luminosity , volcanic eruptions, and variations in the Earth's orbit around the Sun. [90] To determine the human contribution to climate change, unique "fingerprints" for all potential causes are developed and compared with both observed patterns and known internal climate variability . [91] For example, solar forcing—whose fingerprint involves warming the entire atmosphere—is ruled out because only the lower atmosphere has warmed. [92] Atmospheric aerosols produce a smaller, cooling effect. Other drivers, such as changes in albedo , are less impactful. [93] Greenhouse gases Main articles: Greenhouse gas , Greenhouse gas emissions , Greenhouse effect , and Carbon dioxide in Earth's atmosphere CO2 concentrations over the last 800,000 years as measured from ice cores [94] [95] [96] [97] (blue/green) and directly [98] (black) Greenhouse gases are transparent to sunlight , and thus allow it to pass through the atmosphere to heat the Earth's surface. The Earth radiates it as heat , and greenhouse gases absorb a portion of it. This absorption slows the rate at which heat escapes into space, trapping heat near the Earth's surface and warming it over time. [99] While water vapour (≈50%) and clouds (≈25%) are the biggest contributors to the greenhouse effect, they primarily change as a function of temperature and are therefore mostly considered to be feedbacks that change climate sensitivity . On the other hand, concentrations of gases such as CO2 (≈20%), tropospheric ozone , [100] CFCs and nitrous oxide are added or removed independently from temperature, and are therefore considered to be external forcings that change global temperatures. [101] Before the Industrial Revolution , naturally-occurring amounts of greenhouse gases caused the air near the surface to be about 33 °C warmer than it would have been in their absence. [102] [103] Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels ( coal , oil , and natural gas ), [104] has increased the amount of greenhouse gases in the atmosphere, resulting in a radiative imbalance . In 2019, the concentrations of CO2 and methane had increased by about 48% and 160%, respectively, since 1750. [105] These CO2 levels are higher than they have been at any time during the last 2 million years. Concentrations of methane are far higher than they were over the last 800,000 years. [106] The Global Carbon Project shows how additions to CO2 since 1880 have been caused by different sources ramping up one after another. Global anthropogenic greenhouse gas emissions in 2019 were equivalent to 59 billion tonnes of CO2. Of these emissions, 75% was CO2, 18% was methane , 4% was nitrous oxide, and 2% was fluorinated gases . [107] CO2 emissions primarily come from burning fossil fuels to provide energy for transport , manufacturing, heating , and electricity. [5] Additional CO2 emissions come from deforestation and industrial processes , which include the CO2 released by the chemical reactions for making cement , steel , aluminum , and fertiliser . [108] Methane emissions come from livestock , manure, rice cultivation , landfills, wastewater, and coal mining , as well as oil and gas extraction . [109] Nitrous oxide emissions largely come from the microbial decomposition of fertiliser . [110] While methane only lasts in the atmosphere for an average of 12 years, [111] CO2 lasts much longer. The Earth's surface absorbs CO2 as part of the carbon cycle . While plants on land and in the ocean absorb most excess emissions of CO2 every year, that CO2 is returned to the atmosphere when biological matter is digested, burns, or decays. [112] Land-surface carbon sink processes, such as carbon fixation in the soil and photosynthesis, remove about 29% of annual global CO2 emissions. [113] The ocean has absorbed 20 to 30% of emitted CO2 over the last 2 decades. [114] CO2 is only removed from the atmosphere for the long term when it is stored in the Earth's crust, which is a process that can take millions of years to complete. [112] Land surface changes The rate of global tree cover loss has approximately doubled since 2001, to an annual loss approaching an area the size of Italy. [115] According to Food and Agriculture Organization , around 30% of Earth's land area is largely unusable for humans ( glaciers , deserts , etc.), 26% is forests , 10% is shrubland and 34% is agricultural land . [116] Deforestation is the main land use change contributor to global warming, [117] as the destroyed trees release CO2, and are not replaced by new trees, removing that carbon sink . [25] Between 2001 and 2018, 27% of deforestation was from permanent clearing to enable agricultural expansion for crops and livestock. Another 24% has been lost to temporary clearing under the shifting cultivation agricultural systems. 26% was due to logging for wood and derived products, and wildfires have accounted for the remaining 23%. [118] Some forests have not been fully cleared, but were already degraded by these impacts. Restoring these forests also recovers their potential as a carbon sink. [119] Local vegetation cover impacts how much of the sunlight gets reflected back into space ( albedo ), and how much heat is lost by evaporation . For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also modify the release of chemical compounds that influence clouds, and by changing wind patterns. [120] In tropic and temperate areas the net effect is to produce significant warming, and forest restoration can make local temperatures cooler. [119] At latitudes closer to the poles, there is a cooling effect as forest is replaced by snow-covered (and more reflective) plains. [120] Globally, these increases in surface albedo have been the dominant direct influence on temperature from land use change. Thus, land use change to date is estimated to have a slight cooling effect. [121] Other factors Aerosols and clouds Air pollution, in the form of aerosols, affects the climate on a large scale. [122] Aerosols scatter and absorb solar radiation. From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed. This phenomenon is popularly known as global dimming , [123] and is primarily attributed to sulfate aerosols produced by the combustion of fossil fuels with heavy sulfur concentrations like coal and bunker fuel . [46] Smaller contributions come from black carbon , organic carbon from combustion of fossil fuels and biofuels, and from anthropogenic dust. [124] [45] [125] [126] [127] Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much. [128] [46] Aerosols also have indirect effects on the Earth's energy budget . Sulfate aerosols act as cloud condensation nuclei and lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets. [129] They also reduce the growth of raindrops , which makes clouds more reflective to incoming sunlight. [130] Indirect effects of aerosols are the largest uncertainty in radiative forcing . [131] While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise. [132] Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2 °C by 2050. [133] The effect of decreasing sulfur content of fuel oil for ships since 2020 [134] is estimated to cause an additional 0.05 °C increase in global mean temperature by 2050. [135] Solar and volcanic activity Further information: Solar activity and climate The Fourth National Climate Assessment ("NCA4", USGCRP, 2017) includes charts illustrating that neither solar nor volcanic activity can explain the observed warming. [136] [137] As the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect the climate system . [131] Solar irradiance has been measured directly by satellites , [138] and indirect measurements are available from the early 1600s onwards. [131] Since 1880, there has been no upward trend in the amount of the Sun's energy reaching the Earth, in contrast to the warming of the lower atmosphere (the troposphere ). [139] The upper atmosphere (the stratosphere ) would also be warming if the Sun was sending more energy to Earth, but instead, it has been cooling. [92] This is consistent with greenhouse gases preventing heat from leaving the Earth's atmosphere. [140] Explosive volcanic eruptions can release gases, dust and ash that partially block sunlight and reduce temperatures, or they can send water vapor into the atmosphere, which adds to greenhouse gases and increases temperatures. [141] These impacts on temperature only last for several years, because both water vapor and volcanic material have low persistence in the atmosphere. [142] volcanic CO2 emissions are more persistent, but they are equivalent to less than 1% of current human-caused CO2 emissions. [143] Volcanic activity still represents the single largest natural impact (forcing) on temperature in the industrial era. Yet, like the other natural forcings, it has had negligible impacts on global temperature trends since the Industrial Revolution. [142] Climate change feedbacks Main articles: Climate change feedbacks and Climate sensitivity Sea ice reflects 50% to 70% of incoming sunlight, while the ocean, being darker, reflects only 6%. As an area of sea ice melts and exposes more ocean, more heat is absorbed by the ocean, raising temperatures that melt still more ice. This is a positive feedback process . [144] The response of the climate system to an initial forcing is modified by feedbacks: increased by "self-reinforcing" or "positive" feedbacks and reduced by "balancing" or "negative" feedbacks . [145] The main reinforcing feedbacks are the water-vapour feedback , the ice–albedo feedback , and the net effect of clouds. [146] [147] The primary balancing mechanism is radiative cooling , as Earth's surface gives off more heat to space in response to rising temperature. [148] In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilising effect of CO2 on plant growth. [149] Uncertainty over feedbacks, particularly cloud cover, [150] is the major reason why different climate models project different magnitudes of warming for a given amount of emissions. [151] As air warms, it can hold more moisture . Water vapour, as a potent greenhouse gas, holds heat in the atmosphere. [146] If cloud cover increases, more sunlight will be reflected back into space, cooling the planet. If clouds become higher and thinner, they act as an insulator, reflecting heat from below back downwards and warming the planet. [152] Another major feedback is the reduction of snow cover and sea ice in the Arctic, which reduces the reflectivity of the Earth's surface. [153] More of the Sun's energy is now absorbed in these regions, contributing to amplification of Arctic temperature changes . [154] Arctic amplification is also thawing permafrost , which releases methane and CO2 into the atmosphere. [155] Climate change can also cause methane releases from wetlands , marine systems, and freshwater systems. [156] Overall, climate feedbacks are expected to become increasingly positive. [157] Around half of human-caused CO2 emissions have been absorbed by land plants and by the oceans. [158] This fraction is not static and if future CO2 emissions decrease, the Earth will be able to absorb up to around 70%. If they increase substantially, it'll still absorb more carbon than now, but the overall fraction will decrease to below 40%. [159] This is because climate change increases droughts and heat waves that eventually inhibit plant growth on land, and soils will release more carbon from dead plants when they are warmer . [160] [161] The rate at which oceans absorb atmospheric carbon will be lowered as they become more acidic and experience changes in thermohaline circulation and phytoplankton distribution. [162] [163] [73] Modelling Further information: Climate model and Climate change scenario Energy flows between space, the atmosphere, and Earth's surface. Most sunlight passes through the atmosphere to heat the Earth's surface, then greenhouse gases absorb most of the heat the Earth radiates in response. Adding to greenhouse gases increases this insulating effect, causing an energy imbalance that heats the planet up. A climate model is a representation of the physical, chemical and biological processes that affect the climate system. [164] Models include natural processes like changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing. [165] Models are used to estimate the degree of warming future emissions will cause when accounting for the strength of climate feedbacks . [166] [167] Models also predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere. [168] The physical realism of models is tested by examining their ability to simulate contemporary or past climates. [169] Past models have underestimated the rate of Arctic shrinkage [170] and underestimated the rate of precipitation increase. [171] Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations. [172] The 2017 United States-published National Climate Assessment notes that "climate models may still be underestimating or missing relevant feedback processes". [173] Additionally, climate models may be unable to adequately predict short-term regional climatic shifts. [174] A subset of climate models add societal factors to a physical climate model. These models simulate how population, economic growth , and energy use affect—and interact with—the physical climate. With this information, these models can produce scenarios of future greenhouse gas emissions. This is then used as input for physical climate models and carbon cycle models to predict how atmospheric concentrations of greenhouse gases might change. [175] [176] Depending on the socioeconomic scenario and the mitigation scenario, models produce atmospheric CO2 concentrations that range widely between 380 and 1400 ppm. [177] Impacts Main article: Effects of climate change The sixth IPCC Assessment Report projects changes in average soil moisture that can disrupt agriculture and ecosystems. A reduction in soil moisture by one standard deviation means that average soil moisture will approximately match the ninth driest year between 1850 and 1900 at that location. Environmental effects Further information: Effects of climate change on oceans and Effects of climate change on the water cycle The environmental effects of climate change are broad and far-reaching, affecting oceans , ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations. [178] Since the 1950s, droughts and heat waves have appeared simultaneously with increasing frequency. [179] Extremely wet or dry events within the monsoon period have increased in India and East Asia. [180] Monsoonal precipitation over the Northern Hemisphere has increased since 1980. [181] The rainfall rate and intensity of hurricanes and typhoons is likely increasing , [182] and the geographic range likely expanding poleward in response to climate warming. [183] Frequency of tropical cyclones has not increased as a result of climate change. [184] Historical sea level reconstruction and projections up to 2100 published in 2017 by the U.S. Global Change Research Program [185] Global sea level is rising as a consequence of thermal expansion and the melting of glaciers and ice sheets . Between 1993 and 2020, the rise increased over time, averaging 3.3 ± 0.3 mm per year. [186] Over the 21st century, the IPCC projects 32–62 cm of sea level rise under a low emission scenario, 44–76 cm under an intermediate one and 65–101 cm under a very high emission scenario. [187] Marine ice sheet instability processes in Antarctica may add substantially to these values, [188] including the possibility of a 2-meter sea level rise by 2100 under high emissions. [189] Climate change has led to decades of shrinking and thinning of the Arctic sea ice . [190] While ice-free summers are expected to be rare at 1.5 °C degrees of warming, they are set to occur once every three to ten years at a warming level of 2 °C. [191] Higher atmospheric CO2 concentrations cause more CO2 to dissolve in the oceans, which is making them more acidic . [192] Because oxygen is less soluble in warmer water, [193] its concentrations in the ocean are decreasing , and dead zones are expanding. [194] Tipping points and long-term impacts Different levels of global warming may cause different parts of Earth's climate system to reach tipping points that cause transitions to different states. [195] [196] Main article: Tipping points in the climate system Greater degrees of global warming increase the risk of passing through ' tipping points '—thresholds beyond which certain major impacts can no longer be avoided even if temperatures return to their previous state. [197] [198] For instance, the Greenland ice sheet is already melting, but if global warming reaches levels between 1.7 °C and 2.3 °C, its melting will continue until it fully disappears. If the warming is later reduced to 1.5 °C or less, it will still lose a lot more ice than if the warming was never allowed to reach the threshold in the first place. [199] While the ice sheets would melt over millennia, other tipping points would occur faster and give societies less time to respond. The collapse of major ocean currents like the Atlantic meridional overturning circulation (AMOC), and irreversible damage to key ecosystems like the Amazon rainforest and coral reefs can unfold in a matter of decades. [196] The long-term effects of climate change on oceans include further ice melt, ocean warming , sea level rise, ocean acidification and ocean deoxygenation. [200] The timescale of long-term impacts are centuries to millennia due to CO2's long atmospheric lifetime. [201] When net emissions stabilise surface air temperatures will also stabilise, but oceans and ice caps will continue to absorb excess heat from the atmosphere. The result is an estimated total sea level rise of 2.3 metres per degree Celsius (4.2 ft/°F) after 2000 years. [202] Oceanic CO2 uptake is slow enough that ocean acidification will also continue for hundreds to thousands of years. [203] Deep oceans (below 2,000 metres (6,600 ft)) are also already committed to losing over 10% of their dissolved oxygen by the warming which occurred to date. [204] Further, West Antarctic ice sheet appears committed to practically irreversible melting, which would increase the sea levels by at least 3.3 m (10 ft 10 in) over approximately 2000 years. [196] [205] [206] [207] [208] [209] [210] [211] Nature and wildlife Further information: Effects of climate change on oceans and Effects of climate change on biomes Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes . [212] Higher atmospheric CO2 levels and an extended growing season have resulted in global greening. However, heatwaves and drought have reduced ecosystem productivity in some regions. The future balance of these opposing effects is unclear. [213] Climate change has contributed to the expansion of drier climate zones, such as the expansion of deserts in the subtropics . [214] The size and speed of global warming is making abrupt changes in ecosystems more likely. [215] Overall, it is expected that climate change will result in the extinction of many species. [216] The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land. [217] Just as on land, heat waves in the ocean occur more frequently due to climate change, harming a wide range of organisms such as corals, kelp , and seabirds . [218] Ocean acidification makes it harder for marine calcifying organisms such as mussels , barnacles and corals to produce shells and skeletons ; and heatwaves have bleached coral reefs . [219] Harmful algal blooms enhanced by climate change and eutrophication lower oxygen levels, disrupt food webs and cause great loss of marine life. [220] Coastal ecosystems are under particular stress. Almost half of global wetlands have disappeared due to climate change and other human impacts. [221] Climate change impacts on the environment
Toggle the table of contents Climate change From Wikipedia, the free encyclopedia Current rise in Earth's average temperature and its effects "Global warming" redirects here. For other uses, see Climate change (disambiguation) and Global warming (disambiguation) . This article is about contemporary climate change. For historical climate trends, see Climate variability and change . Changes in surface air temperature over the past 50 years. [1] The Arctic has warmed the most, and temperatures on land have generally increased more than sea surface temperatures . Earth's average surface air temperature has increased almost 1.5 °C (about 2.5 °F) since the Industrial Revolution . Natural forces cause some variability, but the 20-year average shows the progressive influence of human activity. [2] Examples of some effects of climate change : Wildfire intensified by heat and drought, bleaching of corals occurring more often due to marine heatwaves , and worsening droughts compromising water supplies. Many climate change impacts have been felt in recent years, with 2023 the warmest on record at +1.48 °C (2.66 °F). [18] Additional warming will increase these impacts and can trigger tipping points , such as melting all of the Greenland ice sheet . [19] Under the 2015 Paris Agreement , nations collectively agreed to keep warming "well under 2 °C". However, with pledges made under the Agreement, global warming would still reach about 2.7 °C (4.9 °F) by the end of the century. [20] Limiting warming to 1.5 °C will require halving emissions by 2030 and achieving net-zero emissions by 2050. [21] Fossil fuel use can be phased out by conserving energy and switching to energy sources that do not produce significant carbon pollution. These energy sources include wind , solar , hydro , and nuclear power . [22] [23] Cleanly generated electricity can replace fossil fuels for powering transportation , heating buildings , and running industrial processes. [24] Carbon can also be removed from the atmosphere , for instance by increasing forest cover and farming with methods that capture carbon in soil . [25] [26] Terminology Before the 1980s it was unclear whether the warming effect of increased greenhouse gases was stronger than the cooling effect of airborne particulates in air pollution . Scientists used the term inadvertent climate modification to refer to human impacts on the climate at this time. [27] In the 1980s, the terms global warming and climate change became more common, often being used interchangeably. [28] Scientifically, global warming refers only to increased surface warming, while climate change describes both global warming and its effects on Earth's climate system , such as precipitation changes. [27] Climate change can also be used more broadly to include changes to the climate that have happened throughout Earth's history. [29] Global warming—used as early as 1975 [30] —became the more popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate . [31] Since the 2000s, climate change has increased usage. [32] Various scientists, politicians and media now use the terms climate crisis or climate emergency to talk about climate change, and global heating instead of global warming. [33] Global temperature rise Global surface temperature reconstruction over the last 2000 years using proxy data from tree rings, corals, and ice cores in blue. [34] Directly observed data is in red. [35] Temperature records prior to global warming Main articles: Climate variability and change ; Temperature record of the last 2,000 years ; and Paleoclimatology Prior to human evolution the record includes hotter temperatures and occasional abrupt changes , such as the Paleocene–Eocene Thermal Maximum 55.5 million years ago. [36] Over the last few million years Human beings evolved in a climate that cycled through ice ages , with global average temperature ranging between current levels and 5–6 °C colder than today. [37] [38] Historical patterns of warming and cooling, like the Medieval Warm Period and the Little Ice Age , did not occur at the same time across different regions. Temperatures may have reached as high as those of the late 20th century in a limited set of regions. [39] Climate information for that period comes from climate proxies , such as trees and ice cores . [40] Warming since the Industrial Revolution Main article: Instrumental temperature record In recent decades, new high temperature records have substantially outpaced new low temperature records on a growing portion of Earth's surface. [41] There has been an increase in ocean heat content during recent decades as the oceans absorb over 90% of the heat from global warming . [42] Around 1850 thermometer records began to provide global coverage. [43] Between the 18th century and 1970 there was little net warming, as the warming impact of greenhouse gas emissions was offset by cooling from sulfur dioxide emissions. Sulfur dioxide causes acid rain , but it also produces sulfate aerosols in the atmosphere, which reflect sunlight and cause so-called global dimming . After 1970, the increasing accumulation of greenhouse gases and controls on sulfur pollution led to a marked increase in temperature. [44] [45] [46] Multiple independent datasets all show worldwide increases in surface temperature, [47] at a rate of around 0.2 °C per decade. [48] The 2013-2022 decade warmed to an average 1.15 °C [1.00–1.25 °C] compared to the pre-industrial baseline (1850–1900). [49] Not every single year was warmer than the last: internal climate variability processes can make any year 0.2 °C warmer or colder than the average. [50] From 1998 to 2013, negative phases of two such processes, Pacific Decadal Oscillation (PDO) [51] and Atlantic Multidecadal Oscillation (AMO) [52] caused a so-called " global warming hiatus ". [53] After the hiatus, the opposite occurred, with years like 2023 exhibiting temperatures well above even the recent average. [54] This is why the temperature change is defined in terms of a 20-year average, which minimises the noise of hot and cold years and decadal climate patterns, and detects the long-term signal. [55] : 5 [56] A wide range of other observations reinforce the evidence of warming. [57] [58] The upper atmosphere is cooling, because greenhouse gases are trapping heat near the Earth's surface, and so less heat is radiating into space. [59] Warming reduces average snow cover and forces the retreat of glaciers . At the same time, warming also causes greater evaporation from the oceans , leading to more atmospheric humidity , more and heavier precipitation . [60] Plants are flowering earlier in spring, and thousands of animal species have been permanently moving to cooler areas. [61] Differences by region Different regions of the world warm at different rates . The pattern is independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, the average surface temperature over land regions has increased almost twice as fast as the global average surface temperature. [62] This is because oceans lose more heat by evaporation and oceans can store a lot of heat . [63] The thermal energy in the global climate system has grown with only brief pauses since at least 1970, and over 90% of this extra energy has been stored in the ocean . [64] [65] The rest has heated the atmosphere , melted ice, and warmed the continents. [66] The Northern Hemisphere and the North Pole have warmed much faster than the South Pole and Southern Hemisphere . The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice . As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they start absorbing more heat . [67] Local black carbon deposits on snow and ice also contribute to Arctic warming. [68] Arctic surface temperatures are increasing between three and four times faster than in the rest of the world. [69] [70] [71] Melting of ice sheets near the poles weakens both the Atlantic and the Antarctic limb of thermohaline circulation , which further changes the distribution of heat and precipitation around the globe. [72] [73] [74] [75] Future global temperatures Further information: Carbon budget and Earth's energy budget CMIP6 multi-model projections of global surface temperature changes for the year 2090 relative to the 1850–1900 average. The current trajectory for warming by the end of the century is roughly halfway between these two extremes. [20] [76] [77] The World Meteorological Organization estimates a 66% chance of global temperatures exceeding 1.5 °C warming from the preindustrial baseline for at least one year between 2023 and 2027. [78] [79] Because the IPCC uses a 20-year average to define global temperature changes, a single year exceeding 1.5 °C does not break the limit. The IPCC expects the 20-year average global temperature to exceed +1.5 °C in the early 2030s. [80] The IPCC Sixth Assessment Report (2023) included projections that by 2100 global warming is very likely to reach 1.0-1.8 °C under a scenario with very low emissions of greenhouse gases , 2.1-3.5 °C under an intermediate emissions scenario ,  or 3.3-5.7 °C under a very high emissions scenario . [81] In the intermediate and high emission scenarios, the warming will continue past 2100. [82] [83] The remaining carbon budget for staying beneath certain temperature increases is determined by modelling the carbon cycle and climate sensitivity to greenhouse gases. [84] According to the IPCC, global warming can be kept below 1.5 °C with a two-thirds chance if emissions after 2018 do not exceed 420 or 570 gigatonnes of CO2. This corresponds to 10 to 13 years of current emissions.  There are high uncertainties about the budget. For instance, it may be 100 gigatonnes of CO2 equivalent smaller due to CO2 and methane release from permafrost and wetlands . [85] However, it is clear that fossil fuel resources need to be proactively kept in the ground to prevent substantial warming. Otherwise, their shortages would not occur until the emissions have already locked in significant long-term impacts. [86] Causes of recent global temperature rise Main article: Causes of climate change Drivers of climate change from 1850–1900 to 2010–2019. There was no significant contribution from internal variability or solar and volcanic drivers. The climate system experiences various cycles on its own which can last for years, decades or even centuries. For example, El Niño events cause short-term spikes in surface temperature while La Niña events cause short term cooling. [87] Their relative frequency can affect global temperature trends on a decadal timescale. [88] Other changes are caused by an imbalance of energy from external forcings . [89] Examples of these include changes in the concentrations of greenhouse gases , solar luminosity , volcanic eruptions, and variations in the Earth's orbit around the Sun. [90] To determine the human contribution to climate change, unique "fingerprints" for all potential causes are developed and compared with both observed patterns and known internal climate variability . [91] For example, solar forcing—whose fingerprint involves warming the entire atmosphere—is ruled out because only the lower atmosphere has warmed. [92] Atmospheric aerosols produce a smaller, cooling effect. Other drivers, such as changes in albedo , are less impactful. [93] Greenhouse gases Main articles: Greenhouse gas , Greenhouse gas emissions , Greenhouse effect , and Carbon dioxide in Earth's atmosphere CO2 concentrations over the last 800,000 years as measured from ice cores [94] [95] [96] [97] (blue/green) and directly [98] (black) Greenhouse gases are transparent to sunlight , and thus allow it to pass through the atmosphere to heat the Earth's surface. The Earth radiates it as heat , and greenhouse gases absorb a portion of it. This absorption slows the rate at which heat escapes into space, trapping heat near the Earth's surface and warming it over time. [99] While water vapour (≈50%) and clouds (≈25%) are the biggest contributors to the greenhouse effect, they primarily change as a function of temperature and are therefore mostly considered to be feedbacks that change climate sensitivity . On the other hand, concentrations of gases such as CO2 (≈20%), tropospheric ozone , [100] CFCs and nitrous oxide are added or removed independently from temperature, and are therefore considered to be external forcings that change global temperatures. [101] Before the Industrial Revolution , naturally-occurring amounts of greenhouse gases caused the air near the surface to be about 33 °C warmer than it would have been in their absence. [102] [103] Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels ( coal , oil , and natural gas ), [104] has increased the amount of greenhouse gases in the atmosphere, resulting in a radiative imbalance . In 2019, the concentrations of CO2 and methane had increased by about 48% and 160%, respectively, since 1750. [105] These CO2 levels are higher than they have been at any time during the last 2 million years. Concentrations of methane are far higher than they were over the last 800,000 years. [106] The Global Carbon Project shows how additions to CO2 since 1880 have been caused by different sources ramping up one after another. Global anthropogenic greenhouse gas emissions in 2019 were equivalent to 59 billion tonnes of CO2. Of these emissions, 75% was CO2, 18% was methane , 4% was nitrous oxide, and 2% was fluorinated gases . [107] CO2 emissions primarily come from burning fossil fuels to provide energy for transport , manufacturing, heating , and electricity. [5] Additional CO2 emissions come from deforestation and industrial processes , which include the CO2 released by the chemical reactions for making cement , steel , aluminum , and fertiliser . [108] Methane emissions come from livestock , manure, rice cultivation , landfills, wastewater, and coal mining , as well as oil and gas extraction . [109] Nitrous oxide emissions largely come from the microbial decomposition of fertiliser . [110] While methane only lasts in the atmosphere for an average of 12 years, [111] CO2 lasts much longer. The Earth's surface absorbs CO2 as part of the carbon cycle . While plants on land and in the ocean absorb most excess emissions of CO2 every year, that CO2 is returned to the atmosphere when biological matter is digested, burns, or decays. [112] Land-surface carbon sink processes, such as carbon fixation in the soil and photosynthesis, remove about 29% of annual global CO2 emissions. [113] The ocean has absorbed 20 to 30% of emitted CO2 over the last 2 decades. [114] CO2 is only removed from the atmosphere for the long term when it is stored in the Earth's crust, which is a process that can take millions of years to complete. [112] Land surface changes The rate of global tree cover loss has approximately doubled since 2001, to an annual loss approaching an area the size of Italy. [115] According to Food and Agriculture Organization , around 30% of Earth's land area is largely unusable for humans ( glaciers , deserts , etc.), 26% is forests , 10% is shrubland and 34% is agricultural land . [116] Deforestation is the main land use change contributor to global warming, [117] as the destroyed trees release CO2, and are not replaced by new trees, removing that carbon sink . [25] Between 2001 and 2018, 27% of deforestation was from permanent clearing to enable agricultural expansion for crops and livestock. Another 24% has been lost to temporary clearing under the shifting cultivation agricultural systems. 26% was due to logging for wood and derived products, and wildfires have accounted for the remaining 23%. [118] Some forests have not been fully cleared, but were already degraded by these impacts. Restoring these forests also recovers their potential as a carbon sink. [119] Local vegetation cover impacts how much of the sunlight gets reflected back into space ( albedo ), and how much heat is lost by evaporation . For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also modify the release of chemical compounds that influence clouds, and by changing wind patterns. [120] In tropic and temperate areas the net effect is to produce significant warming, and forest restoration can make local temperatures cooler. [119] At latitudes closer to the poles, there is a cooling effect as forest is replaced by snow-covered (and more reflective) plains. [120] Globally, these increases in surface albedo have been the dominant direct influence on temperature from land use change. Thus, land use change to date is estimated to have a slight cooling effect. [121] Other factors Aerosols and clouds Air pollution, in the form of aerosols, affects the climate on a large scale. [122] Aerosols scatter and absorb solar radiation. From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed. This phenomenon is popularly known as global dimming , [123] and is primarily attributed to sulfate aerosols produced by the combustion of fossil fuels with heavy sulfur concentrations like coal and bunker fuel . [46] Smaller contributions come from black carbon , organic carbon from combustion of fossil fuels and biofuels, and from anthropogenic dust. [124] [45] [125] [126] [127] Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much. [128] [46] Aerosols also have indirect effects on the Earth's energy budget . Sulfate aerosols act as cloud condensation nuclei and lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets. [129] They also reduce the growth of raindrops , which makes clouds more reflective to incoming sunlight. [130] Indirect effects of aerosols are the largest uncertainty in radiative forcing . [131] While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise. [132] Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2 °C by 2050. [133] The effect of decreasing sulfur content of fuel oil for ships since 2020 [134] is estimated to cause an additional 0.05 °C increase in global mean temperature by 2050. [135] Solar and volcanic activity Further information: Solar activity and climate The Fourth National Climate Assessment ("NCA4", USGCRP, 2017) includes charts illustrating that neither solar nor volcanic activity can explain the observed warming. [136] [137] As the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect the climate system . [131] Solar irradiance has been measured directly by satellites , [138] and indirect measurements are available from the early 1600s onwards. [131] Since 1880, there has been no upward trend in the amount of the Sun's energy reaching the Earth, in contrast to the warming of the lower atmosphere (the troposphere ). [139] The upper atmosphere (the stratosphere ) would also be warming if the Sun was sending more energy to Earth, but instead, it has been cooling. [92] This is consistent with greenhouse gases preventing heat from leaving the Earth's atmosphere. [140] Explosive volcanic eruptions can release gases, dust and ash that partially block sunlight and reduce temperatures, or they can send water vapor into the atmosphere, which adds to greenhouse gases and increases temperatures. [141] These impacts on temperature only last for several years, because both water vapor and volcanic material have low persistence in the atmosphere. [142] volcanic CO2 emissions are more persistent, but they are equivalent to less than 1% of current human-caused CO2 emissions. [143] Volcanic activity still represents the single largest natural impact (forcing) on temperature in the industrial era. Yet, like the other natural forcings, it has had negligible impacts on global temperature trends since the Industrial Revolution. [142] Climate change feedbacks Main articles: Climate change feedbacks and Climate sensitivity Sea ice reflects 50% to 70% of incoming sunlight, while the ocean, being darker, reflects only 6%. As an area of sea ice melts and exposes more ocean, more heat is absorbed by the ocean, raising temperatures that melt still more ice. This is a positive feedback process . [144] The response of the climate system to an initial forcing is modified by feedbacks: increased by "self-reinforcing" or "positive" feedbacks and reduced by "balancing" or "negative" feedbacks . [145] The main reinforcing feedbacks are the water-vapour feedback , the ice–albedo feedback , and the net effect of clouds. [146] [147] The primary balancing mechanism is radiative cooling , as Earth's surface gives off more heat to space in response to rising temperature. [148] In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilising effect of CO2 on plant growth. [149] Uncertainty over feedbacks, particularly cloud cover, [150] is the major reason why different climate models project different magnitudes of warming for a given amount of emissions. [151] As air warms, it can hold more moisture . Water vapour, as a potent greenhouse gas, holds heat in the atmosphere. [146] If cloud cover increases, more sunlight will be reflected back into space, cooling the planet. If clouds become higher and thinner, they act as an insulator, reflecting heat from below back downwards and warming the planet. [152] Another major feedback is the reduction of snow cover and sea ice in the Arctic, which reduces the reflectivity of the Earth's surface. [153] More of the Sun's energy is now absorbed in these regions, contributing to amplification of Arctic temperature changes . [154] Arctic amplification is also thawing permafrost , which releases methane and CO2 into the atmosphere. [155] Climate change can also cause methane releases from wetlands , marine systems, and freshwater systems. [156] Overall, climate feedbacks are expected to become increasingly positive. [157] Around half of human-caused CO2 emissions have been absorbed by land plants and by the oceans. [158] This fraction is not static and if future CO2 emissions decrease, the Earth will be able to absorb up to around 70%. If they increase substantially, it'll still absorb more carbon than now, but the overall fraction will decrease to below 40%. [159] This is because climate change increases droughts and heat waves that eventually inhibit plant growth on land, and soils will release more carbon from dead plants when they are warmer . [160] [161] The rate at which oceans absorb atmospheric carbon will be lowered as they become more acidic and experience changes in thermohaline circulation and phytoplankton distribution. [162] [163] [73] Modelling Further information: Climate model and Climate change scenario Energy flows between space, the atmosphere, and Earth's surface. Most sunlight passes through the atmosphere to heat the Earth's surface, then greenhouse gases absorb most of the heat the Earth radiates in response. Adding to greenhouse gases increases this insulating effect, causing an energy imbalance that heats the planet up. A climate model is a representation of the physical, chemical and biological processes that affect the climate system. [164] Models include natural processes like changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing. [165] Models are used to estimate the degree of warming future emissions will cause when accounting for the strength of climate feedbacks . [166] [167] Models also predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere. [168] The physical realism of models is tested by examining their ability to simulate contemporary or past climates. [169] Past models have underestimated the rate of Arctic shrinkage [170] and underestimated the rate of precipitation increase. [171] Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations. [172] The 2017 United States-published National Climate Assessment notes that "climate models may still be underestimating or missing relevant feedback processes". [173] Additionally, climate models may be unable to adequately predict short-term regional climatic shifts. [174] A subset of climate models add societal factors to a physical climate model. These models simulate how population, economic growth , and energy use affect—and interact with—the physical climate. With this information, these models can produce scenarios of future greenhouse gas emissions. This is then used as input for physical climate models and carbon cycle models to predict how atmospheric concentrations of greenhouse gases might change. [175] [176] Depending on the socioeconomic scenario and the mitigation scenario, models produce atmospheric CO2 concentrations that range widely between 380 and 1400 ppm. [177] Impacts Main article: Effects of climate change The sixth IPCC Assessment Report projects changes in average soil moisture that can disrupt agriculture and ecosystems. A reduction in soil moisture by one standard deviation means that average soil moisture will approximately match the ninth driest year between 1850 and 1900 at that location. Environmental effects Further information: Effects of climate change on oceans and Effects of climate change on the water cycle The environmental effects of climate change are broad and far-reaching, affecting oceans , ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations. [178] Since the 1950s, droughts and heat waves have appeared simultaneously with increasing frequency. [179] Extremely wet or dry events within the monsoon period have increased in India and East Asia. [180] Monsoonal precipitation over the Northern Hemisphere has increased since 1980. [181] The rainfall rate and intensity of hurricanes and typhoons is likely increasing , [182] and the geographic range likely expanding poleward in response to climate warming. [183] Frequency of tropical cyclones has not increased as a result of climate change. [184] Historical sea level reconstruction and projections up to 2100 published in 2017 by the U.S. Global Change Research Program [185] Global sea level is rising as a consequence of thermal expansion and the melting of glaciers and ice sheets . Between 1993 and 2020, the rise increased over time, averaging 3.3 ± 0.3 mm per year. [186] Over the 21st century, the IPCC projects 32–62 cm of sea level rise under a low emission scenario, 44–76 cm under an intermediate one and 65–101 cm under a very high emission scenario. [187] Marine ice sheet instability processes in Antarctica may add substantially to these values, [188] including the possibility of a 2-meter sea level rise by 2100 under high emissions. [189] Climate change has led to decades of shrinking and thinning of the Arctic sea ice . [190] While ice-free summers are expected to be rare at 1.5 °C degrees of warming, they are set to occur once every three to ten years at a warming level of 2 °C. [191] Higher atmospheric CO2 concentrations cause more CO2 to dissolve in the oceans, which is making them more acidic . [192] Because oxygen is less soluble in warmer water, [193] its concentrations in the ocean are decreasing , and dead zones are expanding. [194] Tipping points and long-term impacts Different levels of global warming may cause different parts of Earth's climate system to reach tipping points that cause transitions to different states. [195] [196] Main article: Tipping points in the climate system Greater degrees of global warming increase the risk of passing through ' tipping points '—thresholds beyond which certain major impacts can no longer be avoided even if temperatures return to their previous state. [197] [198] For instance, the Greenland ice sheet is already melting, but if global warming reaches levels between 1.7 °C and 2.3 °C, its melting will continue until it fully disappears. If the warming is later reduced to 1.5 °C or less, it will still lose a lot more ice than if the warming was never allowed to reach the threshold in the first place. [199] While the ice sheets would melt over millennia, other tipping points would occur faster and give societies less time to respond. The collapse of major ocean currents like the Atlantic meridional overturning circulation (AMOC), and irreversible damage to key ecosystems like the Amazon rainforest and coral reefs can unfold in a matter of decades. [196] The long-term effects of climate change on oceans include further ice melt, ocean warming , sea level rise, ocean acidification and ocean deoxygenation. [200] The timescale of long-term impacts are centuries to millennia due to CO2's long atmospheric lifetime. [201] When net emissions stabilise surface air temperatures will also stabilise, but oceans and ice caps will continue to absorb excess heat from the atmosphere. The result is an estimated total sea level rise of 2.3 metres per degree Celsius (4.2 ft/°F) after 2000 years. [202] Oceanic CO2 uptake is slow enough that ocean acidification will also continue for hundreds to thousands of years. [203] Deep oceans (below 2,000 metres (6,600 ft)) are also already committed to losing over 10% of their dissolved oxygen by the warming which occurred to date. [204] Further, West Antarctic ice sheet appears committed to practically irreversible melting, which would increase the sea levels by at least 3.3 m (10 ft 10 in) over approximately 2000 years. [196] [205] [206] [207] [208] [209] [210] [211] Nature and wildlife Further information: Effects of climate change on oceans and Effects of climate change on biomes Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes . [212] Higher atmospheric CO2 levels and an extended growing season have resulted in global greening. However, heatwaves and drought have reduced ecosystem productivity in some regions. The future balance of these opposing effects is unclear. [213] Climate change has contributed to the expansion of drier climate zones, such as the expansion of deserts in the subtropics . [214] The size and speed of global warming is making abrupt changes in ecosystems more likely. [215] Overall, it is expected that climate change will result in the extinction of many species. [216] The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land. [217] Just as on land, heat waves in the ocean occur more frequently due to climate change, harming a wide range of organisms such as corals, kelp , and seabirds . [218] Ocean acidification makes it harder for marine calcifying organisms such as mussels , barnacles and corals to produce shells and skeletons ; and heatwaves have bleached coral reefs . [219] Harmful algal blooms enhanced by climate change and eutrophication lower oxygen levels, disrupt food webs and cause great loss of marine life. [220] Coastal ecosystems are under particular stress. Almost half of global wetlands have disappeared due to climate change and other human impacts. [221] Climate change impacts on the environment
Toggle the table of contents Deforestation From Wikipedia, the free encyclopedia Conversion of forest to non-forest for human use "Forest clearing" redirects here. For a gap in a forest, see Glade (geography) . "Deforest" redirects here. For other uses, see DeForest (disambiguation) . Deforestation in Riau province, Sumatra, Indonesia to make way for an oil palm plantation in 2007. Deforestation in the city of Rio de Janeiro in Brazil's Rio de Janeiro state, 2009 Deforestation or forest clearance is the removal and destruction of a forest or stand of trees from land that is then converted to non-forest use. [1] Deforestation can involve conversion of forest land to farms , ranches , or urban use.  About 31% of Earth's land surface is covered by forests at present. [2] This is one-third less than the forest cover before the expansion of agriculture, with half of that loss occurring in the last century. [3] Between 15 million to 18 million hectares of forest, an area the size of Bangladesh , are destroyed every year. On average 2,400 trees are cut down each minute. [4] Estimates vary widely as to the extent of deforestation in the tropics . [5] [6] In 2019, nearly a third of the overall tree cover loss, or 3.8 million hectares, occurred within humid tropical primary forests . These are areas of mature rainforest that are especially important for biodiversity and carbon storage . [7] [8] The direct cause of most deforestation is agriculture by far. [9] More than 80% of deforestation was attributed to agriculture in 2018. [10] Forests are being converted to plantations for coffee , palm oil , rubber and various other popular products. [11] Livestock grazing also drives deforestation. Further drivers are the wood industry ( logging ), urbanization and mining . The effects of climate change are another cause via the increased risk of wildfires (see deforestation and climate change ). Deforestation results in habitat destruction which in turn leads to biodiversity loss . Deforestation also leads to extinction of animals and plants, changes to the local climate, and displacement of indigenous people who live in forests. Deforested regions often also suffer from other environmental problems such as desertification and soil erosion . Another problem is that deforestation reduces the uptake of carbon dioxide ( carbon sequestration ) from the atmosphere. This reduces the potential of forests to assist with climate change mitigation . The role of forests in capturing and storing carbon and mitigating climate change is also important for the agricultural sector. [12] The reason for this linkage is because the effects of climate change on agriculture pose new risks to global food systems . [12] Definition Forest area net change rate per country in 2020 Deforestation is defined as the conversion of forest to other land uses (regardless of whether it is human-induced). [13] Deforestation and forest area net change are not the same: the latter is the sum of all forest losses (deforestation) and all forest gains (forest expansion) in a given period. Net change, therefore, can be positive or negative, depending on whether gains exceed losses, or vice versa. [13] Current status Annual deforestation Annual change in forest area The FAO estimates that the global forest carbon stock has decreased 0.9%, and tree cover 4.2% between 1990 and 2020. [14] : 16, 52 Changes in forest carbon stock by regionFigures in gigatons [14] : 52, table 43 Region South and Southeast Asia combined 45.8 161.8 144.8 As of 2019 there is still disagreement about whether the global forest is shrinking or not: "While above-ground biomass carbon stocks are estimated to be declining in the tropics, they are increasing globally due to increasing stocks in temperate and boreal forest. [15] : 385 Deforestation in many countries —both naturally occurring [16] and human-induced —is an ongoing issue. [17] Between 2000 and 2012, 2.3 million square kilometres (890,000 sq mi) of forests around the world were cut down. [18] Deforestation and forest degradation continue to take place at alarming rates, which contributes significantly to the ongoing loss of biodiversity . [12] The amount of globally needed agricultural land would be reduced by three quarters if the entire population adopted a vegan diet. [19] Deforestation is more extreme in tropical and subtropical forests in emerging economies. More than half of all plant and land animal species in the world live in tropical forests . [20] As a result of deforestation, only 6.2 million square kilometres (2.4 million square miles) remain of the original 16 million square kilometres (6 million square miles) of tropical rainforest that formerly covered the Earth. [18] An area the size of a football pitch is cleared from the Amazon rainforest every minute, with 136 million acres (55 million hectares) of rainforest cleared for animal agriculture overall. [21] More than 3.6 million hectares of virgin tropical forest was lost in 2018. [22] The global annual net loss of trees is estimated to be approximately 10 billion. [23] [24] According to the Global Forest Resources Assessment 2020 the global average annual deforested land in the 2015–2020 demi-decade was 10 million hectares and the average annual forest area net loss in the 2000–2010 decade was 4.7 million hectares. [13] The world has lost 178 million ha of forest since 1990, which is an area about the size of Libya. [13] An analysis of global deforestation patterns in 2021 showed that patterns of trade, production, and consumption drive deforestation rates in complex ways. While the location of deforestation can be mapped, it does not always match where the commodity is consumed. For example, consumption patterns in G7 countries are estimated to cause an average loss of 3.9 trees per person per year. In other words, deforestation can be directly related to imports—for example, coffee. [25] [26] In 2023, the Global Forest Watch reported a 9% decline in tropical primary forest loss compared to the previous year, with significant regional reductions in Brazil and Colombia overshadowed by increases elsewhere, leading to a 3.2% rise in global deforestation. Massive wildfires in Canada , exacerbated by climate change , contributed to a 24% increase in global tree cover loss, highlighting the ongoing threats to forests essential for carbon storage and biodiversity . Despite some progress, the overall trends in forest destruction and climate impacts remain alarming. [27] Rates of deforestation The period since 1950 has brought "the most rapid transformation of the human relationship with the natural world in the history of humankind". [28] Through 2018, humans have reduced forest area by ~30% and grasslands/shrubs by ~68%, to make way for livestock grazing and crops for humans. [29] Global deforestation [30] sharply accelerated around 1852. [31] [32] As of 1947, the planet had 15 million to 16 million km2 (5.8 million to 6.2 million sq mi) of mature tropical forests , [33] but by 2015, it was estimated that about half of these had been destroyed. [34] [20] [35] Total land coverage by tropical rainforests decreased from 14% to 6%. Much of this loss happened between 1960 and 1990, when 20% of all tropical rainforests were destroyed. At this rate, extinction of such forests is projected to occur by the mid-21st century. [36] In the early 2000s, some scientists predicted that unless significant measures (such as seeking out and protecting old growth forests that have not been disturbed) [33] are taken on a worldwide basis, by 2030 there will only be 10% remaining, [31] [35] with another 10% in a degraded condition . [31] 80% will have been lost, and with them hundreds of thousands of irreplaceable species. [31] Estimates vary widely as to the extent of deforestation in the tropics. [5] [6] In 2019, the world lost nearly 12 million hectares of tree cover. Nearly a third of that loss, 3.8 million hectares, occurred within humid tropical primary forests, areas of mature rainforest that are especially important for biodiversity and carbon storage. This is equivalent to losing an area of primary forest the size of a football pitch every six seconds. [7] [8] Rates of change In decades since 1990, South America and Africa have shown the greatest loss of forest area, with global net loss in the 2010s still about 60% of the 1990s value. [37] The rate of global tree cover loss has approximately doubled since 2001, to an annual loss approaching an area the size of Italy. [38] Loss of primary (old-growth) forest in the tropics has continued its upward trend, with fire-related losses contributing an increasing portion. [39] A 2002 analysis of satellite imagery suggested that the rate of deforestation in the humid tropics (approximately 5.8 million hectares per year) was roughly 23% lower than the most commonly quoted rates. [40] A 2005 report by the United Nations Food and Agriculture Organization (FAO) estimated that although the Earth's total forest area continued to decrease at about 13 million hectares per year, the global rate of deforestation had been slowing. [41] [42] On the other hand, a 2005 analysis of satellite images reveals that deforestation of the Amazon rainforest is twice as fast as scientists previously estimated. [43] [44] From 2010 to 2015, worldwide forest area decreased by 3.3 million ha per year, according to FAO . During this five-year period, the biggest forest area loss occurred in the tropics, particularly in South America and Africa. Per capita forest area decline was also greatest in the tropics and subtropics but is occurring in every climatic domain (except in the temperate) as populations increase. [45] An estimated 420 million ha of forest has been lost worldwide through deforestation since 1990, but the rate of forest loss has declined substantially. In the most recent five-year period (2015–2020), the annual rate of deforestation was estimated at 10 million ha, down from 12 million ha in 2010–2015. [13] Home to much of the Amazon rainforest , Brazil's tropical primary (old-growth) forest loss greatly exceeds that of other countries. [39] Overall, 20% of the Amazon rainforest has been "transformed" (deforested) and another 6% has been "highly degraded", causing Amazon Watch to warn that the Amazonia is in the midst of a tipping point crisis. [46] Africa had the largest annual rate of net forest loss in 2010–2020, at 3.9 million ha, followed by South America, at 2.6 million ha. The rate of net forest loss has increased in Africa in each of the three decades since 1990. It has declined substantially in South America, however, to about half the rate in 2010–2020 compared with 2000–2010. Asia had the highest net gain of forest area in 2010–2020, followed by Oceania and Europe. Nevertheless, both Europe and Asia recorded substantially lower rates of net gain in 2010–2020 than in 2000–2010. Oceania experienced net losses of forest area in the decades 1990–2000 and 2000–2010. [13] Some claim that rainforests are being destroyed at an ever-quickening pace. [47] The London-based Rainforest Foundation notes that "the UN figure is based on a definition of forest as being an area with as little as 10% actual tree cover, which would therefore include areas that are actually savanna-like ecosystems and badly damaged forests". [48] Other critics of the FAO data point out that they do not distinguish between forest types, [49] and that they are based largely on reporting from forestry departments of individual countries, [50] which do not take into account unofficial activities like illegal logging. [51] Despite these uncertainties, there is agreement that destruction of rainforests remains a significant environmental problem. The rate of net forest loss declined from 7.8 million ha per year in the decade 1990–2000 to 5.2 million ha per year in 2000–2010 and 4.7 million ha per year in 2010–2020. The rate of decline of net forest loss slowed in the most recent decade due to a reduction in the rate of forest expansion. [13] Reforestation and afforestation Main articles: Reforestation and Afforestation In many parts of the world, especially in East Asian countries, reforestation and afforestation are increasing the area of forested lands. [52] The amount of forest has increased in 22 of the world's 50 most forested nations. Asia as a whole gained 1 million hectares of forest between 2000 and 2005. Tropical forest in El Salvador expanded more than 20% between 1992 and 2001. Based on these trends, one study projects that global forestation will increase by 10%—an area the size of India—by 2050. [53] 36% of globally planted forest area is in East Asia - around 950,000 square kilometers. From those 87% are in China. [54] Status by region Main article: Deforestation by continent Rates of deforestation vary around the world. Up to 90% of West Africa 's coastal rainforests have disappeared since 1900. [55] Madagascar has lost 90% of its eastern rainforests. [56] [57] In South Asia , about 88% of the rainforests have been lost. [58] Deforestation in Ecuador . Much of what remains of the world's rainforests is in the Amazon basin , where the Amazon Rainforest covers approximately 4 million square kilometres. [61] Some 80% of the deforestation of the Amazon can be attributed to cattle ranching, [62] as Brazil is the largest exporter of beef in the world. [63] The Amazon region has become one of the largest cattle ranching territories in the world. [64] The regions with the highest tropical deforestation rate between 2000 and 2005 were Central America —which lost 1.3% of its forests each year—and tropical Asia. [48] In Central America , two-thirds of lowland tropical forests have been turned into pasture since 1950 and 40% of all the rainforests have been lost in the last 40 years. [65] Brazil has lost 90–95% of its Mata Atlântica forest. [66] Deforestation in Brazil increased by 88% for the month of June 2019, as compared with the previous year. [67] However, Brazil still destroyed 1.3 million hectares in 2019. [7] Brazil is one of several countries that have declared their deforestation a national emergency. [68] [69] Paraguay was losing its natural semi-humid forests in the country's western regions at a rate of 15,000 hectares at a randomly studied 2-month period in 2010. [70] In 2009, Paraguay's parliament refused to pass a law that would have stopped cutting of natural forests altogether. [71] As of 2007, less than 50% of Haiti's forests remained . [72] From 2015 to 2019, the rate of deforestation in the Democratic Republic of the Congo doubled. [73] In 2021, deforestation of the Congolese rainforest increased by 5%. [74] The World Wildlife Fund 's ecoregion project catalogues habitat types throughout the world, including habitat loss such as deforestation, showing for example that even in the rich forests of parts of Canada such as the Mid-Continental Canadian forests of the prairie provinces half of the forest cover has been lost or altered. In 2011, Conservation International listed the top 10 most endangered forests, characterized by having all lost 90% or more of their original habitat , and each harboring at least 1500 endemic plant species (species found nowhere else in the world). [75] As of 2015 [update] , it is estimated that 70% of the world's forests are within one kilometer of a forest edge, where they are most prone to human interference and destruction. [76] [77] Top 10 Most Endangered Forests in 2011 [75] Endangered forest Tropical and subtropical moist broadleaf forests See note for region covered. [79] Tropical and subtropical moist broadleaf forests Western half of the Indo-Malayan archipelago including southern Borneo and Sumatra . [80] Tropical and subtropical moist broadleaf forests Forests over the entire country including 7,100 islands. [81] Forests along Brazil 's Atlantic coast, extends to parts of Paraguay , Argentina and Uruguay . [82] Mountains of Southwest China Tropical and subtropical moist broadleaf forests Montane grasslands and shrublands Forests scattered along the eastern edge of Africa, from Saudi Arabia in the north to Zimbabwe in the south. [87] By country Drivers of deforestation and forest degradation by region, 2000–2010 [12] Drivers of tropical deforestration The last batch of sawnwood from the peat forest in Indragiri Hulu , Sumatra , Indonesia . Deforestation for oil palm plantation. Agricultural expansion continues to be the main driver of deforestation and forest fragmentation and the associated loss of forest biodiversity. [12] Large-scale commercial agriculture (primarily cattle ranching and cultivation of soya bean and oil palm) accounted for 40 percent of tropical deforestation between 2000 and 2010, and local subsistence agriculture for another 33 percent. [12] Trees are cut down for use as building material, timber or sold as fuel (sometimes in the form of charcoal or timber ), while cleared land is used as pasture for livestock and agricultural crops. The vast majority of agricultural activity resulting in deforestation is subsidized by government tax revenue . [88] Disregard of ascribed value, lax forest management, and deficient environmental laws are some of the factors that lead to large-scale deforestation. The types of drivers vary greatly depending on the region in which they take place. The regions with the greatest amount of deforestation for livestock and row crop agriculture are Central and South America, while commodity crop deforestation was found mainly in Southeast Asia. The region with the greatest forest loss due to shifting agriculture was sub-Saharan Africa. [89] Agriculture Further information: Agricultural expansion The overwhelming direct cause of deforestation is agriculture. [9] Subsistence farming is responsible for 48% of deforestation; commercial agriculture is responsible for 32%; logging is responsible for 14%, and fuel wood removals make up 5%. [9] More than 80% of deforestation was attributed to agriculture in 2018. [10] Forests are being converted to plantations for coffee, tea, palm oil , rice, rubber , and various other popular products. [11] The rising demand for certain products and global trade arrangements causes forest conversions , which ultimately leads to soil erosion . [90] The top soil oftentimes erodes after forests are cleared which leads to sediment increase in rivers and streams. Anthropogenic biomes of the world Most deforestation also occurs in tropical regions. The estimated amount of total land mass used by agriculture is around 38%. [91] Since 1960, roughly 15% of the Amazon has been removed with the intention of replacing the land with agricultural practices. [92] It is no coincidence that Brazil has recently become the world's largest beef exporter at the same time that the Amazon rainforest is being clear cut. [93] Another prevalent method of agricultural deforestation is slash-and-burn agriculture , which was primarily used by subsistence farmers in tropical regions but has now become increasingly less sustainable. The method does not leave land for continuous agricultural production but instead cuts and burns small plots of forest land which are then converted into agricultural zones. The farmers then exploit the nutrients in the ashes of the burned plants. [94] [95] As well as, intentionally set fires can possibly lead to devastating measures when unintentionally spreading fire to more land, which can result in the destruction of the protective canopy. [96] The repeated cycle of low yields and shortened fallow periods eventually results in less vegetation being able to grow on once burned lands and a decrease in average soil biomass. [97] In small local plots sustainability is not an issue because of longer fallow periods and lesser overall deforestation. The relatively small size of the plots allowed for no net input of CO2 to be released. [98] Livestock ranching Consumption and production of beef is the primary driver of deforestation in the Amazon , with around 80% of all converted land being used to rear cattle. [99] [100] 91% of Amazon land deforested since 1970 has been converted to cattle ranching. [101] [102] Livestock ranching requires large portions of land to raise herds of animals and livestock crops for consumer needs. According to the World Wildlife Fund , "Extensive cattle ranching is the number one culprit of deforestation in virtually every Amazon country, and it accounts for 80% of current deforestation." [103] The cattle industry is responsible for a significant amount of methane emissions since 60% of all mammals on earth are livestock cows. [104] [105] Replacing forest land with pastures creates a loss of forest stock , which leads to the implication of increased greenhouse gas emissions by burning agriculture methodologies and land-use change . [106] Wood industry Further information: Wood industry A large contributing factor to deforestation is the lumber industry . A total of almost 4 million hectares (9.9×10 6 acres) of timber, [107] or about 1.3% of all forest land, is harvested each year. In addition, the increasing demand for low-cost timber products only supports the lumber company to continue logging. [108] Experts do not agree on whether industrial logging is an important contributor to global deforestation. [109] [110] Some argue that poor people are more likely to clear forest because they have no alternatives, others that the poor lack the ability to pay for the materials and labour needed to clear forest. [109] Economic development Other causes of contemporary deforestation may include corruption of government institutions, [111] [112] [113] the inequitable distribution of wealth and power , [114] population growth [115] and overpopulation , [116] [117] and urbanization . [118] [119] The impact of population growth on deforestation has been contested. One study found that population increases due to high fertility rates were a primary driver of tropical deforestation in only 8% of cases. [120] In 2000 the United Nations Food and Agriculture Organization (FAO) found that "the role of population dynamics in a local setting may vary from decisive to negligible", and that deforestation can result from "a combination of population pressure and stagnating economic, social and technological conditions". [115] Globalization is often viewed as another root cause of deforestation, [121] [122] though there are cases in which the impacts of globalization (new flows of labor, capital, commodities, and ideas) have promoted localized forest recovery. [123] Illegal gold mining in Madre de Dios, Peru . The degradation of forest ecosystems has also been traced to economic incentives that make forest conversion appear more profitable than forest conservation. [124] Many important forest functions have no markets, and hence, no economic value that is readily apparent to the forests' owners or the communities that rely on forests for their well-being. [124] From the perspective of the developing world, the benefits of forest as carbon sinks or biodiversity reserves go primarily to richer developed nations and there is insufficient compensation for these services. Developing countries feel that some countries in the developed world, such as the United States of America, cut down their forests centuries ago and benefited economically from this deforestation, and that it is hypocritical to deny developing countries the same opportunities, i.e. that the poor should not have to bear the cost of preservation when the rich created the problem. [125] Some commentators have noted a shift in the drivers of deforestation over the past 30 years. [126] Whereas deforestation was primarily driven by subsistence activities and government-sponsored development projects like transmigration in countries like Indonesia and colonization in Latin America , India , Java , and so on, during the late 19th century and the first half of the 20th century, by the 1990s the majority of deforestation was caused by industrial factors, including extractive industries, large-scale cattle ranching, and extensive agriculture. [127] Since 2001, commodity-driven deforestation, which is more likely to be permanent, has accounted for about a quarter of all forest disturbance, and this loss has been concentrated in South America and Southeast Asia. [128] As the human population grows, new homes, communities, and expansions of cities will occur, leading to an increase in roads to connect these communities. Rural roads promote economic development but also facilitate deforestation. [129] About 90% of the deforestation has occurred within 100 km of roads in most parts of the Amazon. [130] The European Union is one of the largest importer of products made from illegal deforestation . [131] [ obsolete source ] Some have argued that deforestation trends may follow a Kuznets curve , [132] which if true would nonetheless fail to eliminate the risk of irreversible loss of non-economic forest values (for example, the extinction of species). [133] [134] Mining The importance of mining as a cause of deforestation increased quickly in the beginning the 21st century, among other because of increased demand for minerals. The direct impact of mining is relatively small, but the indirect impacts are much more significant. More than a third of the earth's forests are possibly impacted, at some level and in the years 2001–2021, "755,861 km2... ...had been deforested by causes indirectly related to mining activities alongside other deforestation drivers (based on data from WWF)" [135] Climate change Emerging signals of declining forest resilience under climate change [136] Temporal variations of forest resilience and its key drivers [136] Another cause of deforestation is due to the effects of climate change : More wildfires , [137] insect outbreaks, invasive species , and more frequent extreme weather events (such as storms) are factors that increase deforestation. [138] A study suggests that "tropical, arid and temperate forests are experiencing a significant decline in resilience, probably related to increased water limitations and climate variability" which may shift ecosystems towards critical transitions and ecosystem collapses . [136] By contrast, "boreal forests show divergent local patterns with an average increasing trend in resilience, probably benefiting from warming and CO2 fertilization, which may outweigh the adverse effects of climate change". [136] It has been proposed that a loss of resilience in forests "can be detected from the increased temporal autocorrelation (TAC) in the state of the system, reflecting a decline in recovery rates due to the critical slowing down (CSD) of system processes that occur at thresholds". [136] 23% of tree cover losses result from wildfires and climate change increase their frequency and power. [139] The rising temperatures cause massive wildfires especially in the Boreal forests . One possible effect is the change of the forest composition. [140] Deforestation can also cause forests to become more fire prone through mechanisms such as logging. [141] Military causes U.S. Army Huey helicopter spraying Agent Orange during the Vietnam War Operations in war can also cause deforestation. For example, in the 1945 Battle of Okinawa , bombardment and other combat operations reduced a lush tropical landscape into "a vast field of mud, lead, decay and maggots". [142] Deforestation can also result from the intentional tactics of military forces . Clearing forests became an element in the Russian Empire's successful conquest of the Caucasus in the mid-19th century. [143] The British (during the Malayan Emergency ) and the United States (in the Korean War [144] and in the Vietnam War ) used defoliants (like Agent Orange or others). [145] [146] [147] [ need quotation to verify ] The destruction of forests in Vietnam War is one of the most commonly used examples of ecocide , including by Swedish Prime Minister Olof Palme , lawyers, historians and other academics. [148] [149] [150] Impacts Biophysical mechanisms by which forests influence climate. [151] Per capita CO2 emissions from deforestation for food production Illegal " slash-and-burn " practice in Madagascar , 2010 Mean annual carbon loss from tropical deforestation. [152] Deforestation is a major contributor to climate change . [153] [154] [155] It is often cited as one of the major causes of the enhanced greenhouse effect . Recent calculations suggest that CO2 emissions from deforestation and forest degradation (excluding peatland emissions) contribute about 12% of total anthropogenic CO2 emissions, with a range from 6% to 17%. [156] A 2022 study shows annual carbon emissions from tropical deforestation have doubled during the last two decades and continue to increase: by 0.97 ± 0.16 PgC ( petagrams of carbon, i.e. billions of tons) per year in 2001–2005 to 1.99 ± 0.13 PgC per year in 2015–2019. [157] [152] According to a review, north of 50°N, large scale deforestation leads to an overall net global cooling; but deforestation in the tropics leads to substantial warming: not just due to CO2 impacts, but also due to other biophysical mechanisms (making carbon-centric metrics inadequate). Moreover, it suggests that standing tropical forests help cool the average global temperature by more than 1 °C. [158] [151] The incineration and burning of forest plants to clear land releases large amounts of CO2, which contributes to global warming. [159] Scientists also state that tropical deforestation releases 1.5 billion tons of carbon each year into the atmosphere. [160] Carbon sink or source See also: Carbon sequestration , Carbon sink , Biomass (energy) § Climate impacts , and Sustainable energy § Bioenergy A study suggests logged and structurally degraded tropical forests are carbon sources for at least a decade – even when recovering[ clarification needed ] – due to larger carbon losses from soil organic matter and deadwood, indicating that the tropical forest carbon sink (at least in South Asia) "may be much smaller than previously estimated", contradicting that "recovering logged and degraded tropical forests are net carbon sinks". [161] This section is an excerpt from Carbon sink § Forests .[ edit ] Proportion of carbon stock in forest carbon pools, 2020 [162] On the environment According to a 2020 study, if deforestation continues at current rates it can trigger a total or almost total extinction of humanity in the next 20 to 40 years. They conclude that "from a statistical point of view . . . the probability that our civilisation survives itself is less than 10% in the most optimistic scenario." To avoid this collapse, humanity should pass from a civilization dominated by the economy to "cultural society" that "privileges the interest of the ecosystem above the individual interest of its components, but eventually in accordance with the overall communal interest." [163] [164] Changes to the water cycle The water cycle is also affected by deforestation. Trees extract groundwater through their roots and release it into the atmosphere. When part of a forest is removed, the trees no longer transpire this water, resulting in a much drier climate . Deforestation reduces the content of water in the soil and groundwater as well as atmospheric moisture. The dry soil leads to lower water intake for the trees to extract. [165] Deforestation reduces soil cohesion, so that erosion , flooding and landslides ensue. [166] [167] Shrinking forest cover lessens the landscape's capacity to intercept, retain and transpire precipitation. Instead of trapping precipitation, which then percolates to groundwater systems, deforested areas become sources of surface water runoff, which moves much faster than subsurface flows. Forests return most of the water that falls as precipitation to the atmosphere by transpiration. In contrast, when an area is deforested, almost all precipitation is lost as run-off. [168] That quicker transport of surface water can translate into flash flooding and more localized floods than would occur with the forest cover. Deforestation also contributes to decreased evapotranspiration , which lessens atmospheric moisture which in some cases affects precipitation levels downwind from the deforested area, as water is not recycled to downwind forests, but is lost in runoff and returns directly to the oceans. According to one study, in deforested north and northwest China, the average annual precipitation decreased by one third between the 1950s and the 1980s. [169] Deforestation of the Highland Plateau in Madagascar has led to extensive siltation and unstable flows of western rivers. Trees, and plants in general, affect the water cycle significantly: [170] their canopies intercept a proportion of precipitation , which is then evaporated back to the atmosphere ( canopy interception ); their litter, stems and trunks slow down surface runoff ; their roots create macropores – large conduits – in the soil that increase infiltration of water; they contribute to terrestrial evaporation and reduce soil moisture via transpiration ; their litter and other organic residue change soil properties that affect the capacity of soil to store water. their leaves control the humidity of the atmosphere by transpiring . 99% of the water absorbed by the roots moves up to the leaves and is transpired. [171] As a result, the presence or absence of trees can change the quantity of water on the surface, in the soil or groundwater, or in the atmosphere. This in turn changes erosion rates and the availability of water for either ecosystem functions or human services. Deforestation on lowland plains moves cloud formation and rainfall to higher elevations. [172] The forest may have little impact on flooding in the case of large rainfall events, which overwhelm the storage capacity of forest soil if the soils are at or close to saturation. Tropical rainforests produce about 30% of Earth's fresh water . [173] Deforestation disrupts normal weather patterns creating hotter and drier weather thus increasing drought, desertification, crop failures, melting of the polar ice caps, coastal flooding and displacement of major vegetation regimes. [174] Soil erosion Deforestation in France . Due to surface plant litter , forests that are undisturbed have a minimal rate of erosion . The rate of erosion occurs from deforestation, because it decreases the amount of litter cover, which provides protection from surface runoff . [175] The rate of erosion is around 2 metric tons per square kilometre. [176] [ self-published source? ] This can be an advantage in excessively leached tropical rain forest soils. Forestry operations themselves also increase erosion through the development of ( forest ) roads and the use of mechanized equipment. [76] Deforestation in China's Loess Plateau many years ago has led to soil erosion; this erosion has led to valleys opening up. The increase of soil in the runoff causes the Yellow River to flood and makes it yellow-colored. [176] Greater erosion is not always a consequence of deforestation, as observed in the southwestern regions of the US. In these areas, the loss of grass due to the presence of trees and other shrubbery leads to more erosion than when trees are removed. [176] Soils are reinforced by the presence of trees, which secure the soil by binding their roots to soil bedrock. Due to deforestation, the removal of trees causes sloped lands to be more susceptible to landslides. [170] Other changes to the soil Clearing forests changes the environment of the microbial communities within the soil , and causes a loss of biodiversity in regards to the microbes since biodiversity is actually highly dependent on soil texture . [177] Although the effect of deforestation has much more profound consequences on sandier soils compared to clay-like soils, the disruptions caused by deforestation ultimately reduces properties of soil such as hydraulic conductivity and water storage, thus reducing the efficiency of water and heat absorption. [177] [178] In a simulation of the deforestation process in the Amazon, researchers found that surface and soil temperatures increased by 1 to 3 degrees Celsius demonstrating the loss of the soil's ability to absorb radiation and moisture. [178] Furthermore, soils that are rich in organic decay matter are more susceptible to fire, especially during long droughts. [177] Changes in soil properties could turn the soil itself into a carbon source rather than a carbon sink . [179] Biodiversity loss Further information: Biodiversity loss Deforestation on a human scale results in decline in biodiversity , [180] and on a natural global scale is known to cause the extinction of many species. [181] [182] The removal or destruction of areas of forest cover has resulted in a degraded environment with reduced biodiversity . [117] Forests support biodiversity, providing habitat for wildlife ; [183] moreover, forests foster medicinal conservation . [184] With forest biotopes being irreplaceable source of new drugs (such as taxol ), deforestation can destroy genetic variations (such as crop resistance) irretrievably. [185] Illegal logging in Madagascar . In 2009, the vast majority of the illegally obtained rosewood was exported to China . Since the tropical rainforests are the most diverse ecosystems on Earth [186] [187] and about 80% of the world's known biodiversity can be found in tropical rainforests, [188] [189] removal or destruction of significant areas of forest cover has resulted in a degraded [190] environment with reduced biodiversity. [181] [191] Road construction and development of adjacent land, which greatly reduces the area of intact wilderness and causes soil erosion, is a major contributing factor to the loss of biodiversity in tropical regions. [76] A study in Rondônia , Brazil, has shown that deforestation also removes the microbial community which is involved in the recycling of nutrients, the production of clean water and the removal of pollutants. [192] It has been estimated that 137 plant, animal and insect species go extinct every day due to rainforest deforestation, which equates to 50,000 species a year. [193] Others state that tropical rainforest deforestation is contributing to the ongoing Holocene mass extinction . [194] [195] The known extinction rates from deforestation rates are very low, approximately one species per year from mammals and birds, which extrapolates to approximately 23,000 species per year for all species. Predictions have been made that more than 40% of the animal and plant species in Southeast Asia could be wiped out in the 21st century. [196] Such predictions were called into question by 1995 data that show that within regions of Southeast Asia much of the original forest has been converted to monospecific plantations, but that potentially endangered species are few and tree flora remains widespread and stable. [197] Location of tropical (dark green) and temperate/subtropical (light green) rainforests in the world Scientific understanding of the process of extinction is insufficient to accurately make predictions about the impact of deforestation on biodiversity. [198] Most predictions of forestry related biodiversity loss are based on species-area models, with an underlying assumption that as the forest declines species diversity will decline similarly. [199] However, many such models have been proven to be wrong and loss of habitat does not necessarily lead to large scale loss of species. [199] Species-area models are known to overpredict the number of species known to be threatened in areas where actual deforestation is ongoing, and greatly overpredict the number of threatened species that are widespread. [197] In 2012, a study of the Brazilian Amazon predicts that despite a lack of extinctions thus far, up to 90 percent of predicted extinctions will finally occur in the next 40 years. [200] Oxygen-supply misconception Rainforests are widely believed by lay persons to contribute a significant amount of the world's oxygen, [173] although it is now accepted by scientists that rainforests contribute little net oxygen to the atmosphere and deforestation has only a minor effect on atmospheric oxygen levels. [201] [202] In fact about 50 percent of oxygen on Earth is produced by algae. [203] On human health Infectious diseases The degradation and loss of forests disrupts nature's balance. [12] Indeed, deforestation eliminates a great number of species of plants and animals which also often results in an increase in disease, [204] and exposure of people to zoonotic diseases . [12] [205] [206] [207] Deforestation can also create a path for non-native species to flourish such as certain types of snails, which have been correlated with an increase in schistosomiasis cases. [204] [208] Forest-associated diseases include malaria, Chagas disease (also known as American trypanosomiasis), African trypanosomiasis (sleeping sickness), leishmaniasis, Lyme disease, HIV and Ebola. [12] The majority of new infectious diseases affecting humans, including the SARS-CoV-2 virus that caused the COVID-19 pandemic , are zoonotic and their emergence may be linked to habitat loss due to forest area change and the expansion of human populations into forest areas, which both increase human exposure to wildlife. [12] Deforestation is occurring all over the world and has been coupled with an increase in the occurrence of disease outbreaks. In Malaysia , thousands of acres of forest have been cleared for pig farms. This has resulted in an increase in the spread of the Nipah virus . [209] [210] In Kenya , deforestation has led to an increase in malaria cases which is now the leading cause of morbidity and mortality the country. [211] [212] A 2017 study in the American Economic Review found that deforestation substantially increased the incidence of malaria in Nigeria. [213] Another pathway through which deforestation affects disease is the relocation and dispersion of disease-carrying hosts. This disease emergence pathway can be called " range expansion ", whereby the host's range (and thereby the range of pathogens) expands to new geographic areas. [214] Through deforestation, hosts and reservoir species are forced into neighboring habitats. Accompanying the reservoir species are pathogens that have the ability to find new hosts in previously unexposed regions. As these pathogens and species come into closer contact with humans, they are infected both directly and indirectly. Another example of range expansion due to deforestation and other anthropogenic habitat impacts includes the Capybara rodent in Paraguay . [215] Deforestation reduces safe working hours for millions of people in the tropics, especially for those performing heavy labour outdoors. Continued global heating and forest loss is expected to amplify these impacts, reducing work hours for vulnerable groups even more. [216] A study conducted from 2002 to 2018 also determined that the increase in temperature as a result of climate change, and the lack of shade due to deforestation, has increased the mortality rate of workers in Indonesia . [217] A link between deforestation and infant mortality was found in Indonesia as well. The study shows documentation of deforestation and pregnancy order, [218] as children born from first pregnancies face higher mortality risks due to in-utero exposure. The study's results suggest that women during their first pregnancy could have been affected by deforestation-induced malaria. [218] It has been affirmed that in preserved regions, likely reasons including commercial activity, perinatal health care, alongside air pollution are not identifiable triggers of the weighty impression left by deforestation on newborn fatality. [218] According to the World Economic Forum , 31% of emerging diseases are linked to deforestation. [219] A publication by the United Nations Environment Programme in 2016 found that deforestation, climate change , and livestock agriculture are among the main causes that increase the risk of zoonotic diseases , that is diseases that pass from animals to humans. [220] COVID-19 pandemic See also: COVID-19 pandemic and Pandemic prevention § Environmental policy and economics Scientists have linked the Coronavirus pandemic to the destruction of nature, especially to deforestation, habitat loss in general and wildlife trade . [221] According to the United Nations Environment Programme (UNEP) the Coronavirus disease 2019 is zoonotic, e.g., the virus passed from animals to humans. UNEP concludes that: "The most fundamental way to protect ourselves from zoonotic diseases is to prevent destruction of nature. Where ecosystems are healthy and biodiverse, they are resilient, adaptable and help to regulate diseases. [222] On the economy and agriculture A satellite image showing deforestation for a palm oil plantation in Malaysia This section needs to be updated. The reason given is: cites are very old. Please help update this article to reflect recent events or newly available information. (June 2020) Economic losses due to deforestation in Brazil could reach around 317 billion dollars per year, approximately 7 times higher in comparison to the cost of all commodities produced through deforestation. [223] The forest products industry is a large part of the economy in both developed and developing countries. Short-term economic gains made by conversion of forest to agriculture, or over-exploitation of wood products, typically leads to a loss of long-term income and long-term biological productivity. West Africa , Madagascar , Southeast Asia and many other regions have experienced lower revenue because of declining timber harvests. Illegal logging causes billions of dollars of losses to national economies annually. [224] The resilience of human food systems and their capacity to adapt to future change is linked to biodiversity – including dryland-adapted shrub and tree species that help combat desertification, forest-dwelling insects, bats and bird species that pollinate crops, trees with extensive root systems in mountain ecosystems that prevent soil erosion , and mangrove species that provide resilience against flooding in coastal areas. [12] With climate change exacerbating the risks to food systems, the role of forests in capturing and storing carbon and mitigating climate change is important for the agricultural sector. [12] Satellite image of Haiti 's border with the Dominican Republic (right) shows the amount of deforestation on the Haitian side Deforestation around Pakke Tiger Reserve, India Monitoring Agents from IBAMA, Brazil 's environmental police, searching for illegal logging activity in Indigenous territory in the Amazon rainforest , 2018 There are multiple methods that are appropriate and reliable for reducing and monitoring deforestation. One method is the "visual interpretation of aerial photos or satellite imagery that is labor-intensive but does not require high-level training in computer image processing or extensive computational resources". [130] Another method includes hot-spot analysis (that is, locations of rapid change) using expert opinion or coarse resolution satellite data to identify locations for detailed digital analysis with high resolution satellite images. [130] Deforestation is typically assessed by quantifying the amount of area deforested, measured at the present time. From an environmental point of view, quantifying the damage and its possible consequences is a more important task, while conservation efforts are more focused on forested land protection and development of land-use alternatives to avoid continued deforestation. [130] Deforestation rate and total area deforested have been widely used for monitoring deforestation in many regions, including the Brazilian Amazon deforestation monitoring by INPE. [160] A global satellite view is available, an example of land change science monitoring of land cover over time. [225] [226] Satellite imaging has become crucial in obtaining data on levels of deforestation and reforestation. Landsat satellite data, for example, has been used to map tropical deforestation as part of NASA 's Landsat Pathfinder Humid Tropical Deforestation Project. The project yielded deforestation maps for the Amazon Basin , Central Africa , and Southeast Asia for three periods in the 1970s, 1980s, and 1990s. [227] Greenpeace has mapped out the forests that are still intact [228] and published this information on the internet. [229] World Resources Institute in turn has made a simpler thematic map [230] showing the amount of forests present just before the age of man (8000 years ago) and the current (reduced) levels of forest. [231] Control International, national and subnational policies An incomplete concept of a framework of policy mix sequencing for zero-deforestation governance. Non-intervention in processes related to beef production via policies may be a main driver of tropical deforestation. Further information: Sustainable development and Universal basic income in Brazil Policies for forest protection include information and education programs, economic measures to increase revenue returns from authorized activities and measures to increase effectiveness of "forest technicians and forest managers". [232] Poverty and agricultural rent were found to be principal factors leading to deforestation. [233] Contemporary domestic and foreign political decision-makers could possibly create and implement policies whose outcomes ensure that economic activities in critical forests are consistent with their scientifically ascribed value for ecosystem services , climate change mitigation and other purposes. Such policies may use and organize the development of complementary technical and economic means – including for lower levels of beef production, sales and consumption (which would also have major benefits for climate change mitigation ), [234] [235] higher levels of specified other economic activities in such areas (such as reforestation, forest protection, sustainable agriculture for specific classes of food products and quaternary work in general), product information requirements , practice- and product-certifications and eco-tariffs , along with the required monitoring and traceability . Inducing the creation and enforcement of such policies could, for instance, achieve a global phase-out of deforestation-associated beef . [236] [237] [238] [ additional citation(s) needed ] With complex polycentric governance measures, goals like sufficient climate change mitigation as decided with e.g. the Paris Agreement and a stoppage of deforestation by 2030 as decided at the 2021 United Nations Climate Change Conference could be achieved. [239] A study has suggested higher income nations need to reduce imports of tropical forest-related products and help with theoretically forest-related socioeconomic development. Proactive government policies and international forest policies "revisit[ing] and redesign[ing] global forest trade" are needed as well. [240] [241] In 2022 the European parliament approved a bill aiming to stop the import linked with deforestation. This EU Deforestation Regulation (EUDR), may cause to Brazil, for example, to stop deforestation for agricultural production and begun to "increase productivity on existing agricultural land". [242] The legislation was adopted with some changes by the European Council in May 2023 and is expected to enter into force several weeks after. The bill requires companies who want to import certain types of products to the European Union to prove the production of those commodities is not linked to areas deforested after 31 of December 2020. It prohibits also import of products linked with Human rights abuse. The list of products includes: palm oil , cattle , wood , coffee , cocoa , rubber and soy . Some derivatives of those products are also included: chocolate , furniture , printed paper and several palm oil based derivates. [243] [244] But unfortunately, as the report Bankrolling ecosystem destruction shows [245] , this regulation of product imports is not enough. The European financial sector is investing billions of euros in the destruction of nature. Banks do not respond positively to requests to stop this, which is why the report calls for European regulation in this area to be tightened and for banks to be banned from continuing to finance deforestation [246] . International pledges In 2014, about 40 countries signed the New York Declaration on Forests , a voluntary pledge to halve deforestation by 2020 and end it by 2030. The agreement was not legally binding, however, and some key countries, such as Brazil, China, and Russia, did not sign onto it. As a result, the effort failed, and deforestation increased from 2014 to 2020. [247] [248] In November 2021, 141 countries (with around 85% of the world's primary tropical forests and 90% of global tree cover ) agreed at the COP26 climate summit in Glasgow to the Glasgow Leaders' Declaration on Forests and Land Use, a pledge to end and reverse deforestation by 2030. [248] [249] [250] The agreement was accompanied by about $19.2 billion in associated funding commitments. [249] The 2021 Glasgow agreement improved on the New York Declaration by now including Brazil and many other countries that did not sign the 2014 agreement. [248] [249] Some key nations with high rates of deforestation (including Malaysia, Cambodia, Laos, Paraguay, and Myanmar) have not signed the Glasgow Declaration. [249] Like the earlier agreement, the Glasgow Leaders' Declaration was entered into outside the UN Framework Convention on Climate Change and is thus not legally binding. [249] In November 2021, the EU executive outlined a draft law requiring companies to prove that the agricultural commodities beef, wood, palm oil, soy, coffee and cocoa destined for the EU's 450 million consumers were not linked to deforestation. [251] In September 2022, the EU Parliament supported and strengthened the plan from the EU’s executive with 453 votes to 57. [252] In 2018 the biggest palm oil trader, Wilmar, decided to control its suppliers to avoid deforestation [253] [ additional citation(s) needed ] In 2021, over 100 world leaders, representing countries containing more than 85% of the world's forests, committed to halt and reverse deforestation and land degradation by 2030. [254] Land rights Transferring land rights to indigenous inhabitants is argued to efficiently conserve forests. Indigenous communities have long been the frontline of resistance against deforestation. [255] Transferring rights over land from public domain to its indigenous inhabitants is argued to be a cost-effective strategy to conserve forests. [256] This includes the protection of such rights entitled in existing laws, such as India's Forest Rights Act . [256] The transferring of such rights in China , perhaps the largest land reform in modern times, has been argued to have increased forest cover. [257] In Brazil , forested areas given tenure to indigenous groups have even lower rates of clearing than national parks . [257] Community concessions in the Congolian rainforests have significantly less deforestation as communities are incentivized to manage the land sustainably, even reducing poverty. [258] Forest management Efforts to stop or slow deforestation have been attempted for many centuries because it has long been known that deforestation can cause environmental damage sufficient in some cases to cause societies to collapse. In Tonga , paramount rulers developed policies designed to prevent conflicts between short-term gains from converting forest to farmland and long-term problems forest loss would cause, [259] while during the 17th and 18th centuries in Tokugawa , Japan, [260] the shōguns developed a highly sophisticated system of long-term planning to stop and even reverse deforestation of the preceding centuries through substituting timber by other products and more efficient use of land that had been farmed for many centuries. In 16th-century Germany, landowners also developed silviculture to deal with the problem of deforestation. However, these policies tend to be limited to environments with good rainfall, no dry season and very young soils (through volcanism or glaciation ). This is because on older and less fertile soils trees grow too slowly for silviculture to be economic, whilst in areas with a strong dry season there is always a risk of forest fires destroying a tree crop before it matures. In the areas where " slash-and-burn " is practiced, switching to " slash-and-char " would prevent the rapid deforestation and subsequent degradation of soils. The biochar thus created, given back to the soil, is not only a durable carbon sequestration method, but it also is an extremely beneficial amendment to the soil. Mixed with biomass it brings the creation of terra preta , one of the richest soils on the planet and the only one known to regenerate itself. Sustainable forest management Further information: Sustainable forest management Bamboo is advocated as a more sustainable alternative for cutting down wood for fuel. [261] Certification, as provided by global certification systems such as Programme for the Endorsement of Forest Certification and Forest Stewardship Council , contributes to tackling deforestation by creating market demand for timber from sustainably managed forests. According to the United Nations Food and Agriculture Organization (FAO), "A major condition for the adoption of sustainable forest management is a demand for products that are produced sustainably and consumer willingness to pay for the higher costs entailed. [...] By promoting the positive attributes of forest products from sustainably managed forests, certification focuses on the demand side of environmental conservation." [262] Financial compensations for reducing emissions from deforestation Main article: Reducing emissions from deforestation and forest degradation Reducing emissions from deforestation and forest degradation (REDD) in developing countries has emerged as a new potential to complement ongoing climate policies. The idea consists in providing financial compensations for the reduction of greenhouse gas (GHG) emissions from deforestation and forest degradation". [263] REDD can be seen as an alternative to the emissions trading system as in the latter, polluters must pay for permits for the right to emit certain pollutants (i.e. CO2). Main international organizations including the United Nations and the World Bank, have begun to develop programs aimed at curbing deforestation. The blanket term Reducing Emissions from Deforestation and Forest Degradation (REDD) describes these sorts of programs, which use direct monetary or other incentives to encourage developing countries to limit and/or roll back deforestation. Funding has been an issue, but at the UN Framework Convention on Climate Change (UNFCCC) Conference of the Parties-15 (COP-15) in Copenhagen in December 2009, an accord was reached with a collective commitment by developed countries for new and additional resources, including forestry and investments through international institutions, that will approach US$30 billion for the period 2010–2012. [264] Significant work is underway on tools for use in monitoring developing countries' adherence to their agreed REDD targets. These tools, which rely on remote forest monitoring using satellite imagery and other data sources, include the Center for Global Development 's FORMA (Forest Monitoring for Action) initiative [265] and the Group on Earth Observations ' Forest Carbon Tracking Portal. [266] Methodological guidance for forest monitoring was also emphasized at COP-15. [267] The environmental organization Avoided Deforestation Partners leads the campaign for development of REDD through funding from the U.S. government. [268] History Further information: Timeline of environmental history Prehistory The Carboniferous Rainforest Collapse [181] was an event that occurred 300 million years ago. Climate change devastated tropical rainforests causing the extinction of many plant and animal species. The change was abrupt, specifically, at this time climate became cooler and drier, conditions that are not favorable to the growth of rainforests and much of the biodiversity within them. Rainforests were fragmented forming shrinking 'islands' further and further apart. Populations such as the sub class Lissamphibia were devastated, whereas Reptilia survived the collapse. The surviving organisms were better adapted to the drier environment left behind and served as legacies in succession after the collapse. [36] [ self-published source? ] An array of Neolithic artifacts, including bracelets, ax heads, chisels, and polishing tools. Rainforests once covered 14% of the earth's land surface; now they cover a mere 6% and experts estimate that the last remaining rainforests could be consumed in less than 40 years. [269] Small scale deforestation was practiced by some societies for tens of thousands of years before the beginnings of civilization. [270] The first evidence of deforestation appears in the Mesolithic period . [271] It was probably used to convert closed forests into more open ecosystems favourable to game animals. [270] With the advent of agriculture, larger areas began to be deforested, and fire became the prime tool to clear land for crops. In Europe there is little solid evidence before 7000 BC. Mesolithic foragers used fire to create openings for red deer and wild boar . In Great Britain, shade-tolerant species such as oak and ash are replaced in the pollen record by hazels , brambles, grasses and nettles. Removal of the forests led to decreased transpiration , resulting in the formation of upland peat bogs . Widespread decrease in elm pollen across Europe between 8400 and 8300 BC and 7200–7000 BC, starting in southern Europe and gradually moving north to Great Britain, may represent land clearing by fire at the onset of Neolithic agriculture. The Neolithic period saw extensive deforestation for farming land . [272] [273] Stone axes were being made from about 3000 BC not just from flint, but from a wide variety of hard rocks from across Britain and North America as well. They include the noted Langdale axe industry in the English Lake District , quarries developed at Penmaenmawr in North Wales and numerous other locations. Rough-outs were made locally near the quarries, and some were polished locally to give a fine finish. This step not only increased the mechanical strength of the axe, but also made penetration of wood easier. Flint was still used from sources such as Grimes Graves but from many other mines across Europe. Evidence of deforestation has been found in Minoan Crete ; for example the environs of the Palace of Knossos were severely deforested in the Bronze Age . [274] Pre-industrial history Easter Island , deforested. Just as archaeologists have shown that prehistoric farming societies had to cut or burn forests before planting, documents and artifacts from early civilizations often reveal histories of deforestation. Some of the most dramatic are eighth century BCE Assyrian reliefs depicting logs being floated downstream from conquered areas to the less forested capital region as spoils of war. Ancient Chinese texts make clear that some areas of the Yellow River valley had already destroyed many of their forests over 2000 years ago and had to plant trees as crops or import them from long distances. [275] In South China much of the land came to be privately owned and used for the commercial growing of timber. [276] Three regional studies of historic erosion and alluviation in ancient Greece found that, wherever adequate evidence exists, a major phase of erosion follows the introduction of farming in the various regions of Greece by about 500–1,000 years, ranging from the later Neolithic to the Early Bronze Age. [277] The thousand years following the mid-first millennium BC saw serious, intermittent pulses of soil erosion in numerous places. The historic silting of ports along the southern coasts of Asia Minor (e.g. Clarus , and the examples of Ephesus , Priene and Miletus , where harbors had to be abandoned because of the silt deposited by the Meander) and in coastal Syria during the last centuries BC. [278] [279] Easter Island has suffered from heavy soil erosion in recent centuries, aggravated by agriculture and deforestation. [280] The disappearance of the island's trees seems to coincide with a decline of its civilization around the 17th and 18th century. Scholars have attributed the collapse to deforestation and over-exploitation of all resources. [281] [282] The famous silting up of the harbor for Bruges , which moved port commerce to Antwerp , also followed a period of increased settlement growth (and apparently of deforestation) in the upper river basins. In early medieval Riez in upper Provence , alluvial silt from two small rivers raised the riverbeds and widened the floodplain, which slowly buried the Roman settlement in alluvium and gradually moved new construction to higher ground; concurrently the headwater valleys above Riez were being opened to pasturage. [283] A typical progress trap was that cities were often built in a forested area, which would provide wood for some industry (for example, construction, shipbuilding, pottery). When deforestation occurs without proper replanting, however; local wood supplies become difficult to obtain near enough to remain competitive, leading to the city's abandonment, as happened repeatedly in Ancient Asia Minor . Because of fuel needs, mining and metallurgy often led to deforestation and city abandonment. [284] Deforestation of Brazil's Atlantic Forest c. 1820–1825 With most of the population remaining active in (or indirectly dependent on) the agricultural sector, the main pressure in most areas remained land clearing for crop and cattle farming. Enough wild green was usually left standing (and partially used, for example, to collect firewood, timber and fruits, or to graze pigs) for wildlife to remain viable. The elite's (nobility and higher clergy) protection of their own hunting privileges and game often protected significant woodland. [285] Major parts in the spread (and thus more durable growth) of the population were played by monastical 'pioneering' (especially by the Benedictine and Commercial orders) and some feudal lords' recruiting farmers to settle (and become tax payers) by offering relatively good legal and fiscal conditions. Even when speculators sought to encourage towns, settlers needed an agricultural belt around or sometimes within defensive walls. When populations were quickly decreased by causes such as the Black Death , the colonization of the Americas , [286] or devastating warfare (for example, Genghis Khan 's Mongol hordes in eastern and central Europe, Thirty Years' War in Germany), this could lead to settlements being abandoned. The land was reclaimed by nature, but the secondary forests usually lacked the original biodiversity . The Mongol invasions and conquests alone resulted in the reduction of 700 million tons of carbon from the atmosphere by enabling the re-growth of carbon-absorbing forests on depopulated lands over a significant period of time. [287] [288] Deforestation in Suriname c. 1880–1900 From 1100 to 1500 AD, significant deforestation took place in Western Europe as a result of the expanding human population . [289] The large-scale building of wooden sailing ships by European (coastal) naval owners since the 15th century for exploration, colonisation , slave trade , and other trade on the high seas, consumed many forest resources and became responsible for the introduction of numerous bubonic plague outbreaks in the 14th century. Piracy also contributed to the over harvesting of forests, as in Spain. This led to a weakening of the domestic economy after Columbus' discovery of America, as the economy became dependent on colonial activities (plundering, mining, cattle, plantations, trade, etc.) [285] The massive use of charcoal on an industrial scale in Early Modern Europe was a new type of consumption of western forests. [290] Each of Nelson's Royal Navy war ships at Trafalgar (1805) required 6,000 mature oaks for its construction.[ citation needed ] In France, Colbert planted oak forests to supply the French navy in the future. When the oak plantations matured in the mid-19th century, the masts were no longer required because shipping had changed.[ citation needed ] 19th and 20th centuries See also: Deforestation by continent Steamboats In the 19th century, introduction of steamboats in the United States was the cause of deforestation of banks of major rivers, such as the Mississippi River , with increased and more severe flooding one of the environmental results. The steamboat crews cut wood every day from the riverbanks to fuel the steam engines. Between St. Louis and the confluence with the Ohio River to the south, the Mississippi became more wide and shallow, and changed its channel laterally. Attempts to improve navigation by the use of snag pullers often resulted in crews' clearing large trees 100 to 200 feet (61 m) back from the banks. Several French colonial towns of the Illinois Country , such as Kaskaskia , Cahokia and St. Philippe, Illinois , were flooded and abandoned in the late 19th century, with a loss to the cultural record of their archeology . [291] Society and culture Different cultures of different places in the world have different interpretations of the actions of the cutting down of trees. For example, in Meitei mythology and Meitei folklore of Manipur (India), deforestation is mentioned as one of the reasons to make mother nature weep and mourn for the death of her precious children. [292] [293] [294] See also
Toggle the table of contents Desertification From Wikipedia, the free encyclopedia Process by which fertile areas of land become increasingly arid Not to be confused with Decertification or Desertion . World map from 1998 showing global desertification vulnerability Desertification is a type of gradual land degradation of fertile land into arid desert due to a combination of natural processes and human activities. [1] This spread of arid areas is caused by a variety of factors, such as overexploitation of soil as a result of human activity and the effects of climate change . [2] [3] Geographic areas most affected are located in Africa ( Sahel region), Asia ( Gobi Desert and Mongolia ) and parts of South America . Drylands occupy approximately 40–41% of Earth's land area and are home to more than 2 billion people. [4] Effects of desertification include sand and dust storms , food insecurity , and poverty . Humans can fight desertification in various ways. For instance, improving soil quality , greening deserts , managing grazing better, and planting trees ( reforestation and afforestation ) can all help reverse desertification. Throughout geological history, the development of deserts has occurred naturally over long intervals of time. [5] The modern study of desertification emerged from the study of the 1980s drought in the Sahel . [6] Definitions[ edit ] As recently as 2005, considerable controversy existed over the proper definition of the term "desertification." Helmut Geist (2005) identified more than 100 formal definitions. [7] The most widely accepted of these was that of the Princeton University Dictionary which defined it as "the process of fertile land transforming into desert typically as a result of deforestation , drought or improper/inappropriate agriculture".[ citation needed ] This definition clearly demonstrated the interconnectedness of desertification and human activities, in particular land use and land management practices. It also highlighted the economic, social and environmental implications of desertification.However, this original understanding that desertification involved the physical expansion of deserts has been rejected as the concept has further evolved since then. [8] Desertification has been defined in the text of the United Nations Convention to Combat Desertification (UNCCD) as "land degradation in arid, semi-arid and dry sub-humid regions resulting from various factors, including climatic variations and human activities according to Hulme and Kelly, (1993)." [9] There exists also controversy around the sub-grouping of types of desertification, including, for example, the validity and usefulness of such terms as "man-made desert" and "non-pattern desert". [10] Geographic areas affected[ edit ] Drylands occupy approximately 40–41% of Earth's land area and are home to more than 2 billion people. [11] [4] It has been estimated that some 10–20% of drylands are already degraded, the total area affected by desertification being between 6 and 12 million square kilometers, that about 1–6% of the inhabitants of drylands live in desertified areas, and that a billion people are under threat from further desertification. [12] [13] Further information: Sahel § Desertification and soil loss The impact of climate change and human activities on desertification are exemplified in the Sahel region of Africa. The region is characterized by a dry hot climate, high temperatures and low rainfall (100–600 mm per year). [14] So, droughts are the rule in the Sahel region. [15] The Sahel has lost approximately 650,000 km2 of its productive agricultural land over the past 50 years; [16] the propagation of desertification in this area is considerable. [17] [18] Sahel region of Mali The climate of the Sahara has undergone enormous variations over the last few hundred thousand years, [19] oscillating between wet (grassland) and dry (desert) every 20,000 years [20] (a phenomenon believed to be caused by long-term changes in the North African climate cycle that alters the path of the North African Monsoon , caused by an approximately 40,000-year cycle in which the axial tilt of the earth changes between 22° and 24.5°). [21] Some statistics have shown that, since 1900, the Sahara has expanded by 250 km to the south over a stretch of land from west to east 6,000 km long. [22] Lake Chad , located in the Sahel region, has undergone desiccation due to water withdrawal for irrigation and decrease in rainfall. [23] The lake has shrunk by over 90% since 1987, displacing millions of inhabitants. [24] Recent efforts have managed to make some progress toward its restoration, but it is still considered to be at risk of disappearing entirely. [25] To limit desertification the Great Green Wall (Africa) initiative was started in 2007 involving the planting of vegetation along a stretch of 7,775 kms, 15 kms wide, involving 22 countries to 2030. [26] The purpose of this mammoth planting initiative is to enhance retention of water in the ground following the seasonal rainfall, thus promoting land rehabilitation and future agriculture. Senegal has already contributed to the project by planting 50,000 acres of trees. It is said to have improved land quality and caused an increase in economic opportunity in the region. [27] Gobi Desert and Mongolia[ edit ] See also: Environmental issues in Mongolia § Desertification Another major area that is being impacted by desertification is the Gobi Desert located in Northern China and Southern Mongolia. The Gobi Desert is the fastest expanding desert on Earth, as it transforms over 3,600 square kilometres (1,400 square miles) of grassland into wasteland annually. [28] Although the Gobi Desert itself is still a distance away from Beijing , reports from field studies state there are large sand dunes forming only 70 km (43.5 mi) outside the city. [29] [30] In Mongolia , around 90% of grassland is considered vulnerable to desertification by the UN. An estimated 13% of desertification in Mongolia is caused by natural factors; the rest is due to human influence particularly overgrazing and increased erosion of soils in cultivated areas. [31] [32] During the period 1940 to 2015, the mean air temperature increased by 2.24 °C. [33] The warmest ten-year period was during the latest decade to 2021. Precipitation has decreased by 7% over this period resulting in increased arid conditions throughout Mongolia. The Gobi desert continues to expand northward, with over 70% of Mongolia's land degraded through overgrazing, deforestation, and climate change. [34] In addition, the Mongolia government has listed forest fires , blights , unsustainable forestry and mining activities as leading causes of desertification in the country. [35] The transition from sheep to goat farming in order to meet export demands for cashmere wool has caused degradation of grazing lands. Compared to sheep, goats do more damage to grazing lands by eating roots and flowers. [36] This section is an excerpt from Gobi Desert § Desertification .[ edit ] The Gobi Desert is expanding through desertification, most rapidly on the southern edge into China, which is seeing 3,600 km2 (1,390 sq mi) of grassland overtaken every year. Dust storms increased in frequency between 1996 and 2016, causing further damage to China's agriculture economy. However, in some areas desertification has been slowed or reversed. [37] The northern and eastern boundaries between desert and grassland are constantly changing. This is mostly due to the climate conditions before the growing season, which influence the rate of evapotranspiration and subsequent plant growth. [38] The expansion of the Gobi is attributed mostly to human activities, locally driven by deforestation , overgrazing , and depletion of water resources, as well as to climate change . [37] China has tried various plans to slow the expansion of the desert, which have met with some success. [39] The Three-North Shelter Forest Program (or "Green Great Wall") is a Chinese government tree-planting project begun in 1978 and set to continue through 2050. The goal of the program is to reverse desertification by planting aspen and other fast-growing trees on some 36.5 million hectares across some 551 counties in 12 provinces of northern China. [40] [41] South America[ edit ] South America is another area vulnerable by desertification, as 25% of the land is classified as drylands [42] and over 68% of the land area has undergone soil erosion as a result of deforestation and overgrazing. [43] 27 to 43% of the land areas in Bolivia, Chile, Ecuador and Peru are at risk due to desertification. In Argentina, Mexico and Paraguay, greater than half the land area is degraded by desertification and cannot be used for agriculture. In Central America, drought has caused increased unemployment and decreased food security - also causing migration of people. Similar impacts have been seen in rural parts of Mexico where about 1,000 square kms of land have been lost yearly due to desertification. [43] In Argentina , desertification has the potential to disrupt the nation's food supply. [44] See also: Deforestation § Causes Preventing man-made overgrazing Goats inside of a pen in Norte Chico, Chile. Overgrazing of drylands by poorly managed traditional herding is one of the primary causes of desertification. Wildebeest in Masai Mara during the Great Migration. Overgrazing is not necessarily caused by nomadic grazers in large travelling herd populations. [45] [46] Immediate causes[ edit ] The immediate cause of desertification is the loss of most vegetation. This is driven by a number of factors, alone or in combination, such as drought, climatic shifts, tillage for agriculture, overgrazing and deforestation for fuel or construction materials. Though vegetation plays a major role in determining the biological composition of the soil , studies have shown that, in many environments, the rate of erosion and runoff decreases exponentially with increased vegetation cover. [47] Unprotected, dry soil surfaces blow away with the wind or are washed away by flash floods, leaving infertile lower soil layers that bake in the sun and become an unproductive hardpan. Influence of human activities[ edit ] Early studies argued one of the most common causes of desertification was overgrazing, over consumption of vegetation by cattle or other livestock. [48] However, the role of local overexploitation in driving desertification in the recent past is controversial. [6] Drought in the Sahel region is now thought to be principally the result of seasonal variability in rainfall caused by large-scale sea surface temperature variations, largely driven by natural variability and anthropogenic emissions of aerosols (reflective sulphate particles ) and greenhouse gases. [49] As a result, changing ocean temperature and reductions in sulfate emissions have caused a re-greening of the region. [49] This has led some scholars to argue that agriculture-induced vegetation loss is a minor factor in desertification. [6] A shepherd guiding his sheep through the high desert outside Marrakech , Morocco Human population dynamics have a considerable impact on overgrazing, over-farming and deforestation, as previously acceptable techniques have become unsustainable. [50] There are multiple reasons farmers use intensive farming as opposed to extensive farming but the main reason is to maximize yields. [51] By increasing productivity, they require a lot more fertilizer, pesticides, and labor to upkeep machinery. This continuous use of the land rapidly depletes the nutrients of the soil causing desertification to spread. [52] [53] Natural variations[ edit ] Scientists agree that the existence of a desert in the place where the Sahara desert is now located is due to natural variations in solar insolation due to orbital precession of the Earth. [54] Such variations influence the strength of the West African Monsoon, inducing feedback in vegetation and dust emission that amplify the cycle of wet and dry Sahara climate. [55] There is also a suggestion the transition of the Sahara from savanna to desert during the mid- Holocene was partially due to overgrazing by the cattle of the local population. [56]
Toggle the table of contents Ecosystem From Wikipedia, the free encyclopedia Community of living organisms together with the nonliving components of their environment For other uses, see Ecosystem (disambiguation) . "Biosystem" redirects here. For the journal, see BioSystems . Left: Coral reef ecosystems are highly productive marine systems. [1] Right: Temperate rainforest , a terrestrial ecosystem . Part of a series on e An ecosystem (or ecological system) is a system that environments and their organisms form through their interaction. [2] : 458 The biotic and abiotic components are linked together through nutrient cycles and energy flows. Ecosystems are controlled by external and internal factors . External factors such as climate , parent material which forms the soil and topography , control the overall structure of an ecosystem but are not themselves influenced by the ecosystem. Internal factors are controlled, for example, by decomposition , root competition, shading, disturbance, succession, and the types of species present. While the resource inputs are generally controlled by external processes, the availability of these resources within the ecosystem is controlled by internal factors. Therefore, internal factors not only control ecosystem processes but are also controlled by them. Ecosystems are dynamic entities—they are subject to periodic disturbances and are always in the process of recovering from some past disturbance. The tendency of an ecosystem to remain close to its equilibrium state, despite that disturbance, is termed its resistance . The capacity of a system to absorb disturbance and reorganize while undergoing change so as to retain essentially the same function, structure, identity, and feedbacks is termed its ecological resilience . Ecosystems can be studied through a variety of approaches—theoretical studies, studies monitoring specific ecosystems over long periods of time, those that look at differences between ecosystems to elucidate how they work and direct manipulative experimentation. Biomes are general classes or categories of ecosystems. However, there is no clear distinction between biomes and ecosystems. Ecosystem classifications are specific kinds of ecological classifications that consider all four elements of the definition of ecosystems : a biotic component, an abiotic complex, the interactions between and within them, and the physical space they occupy. Biotic factors of the ecosystem are living things; such as plants, animals, and bacteria, while abiotic are non-living components; such as water, soil and atmosphere. Plants allow energy to enter the system through photosynthesis ,  building up plant tissue. Animals play an important role in the movement of matter and energy through the system, by feeding on plants and on one another. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter , decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and microbes. Ecosystems provide a variety of goods and services upon which people depend, and may be part of. Ecosystem goods include the "tangible, material products" of ecosystem processes such as water, food, fuel, construction material, and medicinal plants . Ecosystem services , on the other hand, are generally "improvements in the condition or location of things of value". These include things like the maintenance of hydrological cycles , cleaning air and water, the maintenance of oxygen in the atmosphere, crop pollination and even things like beauty, inspiration and opportunities for research. Many ecosystems become degraded through human impacts, such as soil loss , air and water pollution , habitat fragmentation , water diversion , fire suppression , and introduced species and invasive species . These threats can lead to abrupt transformation of the ecosystem or to gradual disruption of biotic processes and degradation of abiotic conditions of the ecosystem. Once the original ecosystem has lost its defining features, it is considered "collapsed ". Ecosystem restoration can contribute to achieving the Sustainable Development Goals . Definition An ecosystem (or ecological system) consists of all the organisms and the abiotic pools (or physical environment) with which they interact. [3] [4] : 5 [2] : 458 The biotic and abiotic components are linked together through nutrient cycles and energy flows. [5] "Ecosystem processes" are the transfers of energy and materials from one pool to another. [2] : 458 Ecosystem processes are known to "take place at a wide range of scales". Therefore, the correct scale of study depends on the question asked. [4] : 5 Origin and development of the term The term "ecosystem" was first used in 1935 in a publication by British ecologist Arthur Tansley . The term was coined by Arthur Roy Clapham , who came up with the word at Tansley's request. [6] Tansley devised the concept to draw attention to the importance of transfers of materials between organisms and their environment. [4] : 9 He later refined the term, describing it as "The whole system, ... including not only the organism-complex, but also the whole complex of physical factors forming what we call the environment". [3] Tansley regarded ecosystems not simply as natural units, but as "mental isolates". [3] Tansley later defined the spatial extent of ecosystems using the term " ecotope ". [7] G. Evelyn Hutchinson , a limnologist who was a contemporary of Tansley's, combined Charles Elton 's ideas about trophic ecology with those of Russian geochemist Vladimir Vernadsky . As a result, he suggested that mineral nutrient availability in a lake limited algal production . This would, in turn, limit the abundance of animals that feed on algae. Raymond Lindeman took these ideas further to suggest that the flow of energy through a lake was the primary driver of the ecosystem. Hutchinson's students, brothers Howard T. Odum and Eugene P. Odum , further developed a "systems approach" to the study of ecosystems. This allowed them to study the flow of energy and material through ecological systems. [4] : 9 Processes Flora of Baja California desert , Cataviña region, Mexico External and internal factors Ecosystems are controlled by both external and internal factors. External factors, also called state factors, control the overall structure of an ecosystem and the way things work within it, but are not themselves influenced by the ecosystem. On broad geographic scales, climate is the factor that "most strongly determines ecosystem processes and structure". [4] : 14 Climate determines the biome in which the ecosystem is embedded. Rainfall patterns and seasonal temperatures influence photosynthesis and thereby determine the amount of energy available to the ecosystem. [8] : 145 Parent material determines the nature of the soil in an ecosystem, and influences the supply of mineral nutrients. Topography also controls ecosystem processes by affecting things like microclimate , soil development and the movement of water through a system. For example, ecosystems can be quite different if situated in a small depression on the landscape, versus one present on an adjacent steep hillside. [9] : 39 [10] : 66 Other external factors that play an important role in ecosystem functioning include time and potential biota , the organisms that are present in a region and could potentially occupy a particular site. Ecosystems in similar environments that are located in different parts of the world can end up doing things very differently simply because they have different pools of species present. [11] : 321 The introduction of non-native species can cause substantial shifts in ecosystem function. [12] Unlike external factors, internal factors in ecosystems not only control ecosystem processes but are also controlled by them. [4] : 16 While the resource inputs are generally controlled by external processes like climate and parent material, the availability of these resources within the ecosystem is controlled by internal factors like decomposition, root competition or shading. [13] Other factors like disturbance, succession or the types of species present are also internal factors. Primary production Global oceanic and terrestrial phototroph abundance, from September 1997 to August 2000. As an estimate of autotroph biomass, it is only a rough indicator of primary production potential and not an actual estimate of it. Main article: Primary production Primary production is the production of organic matter from inorganic carbon sources. This mainly occurs through photosynthesis . The energy incorporated through this process supports life on earth, while the carbon makes up much of the organic matter in living and dead biomass, soil carbon and fossil fuels . It also drives the carbon cycle , which influences global climate via the greenhouse effect . Through the process of photosynthesis, plants capture energy from light and use it to combine carbon dioxide and water to produce carbohydrates and oxygen . The photosynthesis carried out by all the plants in an ecosystem is called the gross primary production (GPP). [8] : 124 About half of the gross GPP is respired by plants in order to provide the energy that supports their growth and maintenance. [14] : 157 The remainder, that portion of GPP that is not used up by respiration, is known as the net primary production (NPP). [14] : 157 Total photosynthesis is limited by a range of environmental factors. These include the amount of light available, the amount of leaf area a plant has to capture light (shading by other plants is a major limitation of photosynthesis), the rate at which carbon dioxide can be supplied to the chloroplasts to support photosynthesis, the availability of water, and the availability of suitable temperatures for carrying out photosynthesis. [8] : 155 Energy flow See also: Food web and Trophic level Energy and carbon enter ecosystems through photosynthesis, are incorporated into living tissue, transferred to other organisms that feed on the living and dead plant matter, and eventually released through respiration. [14] : 157 The carbon and energy incorporated into plant tissues (net primary production) is either consumed by animals while the plant is alive, or it remains uneaten when the plant tissue dies and becomes detritus . In terrestrial ecosystems , the vast majority of the net primary production ends up being broken down by decomposers . The remainder is consumed by animals while still alive and enters the plant-based trophic system. After plants and animals die, the organic matter contained in them enters the detritus-based trophic system. [15] Ecosystem respiration is the sum of respiration by all living organisms (plants, animals, and decomposers) in the ecosystem. [16] Net ecosystem production is the difference between gross primary production (GPP) and ecosystem respiration. [17] In the absence of disturbance, net ecosystem production is equivalent to the net carbon accumulation in the ecosystem. Energy can also be released from an ecosystem through disturbances such as wildfire or transferred to other ecosystems (e.g., from a forest to a stream to a lake) by erosion . In aquatic systems , the proportion of plant biomass that gets consumed by herbivores is much higher than in terrestrial systems. [15] In trophic systems, photosynthetic organisms are the primary producers. The organisms that consume their tissues are called primary consumers or secondary producers — herbivores . Organisms which feed on microbes ( bacteria and fungi ) are termed microbivores . Animals that feed on primary consumers— carnivores —are secondary consumers. Each of these constitutes a trophic level. [15] The sequence of consumption—from plant to herbivore, to carnivore—forms a food chain . Real systems are much more complex than this—organisms will generally feed on more than one form of food, and may feed at more than one trophic level. Carnivores may capture some prey that is part of a plant-based trophic system and others that are part of a detritus-based trophic system (a bird that feeds both on herbivorous grasshoppers and earthworms, which consume detritus). Real systems, with all these complexities, form food webs rather than food chains. [15] Decomposition See also: Decomposition Sequence of a decomposing pig carcass over time The carbon and nutrients in dead organic matter are broken down by a group of processes known as decomposition. This releases nutrients that can then be re-used for plant and microbial production and returns carbon dioxide to the atmosphere (or water) where it can be used for photosynthesis. In the absence of decomposition, the dead organic matter would accumulate in an ecosystem, and nutrients and atmospheric carbon dioxide would be depleted. [18] : 183 Decomposition processes can be separated into three categories— leaching , fragmentation and chemical alteration of dead material. As water moves through dead organic matter, it dissolves and carries with it the water-soluble components. These are then taken up by organisms in the soil, react with mineral soil, or are transported beyond the confines of the ecosystem (and are considered lost to it). [19] : 271–280 Newly shed leaves and newly dead animals have high concentrations of water-soluble components and include sugars , amino acids and mineral nutrients. Leaching is more important in wet environments and less important in dry ones. [10] : 69–77 Fragmentation processes break organic material into smaller pieces, exposing new surfaces for colonization by microbes. Freshly shed leaf litter may be inaccessible due to an outer layer of cuticle or bark , and cell contents are protected by a cell wall . Newly dead animals may be covered by an exoskeleton . Fragmentation processes, which break through these protective layers, accelerate the rate of microbial decomposition. [18] : 184 Animals fragment detritus as they hunt for food, as does passage through the gut. Freeze-thaw cycles and cycles of wetting and drying also fragment dead material. [18] : 186 The chemical alteration of the dead organic matter is primarily achieved through bacterial and fungal action. Fungal hyphae produce enzymes that can break through the tough outer structures surrounding dead plant material. They also produce enzymes that break down lignin , which allows them access to both cell contents and the nitrogen in the lignin. Fungi can transfer carbon and nitrogen through their hyphal networks and thus, unlike bacteria, are not dependent solely on locally available resources. [18] : 186 Decomposition rates Decomposition rates vary among ecosystems. [20] The rate of decomposition is governed by three sets of factors—the physical environment (temperature, moisture, and soil properties), the quantity and quality of the dead material available to decomposers, and the nature of the microbial community itself. [18] : 194 Temperature controls the rate of microbial respiration; the higher the temperature, the faster the microbial decomposition occurs. Temperature also affects soil moisture, which affects decomposition. Freeze-thaw cycles also affect decomposition—freezing temperatures kill soil microorganisms, which allows leaching to play a more important role in moving nutrients around. This can be especially important as the soil thaws in the spring, creating a pulse of nutrients that become available. [19] : 280 Decomposition rates are low under very wet or very dry conditions. Decomposition rates are highest in wet, moist conditions with adequate levels of oxygen. Wet soils tend to become deficient in oxygen (this is especially true in wetlands ), which slows microbial growth. In dry soils, decomposition slows as well, but bacteria continue to grow (albeit at a slower rate) even after soils become too dry to support plant growth. [18] : 200 Dynamics and resilience Further information: Resistance (ecology) and Ecological resilience Ecosystems are dynamic entities. They are subject to periodic disturbances and are always in the process of recovering from past disturbances. [21] : 347 When a perturbation occurs, an ecosystem responds by moving away from its initial state. The tendency of an ecosystem to remain close to its equilibrium state, despite that disturbance, is termed its resistance . The capacity of a system to absorb disturbance and reorganize while undergoing change so as to retain essentially the same function, structure, identity, and feedbacks is termed its ecological resilience . [22] [23] Resilience thinking also includes humanity as an integral part of the biosphere where we are dependent on ecosystem services for our survival and must build and maintain their natural capacities to withstand shocks and disturbances. [24] Time plays a central role over a wide range, for example, in the slow development of soil from bare rock and the faster recovery of a community from disturbance . [14] : 67 Disturbance also plays an important role in ecological processes. F. Stuart Chapin and coauthors define disturbance as "a relatively discrete event in time that removes plant biomass". [21] : 346 This can range from herbivore outbreaks, treefalls, fires, hurricanes, floods, glacial advances , to volcanic eruptions . Such disturbances can cause large changes in plant, animal and microbe populations, as well as soil organic matter content. Disturbance is followed by succession, a "directional change in ecosystem structure and functioning resulting from biotically driven changes in resource supply." [2] : 470 The frequency and severity of disturbance determine the way it affects ecosystem function. A major disturbance like a volcanic eruption or glacial advance and retreat leave behind soils that lack plants, animals or organic matter. Ecosystems that experience such disturbances undergo primary succession . A less severe disturbance like forest fires, hurricanes or cultivation result in secondary succession and a faster recovery. [21] : 348 More severe and more frequent disturbance result in longer recovery times. From one year to another, ecosystems experience variation in their biotic and abiotic environments. A drought , a colder than usual winter, and a pest outbreak all are short-term variability in environmental conditions. Animal populations vary from year to year, building up during resource-rich periods and crashing as they overshoot their food supply. Longer-term changes also shape ecosystem processes. For example, the forests of eastern North America still show legacies of cultivation which ceased in 1850 when large areas were reverted to forests. [21] : 340 Another example is the methane production in eastern Siberian lakes that is controlled by organic matter which accumulated during the Pleistocene . [25] A freshwater lake in Gran Canaria , an island of the Canary Islands . Clear boundaries make lakes convenient to study using an ecosystem approach . Nutrient cycling See also: Nutrient cycle , Biogeochemical cycle , and Nitrogen cycle Biological nitrogen cycling Ecosystems continually exchange energy and carbon with the wider environment . Mineral nutrients, on the other hand, are mostly cycled back and forth between plants, animals, microbes and the soil. Most nitrogen enters ecosystems through biological nitrogen fixation , is deposited through precipitation, dust, gases or is applied as fertilizer . [19] : 266 Most terrestrial ecosystems are nitrogen-limited in the short term making nitrogen cycling an important control on ecosystem production. [19] : 289 Over the long term, phosphorus availability can also be critical. [26] Macronutrients which are required by all plants in large quantities include the primary nutrients (which are most limiting as they are used in largest amounts): Nitrogen, phosphorus, potassium. [27] : 231 Secondary major nutrients (less often limiting) include: Calcium, magnesium, sulfur. Micronutrients required by all plants in small quantities include boron, chloride, copper, iron, manganese, molybdenum, zinc. Finally, there are also beneficial nutrients which may be required  by certain plants or by plants under specific environmental conditions: aluminum, cobalt, iodine, nickel, selenium, silicon, sodium, vanadium. [27] : 231 Until modern times, nitrogen fixation was the major source of nitrogen for ecosystems. Nitrogen-fixing bacteria either live symbiotically with plants or live freely in the soil. The energetic cost is high for plants that support nitrogen-fixing symbionts—as much as 25% of gross primary production when measured in controlled conditions. Many members of the legume plant family support nitrogen-fixing symbionts. Some cyanobacteria are also capable of nitrogen fixation. These are phototrophs , which carry out photosynthesis. Like other nitrogen-fixing bacteria, they can either be free-living or have symbiotic relationships with plants. [21] : 360 Other sources of nitrogen include acid deposition produced through the combustion of fossil fuels, ammonia gas which evaporates from agricultural fields which have had fertilizers applied to them, and dust. [19] : 270 Anthropogenic nitrogen inputs account for about 80% of all nitrogen fluxes in ecosystems. [19] : 270 When plant tissues are shed or are eaten, the nitrogen in those tissues becomes available to animals and microbes. Microbial decomposition releases nitrogen compounds from dead organic matter in the soil, where plants, fungi, and bacteria compete for it. Some soil bacteria use organic nitrogen-containing compounds as a source of carbon, and release ammonium ions into the soil. This process is known as nitrogen mineralization . Others convert ammonium to nitrite and nitrate ions, a process known as nitrification . Nitric oxide and nitrous oxide are also produced during nitrification. [19] : 277 Under nitrogen-rich and oxygen-poor conditions, nitrates and nitrites are converted to nitrogen gas , a process known as denitrification . [19] : 281 Mycorrhizal fungi which are symbiotic with plant roots, use carbohydrates supplied by the plants and in return transfer phosphorus and nitrogen compounds back to the plant roots. [28] [29] This is an important pathway of organic nitrogen transfer from dead organic matter to plants. This mechanism may contribute to more than 70 Tg of annually assimilated plant nitrogen, thereby playing a critical role in global nutrient cycling and ecosystem function. [29] Phosphorus enters ecosystems through weathering . As ecosystems age this supply diminishes, making phosphorus-limitation more common in older landscapes (especially in the tropics). [19] : 287–290 Calcium and sulfur are also produced by weathering, but acid deposition is an important source of sulfur in many ecosystems. Although magnesium and manganese are produced by weathering, exchanges between soil organic matter and living cells account for a significant portion of ecosystem fluxes. Potassium is primarily cycled between living cells and soil organic matter. [19] : 291 Function and biodiversity See also: Ecosystem diversity Loch Lomond in Scotland forms a relatively isolated ecosystem. The fish community of this lake has remained stable over a long period until a number of introductions in the 1970s restructured its food web . [30] Spiny forest at Ifaty, Madagascar , featuring various Adansonia (baobab) species, Alluaudia procera (Madagascar ocotillo) and other vegetation Biodiversity plays an important role in ecosystem functioning. [31] : 449–453 Ecosystem processes are driven by the species in an ecosystem, the nature of the individual species, and the relative abundance of organisms among these species. Ecosystem processes are the net effect of the actions of individual organisms as they interact with their environment. Ecological theory suggests that in order to coexist, species must have some level of limiting similarity —they must be different from one another in some fundamental way, otherwise, one species would competitively exclude the other. [32] Despite this, the cumulative effect of additional species in an ecosystem is not linear: additional species may enhance nitrogen retention, for example. However, beyond some level of species richness, [11] : 331 additional species may have little additive effect unless they differ substantially from species already present. [11] : 324 This is the case for example for exotic species . [11] : 321 The addition (or loss) of species that are ecologically similar to those already present in an ecosystem tends to only have a small effect on ecosystem function. Ecologically distinct species, on the other hand, have a much larger effect. Similarly, dominant species have a large effect on ecosystem function, while rare species tend to have a small effect. Keystone species tend to have an effect on ecosystem function that is disproportionate to their abundance in an ecosystem. [11] : 324 An ecosystem engineer is any organism that creates, significantly modifies, maintains or destroys a habitat . [33] Study approaches
Toggle the table of contents Biodiversity From Wikipedia, the free encyclopedia Variety and variability of life forms "Fauna and flora" redirects here. For the organization, see Fauna and Flora International . An example of the biodiversity of fungi in a forest in Northern Saskatchewan (in this photo, there are also leaf lichens and mosses ). Biodiversity or biological diversity is the variety and variability of life on Earth . Biodiversity is a measure of variation at the genetic ( genetic variability ), species ( species diversity ), and ecosystem ( ecosystem diversity ) levels. [1] Biodiversity is not distributed evenly on Earth ; it is usually greater in the tropics as a result of the warm climate and high primary productivity in the region near the equator . Tropical forest ecosystems cover less than 10% of Earth's terrestrial surface and contain about 50% of the world's species. [2] There are latitudinal gradients in species diversity for both marine and terrestrial taxa. [3] Marine coastal biodiversity is highest globally speaking in the Western Pacific ocean steered mainly by the higher surface temperatures. In all oceans across the planet, marine species diversity peaks in the mid-latitudinal zones. [4] Terrestrial species threatened with mass extinction can be observed in exceptionally dense regional biodiversity hotspots , with high levels of species endemism under threat. [5] There are 36 such hotspot regions which require the world's attention in order to secure global biodiversity. [6] [7] Since life began on Earth , five major mass extinctions and several minor events have led to large and sudden drops in biodiversity. The Phanerozoic aeon (the last 540 million years) marked a rapid growth in biodiversity via the Cambrian explosion —a period during which the majority of multicellular phyla first appeared. The next 400 million years included repeated, massive biodiversity losses classified as mass extinction events. In the Carboniferous , rainforest collapse led to a great loss of plant and animal life. The Permian–Triassic extinction event , 251 million years ago, was the worst; vertebrate recovery took 30 million years. The most recent, the Cretaceous–Paleogene extinction event , occurred 65 million years ago and has often attracted more attention than others because it resulted in the extinction of the non-avian dinosaurs . The period since the emergence of humans has displayed an ongoing biodiversity loss and an accompanying loss of genetic diversity . This process is often referred to as Holocene extinction , or sixth mass extinction. During the last century, decreases in biodiversity have been increasingly observed. It was estimated in 2007 that up to 30% of all species will be extinct by 2050. [8] Habitat destruction for the expansion of agriculture and the overexploitation of wildlife are the most significant drivers of contemporary biodiversity loss, and climate change also plays a role. [9] [10] History of the term[ edit ] 1916 – The term biological diversity was used first by J. Arthur Harris in "The Variable Desert", Scientific American : "The bare statement that the region contains a flora rich in genera and species and of diverse geographic origin or affinity is entirely inadequate as a description of its real biological diversity." [11] 1967 – Raymond F. Dasmann used the term biological diversity in reference to the richness of living nature that conservationists should protect in his book A Different Kind of Country. [12] [13] 1974 – The term natural diversity was introduced by John Terborgh . [14] 1980 – Thomas Lovejoy introduced the term biological diversity to the scientific community in a book. [15] It rapidly became commonly used. [16] 1985 – According to Edward O. Wilson , the contracted form biodiversity was coined by W. G. Rosen: "The National Forum on BioDiversity ... was conceived by Walter G.Rosen ... Dr. Rosen represented the NRC/NAS throughout the planning stages of the project. Furthermore, he introduced the term biodiversity". [17] 1985 – The term "biodiversity" appears in the article, "A New Plan to Conserve the Earth's Biota" by Laura Tangley . [18] 1988 – The term biodiversity first appeared in publication. [19] [20] 1988 to Present – The United Nations Environment Programme (UNEP) Ad Hoc Working Group of Experts on Biological Diversity in began working in November 1988, leading to the publication of the draft Convention on Biological Diversity in May 1992.  Since this time, there have been 15 Conferences of the Parties (COPs) to discuss potential global political responses to biodiversity loss.  Most recently COP 15 in Montreal, Canada in 2022. Definitions[ edit ] Biologists most often define biodiversity as the "totality of genes , species and ecosystems of a region". [21] [22] An advantage of this definition is that it presents a unified view of the traditional types of biological variety previously identified: taxonomic diversity (usually measured at the species diversity level) [23] morphological diversity (which stems from genetic diversity and molecular diversity [24] ) functional diversity (which is a measure of the number of functionally disparate species within a population (e.g. different feeding mechanism, different motility, predator vs prey, etc.) [25] ) Biodiversity is most commonly used to replace the more clearly-defined and long-established terms, species diversity and species richness . [26] Other definitions include (in chronological order): An explicit definition consistent with this interpretation was first given in a paper by Bruce A. Wilcox commissioned by the International Union for the Conservation of Nature and Natural Resources (IUCN) for the 1982 World National Parks Conference. [27] Wilcox's definition was "Biological diversity is the variety of life forms...at all levels of biological systems (i.e., molecular, organismic, population, species and ecosystem)...". [27] A publication by Wilcox in 1984: Biodiversity can be defined genetically as the diversity of alleles, genes and organisms . They study processes such as mutation and gene transfer that drive evolution. [27] The 1992 United Nations Earth Summit defined biological diversity as "the variability among living organisms from all sources, including, inter alia, terrestrial, marine and other aquatic ecosystems and the ecological complexes of which they are part: this includes diversity within species, between species and of ecosystems". [28] This definition is used in the United Nations Convention on Biological Diversity . [28] Gaston and Spicer's definition in their book "Biodiversity: an introduction" in 2004 is "variation of life at all levels of biological organization". [29] The Food and Agriculture Organization of the United Nations (FAO) defined biodiversity in 2019 as "the variability that exists among living organisms (both within and between species) and the ecosystems of which they are part." [30] Number of species[ edit ] Main article: Global biodiversity Discovered and predicted total number of species on land and in the oceans According to Mora and her colleagues' estimation, there are approximately 8.7 million terrestrial species and 2.2 million oceanic species. The authors note that these estimates are strongest for eukaryotic organisms and likely represent the lower bound of prokaryote diversity. [31] Other estimates include: 220,000 vascular plants , estimated using the species-area relation method [32] 0.7-1 million marine species [33] 10–30 million insects ; [34] (of some 0.9 million we know today) [35] 5–10 million bacteria ; [36] 1.5-3 million fungi , estimates based on data from the tropics, long-term non-tropical sites and molecular studies that have revealed cryptic speciation . [37] Some 0.075 million species of fungi had been documented by 2001; [38] 1 million mites [39] The number of microbial species is not reliably known, but the Global Ocean Sampling Expedition dramatically increased the estimates of genetic diversity by identifying an enormous number of new genes from near-surface plankton samples at various marine locations, initially over the 2004–2006 period. [40] The findings may eventually cause a significant change in the way science defines species and other taxonomic categories. [41] [42] Since the rate of extinction has increased, many extant species may become extinct before they are described. [43] Not surprisingly, in the animalia the most studied groups are birds and mammals , whereas fishes and arthropods are the least studied animals groups. [44] Current biodiversity loss[ edit ] Main article: Biodiversity loss The World Wildlife Fund's Living Planet Report 2022 found that wildlife populations declined by an average 69% since 1970. [45] [46] [47] During the last century, decreases in biodiversity have been increasingly observed. It was estimated in 2007 that up to 30% of all species will be extinct by 2050. [8] Of these, about one eighth of known plant species are threatened with extinction . [48] Estimates reach as high as 140,000 species per year (based on Species-area theory ). [49] This figure indicates unsustainable ecological practices, because few species emerge each year.[ citation needed ] The rate of species loss is greater now than at any time in human history, with extinctions occurring at rates hundreds of times higher than background extinction rates. [48] [50] [51] and expected to still grow in the upcoming years. [51] [52] [53] As of 2012, some studies suggest that 25% of all mammal species could be extinct in 20 years. [54] In absolute terms, the planet has lost 58% of its biodiversity since 1970 according to a 2016 study by the World Wildlife Fund. [55] The Living Planet Report 2014 claims that "the number of mammals, birds, reptiles, amphibians, and fish across the globe is, on average, about half the size it was 40 years ago". Of that number, 39% accounts for the terrestrial wildlife gone, 39% for the marine wildlife gone and 76% for the freshwater wildlife gone. Biodiversity took the biggest hit in Latin America , plummeting 83 percent. High-income countries showed a 10% increase in biodiversity, which was canceled out by a loss in low-income countries. This is despite the fact that high-income countries use five times the ecological resources of low-income countries, which was explained as a result of a process whereby wealthy nations are outsourcing resource depletion to poorer nations, which are suffering the greatest ecosystem losses. [56] A 2017 study published in PLOS One found that the biomass of insect life in Germany had declined by three-quarters in the last 25 years. [57] Dave Goulson of Sussex University stated that their study suggested that humans "appear to be making vast tracts of land inhospitable to most forms of life, and are currently on course for ecological Armageddon. If we lose the insects then everything is going to collapse." [58] In 2020 the World Wildlife Foundation published a report saying that "biodiversity is being destroyed at a rate unprecedented in human history". The report claims that 68% of the population of the examined species were destroyed in the years 1970 – 2016. [59] Of 70,000 monitored species, around 48% are experiencing population declines from human activity (in 2023), whereas only 3% have increasing populations. [60] [61] [62] Summary of major biodiversity-related environmental-change categories expressed as a percentage of human-driven change (in red) relative to baseline (blue) Rates of decline in biodiversity in the current sixth mass extinction match or exceed rates of loss in the five previous mass extinction events in the fossil record . [72] Biodiversity loss is in fact "one of the most critical manifestations of the Anthropocene " (since around the 1950s); the continued decline of biodiversity constitutes "an unprecedented threat" to the continued existence of human civilization. [73] The reduction is caused primarily by human impacts , particularly habitat destruction . Loss of biodiversity results in the loss of natural capital that supplies ecosystem goods and services . Species today are being wiped out at a rate 100 to 1,000 times higher than baseline, and the rate of extinctions is increasing. This process destroys the resilience and adaptability of life on Earth. [74] In 2006, many species were formally classified as rare or endangered or threatened ; moreover, scientists have estimated that millions more species are at risk which have not been formally recognized. About 40 percent of the 40,177 species assessed using the IUCN Red List criteria are now listed as threatened with extinction —a total of 16,119. [75] As of late 2022 9251 species were considered part of the IUCN's critically endangered . [76] Numerous scientists and the IPBES Global Assessment Report on Biodiversity and Ecosystem Services assert that human population growth and overconsumption are the primary factors in this decline. [77] [78] [79] [80] [81] However, other scientists have criticized this finding and say that loss of habitat caused by "the growth of commodities for export" is the main driver. [82] Some studies have however pointed out that habitat destruction for the expansion of agriculture and the overexploitation of wildlife are the more significant drivers of contemporary biodiversity loss, not climate change . [9] [10] Distribution[ edit ] Distribution of living terrestrial vertebrate species, highest concentration of diversity shown in red in equatorial regions, declining polewards (towards the blue end of the spectrum) Biodiversity is not evenly distributed, rather it varies greatly across the globe as well as within regions and seasons. Among other factors, the diversity of all living things ( biota ) depends on temperature , precipitation , altitude , soils , geography and the interactions between other species. [83] The study of the spatial distribution of organisms , species and ecosystems , is the science of biogeography . [84] [85] Diversity consistently measures higher in the tropics and in other localized regions such as the Cape Floristic Region and lower in polar regions generally. Rain forests that have had wet climates for a long time, such as Yasuní National Park in Ecuador , have particularly high biodiversity. [86] [87] Terrestrial biodiversity is thought to be up to 25 times greater than ocean biodiversity. [88] Forests harbour most of Earth's terrestrial biodiversity. The conservation of the world's biodiversity is thus utterly dependent on the way in which we interact with and use the world's forests. [89] A new method used in 2011, put the total number of species on Earth at 8.7 million, of which 2.1 million were estimated to live in the ocean. [90] However, this estimate seems to under-represent the diversity of microorganisms. [91] Forests provide habitats for 80 percent of amphibian species , 75 percent of bird species and 68 percent of mammal species. About 60 percent of all vascular plants are found in tropical forests. Mangroves provide breeding grounds and nurseries for numerous species of fish and shellfish and help trap sediments that might otherwise adversely affect seagrass beds and coral reefs, which are habitats for many more marine species. [89] Forests span around 4 billion acres (nearly a third of the Earth's land mass) and are home to approximately 80% of the world's biodiversity. About 1 billion hectares are covered by primary forests. Over 700 million hectares of the world's woods are officially protected. [92] [93] The biodiversity of forests varies considerably according to factors such as forest type, geography, climate and soils – in addition to human use. [89] Most forest habitats in temperate regions support relatively few animal and plant species and species that tend to have large geographical distributions, while the montane forests of Africa, South America and Southeast Asia and lowland forests of Australia, coastal Brazil, the Caribbean islands, Central America and insular Southeast Asia have many species with small geographical distributions. [89] Areas with dense human populations and intense agricultural land use, such as Europe , parts of Bangladesh, China, India and North America, are less intact in terms of their biodiversity. Northern Africa, southern Australia, coastal Brazil, Madagascar and South Africa, are also identified as areas with striking losses in biodiversity intactness. [89] European forests in EU and non-EU nations comprise more than 30% of Europe's land mass (around 227 million hectares), representing an almost 10% growth since 1990. [94] [95] Main article: Latitudinal gradients in species diversity Generally, there is an increase in biodiversity from the poles to the tropics . Thus localities at lower latitudes have more species than localities at higher latitudes . This is often referred to as the latitudinal gradient in species diversity. Several ecological factors may contribute to the gradient, but the ultimate factor behind many of them is the greater mean temperature at the equator compared to that at the poles. [96] Even though terrestrial biodiversity declines from the equator to the poles, [97] some studies claim that this characteristic is unverified in aquatic ecosystems , especially in marine ecosystems . [98] The latitudinal distribution of parasites does not appear to follow this rule. [84] Also, in terrestrial ecosystems the soil bacterial diversity has been shown to be highest in temperate climatic zones, [99] and has been attributed to carbon inputs and habitat connectivity. [100] In 2016, an alternative hypothesis ("the fractal biodiversity") was proposed to explain the biodiversity latitudinal gradient. [101] In this study, the species pool size and the fractal nature of ecosystems were combined to clarify some general patterns of this gradient. This hypothesis considers temperature , moisture , and net primary production (NPP) as the main variables of an ecosystem niche and as the axis of the ecological hypervolume . In this way, it is possible to build fractal hyper volumes, whose fractal dimension rises to three moving towards the equator . [102] Biodiversity Hotspots[ edit ] A biodiversity hotspot is a region with a high level of endemic species that have experienced great habitat loss . [103] The term hotspot was introduced in 1988 by Norman Myers . [104] [105] [106] [107] While hotspots are spread all over the world, the majority are forest areas and most are located in the tropics . [108] Brazil 's Atlantic Forest is considered one such hotspot, containing roughly 20,000 plant species, 1,350 vertebrates and millions of insects, about half of which occur nowhere else. [109] [110] The island of Madagascar and India are also particularly notable. Colombia is characterized by high biodiversity, with the highest rate of species by area unit worldwide and it has the largest number of endemics (species that are not found naturally anywhere else) of any country. About 10% of the species of the Earth can be found in Colombia, including over 1,900 species of bird, more than in Europe and North America combined, Colombia has 10% of the world's mammals species, 14% of the amphibian species and 18% of the bird species of the world. [111] Madagascar dry deciduous forests and lowland rainforests possess a high ratio of endemism . [112] [113] Since the island separated from mainland Africa 66 million years ago, many species and ecosystems have evolved independently. [114] Indonesia 's 17,000 islands cover 735,355 square miles (1,904,560 km2) and contain 10% of the world's flowering plants , 12% of mammals and 17% of reptiles , amphibians and birds —along with nearly 240 million people. [115] Many regions of high biodiversity and/or endemism arise from specialized habitats which require unusual adaptations, for example, alpine environments in high mountains , or Northern European peat bogs . [113] Accurately measuring differences in biodiversity can be difficult. Selection bias amongst researchers may contribute to biased empirical research for modern estimates of biodiversity. In 1768, Rev. Gilbert White succinctly observed of his Selborne, Hampshire "all nature is so full, that that district produces the most variety which is the most examined." [116] Evolution over geologic timeframes[ edit ] Main article: Evolution Biodiversity is the result of 3.5 billion years of evolution . [117] The origin of life has not been established by science, however, some evidence suggests that life may already have been well-established only a few hundred million years after the formation of the Earth . Until approximately 2.5 billion years ago, all life consisted of microorganisms – archaea , bacteria , and single-celled protozoans and protists . [91] * Ice Ages Apparent marine fossil diversity during the Phanerozoic [118] The history of biodiversity during the Phanerozoic (the last 540 million years), starts with rapid growth during the Cambrian explosion —a period during which nearly every phylum of multicellular organisms first appeared. [119] Over the next 400 million years or so, invertebrate diversity showed little overall trend and vertebrate diversity shows an overall exponential trend. [23] This dramatic rise in diversity was marked by periodic, massive losses of diversity classified as mass extinction events. [23] A significant loss occurred when rainforests collapsed in the carboniferous. [120] The worst was the Permian-Triassic extinction event , 251 million years ago. Vertebrates took 30 million years to recover from this event. [121] The biodiversity of the past is called Paleobiodiversity. The fossil record suggests that the last few million years featured the greatest biodiversity in history . [23] However, not all scientists support this view, since there is uncertainty as to how strongly the fossil record is biased by the greater availability and preservation of recent geologic sections. [122] Some scientists believe that corrected for sampling artifacts, modern biodiversity may not be much different from biodiversity 300 million years ago, [119] whereas others consider the fossil record reasonably reflective of the diversification of life. [23] Estimates of the present global macroscopic species diversity vary from 2 million to 100 million, with a best estimate of somewhere near 9 million, [90] the vast majority arthropods . [123] Diversity appears to increase continually in the absence of natural selection. [124] Diversification[ edit ] The existence of a global carrying capacity, limiting the amount of life that can live at once, is debated, as is the question of whether such a limit would also cap the number of species. While records of life in the sea show a logistic pattern of growth, life on land (insects, plants and tetrapods) shows an exponential rise in diversity. [23] As one author states, "Tetrapods have not yet invaded 64 percent of potentially habitable modes and it could be that without human influence the ecological and taxonomic diversity of tetrapods would continue to increase exponentially until most or all of the available eco-space is filled." [23] It also appears that the diversity continues to increase over time, especially after mass extinctions. [125] On the other hand, changes through the Phanerozoic correlate much better with the hyperbolic model (widely used in population biology , demography and macrosociology , as well as fossil biodiversity) than with exponential and logistic models. The latter models imply that changes in diversity are guided by a first-order positive feedback (more ancestors, more descendants) and/or a negative feedback arising from resource limitation. Hyperbolic model implies a second-order positive feedback. [126] Differences in the strength of the second-order feedback due to different intensities of interspecific competition might explain the faster rediversification of ammonoids in comparison to bivalves after the end-Permian extinction . [126] The hyperbolic pattern of the world population growth arises from a second-order positive feedback between the population size and the rate of technological growth. [127] The hyperbolic character of biodiversity growth can be similarly accounted for by a feedback between diversity and community structure complexity. [127] [128] The similarity between the curves of biodiversity and human population probably comes from the fact that both are derived from the interference of the hyperbolic trend with cyclical and stochastic dynamics. [127] [128] Most biologists agree however that the period since human emergence is part of a new mass extinction, named the Holocene extinction event , caused primarily by the impact humans are having on the environment. [129] It has been argued that the present rate of extinction is sufficient to eliminate most species on the planet Earth within 100 years. [130] New species are regularly discovered (on average between 5–10,000 new species each year, most of them insects ) and many, though discovered, are not yet classified (estimates are that nearly 90% of all arthropods are not yet classified). [123] Most of the terrestrial diversity is found in tropical forests and in general, the land has more species than the ocean; some 8.7 million species may exist on Earth, of which some 2.1 million live in the ocean. [90] Species diversity in geologic time frames[ edit ] Further information: History of life and Earliest known life forms It is estimated that 5 to 50 billion species have existed on the planet. [131] Assuming that there may be a maximum of about 50 million species currently alive, [132] it stands to reason that greater than 99% of the planet's species went extinct prior to the evolution of humans. [133] Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented and over 86% have not yet been described. [134] However, a May 2016 scientific report estimates that 1 trillion species are currently on Earth, with only one-thousandth of one percent described. [135] The total amount of related DNA base pairs on Earth is estimated at 5.0 x 1037 and weighs 50 billion tonnes . In comparison, the total mass of the biosphere has been estimated to be as much as four trillion tons of carbon . [136] In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth. [137] The age of Earth is about 4.54 billion years. [138] [139] [140] The earliest undisputed evidence of life dates at least from 3.7 billion years ago, during the Eoarchean era after a geological crust started to solidify following the earlier molten Hadean eon. [141] [142] [143] There are microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia . Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old meta-sedimentary rocks discovered in Western Greenland .. [144] [145] More recently, in 2015, "remains of biotic life " were found in 4.1 billion-year-old rocks in Western Australia . According to one of the researchers, "If life arose relatively quickly on Earth...then it could be common in the universe ." [146] Role and benefits of biodiversity[ edit ] Summer field in Belgium (Hamois). The blue flowers are Centaurea cyanus and the red are Papaver rhoeas . General ecosystem services[ edit ] Further information: Ecosystem services From the perspective of the method known as Natural Economy the economic value of 17 ecosystem services for Earth's biosphere (calculated in 1997) has an estimated value of US$33 trillion (3.3x1013) per year. [147] "Ecosystem services are the suite of benefits that ecosystems provide to humanity." [148] The natural species, or biota, are the caretakers of all ecosystems. It is as if the natural world is an enormous bank account of capital assets capable of paying life sustaining dividends indefinitely, but only if the capital is maintained. [149] These services come in three flavors: Provisioning services which involve the production of renewable resources (e.g.: food, wood, fresh water) [148] Regulating services which are those that lessen environmental change (e.g.: climate regulation, pest/disease control) [148] Cultural services represent human value and enjoyment (e.g.: landscape aesthetics, cultural heritage, outdoor recreation and spiritual significance) [150] There have been many claims about biodiversity's effect on these ecosystem services, especially provisioning and regulating services. [148] After an exhaustive survey through peer-reviewed literature to evaluate 36 different claims about biodiversity's effect on ecosystem services, 14 of those claims have been validated, 6 demonstrate mixed support or are unsupported, 3 are incorrect and 13 lack enough evidence to draw definitive conclusions. [148] Greater species diversity of plants increases fodder yield (synthesis of 271 experimental studies). [85] of plants (i.e. diversity within a single species) increases overall crop yield (synthesis of 575 experimental studies). [151] Although another review of 100 experimental studies reports mixed evidence. [152] of trees increases overall wood production (Synthesis of 53 experimental studies). [153] However, there is not enough data to draw a conclusion about the effect of tree trait diversity on wood production. [148] Regulating services Greater species diversity of fish increases the stability of fisheries yield (Synthesis of 8 observational studies) [148] of natural pest enemies decreases herbivorous pest populations (Data from two separate reviews; Synthesis of 266 experimental and observational studies; [154] Synthesis of 18 observational studies. [155] [156] Although another review of 38 experimental studies found mixed support for this claim, suggesting that in cases where mutual intraguild predation occurs, a single predatory species is often more effective [157] of plants decreases disease prevalence on plants (Synthesis of 107 experimental studies) [158] of plants increases resistance to plant invasion (Data from two separate reviews; Synthesis of 105 experimental studies; [158] Synthesis of 15 experimental studies [159] ) of plants increases carbon sequestration , but note that this finding only relates to actual uptake of carbon dioxide and not long-term storage, see below; Synthesis of 479 experimental studies) [85] plants increases soil nutrient remineralization (Synthesis of 103 experimental studies) [158] of plants increases soil organic matter (Synthesis of 85 experimental studies) [158] Services with mixed evidence[ edit ] Provisioning services None to date Regulating services Greater species diversity of plants may or may not decrease herbivorous pest populations. Data from two separate reviews suggest that greater diversity decreases pest populations (Synthesis of 40 observational studies; [160] Synthesis of 100 experimental studies). [152] One review found mixed evidence (Synthesis of 287 experimental studies [161] ), while another found contrary evidence (Synthesis of 100 experimental studies [158] ) Greater species diversity of animals may or may not decrease disease prevalence on those animals (Synthesis of 45 experimental and observational studies), [162] although a 2013 study offers more support showing that biodiversity may in fact enhance disease resistance within animal communities, at least in amphibian frog ponds. [163] Many more studies must be published in support of diversity to sway the balance of evidence will be such that we can draw a general rule on this service. Greater species and trait diversity of plants may or may not increase long term carbon storage (Synthesis of 33 observational studies) [148] Greater pollinator diversity may or may not increase pollination (Synthesis of 7 observational studies), [148] but a publication from March 2013 suggests that increased native pollinator diversity enhances pollen deposition (although not necessarily fruit set as the authors would have you believe, for details explore their lengthy supplementary material). [164] Greater species diversity of plants reduces primary production (Synthesis of 7 experimental studies) [85] Regulating services greater genetic and species diversity of a number of organisms reduces freshwater purification (Synthesis of 8 experimental studies, although an attempt by the authors to investigate the effect of detritivore diversity on freshwater purification was unsuccessful due to a lack of available evidence (only 1 observational study was found [148] Effect of species diversity of plants on biofuel yield (In a survey of the literature, the investigators only found 3 studies) [148] Effect of species diversity of fish on fishery yield (In a survey of the literature, the investigators only found 4 experimental studies and 1 observational study) [148] Regulating services Effect of species diversity on the stability of biofuel yield (In a survey of the literature, the investigators did not find any studies) [148] Effect of species diversity of plants on the stability of fodder yield (In a survey of the literature, the investigators only found 2 studies) [148] Effect of species diversity of plants on the stability of crop yield (In a survey of the literature, the investigators only found 1 study) [148] Effect of genetic diversity of plants on the stability of crop yield (In a survey of the literature, the investigators only found 2 studies) [148] Effect of diversity on the stability of wood production (In a survey of the literature, the investigators could not find any studies) [148] Effect of species diversity of multiple taxa on erosion control (In a survey of the literature, the investigators could not find any studies – they did, however, find studies on the effect of species diversity and root biomass) [148] Effect of diversity on flood regulation (In a survey of the literature, the investigators could not find any studies) [148] Effect of species and trait diversity of plants on soil moisture (In a survey of the literature, the investigators only found 2 studies) [148] Other sources have reported somewhat conflicting results and in 1997 Robert Costanza and his colleagues reported the estimated global value of ecosystem services (not captured in traditional markets) at an average of $33 trillion annually. [165] Since the Stone Age , species loss has accelerated above the average basal rate, driven by human activity. Estimates of species losses are at a rate 100–10,000 times as fast as is typical in the fossil record. [166] Biodiversity also affords many non-material benefits including spiritual and aesthetic values, knowledge systems and education. [166] Amazon Rainforest in South America Agricultural diversity can be divided into two categories: intraspecific diversity , which includes the genetic variation within a single species, like the potato ( Solanum tuberosum ) that is composed of many different forms and types (e.g. in the U.S. they might compare russet potatoes with new potatoes or purple potatoes, all different, but all part of the same species, S. tuberosum). The other category of agricultural diversity is called interspecific diversity and refers to the number and types of different species. Thinking about this diversity we might note that many small vegetable farmers grow many different crops like potatoes and also carrots, peppers, lettuce, etc. Agricultural diversity can also be divided by whether it is 'planned' diversity or 'associated' diversity. This is a functional classification that we impose and not an intrinsic feature of life or diversity. Planned diversity includes the crops which a farmer has encouraged, planted or raised (e.g. crops, covers, symbionts, and livestock, among others), which can be contrasted with the associated diversity that arrives among the crops, uninvited (e.g. herbivores, weed species and pathogens, among others). [167] Associated biodiversity can be damaging or beneficial. The beneficial associated biodiversity include for instance wild pollinators such as wild bees and syrphid flies that pollinate crops [168] and natural enemies and antagonists to pests and pathogens. Beneficial associated biodiversity occurs abundantly in crop fields and provide multiple ecosystem services such as pest control, nutrient cycling and pollination that support crop production. [169] The control of damaging associated biodiversity is one of the great agricultural challenges that farmers face. On monoculture farms, the approach is generally to suppress damaging associated diversity using a suite of biologically destructive pesticides , mechanized tools and transgenic engineering techniques , then to rotate crops . Although some polyculture farmers use the same techniques, they also employ integrated pest management strategies as well as more labor-intensive strategies, but generally less dependent on capital, biotechnology, and energy. Interspecific crop diversity is, in part, responsible for offering variety in what we eat. Intraspecific diversity, the variety of alleles within a single species, also offers us a choice in our diets. If a crop fails in a monoculture, we rely on agricultural diversity to replant the land with something new. If a wheat crop is destroyed by a pest we may plant a hardier variety of wheat the next year, relying on intraspecific diversity. We may forgo wheat production in that area and plant a different species altogether, relying on interspecific diversity. Even an agricultural society that primarily grows monocultures relies on biodiversity at some point. The Irish potato blight of 1846 was a major factor in the deaths of one million people and the emigration of about two million. It was the result of planting only two potato varieties, both vulnerable to the blight, Phytophthora infestans , which arrived in 1845 [167] When rice grassy stunt virus struck rice fields from Indonesia to India in the 1970s, 6,273 varieties were tested for resistance. [170] Only one was resistant, an Indian variety and known to science only since 1966. [170] This variety formed a hybrid with other varieties and is now widely grown. [170] Coffee rust attacked coffee plantations in Sri Lanka , Brazil and Central America in 1970. A resistant variety was found in Ethiopia. [171] The diseases are themselves a form of biodiversity. Monoculture was a contributing factor to several agricultural disasters, including the European wine industry collapse in the late 19th century and the US southern corn leaf blight epidemic of 1970. [172] Although about 80 percent of humans' food supply comes from just 20 kinds of plants, [173] humans use at least 40,000 species. [174] Earth's surviving biodiversity provides resources for increasing the range of food and other products suitable for human use, although the present extinction rate shrinks that potential. [130] Human health[ edit ] The diverse forest canopy on Barro Colorado Island , Panama, yielded this display of different fruit Biodiversity's relevance to human health is becoming an international political issue, as scientific evidence builds on the global health implications of biodiversity loss. [175] [176] [177] This issue is closely linked with the issue of climate change , [178] as many of the anticipated health risks of climate change are associated with changes in biodiversity (e.g. changes in populations and distribution of disease vectors, scarcity of fresh water, impacts on agricultural biodiversity and food resources etc.). This is because the species most likely to disappear are those that buffer against infectious disease transmission, while surviving species tend to be the ones that increase disease transmission, such as that of West Nile Virus, Lyme disease and Hantavirus, according to a study done co-authored by Felicia Keesing, an ecologist at Bard College and Drew Harvell, associate director for Environment of the Atkinson Center for a Sustainable Future (ACSF) at Cornell University . [179] The growing demand and lack of drinkable water on the planet presents an additional challenge to the future of human health. Partly, the problem lies in the success of water suppliers to increase supplies and failure of groups promoting the preservation of water resources. [180] While the distribution of clean water increases, in some parts of the world it remains unequal. According to the World Health Organisation (2018), only 71% of the global population used a safely managed drinking-water service. [181] Some of the health issues influenced by biodiversity include dietary health and nutrition security, infectious disease, medical science and medicinal resources, social and psychological health. [182] Biodiversity is also known to have an important role in reducing disaster risk and in post-disaster relief and recovery efforts. [183] [184] According to the United Nations Environment Programme a pathogen , like a virus , have more chances to meet resistance in a diverse population.Therefore, in a population genetically similar it expands more easily. For example, the COVID-19 pandemic had less chances to occur in a world with higher biodiversity. [185] A  broad literature review published in 2010 by Nature (journal) , Impacts of biodiversity on the emergence and transmission of infectious disease, found this to be broadly true within real environments. [186] Although some small population exceptions were found to exist, on average a collapse in biodiversity significantly increased the spread & spillover of infectious diseases. Biodiversity provides critical support for drug discovery and the availability of medicinal resources. [187] [188] A significant proportion of drugs are derived, directly or indirectly, from biological sources: at least 50% of the pharmaceutical compounds on the US market are derived from plants, animals and microorganisms , while about 80% of the world population depends on medicines from nature (used in either modern or traditional medical practice) for primary healthcare. [176] Only a tiny fraction of wild species has been investigated for medical potential. Biodiversity has been critical to advances throughout the field of bionics . Evidence from market analysis and biodiversity science indicates that the decline in output from the pharmaceutical sector since the mid-1980s can be attributed to a move away from natural product exploration ("bioprospecting") in favour of genomics and synthetic chemistry, indeed claims about the value of undiscovered pharmaceuticals may not provide enough incentive for companies in free markets to search for them because of the high cost of development; [189] meanwhile, natural products have a long history of supporting significant economic and health innovation. [190] [191] Marine ecosystems are particularly important, [192] although inappropriate bioprospecting can increase biodiversity loss, as well as violating the laws of the communities and states from which the resources are taken. [193] [194] [195] Business and industry[ edit ] Agriculture production, pictured is a tractor and a chaser bin Many industrial materials derive directly from biological sources. These include building materials, fibers, dyes, rubber, and oil. Biodiversity is also important to the security of resources such as water, timber, paper, fiber, and food. [196] [197] [198] As a result, biodiversity loss is a significant risk factor in business development and a threat to long-term economic sustainability. [199] [200] Many logging companies do not abide by the rules and legislation established by their respective governments; likewise, governments of timer-producing countries also fail in enforcing their own forestry legislations. [201] Leisure, cultural and aesthetic value[ edit ] Biodiversity enriches leisure activities such as birdwatching or natural history study. Popular activities such as gardening and fishkeeping strongly depend on biodiversity. The number of species involved in such pursuits is in the tens of thousands, though the majority do not enter commerce.[ clarification needed ] The relationships between the original natural areas of these often exotic animals and plants and commercial collectors, suppliers, breeders, propagators and those who promote their understanding and enjoyment are complex and poorly understood. The general public responds well to exposure to rare and unusual organisms, reflecting their inherent value. Philosophically it could be argued that biodiversity has intrinsic aesthetic and spiritual value to mankind in and of itself. This idea can be used as a counterweight to the notion that tropical forests and other ecological realms are only worthy of conservation because of the services they provide. [202] Biodiversity supports many ecosystem services : "There is now unequivocal evidence that biodiversity loss reduces the efficiency by which ecological communities capture biologically essential resources, produce biomass, decompose and recycle biologically essential nutrients... There is mounting evidence that biodiversity increases the stability of ecosystem functions through time... Diverse communities are more productive because they contain key species that have a large influence on productivity and differences in functional traits among organisms increase total resource capture... The impacts of diversity loss on ecological processes might be sufficiently large to rival the impacts of many other global drivers of environmental change... Maintaining multiple ecosystem processes at multiple places and times requires higher levels of biodiversity than does a single process at a single place and time." [148] It plays a part in regulating the chemistry of our atmosphere and water supply . Biodiversity is directly involved in water purification , recycling nutrients and providing fertile soils. Experiments with controlled environments have shown that humans cannot easily build ecosystems to support human needs; [203] for example insect pollination cannot be mimicked, though there have been attempts to create artificial pollinators using unmanned aerial vehicles . [204] The economic activity of pollination alone represented between $2.1–14.6 billion in 2003. [205] This section is an excerpt from Measurement of biodiversity .[ edit ] A variety of objective means exist to empirically measure biodiversity . Each measure relates to a particular use of the data, and is likely to be associated with the variety of genes. Biodiversity is commonly measured in terms of taxonomic richness of a geographic area over a time interval. In order to calculate biodiversity, species evenness, species richness, and species diversity are to be obtained first. Species evenness [206] is the relative number of individuals of each species in a given area. Species richness [207] is the number of species present in a given area. Species diversity [208] is the relationship between species evenness and species richness. There are many ways to measure biodiversity within a given ecosystem. However, the two most popular are Shannon-Weaver diversity index , [209] commonly referred to as Shannon diversity index, and the other is Simpsons diversity index . [210] Although many scientists prefer to use Shannon's diversity index simply because it takes into account species richness. [211] Analytical limits[ edit ] Less than 1% of all species that have been described have been studied beyond noting their existence. [212] The vast majority of Earth's species are microbial. Contemporary biodiversity physics is "firmly fixated on the visible [macroscopic] world". [213] For example, microbial life is metabolically and environmentally more diverse than multicellular life (see e.g., extremophile ). "On the tree of life, based on analyses of small-subunit ribosomal RNA , visible life consists of barely noticeable twigs. The inverse relationship of size and population recurs higher on the evolutionary ladder—to a first approximation, all multicellular species on Earth are insects". [214] Insect extinction rates are high—supporting the Holocene extinction hypothesis. [215] [70] Biodiversity changes (other than losses)[ edit ] Natural seasonal variations[ edit ] Biodiversity naturally varies due to seasonal shifts. Spring's arrival enhances biodiversity as numerous species breed and feed, while winter's onset temporarily reduces it as some insects perish and migrating animals leave. Additionally, the seasonal fluctuation in plant and invertebrate populations influences biodiversity. [216] Introduced and invasive species[ edit ] Main articles: Introduced species and Invasive species Male Lophura nycthemera ( silver pheasant ), a native of East Asia that has been introduced into parts of Europe for ornamental reasons Barriers such as large rivers , seas , oceans , mountains and deserts encourage diversity by enabling independent evolution on either side of the barrier, via the process of allopatric speciation . The term invasive species is applied to species that breach the natural barriers that would normally keep them constrained. Without barriers, such species occupy new territory, often supplanting native species by occupying their niches, or by using resources that would normally sustain native species. The number of species invasions has been on the rise at least since the beginning of the 1900s. Species are increasingly being moved by humans (on purpose and accidentally). In some cases the invaders are causing drastic changes and damage to their new habitats (e.g.: zebra mussels and the emerald ash borer in the Great Lakes region and the lion fish along the North American Atlantic coast). Some evidence suggests that invasive species are competitive in their new habitats because they are subject to less pathogen disturbance. [217] Others report confounding evidence that occasionally suggest that species-rich communities harbor many native and exotic species simultaneously [218] while some say that diverse ecosystems are more resilient and resist invasive plants and animals. [219] An important question is, "do invasive species cause extinctions?" Many studies cite effects of invasive species on natives, [220] but not extinctions. Invasive species seem to increase local (i.e.: alpha diversity ) diversity, which decreases turnover of diversity (i.e.: beta diversity ). Overall gamma diversity may be lowered because species are going extinct because of other causes, [221] but even some of the most insidious invaders (e.g.: Dutch elm disease, emerald ash borer, chestnut blight in North America) have not caused their host species to become extinct. Extirpation , population decline and homogenization of regional biodiversity are much more common. Human activities have frequently been the cause of invasive species circumventing their barriers, [222] by introducing them for food and other purposes. Human activities therefore allow species to migrate to new areas (and thus become invasive) occurred on time scales much shorter than historically have been required for a species to extend its range. Not all introduced species are invasive, nor all invasive species deliberately introduced. In cases such as the zebra mussel , invasion of US waterways was unintentional. In other cases, such as mongooses in Hawaii , the introduction is deliberate but ineffective ( nocturnal rats were not vulnerable to the diurnal mongoose). In other cases, such as oil palms in Indonesia and Malaysia, the introduction produces substantial economic benefits, but the benefits are accompanied by costly unintended consequences . Finally, an introduced species may unintentionally injure a species that depends on the species it replaces. In Belgium , Prunus spinosa from Eastern Europe leafs much sooner than its West European counterparts, disrupting the feeding habits of the Thecla betulae butterfly (which feeds on the leaves). Introducing new species often leaves endemic and other local species unable to compete with the exotic species and unable to survive. The exotic organisms may be predators , parasites , or may outcompete indigenous species for nutrients, water and light. At present, several countries have already imported so many exotic species, particularly agricultural and ornamental plants, that their indigenous fauna/flora may be outnumbered. For example, the introduction of kudzu from Southeast Asia to Canada and the United States has threatened biodiversity in certain areas. [223] Another example are pines , which have invaded forests, shrublands and grasslands in the southern hemisphere. [224] Hybridization and genetic pollution[ edit ] The Yecoro wheat (right) cultivar is sensitive to salinity, plants resulting from a hybrid cross with cultivar W4910 (left) show greater tolerance to high salinity Endemic species can be threatened with extinction [225] through the process of genetic pollution , i.e. uncontrolled hybridization , introgression and genetic swamping. Genetic pollution leads to homogenization or replacement of local genomes as a result of either a numerical and/or fitness advantage of an introduced species. [226] Hybridization and introgression are side-effects of introduction and invasion. These phenomena can be especially detrimental to rare species that come into contact with more abundant ones. The abundant species can interbreed with the rare species, swamping its gene pool . This problem is not always apparent from morphological (outward appearance) observations alone. Some degree of gene flow is normal adaptation and not all gene and genotype constellations can be preserved. However, hybridization with or without introgression may, nevertheless, threaten a rare species' existence. [227] [228] In agriculture and animal husbandry , the Green Revolution popularized the use of conventional hybridization to increase yield. Often hybridized breeds originated in developed countries and were further hybridized with local varieties in the developing world to create high yield strains resistant to local climate and diseases. Local governments and industry have been pushing hybridization. Formerly huge gene pools of various wild and indigenous breeds have collapsed causing widespread genetic erosion and genetic pollution. This has resulted in the loss of genetic diversity and biodiversity as a whole. [229] Genetically modified organisms contain genetic material that is altered through genetic engineering . Genetically modified crops have become a common source for genetic pollution in not only wild varieties, but also in domesticated varieties derived from classical hybridization. [230] [231] [232] [233] [234] Genetic erosion and genetic pollution have the potential to destroy unique genotypes , threatening future access to food security . A decrease in genetic diversity weakens the ability of crops and livestock to be hybridized to resist disease and survive changes in climate. [229] Main article: Conservation biology A schematic image illustrating the relationship between biodiversity, ecosystem services, human well-being and poverty. [235] The illustration shows where conservation action, strategies, and plans can influence the drivers of the current biodiversity crisis at local, regional, to global scales. The retreat of Aletsch Glacier in the Swiss Alps (situation in 1979, 1991 and 2002), due to global warming . Conservation biology matured in the mid-20th century as ecologists , naturalists and other scientists began to research and address issues pertaining to global biodiversity declines. [236] [237] [238] The conservation ethic advocates management of natural resources for the purpose of sustaining biodiversity in species , ecosystems , the evolutionary process and human culture and society. [66] [236] [238] [239] [240] Conservation biology is reforming around strategic plans to protect biodiversity. [236] [241] [242] [243] Preserving global biodiversity is a priority in strategic conservation plans that are designed to engage public policy and concerns affecting local, regional and global scales of communities, ecosystems and cultures. [244] Action plans identify ways of sustaining human well-being, employing natural capital , market capital and ecosystem services . [245] [246] In the EU Directive 1999/22/EC zoos are described as having a role in the preservation of the biodiversity of wildlife animals by conducting research or participation in breeding programs . [247] Protection and restoration techniques[ edit ] Removal of exotic species will allow the species that they have negatively impacted to recover their ecological niches. Exotic species that have become pests can be identified taxonomically (e.g., with Digital Automated Identification SYstem (DAISY), using the barcode of life ). [248] [249] Removal is practical only given large groups of individuals due to the economic cost. As sustainable populations of the remaining native species in an area become assured, "missing" species that are candidates for reintroduction can be identified using databases such as the Encyclopedia of Life and the Global Biodiversity Information Facility . Biodiversity banking places a monetary value on biodiversity. One example is the Australian Native Vegetation Management Framework . Gene banks are collections of specimens and genetic material. Some banks intend to reintroduce banked species to the ecosystem (e.g., via tree nurseries). [250] Reduction and better targeting of pesticides allows more species to survive in agricultural and urbanized areas. Location-specific approaches may be less useful for protecting migratory species. One approach is to create wildlife corridors that correspond to the animals' movements. National and other boundaries can complicate corridor creation. [251] Priorities for resource allocation[ edit ] Focusing on limited areas of higher potential biodiversity promises greater immediate return on investment than spreading resources evenly or focusing on areas of little diversity but greater interest in biodiversity. [252] A second strategy focuses on areas that retain most of their original diversity, which typically require little or no restoration. These are typically non-urbanized, non-agricultural areas. Tropical areas often fit both criteria, given their natively high diversity and relative lack of development. [253] Further information: Protected areas Mother and child at an orangutan rehab facility in Malaysia Protected areas, including forest reserves and biosphere reserves, serve many functions including for affording protection to wild animals and their habitat. [254] Protected areas have been set up all over the world with the specific aim of protecting and conserving plants and animals. Some scientists have called on the global community to designate as protected areas of 30 percent of the planet by 2030, and 50 percent by 2050, in order to mitigate biodiversity loss from anthropogenic causes. [255] [256] The target of protecting 30% of the area of the planet by the year 2030 ( 30 by 30 ) was adopted by almost 200 countries in the 2022 United Nations Biodiversity Conference . At the moment of adoption (December 2022) 17% of land territory and 10% of ocean territory were protected. [257] In a study published 4 September 2020 in Science Advances researchers mapped out regions that can help meet critical conservation and climate goals. [258] Protected areas safeguard nature and cultural resources and contribute to livelihoods, particularly at local level. There are over 238 563 designated protected areas worldwide, equivalent to 14.9 percent of the earth's land surface, varying in their extension, level of protection, and type of management (IUCN, 2018). [259] Percentage of forest in legally protected areas (as of 2020). [89] Forest protected areas are a subset of all protected areas in which a significant portion of the area is forest. [89] This may be the whole or only a part of the protected area. [89] Globally, 18 percent of the world's forest area, or more than 700 million hectares, fall within legally established protected areas such as national parks, conservation areas and game reserves. [89] The benefits of protected areas extend beyond their immediate environment and time. In addition to conserving nature, protected areas are crucial for securing the long-term delivery of ecosystem services. They provide numerous benefits including the conservation of genetic resources for food and agriculture, the provision of medicine and health benefits, the provision of water, recreation and tourism, and for acting as a buffer against disaster. Increasingly, there is acknowledgement of the wider socioeconomic values of these natural ecosystems and of the ecosystem services they can provide. [260] Forest protected areas in particular play many important roles including as a provider of habitat, shelter, food and genetic materials, and as a buffer against disaster. They deliver stable supplies of many goods and environmental services.  The role of protected areas, especially forest protected areas, in mitigating and adapting to climate change has increasingly been recognized over the last few years. Protected areas not only store and sequester carbon (i.e. the global network of protected areas stores at least 15 percent of terrestrial carbon), but also enable species to adapt to changing climate patterns by providing refuges and migration corridors. Protected areas also protect people from sudden climate events and reduce their vulnerability to weather-induced problems such as floods and droughts (UNEP–WCMC, 2016). Main article: National park A national park is a large natural or near natural area set aside to protect large-scale ecological processes, which also provide a foundation for environmentally and culturally compatible, spiritual, scientific, educational, recreational and visitor opportunities. These areas are selected by governments or private organizations to protect natural biodiversity along with its underlying ecological structure and supporting environmental processes, and to promote education and recreation. The International Union for Conservation of Nature (IUCN), and its World Commission on Protected Areas (WCPA), has defined "National Park" as its Category II type of protected areas. [261] National parks are usually owned and managed by national or state governments. In some cases, a limit is placed on the number of visitors permitted to enter certain fragile areas. Designated trails or roads are created. The visitors are allowed to enter only for study, cultural and recreation purposes. Forestry operations, grazing of animals and hunting of animals are regulated and the exploitation of habitat or wildlife is banned. Wildlife sanctuaries aim only at the conservation of species and have the following features: The boundaries of the sanctuaries are not limited by state legislation. The killing, hunting or capturing of any species is prohibited except by or under the control of the highest authority in the department which is responsible for the management of the sanctuary. Private ownership may be allowed. Forestry and other usages can also be permitted. Forest reserves[ edit ] There is an estimated 726 million ha of forest in protected areas worldwide. Of the six major world regions, South America has the highest share of forests in protected areas, 31 percent. [262] The forests play a vital role in harboring more than 45,000 floral and 81,000 faunal species of which 5150 floral and 1837 faunal species are endemic . [263] In addition, there are 60,065 different tree species in the world. [264] Plant and animal species confined to a specific geographical area are called endemic species. In forest reserves, rights to activities like hunting and grazing are sometimes given to communities living on the fringes of the forest, who sustain their livelihood partially or wholly from forest resources or products. The unclassed forests cover 6.4 percent of the total forest area and they are marked by the following characteristics: They are large inaccessible forests. Many of these are unoccupied. They are ecologically and economically less important. Approximately 50 million hectares (or 24%) of European forest land is protected for biodiversity and landscape protection. Forests allocated for soil, water, and other ecosystem services encompass around 72 million hectares (32% of European forest area). [265] [266] [267] Steps to conserve the forest cover[ edit ] An extensive reforestation / afforestation programme should be followed. Alternative environment-friendly sources of fuel energy such as biogas other than wood should be used. Loss of biodiversity due to forest fire is a major problem, immediate steps to prevent forest fire need to be taken. Overgrazing by cattle can damage a forest seriously. Therefore, certain steps should be taken to prevent overgrazing by cattle. Hunting and poaching should be banned. Zoological parks[ edit ] In zoological parks or zoos, live animals are kept for public recreation , education and conservation purposes. Modern zoos offer veterinary facilities, provide opportunities for threatened species to breed in captivity and usually build environments that simulate the native habitats of the animals in their care. Zoos play a major role in creating awareness about the need to conserve nature. Botanical gardens[ edit ] In botanical gardens , plants are grown and displayed primarily for scientific and educational purposes. They consist of a collection of living plants, grown outdoors or under glass in greenhouses and conservatories. Also, a botanical garden may include a collection of dried plants or herbarium and such facilities as lecture rooms, laboratories, libraries, museums and experimental or research plantings. Role of society[ edit ] Transformative change[ edit ] In 2019, a summary for policymakers of the largest, most comprehensive study to date of biodiversity and ecosystem services, the Global Assessment Report on Biodiversity and Ecosystem Services , was published by the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES). It stated that "the state of nature has deteriorated at an unprecedented and accelerating rate". To fix the problem, humanity will need a transformative change, including sustainable agriculture , reductions in consumption and waste, fishing quotas and collaborative water management. [268] [269] Citizen science[ edit ] Citizen science , also known as public participation in scientific research, has been widely used in environmental sciences and is particularly popular in a biodiversity-related context. It has been used to enable scientists to involve the general public in biodiversity research, thereby enabling the scientists to collect data that they would otherwise not have been able to obtain. An online survey of 1,160 CS participants across 63 biodiversity citizen science projects in Europe, Australia and New Zealand reported positive changes in (a) content, process and nature of science knowledge, (b) skills of science inquiry, (c) self-efficacy for science and the environment, (d) interest in science and the environment, (e) motivation for science and the environment and (f) behaviour towards the environment. [270] Volunteer observers have made significant contributions to on-the-ground knowledge about biodiversity, and recent improvements in technology have helped increase the flow and quality of occurrences from citizen sources. A 2016 study published in Biological Conservation [271] registers the massive contributions that citizen scientists already make to data mediated by the Global Biodiversity Information Facility (GBIF) . Despite some limitations of the dataset-level analysis, it is clear that nearly half of all occurrence records shared through the GBIF network come from datasets with significant volunteer contributions. Recording and sharing observations are enabled by several global-scale platforms, including iNaturalist and eBird . [272] [273] Legal status[ edit ] A great deal of work is occurring to preserve the natural characteristics of Hopetoun Falls , Australia while continuing to allow visitor access.
Toggle the table of contents Conservation biology From Wikipedia, the free encyclopedia Study of threats to biological diversity "Biological conservation" and "Conservation ecology" redirect here. For scientific journals, see Conservation Biology (journal) , Biological Conservation (journal) , and Conservation Ecology (journal) . For the popular movement, see Conservationism . 2016 conservation indicator which includes the following indicators: marine protected areas, terrestrial biome protection (global and national), and species protection (global and national) Conservation biology is the study of the conservation of nature and of Earth 's biodiversity with the aim of protecting species , their habitats , and ecosystems from excessive rates of extinction and the erosion of biotic interactions. [1] [2] [3] It is an interdisciplinary subject drawing on natural and social sciences, and the practice of natural resource management . [4] [5] [6] [7] : 478 The conservation ethic is based on the findings of conservation biology. Origins[ edit ] Efforts are made to preserve the natural characteristics of Hopetoun Falls , Australia, without affecting visitors' access. The term conservation biology and its conception as a new field originated with the convening of "The First International Conference on Research in Conservation Biology" held at the University of California, San Diego in La Jolla, California, in 1978 led by American biologists Bruce A. Wilcox and Michael E. Soulé with a group of leading university and zoo researchers and conservationists including Kurt Benirschke , Sir Otto Frankel , Thomas Lovejoy , and Jared Diamond . The meeting was prompted due to concern over tropical deforestation, disappearing species, and eroding genetic diversity within species. [8] The conference and proceedings that resulted [2] sought to initiate the bridging of a gap between theory in ecology and evolutionary genetics on the one hand and conservation policy and practice on the other. [9] Conservation biology and the concept of biological diversity ( biodiversity ) emerged together, helping crystallize the modern era of conservation science and policy . The inherent multidisciplinary basis for conservation biology has led to new subdisciplines including conservation social science, conservation behavior and conservation physiology. [10] It stimulated further development of conservation genetics which Otto Frankel had originated first but is now often considered a subdiscipline as well. Description[ edit ] The rapid decline of established biological systems around the world means that conservation biology is often referred to as a "Discipline with a deadline". [11] Conservation biology is tied closely to ecology in researching the population ecology ( dispersal , migration , demographics , effective population size , inbreeding depression , and minimum population viability ) of rare or endangered species . [12] [13] Conservation biology is concerned with phenomena that affect the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic , population , species , and ecosystem diversity. [5] [6] [7] [13] The concern stems from estimates suggesting that up to 50% of all species on the planet will disappear within the next 50 years, [14] which will increase poverty and starvation, and will reset the course of evolution on this planet. [15] [16] Researchers acknowledge that projections are difficult, given the unknown potential impacts of many variables, including species introduction to new biogeographical settings and a non-analog climate. [17] Conservation biologists research and educate on the trends and process of biodiversity loss , species extinctions , and the negative effect these are having on our capabilities to sustain the well-being of human society. Conservation biologists work in the field and office, in government, universities, non-profit organizations and industry. The topics of their research are diverse, because this is an interdisciplinary network with professional alliances in the biological as well as social sciences. Those dedicated to the cause and profession advocate for a global response to the current biodiversity crisis based on morals , ethics , and scientific reason. Organizations and citizens are responding to the biodiversity crisis through conservation action plans that direct research, monitoring, and education programs that engage concerns at local through global scales. [4] [5] [6] [7] There is increasing recognition that conservation is not just about what is achieved but how it is done. [18] A "conservation acrostic" has been created to emphasize that point where C = co-produced, O = open, N = nimble, S = solutions-oriented, E = empowering, R = relational, V = values-based, A = actionable, T = transdisciplinary, I = inclusive, O = optimistic, and N = nurturing. [18] History[ edit ] The conservation of natural resources is the fundamental problem. Unless we solve that problem, it will avail us little to solve all others. – Theodore Roosevelt [19] Natural resource conservation[ edit ] Conscious efforts to conserve and protect global biodiversity are a recent phenomenon. [7] [20] Natural resource conservation, however, has a history that extends prior to the age of conservation. Resource ethics grew out of necessity through direct relations with nature. Regulation or communal restraint became necessary to prevent selfish motives from taking more than could be locally sustained, therefore compromising the long-term supply for the rest of the community. [7] This social dilemma with respect to natural resource management is often called the " Tragedy of the Commons ". [21] [22] From this principle, conservation biologists can trace communal resource based ethics throughout cultures as a solution to communal resource conflict. [7] For example, the Alaskan Tlingit peoples and the Haida of the Pacific Northwest had resource boundaries, rules, and restrictions among clans with respect to the fishing of sockeye salmon . These rules were guided by clan elders who knew lifelong details of each river and stream they managed. [7] [23] There are numerous examples in history where cultures have followed rules, rituals, and organized practice with respect to communal natural resource management. [24] [25] The Mauryan emperor Ashoka around 250 BC issued edicts restricting the slaughter of animals and certain kinds of birds, as well as opened veterinary clinics. Conservation ethics are also found in early religious and philosophical writings. There are examples in the Tao , Shinto , Hindu , Islamic and Buddhist traditions. [7] [26] In Greek philosophy, Plato lamented about pasture land degradation : "What is left now is, so to say, the skeleton of a body wasted by disease; the rich, soft soil has been carried off and only the bare framework of the district left." [27] In the bible, through Moses, God commanded to let the land rest from cultivation every seventh year. [7] [28] Before the 18th century, however, much of European culture considered it a pagan view to admire nature. Wilderness was denigrated while agricultural development was praised. [29] However, as early as AD 680 a wildlife sanctuary was founded on the Farne Islands by St Cuthbert in response to his religious beliefs. [7] White gyrfalcons drawn by John James Audubon More conservation research is needed for understanding ecology and behaviour of the dhole in central China. Natural history was a major preoccupation in the 18th century, with grand expeditions and the opening of popular public displays in Europe and North America . By 1900 there were 150 natural history museums in Germany , 250 in Great Britain , 250 in the United States , and 300 in France . [30] Preservationist or conservationist sentiments are a development of the late 18th to early 20th centuries. Before Charles Darwin set sail on HMS Beagle, most people in the world, including Darwin, believed in special creation and that all species were unchanged. [31] George-Louis Leclerc was one of the first naturalist that questioned this belief. He proposed in his 44 volume natural history book that species evolve due to environmental influences. [31] Erasmus Darwin was also a naturalist who also suggested that species evolved. Erasmus Darwin noted that some species have vestigial structures which are anatomical structures that have no apparent function in the species currently but would have been useful for the species' ancestors. [31] The thinking of these early 18th century naturalists helped to change the mindset and thinking of the early 19th century naturalists. By the early 19th century biogeography was ignited through the efforts of Alexander von Humboldt , Charles Lyell and Charles Darwin . [32] The 19th-century fascination with natural history engendered a fervor to be the first to collect rare specimens with the goal of doing so before they became extinct by other such collectors. [29] [30] Although the work of many 18th and 19th century naturalists were to inspire nature enthusiasts and conservation organizations , their writings, by modern standards, showed insensitivity towards conservation as they would kill hundreds of specimens for their collections. [30] Main article: Conservation movement The modern roots of conservation biology can be found in the late 18th-century Enlightenment period particularly in England and Scotland . [29] [33] Thinkers including Lord Monboddo described the importance of "preserving nature"; much of this early emphasis had its origins in Christian theology . [33] Scientific conservation principles were first practically applied to the forests of British India . The conservation ethic that began to evolve included three core principles: that human activity damaged the environment , that there was a civic duty to maintain the environment for future generations, and that scientific, empirically based methods should be applied to ensure this duty was carried out. Sir James Ranald Martin was prominent in promoting this ideology, publishing many medico-topographical reports that demonstrated the scale of damage wrought through large-scale deforestation and desiccation, and lobbying extensively for the institutionalization of forest conservation activities in British India through the establishment of Forest Departments . [34] The Madras Board of Revenue started local conservation efforts in 1842, headed by Alexander Gibson , a professional botanist who systematically adopted a forest conservation program based on scientific principles. This was the first case of state conservation management of forests in the world. [35] Governor-General Lord Dalhousie introduced the first permanent and large-scale forest conservation program in the world in 1855, a model that soon spread to other colonies , as well the United States, [36] [37] [38] where Yellowstone National Park was opened in 1872 as the world's first national park. [39] The term conservation came into widespread use in the late 19th century and referred to the management, mainly for economic reasons, of such natural resources as timber , fish, game, topsoil , pastureland , and minerals. In addition it referred to the preservation of forests ( forestry ), wildlife ( wildlife refuge ), parkland, wilderness , and watersheds . This period also saw the passage of the first conservation legislation and the establishment of the first nature conservation societies. The Sea Birds Preservation Act of 1869 was passed in Britain as the first nature protection law in the world [40] after extensive lobbying from the Association for the Protection of Seabirds [41] and the respected ornithologist Alfred Newton . [42] Newton was also instrumental in the passage of the first Game laws from 1872, which protected animals during their breeding season so as to prevent the stock from being brought close to extinction. [43] One of the first conservation societies was the Royal Society for the Protection of Birds , founded in 1889 in Manchester [44] as a protest group campaigning against the use of great crested grebe and kittiwake skins and feathers in fur clothing . Originally known as "the Plumage League", [45] the group gained popularity and eventually amalgamated with the Fur and Feather League in Croydon, and formed the RSPB. [46] The National Trust formed in 1895 with the manifesto to "...promote the permanent preservation, for the benefit of the nation, of lands, ... to preserve (so far practicable) their natural aspect." In May 1912, a month after the Titanic sank, banker and expert naturalist Charles Rothschild held a meeting at the Natural History Museum in London to discuss his idea for a new organisation to save the best places for wildlife in the British Isles. This meeting led to the formation of the Society for the Promotion of Nature Reserves, which later became the Wildlife Trusts . Some biodiversity loss is more insidious than others due to systemic neglect. For example, sport killing and wanton waste of tons of native fishes from unregulated 21st century bowfishing in the United States. [47] New conservation movements are needed to deter irreparable biodiversity loss to fragile freshwater ecosystems. In the United States , the Forest Reserve Act of 1891 gave the President power to set aside forest reserves from the land in the public domain. John Muir founded the Sierra Club in 1892, and the New York Zoological Society was set up in 1895. A series of national forests and preserves were established by Theodore Roosevelt from 1901 to 1909. [48] [49] The 1916 National Parks Act, included a 'use without impairment' clause, sought by John Muir, which eventually resulted in the removal of a proposal to build a dam in Dinosaur National Monument in 1959. [50] In the 20th century, Canadian civil servants, including Charles Gordon Hewitt [51] and James Harkin , spearheaded the movement toward wildlife conservation . [52] In the 21st century professional conservation officers have begun to collaborate with indigenous communities for protecting wildlife in Canada. [53] Some conservation efforts are yet to fully take hold due to ecological neglect. [54] [55] [56] For example in the USA, 21st century bowfishing of native fishes, which amounts to killing wild animals for recreation and disposing of them immediately afterwards, remains unregulated and unmanaged. [47] Global conservation efforts[ edit ] In the mid-20th century, efforts arose to target individual species for conservation, notably efforts in big cat conservation in South America led by the New York Zoological Society. [57] In the early 20th century the New York Zoological Society was instrumental in developing concepts of establishing preserves for particular species and conducting the necessary conservation studies to determine the suitability of locations that are most appropriate as conservation priorities; the work of Henry Fairfield Osborn Jr., Carl E. Akeley , Archie Carr and his son Archie Carr III is notable in this era. [58] [59] [60] Akeley for example, having led expeditions to the Virunga Mountains and observed the mountain gorilla in the wild, became convinced that the species and the area were conservation priorities. He was instrumental in persuading Albert I of Belgium to act in defense of the mountain gorilla and establish Albert National Park (since renamed Virunga National Park ) in what is now Democratic Republic of Congo . [61] By the 1970s, led primarily by work in the United States under the Endangered Species Act [62] along with the Species at Risk Act (SARA) of Canada, Biodiversity Action Plans developed in Australia , Sweden , the United Kingdom , hundreds of species specific protection plans ensued. Notably the United Nations acted to conserve sites of outstanding cultural or natural importance to the common heritage of mankind. The programme was adopted by the General Conference of UNESCO in 1972. As of 2006, a total of 830 sites are listed: 644 cultural, 162 natural. The first country to pursue aggressive biological conservation through national legislation was the United States, which passed back to back legislation in the Endangered Species Act [63] (1966) and National Environmental Policy Act (1970), [64] which together injected major funding and protection measures to large-scale habitat protection and threatened species research. Other conservation developments, however, have taken hold throughout the world. India, for example, passed the Wildlife Protection Act of 1972 . [65] In 1980, a significant development was the emergence of the urban conservation movement. A local organization was established in Birmingham , UK, a development followed in rapid succession in cities across the UK, then overseas. Although perceived as a grassroots movement , its early development was driven by academic research into urban wildlife. Initially perceived as radical, the movement's view of conservation being inextricably linked with other human activity has now become mainstream in conservation thought. Considerable research effort is now directed at urban conservation biology. The Society for Conservation Biology originated in 1985. [7] : 2 By 1992, most of the countries of the world had become committed to the principles of conservation of biological diversity with the Convention on Biological Diversity ; [66] subsequently many countries began programmes of Biodiversity Action Plans to identify and conserve threatened species within their borders, as well as protect associated habitats. The late 1990s saw increasing professionalism in the sector, with the maturing of organisations such as the Institute of Ecology and Environmental Management and the Society for the Environment . Since 2000, the concept of landscape scale conservation has risen to prominence, with less emphasis being given to single-species or even single-habitat focused actions. Instead an ecosystem approach is advocated by most mainstream conservationists, although concerns have been expressed by those working to protect some high-profile species. Ecology has clarified the workings of the biosphere ; i.e., the complex interrelationships among humans, other species, and the physical environment. The burgeoning human population and associated agriculture , industry , and the ensuing pollution, have demonstrated how easily ecological relationships can be disrupted. [67] The last word in ignorance is the man who says of an animal or plant: "What good is it?" If the land mechanism as a whole is good, then every part is good, whether we understand it or not. If the biota, in the course of aeons, has built something we like but do not understand, then who but a fool would discard seemingly useless parts? To keep every cog and wheel is the first precaution of intelligent tinkering. Marine extinction intensity during Phanerozoic Millions of years ago O–S The blue graph shows the apparent percentage (not the absolute number) of marine animal genera becoming extinct during any given time interval. It does not represent all marine species, just those that are readily fossilized. The labels of the traditional "Big Five" extinction events and the more recently recognised Capitanian mass extinction event are clickable links; see Extinction event for more details. ( source and image info ) Extinction rates are measured in a variety of ways. Conservation biologists measure and apply statistical measures of fossil records , [1] [68] rates of habitat loss , and a multitude of other variables such as loss of biodiversity as a function of the rate of habitat loss and site occupancy [69] to obtain such estimates. [70] The Theory of Island Biogeography [71] is possibly the most significant contribution toward the scientific understanding of both the process and how to measure the rate of species extinction. The current background extinction rate is estimated to be one species every few years. [72] Actual extinction rates are estimated to be orders of magnitudes higher. [73] While this is important, it's worth noting that there are no models in existence that account for the complexity of unpredictable factors like species movement, a non-analog climate, changing species interactions, evolutionary rates on finer time scales, and many other stochastic variables. [74] [17] The measure of ongoing species loss is made more complex by the fact that most of the Earth's species have not been described or evaluated. Estimates vary greatly on how many species actually exist (estimated range: 3,600,000–111,700,000) [75] to how many have received a species binomial (estimated range: 1.5–8 million). [75] Less than 1% of all species that have been described beyond simply noting its existence. [75] From these figures, the IUCN reports that 23% of vertebrates , 5% of invertebrates and 70% of plants that have been evaluated are designated as endangered or threatened . [76] [77] Better knowledge is being constructed by The Plant List for actual numbers of species. Systematic conservation planning[ edit ] Systematic conservation planning is an effective way to seek and identify efficient and effective types of reserve design to capture or sustain the highest priority biodiversity values and to work with communities in support of local ecosystems. Margules and Pressey identify six interlinked stages in the systematic planning approach: [78] Compile data on the biodiversity of the planning region Identify conservation goals for the planning region Review existing conservation areas Implement conservation actions Maintain the required values of conservation areas Conservation biologists regularly prepare detailed conservation plans for grant proposals or to effectively coordinate their plan of action and to identify best management practices (e.g. [79] ). Systematic strategies generally employ the services of Geographic Information Systems to assist in the decision-making process. The SLOSS debate is often considered in planning. Conservation physiology: a mechanistic approach to conservation[ edit ] Conservation physiology was defined by Steven J. Cooke and colleagues as: [10] An integrative scientific discipline applying physiological concepts, tools, and knowledge to characterizing biological diversity and its ecological implications; understanding and predicting how organisms, populations, and ecosystems respond to environmental change and stressors; and solving conservation problems across the broad range of taxa (i.e. including microbes, plants, and animals). Physiology is considered in the broadest possible terms to include functional and mechanistic responses at all scales, and conservation includes the development and refinement of strategies to rebuild populations, restore ecosystems, inform conservation policy, generate decision-support tools, and manage natural resources. Conservation physiology is particularly relevant to practitioners in that it has the potential to generate cause-and-effect relationships and reveal the factors that contribute to population declines. Conservation biology as a profession[ edit ] The Society for Conservation Biology is a global community of conservation professionals dedicated to advancing the science and practice of conserving biodiversity. Conservation biology as a discipline reaches beyond biology, into subjects such as philosophy , law , economics , humanities , arts , anthropology , and education . [5] [6] Within biology, conservation genetics and evolution are immense fields unto themselves, but these disciplines are of prime importance to the practice and profession of conservation biology. Conservationists introduce bias when they support policies using qualitative description, such as habitat degradation , or healthy ecosystems . Conservation biologists advocate for reasoned and sensible management of natural resources and do so with a disclosed combination of science , reason , logic , and values in their conservation management plans. [5] This sort of advocacy is similar to the medical profession advocating for healthy lifestyle options, both are beneficial to human well-being yet remain scientific in their approach. There is a movement in conservation biology suggesting a new form of leadership is needed to mobilize conservation biology into a more effective discipline that is able to communicate the full scope of the problem to society at large. [80] The movement proposes an adaptive leadership approach that parallels an adaptive management approach. The concept is based on a new philosophy or leadership theory steering away from historical notions of power, authority, and dominance. Adaptive conservation leadership is reflective and more equitable as it applies to any member of society who can mobilize others toward meaningful change using communication techniques that are inspiring, purposeful, and collegial. Adaptive conservation leadership and mentoring programs are being implemented by conservation biologists through organizations such as the Aldo Leopold Leadership Program. [81] Approaches[ edit ] Conservation may be classified as either in-situ conservation , which is protecting an endangered species in its natural habitat , or ex-situ conservation , which occurs outside the natural habitat. [82] In-situ conservation involves protecting or restoring the habitat. Ex-situ conservation, on the other hand, involves protection outside of an organism's natural habitat, such as on reservations or in gene banks , in circumstances where viable populations may not be present in the natural habitat. [82] Also, non-interference may be used, which is termed a preservationist method. Preservationists advocate for giving areas of nature and species a protected existence that halts interference from the humans. [5] In this regard, conservationists differ from preservationists in the social dimension, as conservation biology engages society and seeks equitable solutions for both society and ecosystems. Some preservationists emphasize the potential of biodiversity in a world without humans. Ecological monitoring in conservation[ edit ] Ecological monitoring is the systematic collection of data relevant to the ecology of a species or habitat at repeating intervals with defined methods. [83] Long-term monitoring for environmental and ecological metrics is an important part of any successful conservation initiative. Unfortunately, long-term data for many species and habitats is not available in many cases. [84] A lack of historical data on species populations , habitats, and ecosystems means that any current or future conservation work will have to make assumptions to determine if the work is having any effect on the population or ecosystem health. Ecological monitoring can provide early warning signals of deleterious effects (from human activities or natural changes in an environment) on an ecosystem and its species. [83] In order for signs of negative trends in ecosystem or species health to be detected, monitoring methods must be carried out at appropriate time intervals, and the metric must be able to capture the trend of the population or habitat as a whole. Long-term monitoring can include the continued measuring of many biological, ecological, and environmental metrics including annual breeding success, population size estimates, water quality, biodiversity (which can be measured in many way, i.e. Shannon Index ), and many other methods. When determining which metrics to monitor for a conservation project, it is important to understand how an ecosystem functions and what role different species and abiotic factors have within the system. [85] It is important to have a precise reason for why ecological monitoring is implemented; within the context of conservation, this reasoning is often to track changes before, during, or after conservation measures are put in place to help a species or habitat recover from degradation and/or maintain integrity. [83] Another benefit of ecological monitoring is the hard evidence it provides scientists to use for advising policy makers and funding bodies about conservation efforts. Not only is ecological monitoring data important for convincing politicians, funders, and the public why a conservation program is important to implement, but also to keep them convinced that a program should be continued to be supported. [84] There is plenty of debate on how conservation resources can be used most efficiently; even within ecological monitoring, there is debate on which metrics that money, time and personnel should be dedicated to for the best chance of making a positive impact. One specific general discussion topic is whether monitoring should happen where there is little human impact (to understand a system that has not been degraded by humans), where there is human impact (so the effects from humans can be investigated), or where there is data deserts and little is known about the habitats' and communities' response to human perturbations . [83] The concept of bioindicators / indicator species can be applied to ecological monitoring as a way to investigate how pollution is affecting an ecosystem. [86] Species like amphibians and birds are highly susceptible to pollutants in their environment due to their behaviours and physiological features that cause them to absorb pollutants at a faster rate than other species. Amphibians spend parts of their time in the water and on land, making them susceptible to changes in both environments. [87] They also have very permeable skin that allows them to breath and intake water, which means they also take any air or water-soluble pollutants in as well. Birds often cover a wide range in habitat types annually, and also generally revisit the same nesting site each year. This makes it easier for researchers to track ecological effects at both an individual and a population level for the species. [88] Many conservation researchers believe that having a long-term ecological monitoring program should be a priority for conservation projects, protected areas, and regions where environmental harm mitigation is used. Ethics and values[ edit ] See also: Conservation (ethic) and Land ethic Conservation biologists are interdisciplinary researchers that practice ethics in the biological and social sciences. Chan states [89] that conservationists must advocate for biodiversity and can do so in a scientifically ethical manner by not promoting simultaneous advocacy against other competing values. A conservationist may be inspired by the resource conservation ethic, [7] : 15 which seeks to identify what measures will deliver "the greatest good for the greatest number of people for the longest time." [5] : 13 In contrast, some conservation biologists argue that nature has an intrinsic value that is independent of anthropocentric usefulness or utilitarianism . [7] : 3, 12, 16–17 Aldo Leopold was a classical thinker and writer on such conservation ethics whose philosophy, ethics and writings are still valued and revisited by modern conservation biologists. [7] : 16–17 See also: Biodiversity § Benefits of biodiversity A pie chart image showing the relative biomass representation in a rain forest through a summary of children's perceptions from drawings and artwork (left), through a scientific estimate of actual biomass (middle), and by a measure of biodiversity (right). The biomass of social insects (middle) far outweighs the number of species (right). The International Union for Conservation of Nature (IUCN) has organized a global assortment of scientists and research stations across the planet to monitor the changing state of nature in an effort to tackle the extinction crisis. The IUCN provides annual updates on the status of species conservation through its Red List. [90] The IUCN Red List serves as an international conservation tool to identify those species most in need of conservation attention and by providing a global index on the status of biodiversity. [91] More than the dramatic rates of species loss, however, conservation scientists note that the sixth mass extinction is a biodiversity crisis requiring far more action than a priority focus on rare , endemic or endangered species . Concerns for biodiversity loss covers a broader conservation mandate that looks at ecological processes , such as migration, and a holistic examination of biodiversity at levels beyond the species, including genetic, population and ecosystem diversity. [92] Extensive, systematic, and rapid rates of biodiversity loss threatens the sustained well-being of humanity by limiting supply of ecosystem services that are otherwise regenerated by the complex and evolving holistic network of genetic and ecosystem diversity. While the conservation status of species is employed extensively in conservation management, [91] some scientists highlight that it is the common species that are the primary source of exploitation and habitat alteration by humanity. Moreover, common species are often undervalued despite their role as the primary source of ecosystem services. [93] [94] While most in the community of conservation science "stress the importance" of sustaining biodiversity , [95] there is debate on how to prioritize genes, species, or ecosystems, which are all components of biodiversity (e.g. Bowen, 1999). While the predominant approach to date has been to focus efforts on endangered species by conserving biodiversity hotspots , some scientists (e.g) [96] and conservation organizations, such as the Nature Conservancy , argue that it is more cost-effective, logical, and socially relevant to invest in biodiversity coldspots. [97] The costs of discovering, naming, and mapping out the distribution of every species, they argue, is an ill-advised conservation venture. They reason it is better to understand the significance of the ecological roles of species. [92] Biodiversity hotspots and coldspots are a way of recognizing that the spatial concentration of genes, species, and ecosystems is not uniformly distributed on the Earth's surface. For example, "... 44% of all species of vascular plants and 35% of all species in four vertebrate groups are confined to 25 hotspots comprising only 1.4% of the land surface of the Earth." [98] Those arguing in favor of setting priorities for coldspots point out that there are other measures to consider beyond biodiversity. They point out that emphasizing hotspots downplays the importance of the social and ecological connections to vast areas of the Earth's ecosystems where biomass , not biodiversity, reigns supreme. [99] It is estimated that 36% of the Earth's surface, encompassing 38.9% of the worlds vertebrates, lacks the endemic species to qualify as biodiversity hotspot. [100] Moreover, measures show that maximizing protections for biodiversity does not capture ecosystem services any better than targeting randomly chosen regions. [101] Population level biodiversity (mostly in coldspots) are disappearing at a rate that is ten times that at the species level. [96] [102] The level of importance in addressing biomass versus endemism as a concern for conservation biology is highlighted in literature measuring the level of threat to global ecosystem carbon stocks that do not necessarily reside in areas of endemism. [103] [104] A hotspot priority approach [105] would not invest so heavily in places such as steppes , the Serengeti , the Arctic , or taiga . These areas contribute a great abundance of population (not species) level biodiversity [102] and ecosystem services , including cultural value and planetary nutrient cycling . [97] Summary of 2006 IUCN Red List categories:  EX ( Extinct ) — EW ( Extinct in the Wild ) — CR ( Critically Endangered ) — EN ( Endangered ) — VU ( Vulnerable ) — NT ( Near Threatened ) — LC ( Least Concern ) Those in favor of the hotspot approach point out that species are irreplaceable components of the global ecosystem, they are concentrated in places that are most threatened, and should therefore receive maximal strategic protections. [106] This is a hotspot approach because the priority is set to target species level concerns over population level or biomass. [102] [ failed verification ] Species richness and genetic biodiversity contributes to and engenders ecosystem stability, ecosystem processes, evolutionary adaptability , and biomass. [107] Both sides agree, however, that conserving biodiversity is necessary to reduce the extinction rate and identify an inherent value in nature; the debate hinges on how to prioritize limited conservation resources in the most cost-effective way. Economic values and natural capital[ edit ] See also: Ecosystem services and Biodiversity Conservation biologists have started to collaborate with leading global economists to determine how to measure the wealth and services of nature and to make these values apparent in global market transactions . [108] This system of accounting is called natural capital and would, for example, register the value of an ecosystem before it is cleared to make way for development. [109] The WWF publishes its Living Planet Report and provides a global index of biodiversity by monitoring approximately 5,000 populations in 1,686 species of vertebrate (mammals, birds, fish, reptiles, and amphibians) and report on the trends in much the same way that the stock market is tracked. [110] This method of measuring the global economic benefit of nature has been endorsed by the G8+5 leaders and the European Commission . [108] Nature sustains many ecosystem services [111] that benefit humanity. [112] Many of the Earth's ecosystem services are public goods without a market and therefore no price or value . [108] When the stock market registers a financial crisis, traders on Wall Street are not in the business of trading stocks for much of the planet's living natural capital stored in ecosystems. There is no natural stock market with investment portfolios into sea horses, amphibians, insects, and other creatures that provide a sustainable supply of ecosystem services that are valuable to society. [112] The ecological footprint of society has exceeded the bio-regenerative capacity limits of the planet's ecosystems by about 30 percent, which is the same percentage of vertebrate populations that have registered decline from 1970 through 2005. [110] The ecological credit crunch is a global challenge. The Living Planet Report 2008 tells us that more than three-quarters of the world's people live in nations that are ecological debtors – their national consumption has outstripped their country's biocapacity. Thus, most of us are propping up our current lifestyles, and our economic growth, by drawing (and increasingly overdrawing) upon the ecological capital of other parts of the world. WWF Living Planet Report [110] The inherent natural economy plays an essential role in sustaining humanity, [113] including the regulation of global atmospheric chemistry , pollinating crops , pest control , [114] cycling soil nutrients , purifying our water supply , [115] supplying medicines and health benefits, [116] and unquantifiable quality of life improvements. There is a relationship, a correlation , between markets and natural capital , and social income inequity and biodiversity loss. This means that there are greater rates of biodiversity loss in places where the inequity of wealth is greatest [117] Although a direct market comparison of natural capital is likely insufficient in terms of human value , one measure of ecosystem services suggests the contribution amounts to trillions of dollars yearly. [118] [119] [120] [121] For example, one segment of North American forests has been assigned an annual value of 250 billion dollars; [122] as another example, honey bee pollination is estimated to provide between 10 and 18 billion dollars of value yearly. [123] The value of ecosystem services on one New Zealand island has been imputed to be as great as the GDP of that region. [124] This planetary wealth is being lost at an incredible rate as the demands of human society is exceeding the bio-regenerative capacity of the Earth. While biodiversity and ecosystems are resilient, the danger of losing them is that humans cannot recreate many ecosystem functions through technological innovation . Strategic species concepts[ edit ] Main article: Keystone species Some species, called a keystone species form a central supporting hub unique to their ecosystem. [125] The loss of such a species results in a collapse in ecosystem function, as well as the loss of coexisting species. [5] Keystone species are usually predators due to their ability to control the population of prey in their ecosystem. [125] The importance of a keystone species was shown by the extinction of the Steller's sea cow (Hydrodamalis gigas) through its interaction with sea otters , sea urchins , and kelp . Kelp beds grow and form nurseries in shallow waters to shelter creatures that support the food chain . Sea urchins feed on kelp, while sea otters feed on sea urchins. With the rapid decline of sea otters due to overhunting , sea urchin populations grazed unrestricted on the kelp beds and the ecosystem collapsed . Left unchecked, the urchins destroyed the shallow water kelp communities that supported the Steller's sea cow's diet and hastened their demise. [126] The sea otter was thought to be a keystone species because the coexistence of many ecological associates in the kelp beds relied upon otters for their survival. However this was later questioned by Turvey and Risley, [127] who showed that hunting alone would have driven the Steller's sea cow extinct. Main article: Indicator species An indicator species has a narrow set of ecological requirements, therefore they become useful targets for observing the health of an ecosystem. Some animals, such as amphibians with their semi-permeable skin and linkages to wetlands , have an acute sensitivity to environmental harm and thus may serve as a miner's canary . Indicator species are monitored in an effort to capture environmental degradation through pollution or some other link to proximate human activities. [5] Monitoring an indicator species is a measure to determine if there is a significant environmental impact that can serve to advise or modify practice, such as through different forest silviculture treatments and management scenarios, or to measure the degree of harm that a pesticide may impart on the health of an ecosystem. Government regulators, consultants, or NGOs regularly monitor indicator species, however, there are limitations coupled with many practical considerations that must be followed for the approach to be effective. [128] It is generally recommended that multiple indicators (genes, populations, species, communities, and landscape) be monitored for effective conservation measurement that prevents harm to the complex, and often unpredictable, response from ecosystem dynamics (Noss, 1997 [129] : 88–89 ). Umbrella and flagship species[ edit ] Main articles: Umbrella species and Flagship species An example of an umbrella species is the monarch butterfly , because of its lengthy migrations and aesthetic value. The monarch migrates across North America, covering multiple ecosystems and so requires a large area to exist. Any protections afforded to the monarch butterfly will at the same time umbrella many other species and habitats. An umbrella species is often used as flagship species, which are species, such as the giant panda , the blue whale , the tiger , the mountain gorilla and the monarch butterfly, that capture the public's attention and attract support for conservation measures. [5] Paradoxically, however, conservation bias towards flagship species sometimes threatens other species of chief concern. [130] Context and trends[ edit ] Conservation biologists study trends and process from the paleontological past to the ecological present as they gain an understanding of the context related to species extinction . [1] It is generally accepted that there have been five major global mass extinctions that register in Earth's history. These include: the Ordovician (440 mya ), Devonian (370 mya), Permian–Triassic (245 mya), Triassic–Jurassic (200 mya), and Cretaceous–Paleogene extinction event (66 mya) extinction spasms. Within the last 10,000 years, human influence over the Earth's ecosystems has been so extensive that scientists have difficulty estimating the number of species lost; [131] that is to say the rates of deforestation , reef destruction , wetland draining and other human acts are proceeding much faster than human assessment of species. The latest Living Planet Report by the World Wide Fund for Nature estimates that we have exceeded the bio-regenerative capacity of the planet, requiring 1.6 Earths to support the demands placed on our natural resources. [132] Main article: Holocene extinction An art scape image showing the relative importance of animals in a rain forest through a summary of (a) child's perception compared with (b) a scientific estimate of the importance. The size of the animal represents its importance. The child's mental image places importance on big cats, birds, butterflies, and then reptiles versus the actual dominance of social insects (such as ants). Conservation biologists are dealing with and have published evidence from all corners of the planet indicating that humanity may be causing the sixth and fastest planetary extinction event . [133] [134] [135] It has been suggested that an unprecedented number of species is becoming extinct in what is known as the Holocene extinction event . [136] The global extinction rate may be approximately 1,000 times higher than the natural background extinction rate. [137] It is estimated that two-thirds of all mammal genera and one-half of all mammal species weighing at least 44 kilograms (97 lb) have gone extinct in the last 50,000 years. [127] [138] [139] [140] The Global Amphibian Assessment [141] reports that amphibians are declining on a global scale faster than any other vertebrate group, with over 32% of all surviving species being threatened with extinction. The surviving populations are in continual decline in 43% of those that are threatened. Since the mid-1980s the actual rates of extinction have exceeded 211 times rates measured from the fossil record . [142] However, "The current amphibian extinction rate may range from 25,039 to 45,474 times the background extinction rate for amphibians." [142] The global extinction trend occurs in every major vertebrate group that is being monitored. For example, 23% of all mammals and 12% of all birds are Red Listed by the International Union for Conservation of Nature (IUCN), meaning they too are threatened with extinction. Even though extinction is natural, the decline in species is happening at such an incredible rate that evolution can simply not match, therefore, leading to the greatest continual mass extinction on Earth. [143] Humans have dominated the planet and our high consumption of resources, along with the pollution generated is affecting the environments in which other species live. [143] [144] There are a wide variety of species that humans are working to protect such as the Hawaiian Crow and the Whooping Crane of Texas. [145] People can also take action on preserving species by advocating and voting for global and national policies that improve climate, under the concepts of climate mitigation and climate restoration . The Earth's oceans demand particular attention as climate change continues to alter pH levels, making it uninhabitable for organisms with shells which dissolve as a result. [137] Status of oceans and reefs[ edit ] See also: Coral reef , Marine pollution , Marine conservation , and Human impact on marine life Global assessments of coral reefs of the world continue to report drastic and rapid rates of decline. By 2000, 27% of the world's coral reef ecosystems had effectively collapsed. The largest period of decline occurred in a dramatic "bleaching" event in 1998, where approximately 16% of all the coral reefs in the world disappeared in less than a year. Coral bleaching is caused by a mixture of environmental stresses , including increases in ocean temperatures and acidity , causing both the release of symbiotic algae and death of corals. [146] Decline and extinction risk in coral reef biodiversity has risen dramatically in the past ten years. The loss of coral reefs, which are predicted to go extinct in the next century, threatens the balance of global biodiversity, will have huge economic impacts, and endangers food security for hundreds of millions of people. [147] Conservation biology plays an important role in international agreements covering the world's oceans [146] (and other issues pertaining to biodiversity [148] ). These predictions will undoubtedly appear extreme, but it is difficult to imagine how such changes will not come to pass without fundamental changes in human behavior. J.B. Jackson [16] : 11463 The oceans are threatened by acidification due to an increase in CO2 levels. This is a most serious threat to societies relying heavily upon oceanic natural resources . A concern is that the majority of all marine species will not be able to evolve or acclimate in response to the changes in the ocean chemistry. [149] The prospects of averting mass extinction seems unlikely when "90% of all of the large (average approximately ≥50 kg), open ocean tuna, billfishes, and sharks in the ocean" [16] are reportedly gone. Given the scientific review of current trends, the ocean is predicted to have few surviving multi-cellular organisms with only microbes left to dominate marine ecosystems . [16] Groups other than vertebrates[ edit ] Serious concerns also being raised about taxonomic groups that do not receive the same degree of social attention or attract funds as the vertebrates. These include fungal (including lichen -forming species), [150] invertebrate (particularly insect [14] [151] [152] ) and plant communities [153] where the vast majority of biodiversity is represented. Conservation of fungi and conservation of insects, in particular, are both of pivotal importance for conservation biology. As mycorrhizal symbionts, and as decomposers and recyclers, fungi are essential for sustainability of forests. [150] The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness . The greatest bulk of biomass on land is found in plants, which is sustained by insect relations. This great ecological value of insects is countered by a society that often reacts negatively toward these aesthetically 'unpleasant' creatures. [154] [155] One area of concern in the insect world that has caught the public eye is the mysterious case of missing honey bees (Apis mellifera). Honey bees provide an indispensable ecological services through their acts of pollination supporting a huge variety of agriculture crops. The use of honey and wax have become vastly used throughout the world. [156] The sudden disappearance of bees leaving empty hives or colony collapse disorder (CCD) is not uncommon. However, in 16-month period from 2006 through 2007, 29% of 577 beekeepers across the United States reported CCD losses in up to 76% of their colonies. This sudden demographic loss in bee numbers is placing a strain on the agricultural sector. The cause behind the massive declines is puzzling scientists. Pests , pesticides , and global warming are all being considered as possible causes. [157] [158] Another highlight that links conservation biology to insects, forests, and climate change is the mountain pine beetle (Dendroctonus ponderosae) epidemic of British Columbia , Canada, which has infested 470,000 km2 (180,000 sq mi) of forested land since 1999. [103] An action plan has been prepared by the Government of British Columbia to address this problem. [159] [160] This impact [pine beetle epidemic] converted the forest from a small net carbon sink to a large net carbon source both during and immediately after the outbreak. In the worst year, the impacts resulting from the beetle outbreak in British Columbia were equivalent to 75% of the average annual direct forest fire emissions from all of Canada during 1959–1999. Kurz et al. [104] Main article: Conservation biology of parasites A large proportion of parasite species are threatened by extinction. A few of them are being eradicated as pests of humans or domestic animals; however, most of them are harmless. Parasites also make up a significant amount of global biodiversity, given that they make up a large proportion of all species on earth, [161] making them of increasingly prevalent conservation interest. Threats include the decline or fragmentation of host populations, or the extinction of host species. Parasites are intricately woven into ecosystems and food webs, thereby occupying valuable roles in ecosystem structure and function. [162] [161] Threats to biodiversity[ edit ] Main article: Biodiversity threats Today, many threats to biodiversity exist. An acronym that can be used to express the top threats of present-day H.I.P.P.O stands for Habitat Loss, Invasive Species, Pollution, Human Population, and Overharvesting. [163] The primary threats to biodiversity are habitat destruction (such as deforestation , agricultural expansion , urban development ), and overexploitation (such as wildlife trade ). [131] [164] [165] [166] [167] [168] [169] [170] [171] Habitat fragmentation also poses challenges, because the global network of protected areas only covers 11.5% of the Earth's surface. [172] A significant consequence of fragmentation and lack of linked protected areas is the reduction of animal migration on a global scale. Considering that billions of tonnes of biomass are responsible for nutrient cycling across the earth, the reduction of migration is a serious matter for conservation biology. [173] [174] Human activities are associated directly or indirectly with nearly every aspect of the current extinction spasm. Wake and Vredenburg [133] However, human activities need not necessarily cause irreparable harm to the biosphere. With conservation management and planning for biodiversity at all levels, from genes to ecosystems, there are examples where humans mutually coexist in a sustainable way with nature. [175] Even with the current threats to biodiversity there are ways we can improve the current condition and start anew. Many of the threats to biodiversity, including disease and climate change, are reaching inside borders of protected areas, leaving them 'not-so protected' (e.g. Yellowstone National Park ). [176] Climate change , for example, is often cited as a serious threat in this regard, because there is a feedback loop between species extinction and the release of carbon dioxide into the atmosphere . [103] [104] Ecosystems store and cycle large amounts of carbon which regulates global conditions. [177] In present day, there have been major climate shifts with temperature changes making survival of some species difficult. [163] The effects of global warming add a catastrophic threat toward a mass extinction of global biological diversity. [178] Numerous more species are predicted to face unprecedented levels of extinction risk due to population increase, climate change and economic development in the future. [179] Conservationists have claimed that not all the species can be saved, and they have to decide which their efforts should be used to protect. This concept is known as the Conservation Triage. [163] The extinction threat is estimated to range from 15 to 37 percent of all species by 2050, [178] or 50 percent of all species over the next 50 years. [14] The current extinction rate is 100–100,000 times more rapid today than the last several billion years. [163]
Toggle the table of contents Natural disaster For other uses, see Natural disaster (disambiguation) . Global multihazard proportional economic loss by natural disasters as cyclones, droughts, earthquakes, floods, landslides and volcanoes A natural disaster is the highly harmful impact on a society or community following a natural hazard event. Some examples of natural hazard events include: flooding , drought , earthquake , tropical cyclone , lightning , tsunami , volcanic activity , wildfire . [1] A natural disaster can cause loss of life or damage property, and typically leaves economic damage in its wake. The severity of the damage depends on the affected population's resilience and on the infrastructure available. [2] Scholars have been saying that the term natural disaster is unsuitable and should be abandoned. Instead, the simpler term disaster could be used, while also specifying the category (or type) of hazard. [3] [4] [5] A disaster is a result of a natural or human-made hazard impacting a vulnerable community . It is the combination of the hazard along with exposure of a vulnerable society that results in a disaster. In modern times, the divide between natural, human-made and human-accelerated disasters is quite difficult to draw. [6] [7] [8] Human choices and activities like architecture, [9] fire, [10] [11] resource management [11] [12] and climate change [13] potentially play a role in causing natural disasters. In fact, the term natural disaster was called a misnomer already in 1976. [5] Natural disasters can be aggravated by inadequate building norms, marginalization of people, inequities, overexploitation of resources, extreme urban sprawl and climate change . [6] The rapid growth of the world's population and its increased concentration often in hazardous environments has escalated both the frequency and severity of disasters. Extreme climates (such as those in the Tropics) and unstable landforms , coupled with deforestation , unplanned growth proliferation and non-engineered constructions create more vulnerable interfaces of populated areas with disaster-prone natural spaces . Developing countries which suffer from chronic natural disasters, often have ineffective communication systems combined with insufficient support for disaster prevention and management . [14] An adverse event will not rise to the level of a disaster if it occurs in an area without a vulnerable population . [15] [16] Once a vulnerable population has experienced a disaster, the community can take many years to repair and that repair period can lead to further vulnerability. The disastrous consequences of natural disaster also affect the mental health of affected communities, often leading to post-traumatic symptoms. These increased emotional experiences can be supported through collective processing, leading to resilience and increased community engagement. [17] Terminology A natural disaster is the highly harmful impact on a society or community following a natural hazard event. The term " disaster " itself is defined as follows: "Disasters are serious disruptions to the functioning of a community that exceed its capacity to cope using its own resources. Disasters can be caused by natural, man-made and technological hazards , as well as various factors that influence the exposure and vulnerability of a community." [18] The US Federal Emergency Management Agency (FEMA) explains the relationship between natural disasters and natural hazards as follows: "Natural hazards and natural disasters are related but are not the same. A natural hazard is the threat of an event that will likely have a negative impact. A natural disaster is the negative impact following an actual occurrence of natural hazard in the event that it significantly harms a community. [1] An example of the distinction between a natural hazard and a disaster is that an earthquake is the hazard which caused the 1906 San Francisco earthquake disaster. A natural hazard [19] is a natural phenomenon that might have a negative effect on humans and other animals , or the environment . Natural hazard events can be classified into two broad categories: geophysical and biological . [20] Natural hazards can be provoked or affected by anthropogenic processes , e.g. land-use change , drainage and construction. [21] There are 18 natural hazards included in the National Risk Index of FEMA: avalanche , coastal flooding , cold wave , drought , earthquake , hail , heat wave , tropical cyclone , ice storm , landslide , lightning , riverine flooding, strong wind, tornado , tsunami , volcanic activity , wildfire , winter weather. [1] In addition there are also tornados and dust storms . Critique The term natural disaster has been called a misnomer already in 1976. [5] A disaster is a result of a natural hazard impacting a vulnerable community . But disasters can be avoided. Earthquakes, droughts, floods, storms, and other events lead to disasters because of human action and inaction. Poor land and policy planning and deregulation can create worse conditions. They often involve development activities that ignore or fail to reduce the disaster risks . Nature alone is blamed for disasters even when disasters result from failures in development. Disasters also result from failure of societies to prepare. Examples for such failures include inadequate building norms, marginalization of people, inequities, overexploitation of resources, extreme urban sprawl and climate change . [5] Defining disasters as solely natural events has serious implications when it comes to understanding the causes of a disaster and the distribution of political and financial responsibility in disaster risk reduction , disaster management , compensation, insurance and disaster prevention. [22] Using natural to describe disasters misleads people to think the devastating results are inevitable, out of our control, and are simply part of a natural process. Hazards (earthquakes, hurricanes, pandemics, drought etc.) are inevitable, but the impact they have on society is not. Thus, the term natural disaster is unsuitable and should be abandoned in favour of the simpler term disaster, while also specifying the category (or type) of hazard. [3] Scale Main articles: List of natural disasters by death toll and List of countries by natural disaster risk Number of recorded natural disaster events (1900–2022) Some of the 18 natural hazards included in the National Risk Index of FEMA [1] now have a higher probability of occurring, and at higher intensity, due to the effects of climate change . This applies to heat waves, droughts, wildfire and coastal flooding. [23] : 9 By region and country As of 2019, the countries with the highest share of disability-adjusted life years ( DALY ) lost due to natural disasters are Bahamas , Haiti , Zimbabwe and Armenia (probably mainly due to the Spitak Earthquake ). [24] [25] The Asia-Pacific region is the world's most disaster prone region. [26] A person in Asia-Pacific is five times more likely to be hit by a natural disaster than someone living in other regions. [27] Between 1995 and 2015, the greatest number of natural disasters occurred in America, China and India. [28] In 2012, there were 905 natural disasters worldwide, 93% of which were weather-related disasters. Overall costs were US$170 billion and insured losses $70 billion. 2012 was a moderate year. 45% were meteorological (storms), 36% were hydrological (floods), 12% were climatological (heat waves, cold waves, droughts, wildfires) and 7% were geophysical events (earthquakes and volcanic eruptions). Between 1980 and 2011 geophysical events accounted for 14% of all natural catastrophes. [29] Slow and rapid onset events Natural hazards occur across different time scales as well as area scales. Tornadoes and flash floods are rapid onset events, meaning they occur with a short warning time and are short-lived. Slow onset events can also be very damaging, for example drought is a natural hazards that develops slowly, sometimes over years. [30] Impacts Global death rate from natural disasters (1900–2022) Global damage cost from natural disasters (1980–2022) A natural disaster may cause loss of life, injury or other health impacts, property damage, loss of livelihoods and services, social and economic disruption, or environmental damage. Various phenomena like earthquakes , landslides , volcanic eruptions , floods , hurricanes , tornadoes , blizzards , tsunamis , cyclones , wildfires , and pandemics are all natural hazards that kill thousands of people and destroy billions of dollars of habitat and property each year. [31] However, the rapid growth of the world's population and its increased concentration often in hazardous environments has escalated both the frequency and severity of disasters. With the tropical climate and unstable landforms , coupled with deforestation, unplanned growth proliferation, non-engineered constructions make the disaster-prone areas more vulnerable.[ citation needed ] The death rate from natural disasters is highest in poorly developed countries due to the lower quality of building construction, infrastructure, and medical facilities. [32] Globally, the total number of deaths from natural disasters has been reduced by 75% over the last 100 years, due to the increased development of countries, increased preparedness, better education, better methods, and aid from international organizations. Since the global population has grown over the same time period, the decrease in number of deaths per capita is larger, dropping to 6% of the original amount. [32] On the environment During emergencies such as natural disasters and armed conflicts more waste may be produced, while waste management is given low priority compared with other services. Existing waste management services and infrastructures can be disrupted, leaving communities with unmanaged waste and increased littering. Under these circumstances human health and the environment are often negatively impacted. [33] Natural disasters (e.g. earthquakes, tsunamis, hurricanes) have the potential to generate a significant amount of waste within a short period. Waste management systems can be out of action or curtailed, often requiring considerable time and funding to restore. For example, the tsunami in Japan in 2011 produced huge amounts of debris: estimates of 5 million tonnes of waste were reported by the Japanese Ministry of the Environment . Some of this waste, mostly plastic and styrofoam washed up on the coasts of Canada and the United States in late 2011. Along the west coast of the United States, this increased the amount of litter by a factor of 10 and may have transported alien species. Storms are also important generators of plastic litter. A study by Lo et al. (2020) reported a 100% increase in the amount of microplastics on beaches surveyed following a typhoon in Hong Kong in 2018. [33] A significant amount of plastic waste can be produced during disaster relief operations. Following the 2010 earthquake in Haiti , the generation of waste from relief operations was referred to as a "second disaster". The United States military reported that millions of water bottles and styrofoam food packages were distributed although there was no operational waste management system. Over 700,000 plastic tarpaulins and 100,000 tents were required for emergency shelters. The increase in plastic waste, combined with poor disposal practices, resulted in open drainage channels being blocked, increasing the risk of disease . [33] Conflicts can result in large-scale displacement of communities. People living under these conditions are often provided with minimal waste management facilities. Burn pits are widely used to dispose of mixed wastes, including plastics. Air pollution can lead to respiratory and other illnesses. For example, Sahrawi refugees have been living in five camps near Tindouf, Algeria for nearly 45 years. As waste collection services are underfunded and there is no recycling facility, plastics have flooded the camps’ streets and surroundings. In contrast, the Azraq camp in Jordan for refugees from Syria has waste management services; of 20.7 tonnes of waste produced per day, 15% is recyclable. [33] On vulnerable groups Women Because of the social, political and cultural context of many places throughout the world, women are often disproportionately affected by disaster. [34] In the 2004 Indian Ocean tsunami, more women died than men, partly due to the fact that fewer women knew how to swim. [34] During and after a natural disaster, women are at increased risk of being affected by gender based violence and are increasingly vulnerable to sexual violence. Disrupted police enforcement, lax regulations, and displacement all contribute to increased risk of gender based violence and sexual assault. [34] Women who have been affected by sexual violence are at a significantly increased risk of sexually transmitted infections, unique physical injuries and long term psychological consequences. [34] All of these long-term health outcomes can prevent successful reintegration into society after the disaster recovery period. [34] In addition to LGBT people and immigrants , women are also disproportionately victimised by religion-based scapegoating for natural disasters: fanatical religious leaders or adherents may claim that a god or gods are angry with women's independent, freethinking behaviour, such as dressing 'immodestly', having sex or abortions. [35] For example, Hindutva party Hindu Makkal Katchi and others blamed women's struggle for the right to enter the Sabarimala temple for the August 2018 Kerala floods , purportedly inflicted by the angry god Ayyappan . [36] [37] In response to Iranian Islamic cleric Kazem Seddiqi 's accusation of women dressing immodestly and spreading promiscuity being the cause of earthquakes, American student Jennifer McCreight organised the Boobquake event on 26 April 2010: she encouraged women around the world to participate in dressing immodestly all at the same time while performing regular seismographic checks to prove that such behaviour in women causes no significant increase in earthquake activity. [38] During and after natural disasters, routine health behaviors become interrupted. In addition, health care systems may have broken down as a result of the disaster, further reducing access to contraceptives. [34] Unprotected intercourse during this time can lead to increased rates of childbirth, unintended pregnancies and sexually transmitted infections (STIs). [34] [39] Methods used to prevent STIs (such as condom use) are often forgotten or not accessible during times surrounding a disaster. Lack of health care infrastructure and medical shortages hinder the ability to treat individuals once they acquire an STI. In addition, health efforts to prevent, monitor or treat HIV/AIDS are often disrupted, leading to increased rates of HIV complications and increased transmission of the virus through the population. [34] Pregnant women are one of the groups disproportionately affected by natural disasters. Inadequate nutrition, little access to clean water, lack of health-care services and psychological stress in the aftermath of the disaster can lead to a significant increase in maternal morbidity and mortality. Furthermore, shortage of healthcare resources during this time can convert even routine obstetric complications into emergencies. [40] During and after a disaster, women's prenatal, peri-natal and postpartum care can become disrupted. [39] Among women affected by natural disaster, there are significantly higher rates of low birth weight infants, preterm infants and infants with low head circumference. [34] [41] On governments and voting processes Everyone is desperate for food and water. There's no food, water, or gasoline. The government is missing. — Lian Gogali Aid worker following 2018 Sulawesi earthquake and tsunami . [42] Disasters stress government capacity, as the government tries to conduct routine as well as emergency operations. [43] Some theorists of voting behavior propose that citizens update information about government effectiveness based on their response to disasters, which affects their vote choice in the next election. [44] Indeed, some evidence, based on data from the United States, reveals that incumbent parties can lose votes if citizens perceives them as responsible for a poor disaster response [45] or gain votes based on perceptions of well-executed relief work. [46] The latter study also finds, however, that voters do not reward incumbent parties for disaster preparedness , which may end up affecting government incentives to invest in such preparedness. [46] Other evidence, however, also based on the United States, finds that citizens can simply backlash and blame the incumbent for hardship following a natural disaster, causing the incumbent party to lose votes. [47] One study in India finds that incumbent parties extend more relief following disasters in areas where there is higher newspaper coverage, electoral turnout, and literacy – the authors reason that these results indicate that incumbent parties are more responsive with relief to areas with more politically informed citizens who would be more likely to punish them for poor relief efforts. [48] Violent conflicts within states can exacerbate the impact of natural disasters by weakening the ability of states, communities and individuals to provide disaster relief. Natural disasters can also worsen ongoing conflicts within states by weakening the capacity of states to fight rebels. [49] [50] In Chinese and Japanese history, it has been routine for era names or capital cities and palaces of emperors to be changed after a major natural disaster, chiefly for political reasons such as association with hardships by the populace and fear of upheaval (i.e. in East Asian government chronicles, such fears were recorded in a low profile way as an unlucky name or place requiring change). [51] Disasters caused by geological hazards Landslides This section is an excerpt from Landslide .[ edit ] A landslide near Cusco, Peru , in 2018 A NASA model has been developed to look at how potential landslide activity is changing around the world. Animation of a landslide in San Mateo County, California Landslides , also known as landslips, [52] [53] [54] are several forms of mass wasting that may include a wide range of ground movements, such as rockfalls , mudflows , shallow or deep-seated slope failures and debris flows . [55] Landslides occur in a variety of environments, characterized by either steep or gentle slope gradients, from mountain ranges to coastal cliffs or even underwater, [56] in which case they are called submarine landslides . Gravity is the primary driving force for a landslide to occur, but there are other factors affecting slope stability that produce specific conditions that make a slope prone to failure. In many cases, the landslide is triggered by a specific event (such as a heavy rainfall , an earthquake , a slope cut to build a road, and many others), although this is not always identifiable. Landslides are frequently made worse by human development (such as urban sprawl ) and resource exploitation (such as mining and deforestation ). Land degradation frequently leads to less stabilization of soil by vegetation . [57] Additionally, global warming caused by climate change and other human impact on the environment , can increase the frequency of natural events (such as extreme weather ) which trigger landslides. [58] Landslide mitigation describes the policy and practices for reducing the risk of human impacts of landslides, reducing the risk of natural disaster. A landslide in San Clemente, California in 1966 Avalanches This section is an excerpt from Avalanche .[ edit ] A powder snow avalanche in the Himalayas near Mount Everest . Heavy equipment in action after an avalanche has interrupted service on the Saint-Gervais–Vallorcine railway in Haute-Savoie , France (2006). The terminus of an avalanche in Alaska 's Kenai Fjords . Alaska Railroad track blocked by a snow slide An avalanche is a rapid flow of snow down a slope , such as a hill or mountain. [59] Avalanches can be triggered by spontaneously, by factors such as increased precipitation or snowpack weakening, or by external means such as humans, other animals, and earthquakes . Primarily composed of flowing snow and air, large avalanches have the capability to capture and move ice, rocks, and trees. Avalanches can happen in any mountain range that has an enduring snowpack. They are most frequent in winter or spring, but may occur at any time of the year. In mountainous areas, avalanches are among the most serious natural hazards to life and property, so great efforts are made in avalanche control . Earthquakes
Toggle the table of contents Urban heat island From Wikipedia, the free encyclopedia Urban area that is significantly warmer than its surrounding rural areas This article is about higher temperatures in cities due to urbanization effects. For effects of climate change on city temperatures, see climate change and cities . It has been suggested that Urban dust dome be merged into this article. ( Discuss ) Proposed since October 2023. Dense urban living without green spaces lead to a pronounced urban heat island effect ( Milan , Italy) Urban areas usually experience the urban heat island (UHI) effect, that is, they are significantly warmer than surrounding rural areas . The temperature difference is usually larger at night than during the day, [1] and is most apparent when winds are weak, under block conditions, noticeably during the summer and winter . The main cause of the UHI effect is from the modification of land surfaces while waste heat generated by energy usage is a secondary contributor. [2] [3] [4] A study has shown that heat islands can be affected by proximity to different types of land cover, so that proximity to barren land causes urban land to become hotter and proximity to vegetation makes it cooler. [5] As a population center grows, it tends to expand its area and increase its average temperature. The term heat island is also used; the term can be used to refer to any area that is relatively hotter than the surrounding, but generally refers to human-disturbed areas. [6] Urban areas occupy about 0.5% of the Earth's land surface but host more than half of the world's population. [7] Monthly rainfall is greater downwind of cities, partially due to the UHI. Increases in heat within urban centers increases the length of growing seasons and decreases the occurrence of weak tornadoes . The UHI decreases air quality by increasing the production of pollutants such as ozone , and decreases water quality as warmer waters flow into area streams and put stress on their ecosystems . Not all cities have a distinct urban heat island, and the heat island characteristics depend strongly on the background climate of the area in which the city is located. [8] Effects within a city can vary significantly depending on local environmental conditions. Heat can be reduced by tree cover and green space, which act as sources of shade and promote evaporative cooling. [9] Other options include green roofs , passive daytime radiative cooling applications, and the use of lighter-colored surfaces and less absorptive building materials in urban areas, to reflect more sunlight and absorb less heat. [10] [11] [12] Climate change is not the cause of urban heat islands but it is causing more frequent and more intense heat waves which in turn amplify the urban heat island effect in cities. [13] : 993 Compact, dense urban development may increase the urban heat island effect, leading to higher temperatures and increased exposure. [14] Description[ edit ] Mechanism of the urban heat island effect: the densely-built downtown areas tend to be warmer than suburban residential areas or rural areas. Definition[ edit ] Tokyo , an example of an urban heat island. Normal temperatures of Tokyo go up higher than those of the surrounding area. A definition of urban heat island is: "The relative warmth of a city compared with surrounding rural areas." [15] : 2926 This relative warmth is caused by "heat trapping due to land use, the configuration and design of the built environment , including street layout and building size, the heat-absorbing properties of urban building materials, reduced ventilation, reduced greenery and water features, and domestic and industrial heat emissions generated directly from human activities". [15] : 2926 Diurnal variability[ edit ] Cities often experience stronger urban heat island effects at night; effects can vary with location and topography of metropolitan areas For most cities, the difference in temperature between the urban and surrounding rural area is largest at night. While temperature difference is significant all year round, the difference is generally bigger in winter. [16] [17] The typical temperature difference is several degrees between the city and surrounding areas. The difference in temperature between an inner city and its surrounding suburbs is frequently mentioned in weather reports, as in "68 °F (20 °C) downtown, 64 °F (18 °C) in the suburbs". In the United States, the difference during the day is between 0.6–3.9 °C (1–7 °F), while the difference during the night is 1.1–2.8 °C (2–5 °F). The difference is larger for bigger cities and areas with a high air humidity . [18] [19] Though the warmer air temperature within the UHI is generally most apparent at night, urban heat islands exhibit significant and somewhat paradoxical diurnal behavior. The air temperature difference between the UHI and the surrounding environment is large at night and small during the day. [20] Throughout the daytime, particularly when the skies are cloudless, urban surfaces are warmed by the absorption of solar radiation . Surfaces in the urban areas tend to warm faster than those of the surrounding rural areas. By virtue of their high heat capacities , urban surfaces act as a giant reservoir of heat energy. For example, concrete can hold roughly 2,000 times as much heat as an equivalent volume of air. As a result, the large daytime surface temperature within the UHI is easily seen via thermal remote sensing . [21] As is often the case with daytime heating, this warming also has the effect of generating convective winds within the urban boundary layer . It is theorized that, due to the atmospheric mixing that results, the air temperature perturbation within the UHI is generally minimal or nonexistent during the day, though the surface temperatures can reach extremely high levels. [22] At night, the situation reverses. The absence of solar heating leads to the decrease of atmospheric convection and the stabilization of urban boundary layer. If enough stabilization occurs, an inversion layer is formed. This traps urban air near the surface, and keeping surface air warm from the still-warm urban surfaces, resulting in warmer nighttime air temperatures within the UHI. Other than the heat retention properties of urban areas, the nighttime maximum in urban canyons could also be due to the blocking of "sky view" during cooling: surfaces lose heat at night principally by radiation to the comparatively cool sky, and this is blocked by the buildings in an urban area. Radiative cooling is more dominant when wind speed is low and the sky is cloudless, and indeed the UHI is found to be largest at night in these conditions. [23] [24] Seasonal variability[ edit ] The urban heat island temperature difference is not only usually larger at night than during the day, but also larger in winter than in summer.[ citation needed ] This is especially true in areas where snow is common, as cities tend to hold snow for shorter periods of time than surrounding rural areas (this is due to the higher insulation capacity of cities, as well as human activities such as plowing). This decreases the albedo of the city and thereby magnifies the heating effect. Higher wind speeds in rural areas, particularly in winter, can also function to make them cooler than urban areas. Regions with distinct wet and dry seasons will exhibit a larger urban heat island effect during the dry season.[ citation needed ] Models and simulations[ edit ] If a city or town has a good system of taking weather observations the UHI can be measured directly. [25] An alternative is to use a complex simulation of the location to calculate the UHI, or to use an approximate empirical method. [26] [27] Such models allow the UHI to be included in estimates of future temperatures rises within cities due to climate change. Leonard O. Myrup published the first comprehensive numerical treatment to predict the effects of the urban heat island (UHI) in 1969. [28] The heat island effect was found to be the net result of several competing physical processes. In general, reduced evaporation in the city center and the thermal properties of the city building and paving materials are the dominant parameters. [28] Modern simulation environments include ENVI-met , which simulates all interactions between building and ground surfaces, plants and ambient air. [29] Causes[ edit ] Example of dense urban living: High-rise buildings of Manhattan during sunset Thermal (top) and vegetation (bottom) locations around New York City via infrared satellite imagery. A comparison of the images shows that where vegetation is dense, temperatures are lower. See also: Heatwave § Formation There are several causes of an urban heat island (UHI); for example, dark surfaces absorb significantly more solar radiation , which causes urban concentrations of roads and buildings to heat more than suburban and rural areas during the day; [2] materials commonly used in urban areas for pavement and roofs, such as concrete and asphalt , have significantly different thermal bulk properties (including heat capacity and thermal conductivity ) and surface radiative properties ( albedo and emissivity ) than the surrounding rural areas. This causes a change in the energy budget of the urban area, often leading to higher temperatures than surrounding rural areas. [30] Pavements , parking lots , roads or, more generally speaking transport infrastructure , contribute significantly to the urban heat island effect. [31] For example, pavement infrastructure is a main contributor to urban heat during summer afternoons in Phoenix , United States. [31] Another major reason is the lack of evapotranspiration (for example, through lack of vegetation) in urban areas. [24] The U.S. Forest Service found in 2018 that cities in the United States are losing 36 million trees each year. [32] With a decreased amount of vegetation, cities also lose the shade and evaporative cooling effect of trees. [33] [34] Other causes of a UHI are due to geometric effects. The tall buildings within many urban areas provide multiple surfaces for the reflection and absorption of sunlight, increasing the efficiency with which urban areas are heated. This is called the " urban canyon effect ". Another effect of buildings is the blocking of wind, which also inhibits cooling by convection and prevents pollutants from dissipating. Waste heat from automobiles, air conditioning, industry, and other sources also contributes to the UHI. [4] [35] [36] High levels of pollution in urban areas can also increase the UHI, as many forms of pollution change the radiative properties of the atmosphere. [30] UHI not only raises urban temperatures but also increases ozone concentrations because ozone is a greenhouse gas whose formation will accelerate with the increase of temperature. [37] Climate change as an amplifier[ edit ] Further information: Climate change and cities and Climate change adaptation § Heatwaves Climate change is not a cause but an amplifier of the urban heat island effect. The IPCC Sixth Assessment Report from 2022 summarized the available research accordingly: "Climate change increases heat stress risks in cities [...] and amplifies the urban heat island across Asian cities at 1.5°C and 2°C warming levels, both substantially larger than under present climates [...]." [38] : 66 The report goes on to say: "In a warming world, increasing air temperature makes the urban heat island effect in cities worse. One key risk is heatwaves in cities that are likely to affect half of the future global urban population, with negative impacts on human health and economic productivity." [13] : 993 There are unhelpful interactions between heat and built infrastructure: These interactions increase the risk of heat stress for people living in cities. [13] : 993 Example of urbanization: Dubai On weather and climate[ edit ] Aside from the effect on temperature, UHIs can produce secondary effects on local meteorology, including the altering of local wind patterns, the development of clouds and fog , the humidity , and the rates of precipitation. [39] The extra heat provided by the UHI leads to greater upward motion, which can induce additional shower and thunderstorm activity. In addition, the UHI creates during the day a local low pressure area where relatively moist air from its rural surroundings converges, possibly leading to more favorable conditions for cloud formation. [40] Rainfall rates downwind of cities are increased between 48% and 116%. Partly as a result of this warming, monthly rainfall is about 28% greater between 20 miles (32 km) to 40 miles (64 km) downwind of cities, compared with upwind. [41] Some cities show a total precipitation increase of 51%. [42] One study concluded that cities change the climate in area 2–4 times larger than their own area. [43] One 1999 comparison between urban and rural areas proposed that urban heat island effects have little influence on global mean temperature trends . [44] Others suggested that urban heat islands affect global climate by impacting the jet stream. [45] On human health[ edit ] See also: Heat illness and Effects of climate change on human health § Higher global temperatures and heat waves (direct risk) Image of Atlanta, Georgia , showing temperature distribution, with blue showing cool temperatures, red warm, and hot areas appear white. UHIs have the potential to directly influence the health and welfare of urban residents. As UHIs are characterized by increased temperature, they can potentially increase the magnitude and duration of heat waves within cities. The number of individuals exposed to  extreme temperatures is increased by the UHI-induced warming. [46] The nighttime effect of UHIs can be particularly harmful during a heat wave, as it deprives urban residents of the cool relief found in rural areas during the night. [47] Increased temperatures have been reported to cause heat illnesses , such as heat stroke , heat exhaustion , heat syncope , and heat cramps . [48] High UHI intensity correlates with increased concentrations of air pollutants that gathered at night, which can affect the next day's air quality . [49] These pollutants include volatile organic compounds , carbon monoxide , nitrogen oxides , and particulate matter . [50] The production of these pollutants combined with the higher temperatures in UHIs can quicken the production of ozone . [49] Ozone at surface level is considered to be a harmful pollutant. [49] Studies suggest that increased temperatures in UHIs can increase polluted days but also note that other factors (e.g. air pressure , cloud cover , wind speed ) can also have an effect on pollution. [49] Studies from Hong Kong have found that areas of the city with poorer outdoor urban air ventilation tended to have stronger urban heat island effects [51] and had significantly higher all-cause mortality [52] compared to areas with better ventilation. Another study employing advanced statistical methods in Babol city, Iran, revealed a significant increase in Surface Urban Heat Island Intensity (SUHII) from 1985 to 2017, influenced by both geographic direction and time. This research, enhancing the understanding of SUHII's spatial and temporal variations, emphasizes the need for precise urban planning to mitigate the health impacts of urban heat islands. [53] Surface UHI's are more prominent during the day and are measured using the land surface temperature and remote sensing. [54] On water bodies and aquatic organisms[ edit ] UHIs also impair water quality . Hot pavement and rooftop surfaces transfer their excess heat to stormwater, which then drains into storm sewers and raises water temperatures as it is released into streams, rivers, ponds, and lakes. Additionally, increased urban water body temperatures lead to a decrease in biodiversity in the water. [55] For example, in August 2001, rains over Cedar Rapids, Iowa led to a 10.5 °C (18.9 °F) rise in the nearby stream within one hour, resulting in a fish kill which affected an estimated 188 fish. [56] Since the temperature of the rain was comparatively cool, the deaths could be attributed to the hot pavement of the city. Similar events have been documented across the American Midwest, as well as Oregon and California. [57] Rapid temperature changes can be stressful to aquatic ecosystems. [58] With the temperature of the nearby buildings sometimes reaching a difference of over 50 °F (28 °C) from the near-surface air temperature, precipitation warms rapidly, and run-off into nearby streams, lakes and rivers (or other bodies of water) to provide excessive thermal pollution . The increase in thermal pollution has the potential to increase water temperature by 20 to 30 °F (11 to 17 °C). This increase causes the fish species inhabiting the body of water to undergo thermal stress and shock due to the rapid change in temperature of their habitat. [59] Permeable pavements may reduce these effects by percolating water through the pavement into subsurface storage areas where it can be dissipated through absorption and evaporation. [60] On animals[ edit ] Species that are good at colonizing can use conditions provided by urban heat islands to thrive in regions outside of their normal range. Examples of this include the grey-headed flying fox (Pteropus poliocephalus) and the common house gecko (Hemidactylus frenatus). [61] Grey-headed flying foxes, found in Melbourne, Australia , colonized urban habitats following the increase in temperatures there. Increased temperatures, causing warmer winter conditions, made the city more similar in climate to the more northerly wildland habitat of the species. With temperate climates, urban heat islands will extend the growing season, therefore altering breeding strategies of inhabiting species. [61] This can be best observed in the effects that urban heat islands have on water temperature (see effects on water bodies ). Urban heat islands caused by cities have altered the natural selection process. [61] Selective pressures like temporal variation in food, predation and water are relaxed causing a new set of selective forces to roll out. For example, within urban habitats, insects are more abundant than in rural areas. Insects are ectotherms . This means that they depend on the temperature of the environment to control their body temperature, making the warmer climates of the city perfect for their ability to thrive. A study done in Raleigh, North Carolina conducted on Parthenolecanium quercifex (oak scales), showed that this particular species preferred warmer climates and were therefore found in higher abundance in urban habitats than on oak trees in rural habitats. Over time spent living in urban habitats, they have adapted to thrive in warmer climates than in cooler ones. [62] On energy usage for cooling[ edit ] Images of Salt Lake City , show positive correlation between white reflective roofs and cooler temperatures. Image A depicts an aerial view of Salt Lake City, Utah, site of 865000 sqft white reflective roof. Image B is a thermal infrared image of same area, showing hot (red and yellow) and cool (green and blue) spots. The reflective vinyl roof, not absorbing solar radiation, is shown in blue surrounded by other hot spots. Another consequence of urban heat islands is the increased energy required for air conditioning and refrigeration in cities that are in comparatively hot climates. The heat island effect costs Los Angeles about US$ 100 million per year in energy (in the year 2000). [63] Through the implementation of heat island reduction strategies, significant annual net energy savings have been calculated for northern locations such as Chicago, Salt Lake City, and Toronto. [64] Every year in the U.S. 15% of energy goes towards the air conditioning of buildings in these urban heat islands. It was reported in 1998 that "the air conditioning demand has risen 10% within the last 40 years." [65] Options for reducing heat island effects[ edit ] Botanical Garden in Lublin , Poland Strategies to improve urban resilience by reducing excessive heat in cities include: Planting trees in cities, white roofs and light-coloured concrete, green infrastructure (including green roofs ), passive daytime radiative cooling .[ citation needed ] The temperature difference between urban areas and the surrounding suburban or rural areas can be as much as 5 °C (9.0 °F). Nearly 40 percent of that increase is due to the prevalence of dark roofs, with the remainder coming from dark-colored pavement and the declining presence of vegetation. The heat island effect can be counteracted slightly by using white or reflective materials to build houses, roofs, pavements, and roads, thus increasing the overall albedo of the city. [66] Concentric expansion of cities is unfavorable in terms of the urban heat island phenomenon. It is recommended to plan the development of cities in strips, consistent with the hydrographic network, taking into account green areas with various plant species. [67] In this way, it was planned to build urban settlements stretching over large areas, e.g. Kielce , Szczecin and Gdynia in Poland, Copenhagen in Denmark and Hamburg , Berlin and Kiel in Germany. Planting trees in cities[ edit ] Main articles: Urban forest and Urban forestry Planting trees around the city can be another way of increasing albedo and decreasing the urban heat island effect. It is recommended to plant deciduous trees because they can provide many benefits such as more shade in the summer and not blocking warmth in winter. [68] Trees are a necessary feature in combating most of the urban heat island effect because they reduce air temperatures by 10 °F (5.6 °C), [69] and surface temperatures by up to 20–45 °F (11–25 °C). [70] Another benefit of having trees in a city is that trees also help fight global warming by absorbing CO2 from the atmosphere. White roofs and light-coloured concrete[ edit ] Green roof of Chicago City Hall . Painting rooftops white has become a common strategy to reduce the heat island effect. [71] In cities, there are many dark colored surfaces that absorb the heat of the sun in turn lowering the albedo of the city. [71] White rooftops allow high solar reflectance and high solar emittance, increasing the albedo of the city or area the effect is occurring. [71] Relative to remedying the other sources of the problem, replacing dark roofing requires the least amount of investment for the most immediate return. A cool roof made from a reflective material such as vinyl reflects at least 75 percent of the sun's rays, and emit at least 70 percent of the solar radiation absorbed by the building envelope. Asphalt built-up roofs (BUR), by comparison, reflect 6 percent to 26 percent of solar radiation. [72] Using light-colored concrete has proven effective in reflecting up to 50% more light than asphalt and reducing ambient temperature. [73] A low albedo value, characteristic of black asphalt, absorbs a large percentage of solar heat creating warmer near-surface temperatures. Paving with light-colored concrete, in addition to replacing asphalt with light-colored concrete, communities may be able to lower average temperatures. [74] However, research into the interaction between reflective pavements and buildings has found that, unless the nearby buildings are fitted with reflective glass, solar radiation reflected off light-colored pavements can increase building temperatures, increasing air conditioning demands. [75] [76] There are specific paint formulations for daytime radiative cooling that reflect up to 98.1% of sunlight. [77] [78]
From Wikipedia, the free encyclopedia Displacement of soil by water, wind, and lifeforms An actively eroding rill on an intensively-farmed field in eastern Germany Soil erosion Soil erosion is the denudation or wearing away of the upper layer of soil . It is a form of soil degradation . This natural process is caused by the dynamic activity of erosive agents, that is, water , ice (glaciers), snow , air (wind), plants , and animals (including humans ). In accordance with these agents, erosion is sometimes divided into water erosion, glacial erosion , snow erosion, wind (aeolian) erosion , zoogenic erosion and anthropogenic erosion such as tillage erosion . [1] Soil erosion may be a slow process that continues relatively unnoticed, or it may occur at an alarming rate causing a serious loss of topsoil . The loss of soil from farmland may be reflected in reduced crop production potential, lower surface water quality and damaged drainage networks. Soil erosion could also cause sinkholes . Human activities have increased by 10–50 times the rate at which erosion is occurring world-wide. Excessive (or accelerated) erosion causes both "on-site" and "off-site" problems. On-site impacts include decreases in agricultural productivity and (on natural landscapes ) ecological collapse , both because of loss of the nutrient-rich upper soil layers . In some cases, the eventual result is desertification . Off-site effects include sedimentation of waterways and eutrophication of water bodies, as well as sediment-related damage to roads and houses. Water and wind erosion are the two primary causes of land degradation ; combined, they are responsible for about 84% of the global extent of degraded land, making excessive erosion one of the most significant environmental problems worldwide. [2] [3] [4] Intensive agriculture , deforestation , roads , acid rains , anthropogenic climate change and urban sprawl are amongst the most significant human activities in regard to their effect on stimulating erosion. [5] However, there are many prevention and remediation practices that can curtail or limit erosion of vulnerable soils. Rainfall and surface runoff[ edit ] Soil and water being splashed by the impact of a single raindrop Rainfall , and the surface runoff which may result from rainfall, produces four main types of soil erosion: splash erosion, sheet erosion, rill erosion, and gully erosion. Splash erosion is generally seen as the first and least severe stage in the soil erosion process, which is followed by sheet erosion, then rill erosion and finally gully erosion (the most severe of the four). [6] [7] In splash erosion, the impact of a falling raindrop creates a small crater in the soil, [8] ejecting soil particles. [9] The distance these soil particles travel can be as much as 0.6 m (two feet) vertically and 1.5 m (five feet) horizontally on level ground. If the soil is saturated , or if the rainfall rate is greater than the rate at which water can infiltrate into the soil, surface runoff occurs. If the runoff has sufficient flow energy , it will transport loosened soil particles ( sediment ) down the slope. [10] Sheet erosion is the transport of loosened soil particles by overland flow. [10] A spoil tip covered in rills and gullies due to erosion processes caused by rainfall: Rummu , Estonia Rill erosion refers to the development of small, ephemeral concentrated flow paths which function as both sediment source and sediment delivery systems for erosion on hillslopes. Generally, where water erosion rates on disturbed upland areas are greatest, rills are active. Flow depths in rills are typically of the order of a few centimeters (about an inch) or less and along-channel slopes may be quite steep. This means that rills exhibit hydraulic physics very different from water flowing through the deeper wider channels of streams and rivers. [11] Gully erosion occurs when runoff water accumulates and rapidly flows in narrow channels during or immediately after heavy rains or melting snow, removing soil to a considerable depth. [12] [13] [14] Another cause of gully erosion is grazing, which often results in ground compaction. Because the soil is exposed, it loses the ability to absorb excess water, and erosion can develop in susceptible areas. [15] Rivers and streams[ edit ] Further information on water's erosive ability: Hydraulic action Dobbingstone Burn, Scotland—This photo illustrates two different types of erosion affecting the same place. Valley erosion is occurring due to the flow of the stream, and the boulders and stones (and much of the soil) that are lying on the edges are glacial till that was left behind as ice age glaciers flowed over the terrain. Valley or stream erosion occurs with continued water flow along a linear feature. The erosion is both downward , deepening the valley, and headward , extending the valley into the hillside, creating head cuts and steep banks. In the earliest stage of stream erosion, the erosive activity is dominantly vertical, the valleys have a typical V cross-section and the stream gradient is relatively steep. When some base level is reached, the erosive activity switches to lateral erosion, which widens the valley floor and creates a narrow floodplain. The stream gradient becomes nearly flat, and lateral deposition of sediments becomes important as the stream meanders across the valley floor. In all stages of stream erosion, by far the most erosion occurs during times of flood, when more and faster-moving water is available to carry a larger sediment load. In such processes, it is not the water alone that erodes: suspended abrasive particles, pebbles and boulders can also act erosively as they traverse a surface , in a process known as traction. [16] Bank erosion is the wearing away of the banks of a stream or river . This is distinguished from changes on the bed of the watercourse, which is referred to as scour. Erosion and changes in the form of river banks may be measured by inserting metal rods into the bank and marking the position of the bank surface along the rods at different times. [17] Thermal erosion is the result of melting and weakening permafrost due to moving water. [18] It can occur both along rivers and at the coast. Rapid river channel migration observed in the Lena River of Siberia is due to thermal erosion , as these portions of the banks are composed of permafrost-cemented non-cohesive materials. [19] Much of this erosion occurs as the weakened banks fail in large slumps. Thermal erosion also affects the Arctic coast, where wave action and near-shore temperatures combine to undercut permafrost bluffs along the shoreline and cause them to fail. Annual erosion rates along a 100-kilometre (62-mile) segment of the Beaufort Sea shoreline averaged 5.6 metres (18 feet) per year from 1955 to 2002. [20] Floods[ edit ] At extremely high flows, kolks , or vortices are formed by large volumes of rapidly rushing water. Kolks cause extreme local erosion, plucking bedrock and creating pothole-type geographical features called rock-cut basins . Examples can be seen in the flood regions result from glacial Lake Missoula , which created the channeled scablands in the Columbia Basin region of eastern Washington . [21] Main article: Aeolian processes Wind erosion is a major geomorphological force, especially in arid and semi-arid regions. It is also a major source of land degradation, evaporation, desertification, harmful airborne dust, and crop damage—especially after being increased far above natural rates by human activities such as deforestation , urbanization , and agriculture . [22] [23] Wind erosion is of two primary varieties: deflation , where the wind picks up and carries away loose particles; and abrasion , where surfaces are worn down as they are struck by airborne particles carried by wind. Deflation is divided into three categories: (1) surface creep , where larger, heavier particles slide or roll along the ground; (2) saltation , where particles are lifted a short height into the air, and bounce and saltate across the surface of the soil; and (3) suspension , where very small and light particles are lifted into the air by the wind, and are often carried for long distances. Saltation is responsible for the majority (50–70%) of wind erosion, followed by suspension (30–40%), and then surface creep (5–25%). [24] [25] Silty soils tend to be the most affected by wind erosion; silt particles are relatively easily detached and carried away. [26] Wind erosion is much more severe in arid areas and during times of drought. For example, in the Great Plains , it is estimated that soil loss due to wind erosion can be as much as 6100 times greater in drought years than in wet years. [27] Mass movement[ edit ] Wadi in Makhtesh Ramon, Israel, showing gravity collapse erosion on its banks Mass movement is the downward and outward movement of rock and sediments on a sloped surface, mainly due to the force of gravity . [28] [29] Mass movement is an important part of the erosional process, and is often the first stage in the breakdown and transport of weathered materials in mountainous areas. [30] It moves material from higher elevations to lower elevations where other eroding agents such as streams and glaciers can then pick up the material and move it to even lower elevations. Mass-movement processes are always occurring continuously on all slopes; some mass-movement processes act very slowly; others occur very suddenly, often with disastrous results. Any perceptible down-slope movement of rock or sediment is often referred to in general terms as a landslide . However, landslides can be classified in a much more detailed way that reflects the mechanisms responsible for the movement and the velocity at which the movement occurs. One of the visible topographical manifestations of a very slow form of such activity is a scree slope. [31] Slumping happens on steep hillsides, occurring along distinct fracture zones, often within materials like clay that, once released, may move quite rapidly downhill. They will often show a spoon-shaped isostatic depression , in which the material has begun to slide downhill. In some cases, the slump is caused by water beneath the slope weakening it. In many cases it is simply the result of poor engineering along highways where it is a regular occurrence. [32] Surface creep is the slow movement of soil and rock debris by gravity which is usually not perceptible except through extended observation. However, the term can also describe the rolling of dislodged soil particles 0.5 to 1.0 mm (0.02 to 0.04 in) in diameter by wind along the soil surface. [33] This section is an excerpt from Tillage erosion .[ edit ] Eroded hilltops due to tillage erosion Tillage erosion is a form of soil erosion occurring in cultivated fields due to the movement of soil by tillage . [34] [35] There is growing evidence that tillage erosion is a major soil erosion process in agricultural lands, surpassing water and wind erosion in many fields all around the world, especially on sloping and hilly lands [36] [37] [38] A signature spatial pattern of soil erosion shown in many water erosion handbooks and pamphlets, the eroded hilltops, is actually caused by tillage erosion as water erosion mainly causes soil losses in the midslope and lowerslope segments of a slope, not the hilltops. [39] [34] [36] Tillage erosion results in soil degradation, which can lead to significant reduction in crop yield and, therefore, economic losses for the farm. [40] [41] Tillage erosion in field with diversion terraces Factors affecting soil erosion[ edit ] Climate[ edit ] The amount and intensity of precipitation is the main climatic factor governing soil erosion by water. The relationship is particularly strong if heavy rainfall occurs at times when, or in locations where, the soil's surface is not well protected by vegetation . This might be during periods when agricultural activities leave the soil bare, or in semi-arid regions where vegetation is naturally sparse. Wind erosion requires strong winds, particularly during times of drought when vegetation is sparse and soil is dry (and so is more erodible). Other climatic factors such as average temperature and temperature range may also affect erosion, via their effects on vegetation and soil properties. In general, given similar vegetation and ecosystems, areas with more precipitation (especially high-intensity rainfall), more wind, or more storms are expected to have more erosion. In some areas of the world (e.g. the mid-western US and the Amazon Rainforest ), rainfall intensity is the primary determinant of erosivity, with higher intensity rainfall generally resulting in more soil erosion by water. The size and velocity of rain drops is also an important factor. Larger and higher-velocity rain drops have greater kinetic energy , and thus their impact will displace soil particles by larger distances than smaller, slower-moving rain drops. [42] In other regions of the world (e.g. western Europe ), runoff and erosion result from relatively low intensities of stratiform rainfall falling onto previously saturated soil. In such situations, rainfall amount rather than intensity is the main factor determining the severity of soil erosion by water. [43] Soil structure and composition[ edit ] Erosional gully in unconsolidated Dead Sea (Israel) sediments along the southwestern shore. This gully was excavated by floods from the Judean Mountains in less than a year. The composition, moisture, and compaction of soil are all major factors in determining the erosivity of rainfall. Sediments containing more clay tend to be more resistant to erosion than those with sand or silt, because the clay helps bind soil particles together. [44] Soil containing high levels of organic materials are often more resistant to erosion, because the organic materials coagulate soil colloids and create a stronger, more stable soil structure. [45] The amount of water present in the soil before the precipitation also plays an important role, because it sets limits on the amount of water that can be absorbed by the soil (and hence prevented from flowing on the surface as erosive runoff). Wet, saturated soils will not be able to absorb as much rainwater, leading to higher levels of surface runoff and thus higher erosivity for a given volume of rainfall. [45] [46] Soil compaction also affects the permeability of the soil to water, and hence the amount of water that flows away as runoff. More compacted soils will have a larger amount of surface runoff than less compacted soils. [45] See also: Vegetation and slope stability Vegetation acts as an interface between the atmosphere and the soil . It increases the permeability of the soil to rainwater , thus decreasing runoff. It shelters the soil from winds , which results in decreased wind erosion , as well as advantageous changes in microclimate . The roots of the plants bind the soil together, and interweave with other roots, forming a more solid mass that is less susceptible to both water and wind erosion . The removal of vegetation increases the rate of surface erosion . [47] Topography[ edit ] The topography of the land determines the velocity at which surface runoff will flow, which in turn determines the erosivity of the runoff. Longer, steeper slopes (especially those without adequate vegetative cover) are more susceptible to very high rates of erosion during heavy rains than shorter, less steep slopes. Steeper terrain is also more prone to mudslides, landslides, and other forms of gravitational erosion processes. [48] [49] [50] Human activities that aid soil erosion[ edit ] See also: agricultural pollution and overgrazing Tilled farmland such as this is very susceptible to erosion from rainfall, due to the destruction of vegetative cover and the loosening of the soil during plowing. Unsustainable agricultural practices increase rates of erosion by one to two orders of magnitude over the natural rate and far exceed replacement by soil production. [51] [52] The tillage of agricultural lands, which breaks up soil into finer particles, is one of the primary factors. The problem has been exacerbated in modern times, due to mechanized agricultural equipment that allows for deep plowing , which severely increases the amount of soil that is available for transport by water erosion. Others include monocropping , farming on steep slopes, pesticide and chemical fertilizer usage (which kill organisms that bind soil together), row-cropping, and the use of surface irrigation . [53] [54] A complex overall situation with respect to defining nutrient losses from soils, could arise as a result of the size selective nature of soil erosion events. Loss of total phosphorus , for instance, in the finer eroded fraction is greater relative to the whole soil. [55] Extrapolating this evidence to predict subsequent behaviour within receiving aquatic systems, the reason is that this more easily transported material may support a lower solution P concentration compared to coarser sized fractions. [56] Tillage also increases wind erosion rates, by dehydrating the soil and breaking it up into smaller particles that can be picked up by the wind. Exacerbating this is the fact that most of the trees are generally removed from agricultural fields, allowing winds to have long, open runs to travel over at higher speeds. [57] Heavy grazing reduces vegetative cover and causes severe soil compaction, both of which increase erosion rates. [58] Deforestation[ edit ] In this clearcut , almost all of the vegetation has been stripped from the surface of steep slopes, in an area with very heavy rains. Severe erosion occurs in cases such as this, causing stream sedimentation and the loss of nutrient-rich topsoil . In an undisturbed forest , the mineral soil is protected by a layer of leaf litter and an humus that cover the forest floor. These two layers form a protective mat over the soil that absorbs the impact of rain drops. They are porous and highly permeable to rainfall, and allow rainwater to slow percolate into the soil below, instead of flowing over the surface as runoff . [59] The roots of the trees and plants [60] hold together soil particles, preventing them from being washed away. [59] The vegetative cover acts to reduce the velocity of the raindrops that strike the foliage and stems before hitting the ground, reducing their kinetic energy . [61] However it is the forest floor, more than the canopy, that prevents surface erosion. The terminal velocity of rain drops is reached in about 8 metres (26 feet). Because forest canopies are usually higher than this, rain drops can often regain terminal velocity even after striking the canopy. However, the intact forest floor, with its layers of leaf litter and organic matter, is still able to absorb the impact of the rainfall. [61] [62] Deforestation causes increased erosion rates due to exposure of mineral soil by removing the humus and litter layers from the soil surface, removing the vegetative cover that binds soil together, and causing heavy soil compaction from logging equipment. Once trees have been removed by fire or logging, infiltration rates become high and erosion low to the degree the forest floor remains intact. Severe fires can lead to significant further erosion if followed by heavy rainfall. [63] Globally one of the largest contributors to erosive soil loss in the year 2006 is the slash and burn treatment of tropical forests . In a number of regions of the earth, entire sectors of a country have been rendered unproductive. For example, on the Madagascar high central plateau , comprising approximately ten percent of that country's land area, virtually the entire landscape is sterile of vegetation , with gully erosive furrows typically in excess of 50 metres (160 ft) deep and 1 kilometre (0.6 miles) wide. Shifting cultivation is a farming system which sometimes incorporates the slash and burn method in some regions of the world. This degrades the soil and causes the soil to become less and less fertile. [64] Roads and human impact[ edit ] Erosion polluted the Kasoa highway after downpour in Ghana Human Impact has major effects on erosion processes—first by denuding the land of vegetative cover, altering drainage patterns, and compacting the soil during construction; and next by covering the land in an impermeable layer of asphalt or concrete that increases the amount of surface runoff and increases surface wind speeds. [65] Much of the sediment carried in runoff from urban areas (especially roads) is highly contaminated with fuel, oil, and other chemicals. [66] This increased runoff, in addition to eroding and degrading the land that it flows over, also causes major disruption to surrounding watersheds by altering the volume and rate of water that flows through them, and filling them with chemically polluted sedimentation. The increased flow of water through local waterways also causes a large increase in the rate of bank erosion. [67] Main article: Land degradation The warmer atmospheric temperatures observed over the past decades are expected to lead to a more vigorous hydrological cycle, including more extreme rainfall events. [68] The rise in sea levels that has occurred as a result of climate change has also greatly increased coastal erosion rates. [69] [70] Most part of Accra mostly flooded during rainy season, causing environmental crisis in Ghana Studies on soil erosion suggest that increased rainfall amounts and intensities will lead to greater rates of soil erosion. Thus, if rainfall amounts and intensities increase in many parts of the world as expected, erosion will also increase, unless amelioration measures are taken. Soil erosion rates are expected to change in response to changes in climate for a variety of reasons. The most direct is the change in the erosive power of rainfall. Other reasons include: a) changes in plant canopy caused by shifts in plant biomass production associated with moisture regime; b) changes in litter cover on the ground caused by changes in both plant residue decomposition rates driven by temperature and moisture dependent soil microbial activity as well as plant biomass production rates; c) changes in soil moisture due to shifting precipitation regimes and evapo-transpiration rates, which changes infiltration and runoff ratios; d) soil erodibility changes due to decrease in soil organic matter concentrations in soils that lead to a soil structure that is more susceptible to erosion and increased runoff due to increased soil surface sealing and crusting; e) a shift of winter precipitation from non-erosive snow to erosive rainfall due to increasing winter temperatures; f) melting of permafrost, which induces an erodible soil state from a previously non-erodible one; and g) shifts in land use made necessary to accommodate new climatic regimes. [71] Studies by Pruski and Nearing indicated that, other factors such as land use unconsidered, it is reasonable to expect approximately a 1.7% change in soil erosion for each 1% change in total precipitation under climate change. [72] In recent studies, there are predicted increases of rainfall erosivity by 17% in the United States, [73] by 18% in Europe, [74] and globally 30 to 66% [75] Global environmental effects[ edit ] run-off and filter soxx World map indicating areas that are vulnerable to high rates of water erosion During the 17th and 18th centuries, Easter Island experienced severe erosion due to deforestation and unsustainable agricultural practices. The resulting loss of topsoil ultimately led to ecological collapse, causing mass starvation and the complete disintegration of the Easter Island civilization. [76] [77] Due to the severity of its ecological effects, and the scale on which it is occurring, erosion constitutes one of the most significant global environmental problems we face today. [3] Land degradation[ edit ] Water and wind erosion are now the two primary causes of land degradation ; combined, they are responsible for 84% of degraded acreage. [2] Each year, about 75 billion tons of soil is eroded from the land—a rate that is about 13–40 times as fast as the natural rate of erosion. [78] Approximately 40% of the world's agricultural land is seriously degraded. [79] According to the United Nations , an area of fertile soil the size of Ukraine is lost every year because of drought , deforestation and climate change . [80] In Africa , if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to UNU 's Ghana-based Institute for Natural Resources in Africa. [81] Recent modeling developments have quantified rainfall erosivity at global scale using high temporal resolution (<30 min) and high fidelity rainfall recordings. The results is an extensive global data collection effort produced the Global Rainfall Erosivity Database (GloREDa) which includes rainfall erosivity for 3,625 stations and covers 63 countries. This first ever Global Rainfall Erosivity Database was used to develop a global erosivity map [82] at 30 arc-seconds(~1 km) based on sophisticated geostatistical process. According to a new study [83] published in Nature Communications, almost 36 billion tons of soil is lost every year due to water, and deforestation and other changes in land use make the problem worse. The study investigates global soil erosion dynamics by means of high-resolution spatially distributed modelling (c. 250 × 250 m cell size). The geo-statistical approach allows, for the first time, the thorough incorporation into a global soil erosion model of land use and changes in land use, the extent, types, spatial distribution of global croplands and the effects of different regional cropping systems. The loss of soil fertility due to erosion is further problematic because the response is often to apply chemical fertilizers, which leads to further water and soil pollution , rather than to allow the land to regenerate. [84] Sedimentation of aquatic ecosystems[ edit ] Soil erosion (especially from agricultural activity) is considered to be the leading global cause of diffuse water pollution , due to the effects of the excess sediments flowing into the world's waterways. The sediments themselves act as pollutants, as well as being carriers for other pollutants, such as attached pesticide molecules or heavy metals. [85] The effect of increased sediments loads on aquatic ecosystems can be catastrophic. Silt can smother the spawning beds of fish, by filling in the space between gravel on the stream bed. It also reduces their food supply, and causes major respiratory issues for them as sediment enters their gills . The biodiversity of aquatic plant and algal life is reduced, and invertebrates are also unable to survive and reproduce. While the sedimentation event itself might be relatively short-lived, the ecological disruption caused by the mass die off often persists long into the future. [86] One of the most serious and long-running water erosion problems worldwide is in the People's Republic of China , on the middle reaches of the Yellow River and the upper reaches of the Yangtze River . From the Yellow River , over 1.6 billion tons of sediment flows into the ocean each year. The sediment originates primarily from water erosion in the Loess Plateau region of the northwest. [87] Airborne dust pollution[ edit ] Soil particles picked up during wind erosion of soil are a major source of air pollution , in the form of airborne particulates —"dust". These airborne soil particles are often contaminated with toxic chemicals such as pesticides or petroleum fuels, posing ecological and public health hazards when they later land, or are inhaled/ingested. [88] [89] [90] [91] Dust from erosion acts to suppress rainfall and changes the sky color from blue to white, which leads to an increase in red sunsets[ citation needed ]. Dust events have been linked to a decline in the health of coral reefs across the Caribbean and Florida, primarily since the 1970s. [92] Similar dust plumes originate in the Gobi desert , which combined with pollutants, spread large distances downwind, or eastward, into North America. [93] Monitoring, measuring and modelling soil erosion[ edit ] Terracing is an ancient technique that can significantly slow the rate of water erosion on cultivated slopes. See also: Erosion prediction This section needs expansion. You can help by adding to it . (April 2012) Monitoring and modeling of erosion processes can help people better understand the causes of soil erosion , make predictions of erosion under a range of possible conditions , and plan the implementation of preventative and restorative strategies for erosion . However, the complexity of erosion processes and the number of scientific disciplines that must be considered to understand and model them (e.g. climatology, hydrology, geology, soil science, agriculture, chemistry, physics, etc.) makes accurate modelling challenging. [94] [95] [96] Erosion models are also non-linear, which makes them difficult to work with numerically, and makes it difficult or impossible to scale up to making predictions about large areas from data collected by sampling smaller plots. [97] The most commonly used model for predicting soil loss from water erosion is the Universal Soil Loss Equation (USLE) . This was developed in the 1960s and 1970s. It estimates the average annual soil loss A on a plot-sized area as: [98] A = RKLSCP where R is the rainfall erosivity factor, [99] [100] K is the soil erodibility factor, [101] L and S are topographic factors [102] representing length and slope, [103] C is the cover and management factor [104] and P is the support practices factor. [105] Despite the USLE's plot-scale spatial basis, the model has often been used to estimate soil erosion on much larger areas, such as watersheds , continents , and globally. One major problem is that the USLE cannot simulate gully erosion, and so erosion from gullies is ignored in any USLE-based assessment of erosion. Yet erosion from gullies can be a substantial proportion (10–80%) of total erosion on cultivated and grazed land. [106] During the 50 years since the introduction of the USLE, many other soil erosion models have been developed. [107] But because of the complexity of soil erosion and its constituent processes, all erosion models can only roughly approximate actual erosion rates when validated i.e. when model predictions are compared with real-world measurements of erosion. [108] [109] Thus new soil erosion models continue to be developed. Some of these remain USLE-based, e.g. the G2 model. [110] [111] Other soil erosion models have largely (e.g. the Water Erosion Prediction Project model ) or wholly (e.g. RHEM, the Rangeland Hydrology and Erosion Model [112] ) abandoned usage of USLE elements. Global studies continue to be based on the USLE [75] Prevention and remediation[ edit ] See also: Erosion control and Erosion control examples A windbreak (the row of trees) planted next to an agricultural field, acting as a shield against strong winds. This reduces the effects of wind erosion, and provides many other benefits. The most effective known method for erosion prevention is to increase vegetative cover on the land, which helps prevent both wind and water erosion. [113] Terracing is an extremely effective means of erosion control, which has been practiced for thousands of years by people all over the world. [114] Windbreaks (also called shelterbelts) are rows of trees and shrubs that are planted along the edges of agricultural fields, to shield the fields against winds. [115] In addition to significantly reducing wind erosion, windbreaks provide many other benefits such as improved microclimates for crops (which are sheltered from the dehydrating and otherwise damaging effects of wind), habitat for beneficial bird species, [116] carbon sequestration , [117] and aesthetic improvements to the agricultural landscape. [118] [119] Traditional planting methods, such as mixed-cropping (instead of monocropping ) and crop rotation have also been shown to significantly reduce erosion rates. [120] [121] Crop residues play a role in the mitigation of erosion, because they reduce the impact of raindrops breaking up the soil particles. [122] There is a higher potential for erosion when producing potatoes than when growing cereals, or oilseed crops. [123] Forages have a fibrous root system, which helps combat erosion by anchoring the plants to the top layer of the soil, and covering the entirety of the field, as it is a non-row crop. [124] In tropical coastal systems, properties of mangroves have been examined as a potential means to reduce soil erosion. Their complex root structures are known to help reduce wave damage from storms and flood impacts while binding and building soils. These roots can slow down water flow, leading to the deposition of sediments and reduced erosion rates. However, in order to maintain sediment balance, adequate mangrove forest width needs to be present. [125]
Toggle the table of contents Soil moisture From Wikipedia, the free encyclopedia Water content of the soil Soil moisture is the water content of the soil . It can be expressed in terms of volume or weight. Soil moisture measurement can be based on in situ probes (e.g., capacitance probes , neutron probes ) or remote sensing methods. [1] [2] Water that enters a field is removed from a field by runoff , drainage , evaporation or transpiration . [3] Runoff is the water that flows on the surface to the edge of the field; drainage is the water that flows through the soil downward or toward the edge of the field underground; evaporative water loss from a field is that part of the water that evaporates into the atmosphere directly from the field's surface; transpiration is the loss of water from the field by its evaporation from the plant itself. Water affects soil formation , structure , stability and erosion but is of primary concern with respect to plant growth . [4] Water is essential to plants for four reasons: It constitutes 80–95% of the plant's protoplasm . It is essential for photosynthesis . It is the solvent in which nutrients are carried to, into and throughout the plant. It provides the turgidity by which the plant keeps itself in proper position. [5] In addition, water alters the soil profile by dissolving and re-depositing mineral and organic solutes and colloids , often at lower levels, a process called leaching . In a loam soil , solids constitute half the volume, gas one-quarter of the volume, and water one-quarter of the volume of which only half will be available to most plants, with a strong variation according to matric potential . [6] Water moves in soil under the influence of gravity , osmosis and capillarity . [7] When water enters the soil, it displaces air from interconnected macropores by buoyancy , and breaks aggregates into which air is entrapped, a process called slaking . [8] The rate at which a soil can absorb water depends on the soil and its other conditions. As a plant grows, its roots remove water from the largest pores (macropores) first. Soon the larger pores hold only air, and the remaining water is found only in the intermediate- and smallest-sized pores ( micropores ). The water in the smallest pores is so strongly held to particle surfaces that plant roots cannot pull it away. Consequently, not all soil water is available to plants, with a strong dependence on texture . [9] When saturated, the soil may lose nutrients as the water drains. [10] Water moves in a draining field under the influence of pressure where the soil is locally saturated and by capillarity pull to drier parts of the soil. [11] Most plant water needs are supplied from the suction caused by evaporation from plant leaves ( transpiration ) and a lower fraction is supplied by suction created by osmotic pressure differences between the plant interior and the soil solution. [12] [13] Plant roots must seek out water and grow preferentially in moister soil microsites, [14] but some parts of the root system are also able to remoisten dry parts of the soil. [15] Insufficient water will damage the yield of a crop. [16] Most of the available water is used in transpiration to pull nutrients into the plant. [17] Soil water is also important for climate modeling and numerical weather prediction . The Global Climate Observing System specified soil water as one of the 50 Essential Climate Variables (ECVs). [18] Soil water can be measured in situ with soil moisture sensors or can be estimated at various scales and resolution: from local or wifi measures via sensors in the soil to satellite imagery that combines data capture and hydrological models . Each method exhibits pros and cons, and hence, the integration of different techniques may decrease the drawbacks of a single given method. [19] Moisture level concepts[ edit ] ECMWF soil moisture forecast for the East Asia region, showing the key moisture levels and intermediate measurements Field capacity A flooded field will drain the gravitational water under the influence of gravity until water's adhesive and cohesive forces resist further drainage at which point it is said to have reached field capacity . [20] At that point, plants must apply suction to draw water from a soil. By convention it is defined at 0.33 bar suction. [20] [21] Available water and unavailable water The water that plants may draw from the soil is called the available water . [20] [22] Once the available water is used up the remaining moisture is called unavailable water as the plant cannot produce sufficient suction to draw that water in. Wilting point The wilting point is the minimum amount of water plants need to not wilt and approximates the boundary between available and unavailable water. By convention it is defined as 15 bar suction. At this point, seeds will not germinate, [23] [20] [24] plants begin to wilt and then die unless they are able to recover after water replenishment thanks to species-specific adaptations. [25] Further information: Soil water (retention) and Water retention curve Water is retained in a soil when the adhesive force of attraction that water's hydrogen atoms have for the oxygen of soil particles is stronger than the cohesive forces that water's hydrogen feels for water oxygen atoms. [26] When a field is flooded, the soil pore space is completely filled by water. The field will drain under the force of gravity until it reaches what is called field capacity , at which point the smallest pores are filled with water and the largest with water and gases. [27] The total amount of water held when field capacity is reached is a function of the specific surface area of the soil particles. [28] As a result, high clay and high organic soils have higher field capacities. [29] The potential energy of water per unit volume relative to pure water in reference conditions is called water potential . Total water potential is a sum of matric potential which results from capillary action , osmotic potential for saline soil , and gravitational potential when dealing with downward water movement. Water potential in soil usually has negative values, and therefore it is also expressed in suction , which is defined as the minus of water potential. Suction has a positive value and can be regarded as the total force required to pull or push water out of soil. Water potential or suction is expressed in units of kPa (103 pascal ), bar (100 kPa), or cm H2O (approximately 0.098 kPa). Common logarithm of suction in cm H2O is called pF. [30] Therefore, pF 3 = 1000 cm = 98 kPa = 0.98 bar. The forces with which water is held in soils determine its availability to plants. Forces of adhesion hold water strongly to mineral and humus surfaces and less strongly to itself by cohesive forces. A plant's root may penetrate a very small volume of water that is adhering to soil and be initially able to draw in water that is only lightly held by the cohesive forces. But as the droplet is drawn down, the forces of adhesion of the water for the soil particles produce increasingly higher suction , finally up to 1500 kPa (pF = 4.2). [31] At 1500 kPa suction, the soil water amount is called wilting point . At that suction the plant cannot sustain its water needs as water is still being lost from the plant by transpiration , the plant's turgidity is lost, and it wilts, although stomatal closure may decrease transpiration and thus may retard wilting below the wilting point, in particular under adaptation or acclimatization to drought . [32] The next level, called air-dry, occurs at 100,000 kPa suction (pF = 6). Finally the oven dry condition is reached at 1,000,000 kPa suction (pF = 7). All water below wilting point is called unavailable water. [33] When the soil moisture content is optimal for plant growth, the water in the large and intermediate size pores can move about in the soil and be easily used by plants. [9] The amount of water remaining in a soil drained to field capacity and the amount that is available are functions of the soil type. Sandy soil will retain very little water, while clay will hold the maximum amount. [29] The available water for the silt loam might be 20% whereas for the sand it might be only 6% by volume, as shown in this table. Wilting point, field capacity, and available water of various soil textures (unit: % by volume) [34] Soil Texture
Toggle the table of contents Ocean acidification From Wikipedia, the free encyclopedia Decrease of pH levels in the ocean Ocean acidification means that the average ocean pH value is dropping over time. [1] Ocean acidification is the ongoing decrease in the pH of the Earth's ocean . Between 1950 and 2020, the average pH of the ocean surface fell from approximately 8.15 to 8.05. [2] Carbon dioxide emissions from human activities are the primary cause of ocean acidification, with atmospheric carbon dioxide (CO2) levels exceeding 410 ppm (in 2020). CO2 from the atmosphere is absorbed by the oceans. This chemical reaction produces carbonic acid ( HCO−3) and a hydrogen ion ( H+). The presence of free hydrogen ions ( H+) lowers the pH of the ocean, increasing acidity (this does not mean that seawater is acidic yet; it is still alkaline , with a pH higher than 8). Marine calcifying organisms , such as mollusks and corals , are especially vulnerable because they rely on calcium carbonate to build shells and skeletons. [3] A change in pH by 0.1 represents a 26% increase in hydrogen ion concentration in the world's oceans (the pH scale is logarithmic, so a change of one in pH units is equivalent to a tenfold change in hydrogen ion concentration). Sea-surface pH and carbonate saturation states vary depending on ocean depth and location. Colder and higher latitude waters are capable of absorbing more CO2. This can cause acidity to rise, lowering the pH and carbonate saturation levels in these areas. Other factors that influence the atmosphere-ocean CO2 exchange, and thus local ocean acidification, include: ocean currents and upwelling zones, proximity to large continental rivers, sea ice coverage, and atmospheric exchange with nitrogen and sulfur from fossil fuel burning and agriculture . [4] [5] [6] Decreased ocean pH has a range of potentially harmful effects for marine organisms. These include reduced calcification, depressed metabolic rates, lowered immune responses, and reduced energy for basic functions such as reproduction. [7] The effects of ocean acidification are therefore impacting marine ecosystems that provide food, livelihoods, and other ecosystem services for a large portion of humanity. Some 1 billion people are wholly or partially dependent on the fishing, tourism, and coastal management services provided by coral reefs. Ongoing acidification of the oceans may therefore threaten food chains linked with the oceans. [8] [9] The United Nations Sustainable Development Goal 14 ("Life below Water") has a target to "minimize and address the impacts of ocean acidification". [10] Reducing carbon dioxide emissions (i.e., climate change mitigation measures) is the only solution that addresses the root cause of ocean acidification. Mitigation measures which achieve carbon dioxide removal from the atmosphere would help to reverse ocean acidification. The more specific ocean-based mitigation methods (e.g. ocean alkalinity enhancement, enhanced weathering ) could also reduce ocean acidification. These strategies are being researched, but generally have a low technology readiness level and many risks. [11] [12] [13] Ocean acidification has occurred previously in Earth's history. [14] The resulting ecological collapse in the oceans had long-lasting effects on the global carbon cycle and climate . Cause[ edit ] Spatial distribution of global surface ocean pH (Panel a: the annually-averaged surface ocean pH to be approximate for the year 1770; Panel b: the difference between pH in 2000 and 1770 in the global surface ocean). [4] This diagram of the fast carbon cycle shows the movement of carbon between land, atmosphere, and oceans. Yellow numbers are natural fluxes, and red are human contributions in gigatons of carbon per year. White numbers indicate stored carbon. [15] Video summarizing the impacts of ocean acidification. Source: NOAA Environmental Visualization Laboratory. See also: Oceanic carbon cycle Present-day (2021) atmospheric carbon dioxide (CO2) levels of around 415 ppm are around 50% higher than preindustrial concentrations. [16] The current elevated levels and rapid growth rates are unprecedented in the past 55 million years of the geological record. The sources of this excess CO2 are clearly established as human driven: they include anthropogenic fossil fuel, industrial, and land-use/land-change emissions. The ocean acts as a carbon sink for anthropogenic CO2 and takes up roughly a quarter of total anthropogenic CO2 emissions. [17] However, the additional CO2 in the ocean results in a wholesale shift in seawater acid-base chemistry toward more acidic, lower pH conditions and lower saturation states for carbonate minerals used in many marine organism shells and skeletons. [17] Accumulated since 1850, the ocean sink holds up to 175 ± 35 gigatons of carbon, with more than two-thirds of this amount (120 GtC) being taken up by the global ocean since 1960. Over the historical period, the ocean sink increased in pace with the exponential anthropogenic emissions increase. From 1850 until 2022, the ocean has absorbed 26 % of total anthropogenic emissions. [16] Emissions during the period 1850–2021 amounted to 670 ± 65 gigatons of carbon and were partitioned among the atmosphere (41 %), ocean (26 %), and land (31 %). [16] The carbon cycle describes the fluxes of carbon dioxide (CO2) between the oceans, terrestrial biosphere , lithosphere , [18] and atmosphere . The carbon cycle involves both organic compounds such as cellulose and inorganic carbon compounds such as carbon dioxide , carbonate ion , and bicarbonate ion , together referenced as dissolved inorganic carbon (DIC). These inorganic compounds are particularly significant in ocean acidification, as they include many forms of dissolved CO2 present in the Earth's oceans. [19] When CO2 dissolves, it reacts with water to form a balance of ionic and non-ionic chemical species: dissolved free carbon dioxide (CO2(aq)), carbonic acid (H2CO3), bicarbonate (HCO−3) and carbonate (CO2−3). The ratio of these species depends on factors such as seawater temperature , pressure and salinity (as shown in a Bjerrum plot ). These different forms of dissolved inorganic carbon are transferred from an ocean's surface to its interior by the ocean's solubility pump . The resistance of an area of ocean to absorbing atmospheric CO2 is known as the Revelle factor . Main effects[ edit ] The ocean's chemistry is changing due to the uptake of anthropogenic carbon dioxide (CO2). [4] [20] : 395 Ocean pH, carbonate ion concentrations ([CO32−]), and calcium carbonate mineral saturation states (Ω) have been declining as a result of the uptake of approximately 30% of the anthropogenic carbon dioxide emissions over the past 270 years (since around 1750). This process, commonly referred to as "ocean acidification", is making it harder for marine calcifiers to build a shell or skeletal structure, endangering coral reefs and the broader marine ecosystems. [4] Ocean acidification has been called the "evil twin of global warming " and "the other CO2 problem". [21] [22] Increased ocean temperatures and oxygen loss act concurrently with ocean acidification and constitute the "deadly trio" of climate change pressures on the marine environment. [23] The impacts of this will be most severe for coral reefs and other shelled marine organisms, [24] [25] as well as those populations that depend on the ecosystem services they provide. Reduction in pH value[ edit ] Dissolving CO2 in seawater increases the hydrogen ion (H+) concentration in the ocean, and thus decreases ocean pH, as follows: [26] CO2 (aq) + H2O ⇌ H2CO3 ⇌ HCO3− + H+ ⇌ CO32− + 2 H+. In shallow coastal and shelf regions, a number of factors interplay to affect air-ocean CO2 exchange and resulting pH change. [27] [28] These include biological processes, such as photosynthesis and respiration, [29] as well as water upwelling. [30] Also, ecosystem metabolism in freshwater sources reaching coastal waters can lead to large, but local, pH changes. [27] Freshwater bodies also appear to be acidifying, although this is a more complex and less obvious phenomenon. [31] [32] The absorption of CO2 from the atmosphere does not affect the ocean's alkalinity . [33] : 2252 This is important to know in this context as alkalinity is the capacity of water to resist acidification . [34] Ocean alkalinity enhancement has been proposed as one option to add alkalinity to the ocean and therefore buffer against pH changes. Decreased calcification in marine organisms[ edit ] Various types of foraminifera observed through a microscope using differential interference contrast. Bjerrum plot : Change in carbonate system of seawater from ocean acidification Changes in ocean chemistry can have extensive direct and indirect effects on organisms and their habitats. One of the most important repercussions of increasing ocean acidity relates to the production of shells out of calcium carbonate ( CaCO3). [3] This process is called calcification and is important to the biology and survival of a wide range of marine organisms. Calcification involves the precipitation of dissolved ions into solid CaCO3 structures, structures for many marine organisms, such as coccolithophores , foraminifera , crustaceans , mollusks , etc. After they are formed, these CaCO3 structures are vulnerable to dissolution unless the surrounding seawater contains saturating concentrations of carbonate ions ( CO2−3). Very little of the extra carbon dioxide that is added into the ocean remains as dissolved carbon dioxide. The majority dissociates into additional bicarbonate and free hydrogen ions. The increase in hydrogen is larger than the increase in bicarbonate, [35] creating an imbalance in the reaction: HCO−3 ⇌ CO2−3 + H+ To maintain chemical equilibrium, some of the carbonate ions already in the ocean combine with some of the hydrogen ions to make further bicarbonate. Thus the ocean's concentration of carbonate ions is reduced, removing an essential building block for marine organisms to build shells, or calcify: Ca2+ + CO2−3 ⇌ CaCO3 The increase in concentrations of dissolved carbon dioxide and bicarbonate, and reduction in carbonate, are shown in the Bjerrum plot . Decrease in saturation state[ edit ] Distribution of (A) aragonite and (B) calcite saturation depth in the global oceans [36] The saturation state (known as Ω) of seawater for a mineral is a measure of the thermodynamic potential for the mineral to form or to dissolve, and for calcium carbonate is described by the following equation: Ω p {\displaystyle {\Omega }={\frac {\left[{\ce {Ca^2+}}\right]\left[{\ce {CO3^2-}}\right]}{K_{sp}}}} Here Ω is the product of the concentrations (or activities ) of the reacting ions that form the mineral (Ca2+ and CO32−), divided by the apparent solubility product at equilibrium (Ksp), that is, when the rates of precipitation and dissolution are equal. [37] In seawater, dissolution boundary is formed as a result of temperature, pressure, and depth, and is known as the saturation horizon. [3] Above this saturation horizon, Ω has a value greater than 1, and CaCO3 does not readily dissolve. Most calcifying organisms live in such waters. [3] Below this depth, Ω has a value less than 1, and CaCO3 will dissolve. The carbonate compensation depth is the ocean depth at which carbonate dissolution balances the supply of carbonate to sea floor, therefore sediment below this depth will be void of calcium carbonate. [38] Increasing CO2 levels, and the resulting lower pH of seawater, decreases the concentration of CO32− and the saturation state of CaCO3 therefore increasing CaCO3 dissolution. Calcium carbonate most commonly occurs in two common polymorphs (crystalline forms): aragonite and calcite . Aragonite is much more soluble than calcite, so the aragonite saturation horizon, and aragonite compensation depth, is always nearer to the surface than the calcite saturation horizon. [3] This also means that those organisms that produce aragonite may be more vulnerable to changes in ocean acidity than those that produce calcite. [39] Ocean acidification and the resulting decrease in carbonate saturation states raise the saturation horizons of both forms closer to the surface. [3] This decrease in saturation state is one of the main factors leading to decreased calcification in marine organisms because the inorganic precipitation of CaCO3 is directly proportional to its saturation state and calcifying organisms exhibit stress in waters with lower saturation states. [40] Natural variability and climate feedbacks[ edit ] Further information: Carbonate compensation depth Already now large quantities of water undersaturated in aragonite are upwelling close to the Pacific continental shelf area of North America, from Vancouver to Northern California . [41] These continental shelves play an important role in marine ecosystems, since most marine organisms live or are spawned there. Other shelf areas may be experiencing similar effects. [41] At depths of 1000s of meters in the ocean, calcium carbonate shells begin to dissolve as increasing pressure and decreasing temperature shift the chemical equilibria controlling calcium carbonate precipitation. [42] The depth at which this occurs is known as the carbonate compensation depth . Ocean acidification will increase such dissolution and shallow the carbonate compensation depth on timescales of tens to hundreds of years. [42] Zones of downwelling are being affected first. [43] In the North Pacific and North Atlantic, saturation states are also decreasing (the depth of saturation is getting more shallow). [20] : 396 Ocean acidification is progressing in the open ocean as the CO2 travels to deeper depth as a result of ocean mixing. In the open ocean, this causes carbonate compensation depths to become more shallow, meaning that dissolution of calcium carbonate will occur below those depths. In the North Pacific these carbonate saturations depths are shallowing at a rate of 1–2 m per year. [20] : 396 It is expected that ocean acidification in the future will lead to a significant decrease in the burial of carbonate sediments for several centuries, and even the dissolution of existing carbonate sediments. [44] Measured and estimated values[ edit ] Present day and recent history[ edit ] Time series of atmospheric CO2 at Mauna Loa (in parts per million volume, ppmv; red), surface ocean pCO2 (µatm; blue) and surface ocean pH (green) at Ocean Station ALOHA in the subtropical North Pacific Ocean. [45] [46] Estimated change in seawater pH caused by anthropogenic impact on CO2 levels between the 1700s and the 1990s, from the Global Ocean Data Analysis Project (GLODAP) and the World Ocean Atlas Between 1950 and 2020, the average pH value of the ocean surface is estimated to have decreased from approximately 8.15 to 8.05. [2] This represents an increase of around 26% in hydrogen ion concentration in the world's oceans (the pH scale is logarithmic, so a change of one in pH unit is equivalent to a tenfold change in hydrogen ion concentration). [47] For example, in the 15-year period 1995–2010 alone, acidity has increased 6 percent in the upper 100 meters of the Pacific Ocean from Hawaii to Alaska. [48] The IPCC Sixth Assessment Report in 2021 stated that "present-day surface pH values are unprecedented for at least 26,000 years and current rates of pH change are unprecedented since at least that time. [49] : 76 The pH value of the ocean interior has declined over the last 20–30 years everywhere in the global ocean. [49] : 76 The report also found that "pH in open ocean surface water has declined by about 0.017 to 0.027 pH units per decade since the late 1980s". [50] : 716 The rate of decline differs by region. This is due to complex interactions between different types of forcing mechanisms: [50] : 716 "In the tropical Pacific, its central and eastern upwelling zones exhibited a faster pH decline of minus 0.022 to minus 0.026 pH unit per decade." This is thought to be "due to increased upwelling of CO2-rich sub-surface waters in addition to anthropogenic CO2 uptake." [50] : 716 Some regions exhibited a slower acidification rate: a pH decline of minus 0.010 to minus 0.013 pH unit per decade has been observed in warm pools in the western tropical Pacific. [50] : 716 The rate at which ocean acidification will occur may be influenced by the rate of surface ocean warming , because warm waters will not absorb as much CO2. [51] Therefore, greater seawater warming could limit CO2 absorption and lead to a smaller change in pH for a given increase in CO2. [51] The difference in changes in temperature between basins is one of the main reasons for the differences in acidification rates in different localities. Current rates of ocean acidification have been likened to the greenhouse event at the Paleocene–Eocene boundary (about 56 million years ago), when surface ocean temperatures rose by 5–6 degrees Celsius . In that event, surface ecosystems experienced a variety of impacts, but bottom-dwelling organisms in the deep ocean actually experienced a major extinction. [52] Currently, the rate of carbon addition to the atmosphere-ocean system is about ten times the rate that occurred at the Paleocene–Eocene boundary. [53] Extensive observational systems are now in place or being built for monitoring seawater CO2 chemistry and acidification for both the global open ocean and some coastal systems. [17] Rates of increasing acidity in different marine regions Location Change in pH unitsper decade Period Proxy reconstruction 2016 Rates of pH change for some regions of the world(Many more regions available in source table) [61] : Table 5.SM.3 Station, region Dyfamed (43.42°N, 7.87°E) 1995–2011 Iceland Sea (68°N, 12.67°W) 1985–20081985–2010 Irminger Sea (64.3°N, 28°W) 1983–2004 Ocean acidification has occurred previously in Earth's history. [14] It happened during the Capitanian mass extinction , [62] [63] [64] at the end-Permian extinction , [65] [66] [67] during the end-Triassic extinction , [68] [69] [70] and during the Cretaceous–Palaeogene extinction event . [71] Three of the big five mass extinction events in the geologic past were associated with a rapid increase in atmospheric carbon dioxide, probably due to volcanism and/or thermal dissociation of marine gas hydrates . [72] Elevated CO2 levels impacted biodiversity. [73] Decreased CaCO3 saturation due to seawater uptake of volcanogenic CO2 has been suggested as a possible kill mechanism during the marine mass extinction at the end of the Triassic . [74] The end-Triassic biotic crisis is still the most well-established example of a marine mass extinction due to ocean acidification, because (a) carbon isotope records suggest enhanced volcanic activity that decreased the carbonate sedimentation which reduced the carbonate compensation depth and the carbonate saturation state, and a marine extinction coincided precisely in the stratigraphic record, [70] [69] [75] and (b) there was pronounced selectivity of the extinction against organisms with thick aragonitic skeletons, [70] [76] [77] which is predicted from experimental studies. [78] Ocean acidification has also been suggested as a one cause of the end-Permian mass extinction [66] [65] and the end-Cretaceous crisis. [71] Overall, multiple climatic stressors, including ocean acidification, was likely the cause of geologic extinction events. [72] The most notable example of ocean acidification is the Paleocene-Eocene Thermal Maximum (PETM), which occurred approximately 56 million years ago when massive amounts of carbon entered the ocean and atmosphere, and led to the dissolution of carbonate sediments across many ocean basins. [73] Relatively new geochemical methods of testing for pH in the past indicate the pH dropped 0.3 units across the PETM. [79] [80] One study that solves the marine carbonate system for saturation state shows that it may not change much over the PETM, suggesting the rate of carbon release at our best geological analogy was much slower than human-induced carbon emissions. However, stronger proxy methods to test for saturation state are needed to assess how much this pH change may have affected calcifying organisms. Predicted future values[ edit ] In situ CO2 concentration sensor (SAMI-CO2), attached to a Coral Reef Early Warning System station, utilized in conducting ocean acidification studies near coral reef areas (by NOAA ( AOML )) A moored autonomous CO2 buoy used for measuring CO2 concentration and ocean acidification studies ( NOAA (by PMEL )) Importantly, the rate of change in ocean acidification is much higher than in the geological past. This faster change prevents organisms from gradually adapting, and prevents climate cycle feedbacks from kicking in to mitigate ocean acidification. Ocean acidification is now on a path to reach lower pH levels than at any other point in the last 300 million years. [81] [71] The rate of ocean acidification (i.e. the rate of change in pH value) is also estimated to be unprecedented over that same time scale. [82] [14] These expected changes are considered unprecedented in the geological record. [83] [84] [85] In combination with other ocean biogeochemical changes, this drop in pH value could undermine the functioning of marine ecosystems and disrupt the provision of many goods and services associated with the ocean, beginning as early as 2100. [86] The extent of further ocean chemistry changes, including ocean pH, will depend on climate change mitigation efforts taken by nations and their governments. [49] Different scenarios of projected socioeconomic global changes are modelled by using the Shared Socioeconomic Pathways (SSP) scenarios. Under a very high emission scenario (SSP5-8.5) , model projections estimate that surface ocean pH could decrease by as much as 0.44 units by the end of this century, compared to the end of the 19th century. [87] : 608 This would mean a pH as low as about 7.7, and represents a further increase in H+ concentrations of two to four times beyond the increase to date. Estimated past and future global mean surface pH for different emission scenarios [49] : values estimated from Figure TS.11 (d) Time period Future (2100) with low emission scenario ( SSP 1–2.6) 8.0 Future (2100) with very high emission scenario ( SSP 5–8.5) 7.7 A pteropod shell is shown dissolving over time in seawater with a lower pH. When carbon dioxide is absorbed by the ocean from the atmosphere, the chemistry of the seawater is changed (source: NOAA ) Pterapod shell dissolved in seawater adjusted to an ocean chemistry projected for the year 2100 (source: NOAA ). Unhealthy pteropod showing effects of ocean acidification including ragged, dissolving shell ridges on upper surface, a cloudy shell in lower right quadrant, and severe abrasions and weak spots at 6:30 position on lower whorl of shell (source: NOAA ). Complexity of research findings[ edit ] The full ecological consequences of the changes in calcification due to ocean acidification are complex but it appears likely that many calcifying species will be adversely affected by ocean acidification. [17] [20] : 413 Increasing ocean acidification makes it more difficult for shell-accreting organisms to access carbonate ions, essential for the production of their hard exoskeletal shell. [88] Oceanic calcifying organism span the food chain from autotrophs to heterotrophs and include organisms such as coccolithophores , corals , foraminifera , echinoderms , crustaceans and molluscs . [86] [89] Overall, all marine ecosystems on Earth will be exposed to changes in acidification and several other ocean biogeochemical changes. [90] Ocean acidification may force some organisms to reallocate resources away from productive endpoints in order to maintain calcification. [91] For example, the oyster Magallana gigas is recognized to experience metabolic changes alongside altered calcification rates due to energetic tradeoffs resulting from pH imbalances. [92] Under normal conditions, calcite and aragonite are stable in surface waters since the carbonate ions are supersaturated with respect to seawater. However, as ocean pH falls, the concentration of carbonate ions also decreases. Calcium carbonate thus becomes undersaturated, and structures made of calcium carbonate are vulnerable to calcification stress and dissolution. [93] In particular, studies show that corals, [94] [95] coccolithophores, [89] [27] [96] coralline algae, [97] foraminifera, [98] shellfish and pteropods [99] experience reduced calcification or enhanced dissolution when exposed to elevated CO2. Even with active marine conservation practices it may be impossible to bring back many previous shellfish populations. [100] Some studies have found different responses to ocean acidification, with coccolithophore calcification and photosynthesis both increasing under elevated atmospheric pCO2 , [101] and an equal decline in primary production and calcification in response to elevated CO2, [102] or the direction of the response varying between species. [103] Similarly, the sea star, Pisaster ochraceus , shows enhanced growth in waters with increased acidity. [104] Reduced calcification from ocean acidification may affect the ocean's biologically driven sequestration of carbon from the atmosphere to the ocean interior and seafloor sediment , weakening the so-called biological pump . [71] Seawater acidification could also reduce the size of Antarctic phytoplankton, making them less effective at storing carbon. [105] Such changes are being increasingly studied and synthesized through the use of physiological frameworks, including the Adverse Outcome Pathway (AOP) framework. [92] Coccolithus pelagicus, a species of coccolithophore sampled from the North Atlantic Ocean. Further information: Coccolithophore A coccolithophore is a unicellular , eukaryotic phytoplankton ( alga ). Understanding calcification changes in coccolithophores may be particularly important because a decline in the coccolithophores may have secondary effects on climate: it could contribute to global warming by decreasing the Earth's albedo via their effects on oceanic cloud cover. [106] A study in 2008 examined a sediment core from the North Atlantic and found that the species composition of coccolithophorids remained unchanged over the past 224 years (1780 to 2004). But the average coccolith mass had increased by 40% during the same period. [101] Further information: Coral and Coral reef Warm water corals are clearly in decline, with losses of 50% over the last 30–50 years due to multiple threats from ocean warming, ocean acidification, pollution and physical damage from activities such as fishing, and these pressures are expected to intensify. [107] [20] : 416 The fluid in the internal compartments (the coelenteron) where corals grow their exoskeleton is also extremely important for calcification growth. When the saturation state of aragonite in the external seawater is at ambient levels, the corals will grow their aragonite crystals rapidly in their internal compartments, hence their exoskeleton grows rapidly. If the saturation state of aragonite in the external seawater is lower than the ambient level, the corals have to work harder to maintain the right balance in the internal compartment. When that happens, the process of growing the crystals slows down, and this slows down the rate of how much their exoskeleton is growing. Depending on the aragonite saturation state in the surrounding water, the corals may halt growth because pumping aragonite into the internal compartment will not be energetically favorable. [108] Under the current progression of carbon emissions, around 70% of North Atlantic cold-water corals will be living in corrosive waters by 2050–60. [109] Acidified conditions primarily reduce the coral's capacity to build dense exoskeletons, rather than affecting the linear extension of the exoskeleton. The density of some species of corals could be reduced by over 20% by the end of this century. [110] An in situ experiment, conducted on a 400 m2 patch of the Great Barrier Reef , to decrease seawater CO2 level (raise pH) to near the preindustrial value showed a 7% increase in net calcification. [111] A similar experiment to raise in situ seawater CO2 level (lower pH) to a level expected soon after the 2050 found that net calcification decreased 34%. [112] However, a field study of the coral reef in Queensland and Western Australia from 2007 to 2012 found that corals are more resistant to the environmental pH changes than previously thought, due to internal homeostasis regulation; this makes thermal change ( marine heatwaves ), which leads to coral bleaching , rather than acidification, the main factor for coral reef vulnerability due to climate change. [113] Studies at carbon dioxide seep sites[ edit ] In some places carbon dioxide bubbles out from the sea floor, locally changing the pH and other aspects of the chemistry of the seawater. Studies of these carbon dioxide seeps have documented a variety of responses by different organisms. [114] Coral reef communities located near carbon dioxide seeps are of particular interest because of the sensitivity of some corals species to acidification. In Papua New Guinea , declining pH caused by carbon dioxide seeps is associated with declines in coral species diversity. [115] However, in Palau carbon dioxide seeps are not associated with reduced species diversity of corals, although bioerosion of coral skeletons is much higher at low pH sites. Pteropods and brittle stars[ edit ] Pteropods and brittle stars both form the base of the Arctic food webs and are both seriously damaged from acidification. Pteropods shells dissolve with increasing acidification and the brittle stars lose muscle mass when re-growing appendages . [116] For pteropods to create shells they require aragonite which is produced through carbonate ions and dissolved calcium and strontium. Pteropods are severely affected because increasing acidification levels have steadily decreased the amount of water supersaturated with carbonate. [117] The degradation of organic matter in Arctic waters has amplified ocean acidification; some Arctic waters are already undersaturated with respect to aragonite. [118] [119] [120] The brittle star's eggs die within a few days when exposed to expected conditions resulting from Arctic acidification. [121] Similarly, when exposed in experiments to pH reduced by 0.2 to 0.4, larvae of a temperate brittle star , a relative of the common sea star , fewer than 0.1 percent survived more than eight days. [86] Other impacts on ecosystems[ edit ] This map shows changes in the aragonite saturation level of ocean surface waters between the 1880s and 2006–2015. Aragonite is a form of calcium carbonate that many marine animals use to build their skeletons and shells. The lower the saturation level, the more difficult it is for organisms to build and maintain their skeletons and shells. A negative change represents a decrease in saturation. [122] Other biological impacts[ edit ] Aside from the slowing and/or reversal of calcification, organisms may suffer other adverse effects, either indirectly through negative impacts on food resources, or directly as reproductive or physiological effects. [3] For example, the elevated oceanic levels of CO2 may produce CO2-induced acidification of body fluids, known as hypercapnia . [123] Increasing acidity has been observed to reduce metabolic rates in jumbo squid [124] and depress the immune responses of blue mussels. [125] Atlantic longfin squid eggs took longer to hatch in acidified water, and the squid's statolith was smaller and malformed in animals placed in sea water with a lower pH. [126] However, these studies are ongoing and there is not yet a full understanding of these processes in marine organisms or ecosystems . [127] Acoustic properties[ edit ] Another potential route to ecosystem impacts is through bioacoustics . This may occur as ocean acidification can alter the acoustic properties of seawater, allowing sound to propagate further, and increasing ocean noise. [128] This impacts all animals that use sound for echolocation or communication . [129] Algae and seagrasses[ edit ] Further information: Eutrophication Another possible effect would be an increase in harmful algal bloom events, which could contribute to the accumulation of toxins ( domoic acid , brevetoxin , saxitoxin ) in small organisms such as anchovies and shellfish , in turn increasing occurrences of amnesic shellfish poisoning , neurotoxic shellfish poisoning and paralytic shellfish poisoning . [130] Although algal blooms can be harmful, other beneficial photosynthetic organisms may benefit from increased levels of carbon dioxide. Most importantly, seagrasses will benefit. [131] Research found that as seagrasses increased their photosynthetic activity, calcifying algae's calcification rates rose, likely because localized photosynthetic activity absorbed carbon dioxide and elevated local pH. [131] Fish larvae[ edit ] Ocean acidification can also have effects on marine fish larvae . It internally affects their olfactory systems, which is a crucial part of their early development. Orange clownfish larvae mostly live on oceanic reefs that are surrounded by vegetative islands[ clarification needed ]. [113] Larvae are known to use their sense of smell to detect the differences between reefs surrounded by vegetative islands and reefs not surrounded by vegetative islands. [113] Clownfish larvae need to be able to distinguish between these two destinations to be able to find a suitable area for their growth. Another use for marine fish olfactory systems is to distinguish between their parents and other adult fish, in order to avoid inbreeding. In an experimental aquarium facility, clownfish were sustained in non-manipulated seawater with pH 8.15 ± 0.07, which is similar to our current ocean's pH. [113] To test for effects of different pH levels, the seawater was modified to two other pH levels, which corresponded with climate change models that predict future atmospheric CO2 levels. [113] In the year 2100 the model projects possible CO2 levels of 1,000 ppm, which correlates with the pH of 7.8 ± 0.05. This experiment showed that when larvae are exposed to a pH of 7.8 ± 0.05 their reaction to environmental cues differs drastically from their reaction to cues at pH equal to current ocean levels. [113] At pH 7.6 ± 0.05 larvae had no reaction to any type of cue. However, a meta-analysis published in 2022 found that the effect sizes of published studies testing for ocean acidification effects on fish behavior have declined by an order of magnitude over the past decade, and have been negligible for the past five years. [132] Eel embryos, a "critically endangered" species [133] yet profound[ clarification needed ] in aquaculture, are also being affected by ocean acidification, specifically the European eel . Although they spend most of their lives in fresh water, usually in rivers, streams, or estuaries, they go to spawn and die in the Sargasso Sea . Here is where European eels are experiencing the effects of acidification in one of their key life stages. Fish embryos and larvae are usually more sensitive to pH changes than adults, as organs for pH regulation are not full developed. [134] Because of this, European eel embryos are more vulnerable to changes in pH in the Sargasso Sea. A study of the European Eel in the Sargasso Sea was conducted in 2021 to analyze the specific effects of ocean acidification on embryos. The study found that exposure to predicted end-of-century ocean pCO2 conditions may affect normal development of this species in nature during sensitive early life history stages with limited physiological response capacities, while extreme acidification would negatively influence embryonic survival and development under hatchery conditions. [135] Compounded effects of acidification, warming and deoxygenation[ edit ] Further information: Effects of climate change on oceans and Ocean deoxygenation Drivers of hypoxia and ocean acidification intensification in upwelling shelf systems. Equatorward winds drive the upwelling of low dissolved oxygen (DO), high nutrient, and high dissolved inorganic carbon (DIC) water from above the oxygen minimum zone . Cross-shelf gradients in productivity and bottom water residence times drive the strength of DO (DIC) decrease (increase) as water transits across a productive continental shelf . [136] [137] There is a substantial body of research showing that a combination of ocean acidification and elevated ocean temperature have a compounded effect on marine life and the ocean environment. This effect far exceeds the individual harmful impact of either. [138] In addition, ocean warming, along with increased productivity of phytoplankton from higher CO2 levels exacerbates ocean deoxygenation . Deoxygenation of ocean waters is an additional stressor on marine organisms that increases ocean stratification therefore limiting nutrients over time and reducing biological gradients. [139] [140] Meta analyses have quantified the direction and magnitude of the harmful effects of combined ocean acidification, warming and deoxygenation on the ocean. [141] [142] These meta-analyses have been further tested by mesocosm studies that simulated the interaction of these stressors and found a catastrophic effect on the marine food web: thermal stress more than negates any primary producer to herbivore increase in productivity from elevated CO2. [143] [144] Impacts on the economy and societies[ edit ] The increase of ocean acidity decelerates the rate of calcification in salt water, leading to smaller and slower growing coral reefs which supports approximately 25% of marine life. [145] [146] Impacts are far-reaching from fisheries and coastal environments down to the deepest depths of the ocean. [17] The increase in ocean acidity in not only killing the coral, but also the wildly diverse population of marine inhabitants which coral reefs support. [147] Fishing and tourism industry[ edit ] The threat of acidification includes a decline in commercial fisheries and the coast-based tourism industry . Several ocean goods and services are likely to be undermined by future ocean acidification potentially affecting the livelihoods of some 400 to 800 million people, depending upon the greenhouse gas emission scenario . [86] Some 1 billion people are wholly or partially dependent on the fishing, tourism, and coastal management services provided by coral reefs. Ongoing acidification of the oceans may therefore threaten future food chains linked with the oceans. [8] [9] Arctic[ edit ] In the Arctic, commercial fisheries are threatened because acidification harms calcifying organisms which form the base of the Arctic food webs (pteropods and brittle stars, see above).  Acidification threatens Arctic food webs from the base up. Arctic food webs are considered simple, meaning there are few steps in the food chain from small organisms to larger predators. For example, pteropods are "a key prey item of a number of higher predators – larger plankton, fish, seabirds, whales". [148] Both pteropods and sea stars serve as a substantial food source and their removal from the simple food web would pose a serious threat to the whole ecosystem. The effects on the calcifying organisms at the base of the food webs could potentially destroy fisheries. US commercial fisheries[ edit ] An adult American lobster rests on the sea floor. Rhode Island, Dutch Island, Newport County. The value of fish caught from US commercial fisheries in 2007 was valued at $3.8 billion and of that 73% was derived from calcifiers and their direct predators. [149] Other organisms are directly harmed as a result of acidification. For example, decrease in the growth of marine calcifiers such as the American lobster , ocean quahog , and scallops means there is less shellfish meat available for sale and consumption. [150] Red king crab fisheries are also at a serious threat because crabs are also calcifiers. Baby red king crab when exposed to increased acidification levels experienced 100% mortality after 95 days. [151] In 2006, red king crab accounted for 23% of the total guideline harvest levels and a serious decline in red crab population would threaten the crab harvesting industry. [152]
From Wikipedia, the free encyclopedia Soil frozen for a duration of at least two years This article is about frozen ground. For other uses, see Permafrost (disambiguation) . Permafrost Map showing extent and types of permafrost in the Northern Hemisphere Used in Climate High latitudes, alpine regions Permafrost (from perma- ' permanent ', and frost ) is soil or underwater sediment which continuously remains below 0 °C (32 °F) for two years or more: the oldest permafrost had been continuously frozen for around 700,000 years. [1] While the shallowest permafrost has a vertical extent of below a meter (3 ft), the deepest is greater than 1,500 m (4,900 ft). [2] Similarly, the area of individual permafrost zones may be limited to narrow mountain summits or extend across vast Arctic regions. [3] The ground beneath glaciers and ice sheets is not usually defined as permafrost, so on land, permafrost is generally located beneath a so-called active layer of soil which freezes and thaws depending on the season. [4] Around 15% of the Northern Hemisphere or 11% of the global surface is underlain by permafrost, [5] with the total area of around 18 million km2 (6.9 million sq mi). [6] This includes substantial areas of Alaska , Greenland , Canada, and Siberia . It is also located in high mountain regions, with the Tibetan Plateau a prominent example. Only a minority of permafrost exists in the Southern Hemisphere , where it is consigned to mountain slopes like in the Andes of Patagonia , the Southern Alps of New Zealand, or the highest mountains of Antarctica . [3] [1] Permafrost contains large amounts of dead biomass that have accumulated throughout millennia without having had the chance to fully decompose and release their carbon , making tundra soil a carbon sink . [3] As global warming heats the ecosystem, frozen soil thaws and becomes warm enough for decomposition to start anew, accelerating the permafrost carbon cycle . Depending on conditions at the time of thaw, decomposition can either release carbon dioxide or methane , and these greenhouse gas emissions act as a climate change feedback . [7] [8] [9] The emissions from thawing permafrost will have a sufficient impact on the climate to impact global carbon budgets . Exact estimates of permafrost emissions are hard to model because of the uncertainty about different thaw processes. There is a widespread agreement they will be smaller than human-caused emissions and not large enough to result in " runaway warming ". [10] Instead, projected annual permafrost emissions have been compared to global emissions from deforestation , or to annual emissions of large countries such as Russia, the United States or China. [11] In addition to its climate impact, permafrost thaw brings additional risks. Formerly frozen ground often contains enough ice that when it thaws, hydraulic saturation is suddenly exceeded, so the ground shifts substantially and may even collapse outright. Many buildings and other infrastructure were built on permafrost when it was frozen and stable, and so are vulnerable to collapse if it thaws. [12] Estimates suggest nearly 70% of such infrastructure is at risk by 2050, and that the associated costs could rise to tens of billions of dollars in the second half of the century. [13] Furthermore, between 13,000 and 20,000 sites contaminated with toxic waste are present in the permafrost, [14] as well as the natural mercury deposits, [15] which are all liable to leak and pollute the environment as the warming progresses. [16] Lastly, there have been concerns about potentially pathogenic microorganisms surviving the thaw and contributing to future epidemics and pandemics , [17] [18] although this risk is speculative and is considered implausible by much of the scientific community. [19] [20] [21] Classification and extent[ edit ] Permafrost temperature profile. Permafrost occupies the middle zone, with the active layer above it, while geothermal activity keeps the lowest layer above freezing. The vertical 0 °C or 32 °F line denotes the average annual temperature that is crucial for the upper and lower limit of the permafrost zone, while the red lines represent seasonal temperature changes and seasonal temperature extremes. Solid curved lines at the top show seasonal maximum and minimum temperatures in the active layer, while the red dotted-to-solid line depicts the average temperature profile with depth of soil in a permafrost region. Permafrost is soil , rock or sediment that is frozen for more than two consecutive years. In practice, this means that permafrost occurs at a mean annual temperature of −2 °C (28.4 °F) or below. In the coldest regions, the depth of continuous permafrost can exceed 1,400 m (4,600 ft). [22] It typically exists beneath the so-called active layer , which freezes and thaws annually, and so can support plant growth, as the roots can only take hold in the soil that's thawed. [2] Active layer thickness is measured during its maximum extent at the end of summer: [23] as of 2018, the average thickness in the Northern Hemisphere is ~145 centimetres (4.76 ft), but there are significant regional differences. Northeastern Siberia , Alaska and Greenland have the most solid permafrost with the lowest extent of active layer (less than 50 centimetres (1.6 ft) on average, and sometimes only 30 centimetres (0.98 ft)), while southern Norway and the Mongolian Plateau are the only areas where the average active layer is deeper than 600 centimetres (20 ft), with the record of 10 metres (33 ft). [24] [25] The border between active layer and permafrost itself is sometimes called permafrost table. [26] Around 15% of Northern Hemisphere land that is not completely covered by ice is directly underlain by permafrost; 22% is defined as part of a permafrost zone or region. [5] This is because only slightly more than half of this area is defined as a continuous permafrost zone, where 90%–100% of the land is underlain by permafrost. Around 20% is instead defined as discontinuous permafrost, where the coverage is between 50% and 90%. Finally, the remaining <30% of permafrost regions consists of areas with 10%–50% coverage, which are defined as sporadic permafrost zones, and some areas that have isolated patches of permafrost covering 10% or less of their area. [27] [28] : 435 Most of this area is found in Siberia, northern Canada, Alaska and Greenland. Beneath the active layer annual temperature swings of permafrost become smaller with depth. The greatest depth of permafrost occurs right before the point where geothermal heat maintains a temperature above freezing. Above that bottom limit there may be permafrost with a consistent annual temperature—"isothermal permafrost". [29] Continuity of coverage[ edit ] Permafrost typically forms in any climate where the mean annual air temperature is lower than the freezing point of water. Exceptions are found in humid boreal forests , such as in Northern Scandinavia and the North-Eastern part of European Russia west of the Urals , where snow acts as an insulating blanket. Glaciated areas may also be exceptions. Since all glaciers are warmed at their base by geothermal heat, temperate glaciers , which are near the pressure melting point throughout, may have liquid water at the interface with the ground and are therefore free of underlying permafrost. [30] "Fossil" cold anomalies in the geothermal gradient in areas where deep permafrost developed during the Pleistocene persist down to several hundred metres. This is evident from temperature measurements in boreholes in North America and Europe. [31] Discontinuous permafrost[ edit ] Excavating ice-rich permafrost with a jackhammer in Alaska . The below-ground temperature varies less from season to season than the air temperature, with mean annual temperatures tending to increase with depth as a result of the geothermal crustal gradient. Thus, if the mean annual air temperature is only slightly below 0 °C (32 °F), permafrost will form only in spots that are sheltered (usually with a northern or southern aspect , in north and south hemispheres respectively) creating discontinuous permafrost. Usually, permafrost will remain discontinuous in a climate where the mean annual soil surface temperature is between −5 and 0 °C (23 and 32 °F). In the moist-wintered areas mentioned before, there may not be even discontinuous permafrost down to −2 °C (28 °F). Discontinuous permafrost is often further divided into extensive discontinuous permafrost, where permafrost covers between 50 and 90 percent of the landscape and is usually found in areas with mean annual temperatures between −2 and −4 °C (28 and 25 °F), and sporadic permafrost, where permafrost cover is less than 50 percent of the landscape and typically occurs at mean annual temperatures between 0 and −2 °C (32 and 28 °F). [32] In soil science, the sporadic permafrost zone is abbreviated SPZ and the extensive discontinuous permafrost zone DPZ. [33] Exceptions occur in un-glaciated Siberia and Alaska where the present depth of permafrost is a relic of climatic conditions during glacial ages where winters were up to 11 °C (20 °F) colder than those of today. Estimated extent of alpine permafrost by region [34] Locality Rocky Mountains (US and Canada) 100,000 km2 (39,000 sq mi) 75,000 km2 (29,000 sq mi) Remaining <50,000 km2 (19,000 sq mi) At mean annual soil surface temperatures below −5 °C (23 °F) the influence of aspect can never be sufficient to thaw permafrost and a zone of continuous permafrost (abbreviated to CPZ) forms. A line of continuous permafrost in the Northern Hemisphere [35] represents the most southern border where land is covered by continuous permafrost or glacial ice. The line of continuous permafrost varies around the world northward or southward due to regional climatic changes. In the southern hemisphere , most of the equivalent line would fall within the Southern Ocean if there were land there. Most of the Antarctic continent is overlain by glaciers, under which much of the terrain is subject to basal melting . [36] The exposed land of Antarctica is substantially underlain with permafrost, [37] some of which is subject to warming and thawing along the coastline. [38] Alpine permafrost[ edit ] A range of elevations in both the Northern and Southern Hemisphere are cold enough to support perennially frozen ground: some of the best-known examples include the Canadian Rockies , the European Alps , Himalaya and the Tien Shan . In general, it has been found that extensive alpine permafrost requires mean annual air temperature of −3 °C (27 °F), though this can vary depending on local topography , and some mountain areas are known to support permafrost at −1 °C (30 °F). It is also possible for subsurface alpine permafrost to be covered by warmer, vegetation-supporting soil. [39] Changes in subsea permafrost extent and structure between the Last Glacial Maximum and now. [6] Alpine permafrost is particularly difficult to study, and systematic research efforts did not begin until the 1970s. [39] Consequently, there remain uncertainties about its geography. As recently as 2009, permafrost had been discovered in a new area – Africa's highest peak, Mount Kilimanjaro (4,700 m (15,400 ft) above sea level and approximately 3° south of the equator ). [40] In 2014, a collection of regional estimates of alpine permafrost extent had established a global extent of  3,560,000 km2 (1,370,000 sq mi). [34] Yet, by 2014, alpine permafrost in the Andes has not been fully mapped, [41] although its extent has been modeled to assess the amount of water bound up in these areas. [42] Subsea permafrost[ edit ] Subsea permafrost occurs beneath the seabed and exists in the continental shelves of the polar regions. [2] These areas formed during the last Ice Age , when a larger portion of Earth's water was bound up in ice sheets on land and when sea levels were low. As the ice sheets melted to again become seawater during the Holocene glacial retreat , coastal permafrost became submerged shelves under relatively warm and salty boundary conditions, compared to surface permafrost. Since then, these conditions led to the gradual and ongoing decline of subsea permafrost extent. [6] Nevertheless, its presence remains an important consideration for the "design, construction, and operation of coastal facilities, structures founded on the seabed, artificial islands , sub-sea pipelines , and wells drilled for exploration and production". [43] Subsea permafrost can also overlay deposits of methane clathrate , which were once speculated to be a major climate tipping point in what was known as a clathrate gun hypothesis , but are now no longer believed to play any role in projected climate change. [44] Past extent of permafrost[ edit ] At the Last Glacial Maximum , continuous permafrost covered a much greater area than it does today, covering all of ice-free Europe south to about Szeged (southeastern Hungary ) and the Sea of Azov (then dry land) [45] and East Asia south to present-day Changchun and Abashiri . [46] In North America, only an extremely narrow belt of permafrost existed south of the ice sheet at about the latitude of New Jersey through southern Iowa and northern Missouri , but permafrost was more extensive in the drier western regions where it extended to the southern border of Idaho and Oregon . [47] In the Southern Hemisphere , there is some evidence for former permafrost from this period in central Otago and Argentine Patagonia , but was probably discontinuous, and is related to the tundra. Alpine permafrost also occurred in the Drakensberg during glacial maxima above about 3,000 metres (9,840 ft). [48] [49] Time required for permafrost to reach depth at Prudhoe Bay, Alaska [50] : 35 Time (yr) Base depth[ edit ] Permafrost extends to a base depth where geothermal heat from the Earth and the mean annual temperature at the surface achieve an equilibrium temperature of 0 °C (32 °F). [51] This base depth of permafrost can vary wildly – it is less than a meter (3 ft) in the areas where it is shallowest, [2] yet reaches 1,493 m (4,898 ft) in the northern Lena and Yana River basins in Siberia . [22] Calculations indicate that the formation time of permafrost greatly slows past the first several metres. For instance, over half a million years was required to form the deep permafrost underlying Prudhoe Bay, Alaska , a time period extending over several glacial and interglacial cycles of the Pleistocene . [50] : 18 Base depth is affected by the underlying geology, and particularly by thermal conductivity , which is lower for permafrost in soil than in bedrock . [51] Lower conductivity leaves permafrost less affected by the geothermal gradient , which is the rate of increasing temperature with respect to increasing depth in the Earth's interior. It occurs as the Earth's internal thermal energy is generated by radioactive decay of unstable isotopes and flows to the surface by conduction at a rate of ~47 terawatts (TW). [52] Away from tectonic plate boundaries, this is equivalent to an average heat flow of 25–30 °C/km (124–139 °F/mi) near the surface. [53] Massive ground ice[ edit ] Labelled example of a massive buried ice deposit in Bylot Island , Canada. [54] When the ice content of a permafrost exceeds 250 percent (ice to dry soil by mass) it is classified as massive ice. Massive ice bodies can range in composition, in every conceivable gradation from icy mud to pure ice. Massive icy beds have a minimum thickness of at least 2 m and a short diameter of at least 10 m. [55] First recorded North American observations of this phenomenon were by European scientists at Canning River (Alaska) in 1919. [56] Russian literature provides an earlier date of 1735 and 1739 during the Great North Expedition by P. Lassinius and Khariton Laptev , respectively. Russian investigators including I.A. Lopatin, B. Khegbomov, S. Taber and G. Beskow had also formulated the original theories for ice inclusion in freezing soils. [57] While there are four categories of ice in permafrost – pore ice, ice wedges (also known as vein ice), buried surface ice and intrasedimental (sometimes also called constitutional [57] ) ice – only the last two tend to be large enough to qualify as massive ground ice. [58] [26] These two types usually occur separately, but may be found together, like on the coast of Tuktoyaktuk in western Arctic Canada , where the remains of Laurentide Ice Sheet are located. [59] Buried surface ice may derive from snow, frozen lake or sea ice , aufeis (stranded river ice) and even buried glacial ice from the former Pleistocene ice sheets. The latter hold enormous value for paleoglaciological research, yet even as of 2022, the total extent and volume of such buried ancient ice is unknown. [60] Notable sites with known ancient ice deposits include Yenisei River valley in Siberia , Russia as well as Banks and Bylot Island in Canada's Nunavut and Northwest Territories . [61] [62] [54] Some of the buried ice sheet remnants are known to host thermokarst lakes . [60] Intrasedimental or constitutional ice has been widely observed and studied across Canada. It forms when subterranean waters freeze in place, and is subdivided into intrusive, injection and segregational ice. The latter is the dominant type, formed after crystallizational differentiation in wet sediments , which occurs when water migrates to the freezing front under the influence of van der Waals forces . [56] [55] [58] This is a slow process, which primarily occurs in silts with salinity less than 20% of seawater : silt sediments with higher salinity and clay sediments instead have water movement prior to ice formation dominated by rheological processes. Consequently, it takes between 1 and 1000 years to form intrasedimental ice in the top 2.5 meters of clay sediments, yet it takes between 10 and 10,000 years for peat sediments and between 1,000 and 1,000,000 years for silt sediments. [26] Cliff wall of a retrogressive thaw slump located on the southern coast of Herschel Island within an approximately 22-metre (72 ft) by 1,300-metre (4,300 ft) headwall. See also: Patterned ground Permafrost processes such as thermal contraction generating cracks which eventually become ice wedges and solifluction – gradual movement of soil down the slope as it repeatedly freezes and thaws – often lead to the formation of ground polygons, rings, steps and other forms of patterned ground found in arctic, periglacial and alpine areas. [63] [64] In ice-rich permafrost areas, melting of ground ice initiates thermokarst landforms such as thermokarst lakes , thaw slumps, thermal-erosion gullies, and active layer detachments. [65] [66] Notably, unusually deep permafrost in Arctic moorlands and bogs often attracts meltwater in warmer seasons, which pools and freezes to form ice lenses , and the surrounding ground begins to jut outward at a slope. This can eventually result in the formation of large-scale land forms around this core of permafrost, such as palsas – long (15–150 m (49–492 ft)), wide (10–30 m (33–98 ft)) yet shallow (<1–6 m (3 ft 3 in – 19 ft 8 in) tall) peat mounds – and the even larger pingos , which can be 3–70 m (10–230 ft) high and 30–1,000 m (98–3,281 ft) in diameter . [67] [68] A group of palsas , as seen from above, formed by the growth of ice lenses. A peat plateau complex south of Fort Simpson , Northwest Territories . Only plants with shallow roots can survive in the presence of permafrost. Black spruce tolerates limited rooting zones, and dominates flora where permafrost is extensive. Likewise, animal species which live in dens and burrows have their habitat constrained by the permafrost, and these constraints also have a secondary impact on interactions between species within the ecosystem . [69] Cracks forming at the edges of the Storflaket permafrost bog in Sweden. While permafrost soil is frozen, it is not completely inhospitable to microorganisms , though their numbers can vary widely, typically from 1 to 1000 million per gram of soil. [70] [71] The permafrost carbon cycle (Arctic Carbon Cycle) deals with the transfer of carbon from permafrost soils to terrestrial vegetation and microbes, to the atmosphere, back to vegetation, and finally back to permafrost soils through burial and sedimentation due to cryogenic processes. Some of this carbon is transferred to the ocean and other portions of the globe through the global carbon cycle. The cycle includes the exchange of carbon dioxide and methane between terrestrial components and the atmosphere, as well as the transfer of carbon between land and water as methane, dissolved organic carbon , dissolved inorganic carbon , particulate inorganic carbon and particulate organic carbon . [72] Most of the bacteria and fungi found in permafrost cannot be cultured in the laboratory, but the identity of the microorganisms can be revealed by DNA -based techniques. For instance, analysis of 16S rRNA genes from late Pleistocene permafrost samples in eastern Siberia 's Kolyma Lowland revealed eight phylotypes , which belonged to the phyla Actinomycetota and Pseudomonadota . [73] "Muot-da-Barba-Peider", an alpine permafrost site in eastern Switzerland, was found to host a diverse microbial community in 2016. Prominent bacteria groups included phylum Acidobacteriota , Actinomycetota , AD3, Bacteroidota , Chloroflexota , Gemmatimonadota , OD1, Nitrospirota , Planctomycetota , Pseudomonadota , and Verrucomicrobiota , in addition to eukaryotic fungi like Ascomycota , Basidiomycota , and Zygomycota . In the presently living species, scientists observed a variety of adaptations for sub-zero conditions, including reduced and anaerobic metabolic processes. [74] Construction on permafrost[ edit ] There are only two large cities in the world built in areas of continuous permafrost (where the frozen soil forms an unbroken, below-zero sheet) and both are in Russia – Norilsk in Krasnoyarsk Krai and Yakutsk in the Sakha Republic . [75] Building on permafrost is difficult because the heat of the building (or pipeline ) can spread to the soil, thawing it. As ice content turns to water, the ground's ability to provide structural support is weakened, until the building is destabilized. For instance, during the construction of the Trans-Siberian Railway , a steam engine factory complex built in 1901 began to crumble within a month of operations for these reasons. [76] : 47 Additionally, there is no groundwater available in an area underlain with permafrost. Any substantial settlement or installation needs to make some alternative arrangement to obtain water. [75] [76] : 25 A common solution is placing foundations on wood piles , a technique pioneered by Soviet engineer Mikhail Kim in Norilsk. [77] However, warming-induced change of friction on the piles can still cause movement through creep , even as the soil remains frozen. [78] The Melnikov Permafrost Institute in Yakutsk found that pile foundations should extend down to 15 metres (49 ft) to avoid the risk of buildings sinking. At this depth the temperature does not change with the seasons, remaining at about −5 °C (23 °F). [79] Two other approaches are building on an extensive gravel pad (usually 1–2 m (3 ft 3 in – 6 ft 7 in) thick); or using anhydrous ammonia heat pipes . [80] The Trans-Alaska Pipeline System uses heat pipes built into vertical supports to prevent the pipeline from sinking and the Qingzang railway in Tibet employs a variety of methods to keep the ground cool, both in areas with frost-susceptible soil . Permafrost may necessitate special enclosures for buried utilities, called " utilidors ". [81] A building on elevated piles in permafrost zone. Heat pipes in vertical supports maintain a frozen bulb around portions of the Trans-Alaska Pipeline that are at risk of thawing. [82] Pile foundations in Yakutsk , a city underlain with continuous permafrost. District heating pipes run above ground in Yakutsk. Impacts of climate change[ edit ] See also: Effects of climate change Recently thawed Arctic permafrost and coastal erosion on the Beaufort Sea, Arctic Ocean, near Point Lonely, Alaska in 2013. Globally, permafrost warmed by about 0.3 °C (0.54 °F) between 2007 and 2016, with stronger warming observed in the continuous permafrost zone relative to the discontinuous zone. Observed warming was up to 3 °C (5.4 °F) in parts of Northern Alaska (early 1980s to mid-2000s) and up to 2 °C (3.6 °F) in parts of the Russian European North (1970–2020). This warming inevitably causes permafrost to thaw: active layer thickness has increased in the European and Russian Arctic across the 21st century and at high elevation areas in Europe and Asia since the 1990s. [83] : 1237 Between 2000 and 2018, the average active layer thickness had increased from ~127 centimetres (4.17 ft) to ~145 centimetres (4.76 ft), at an average annual rate of ~0.65 centimetres (0.26 in). [24] In Yukon , the zone of continuous permafrost might have moved 100 kilometres (62 mi) poleward since 1899, but accurate records only go back 30 years. The extent of subsea permafrost is decreasing as well; as of 2019, ~97% of permafrost under Arctic ice shelves is becoming warmer and thinner. [84] [10] : 1281 Based on high agreement across model projections, fundamental process understanding, and paleoclimate evidence, it is virtually certain that permafrost extent and volume will continue to shrink as the global climate warms, with the extent of the losses determined by the magnitude of warming. [83] : 1283 Permafrost thaw is associated with a wide range of issues, and International Permafrost Association (IPA) exists to help address them. It convenes International Permafrost Conferences and maintains Global Terrestrial Network for Permafrost , which undertakes special projects such as preparing databases, maps, bibliographies, and glossaries, and coordinates international field programmes and networks. [85] Climate change feedback[ edit ] Main article: Permafrost carbon cycle Permafrost peatlands (a smaller, carbon-rich subset of permafrost areas) under varying extent of global warming, and the resultant emissions as a fraction of anthropogenic emissions needed to cause that extent of warming. [86] As recent warming deepens the active layer subject to permafrost thaw, this exposes formerly stored carbon to biogenic processes which facilitate its entrance into the atmosphere as carbon dioxide and methane . [11] Because carbon emissions from permafrost thaw contribute to the same warming which facilitates the thaw, it is a well-known example of a positive climate change feedback , [87] and because widespread permafrost thaw is effectively irreversible, it is also considered one of tipping points in the climate system . [88] In the northern circumpolar region, permafrost contains organic matter equivalent to 1400–1650 billion tons of pure carbon, which  was built up over thousands of years. This amount equals almost half of all organic material in all soils , [89] [11] and it is about twice the carbon content of the atmosphere , or around four times larger than the human emissions of carbon between the start of the Industrial Revolution and 2011. [90] Further, most of this carbon (~1,035 billion tons) is stored in what is defined as the near-surface permafrost, no deeper than 3 metres (9.8 ft) below the surface. [89] [11] However, only a fraction of this stored carbon is expected to enter the atmosphere. [91] In general, the volume of permafrost in the upper 3 m of ground is expected to decrease by about 25% per 1 °C (1.8 °F) of global warming, [83] : 1283 yet even under the RCP8.5 scenario associated with over 4 °C (7.2 °F) of global warming by the end of the 21st century, [92] about 5% to 15% of permafrost carbon is expected to be lost "over decades and centuries". [11] The exact amount of carbon that will be released due to warming in a given permafrost area depends on depth of thaw, carbon content within the thawed soil, physical changes to the environment, and microbial and vegetation activity in the soil. [93] Notably, estimates of carbon release alone do not fully represent the impact of permafrost thaw on climate change. This is because carbon can be released through either aerobic or anaerobic respiration , which results in carbon dioxide (CO2) or methane (CH4) emissions, respectively. While methane lasts less than 12 years in the atmosphere, its global warming potential is around 80 times larger than that of CO2 over a 20-year period and about 28 times larger over a 100-year period. [94] [95] While only a small fraction of permafrost carbon will enter the atmosphere as methane, those emissions will cause 40-70% of the total warming caused by permafrost thaw during the 21st century. Much of the uncertainty about the eventual extent of permafrost methane emissions is caused by the difficulty of accounting for the recently discovered abrupt thaw processes, which often increase the fraction of methane emitted over carbon dioxide in comparison to the usual gradual thaw processes. [96] [11] Permafrost thaw ponds on peatland in Hudson Bay , Canada in 2008. [97] Another factor which complicates projections of permafrost carbon emissions is the ongoing "greening" of the Arctic. As climate change warms the air and the soil, the region becomes more hospitable to plants, including larger shrubs and trees which could not survive there before. Thus, the Arctic is losing more and more of its tundra biomes, yet it gains more plants, which proceed to absorb more carbon. Some of the emissions caused by permafrost thaw will be offset by this increased plant growth, but the exact proportion is uncertain. It is considered very unlikely that this greening could offset all of the emissions from permafrost thaw during the 21st century, and even less likely that it could continue to keep pace with those emissions after the 21st century. [11] Further, climate change also increases the risk of wildfires in the Arctic, which can substantially accelerate emissions of permafrost carbon. [87] [98] Impact on global temperatures[ edit ] Nine probable scenarios of greenhouse gas emissions from permafrost thaw during the 21st century, which show a limited, moderate and intense CO2 and CH4 emission response to low, medium and high-emission Representative Concentration Pathways . The vertical bar uses emissions of selected large countries as a comparison: the right-hand side of the scale shows their cumulative emissions since the start of the Industrial Revolution , while the left-hand side shows each country's cumulative emissions for the rest of the 21st century if they remained unchanged from their 2019 levels. [11] Altogether, it is expected that cumulative greenhouse gas emissions from permafrost thaw will be smaller than the cumulative anthropogenic emissions, yet still substantial on a global scale, with some experts comparing them to emissions caused by deforestation . [11] The IPCC Sixth Assessment Report estimates that carbon dioxide and methane released from permafrost could amount to the equivalent of 14–175 billion tonnes of carbon dioxide per 1 °C (1.8 °F) of warming. [83] : 1237 For comparison, by 2019, annual anthropogenic emissions of carbon dioxide alone stood around 40 billion tonnes. [83] : 1237 A major review published in the year 2022 concluded that if the goal of preventing 2 °C (3.6 °F) of warming was realized, then the average annual permafrost emissions throughout the 21st century would be equivalent to the year 2019 annual emissions of Russia. Under RCP4.5, a scenario considered close to the current trajectory and where the warming stays slightly below 3 °C (5.4 °F), annual permafrost emissions would be comparable to year 2019 emissions of Western Europe or the United States, while under the scenario of high global warming and worst-case permafrost feedback response, they would approach year 2019 emissions of China. [11] Fewer studies have attempted to describe the impact directly in terms of warming. A 2018 paper estimated that if global warming was limited to 2 °C (3.6 °F), gradual permafrost thaw would add around 0.09 °C (0.16 °F) to global temperatures by 2100, [99] while a 2022 review concluded that every 1 °C (1.8 °F) of global warming would cause 0.04 °C (0.072 °F) and 0.11 °C (0.20 °F) from abrupt thaw by the year 2100 and 2300. Around 4 °C (7.2 °F) of global warming, abrupt (around 50 years) and widespread collapse of permafrost areas could occur, resulting in an additional warming of 0.2–0.4 °C (0.36–0.72 °F). [88] [100] Thaw-induced ground instability[ edit ] Severe coastal erosion on the Arctic Ocean coast of Alaska . As the water drains or evaporates, soil structure weakens and sometimes becomes viscous until it regains strength with decreasing moisture content. One visible sign of permafrost degradation is the random displacement of trees from their vertical orientation in permafrost areas. [101] Global warming has been increasing permafrost slope disturbances and sediment supplies to fluvial systems, resulting in exceptional increases in river sediment. [102] On the other hands, disturbance of formerly hard soil increases drainage of water reservoirs in northern wetlands . This can dry them out and compromise the survival of plants and animals used to the wetland ecosystem. [103] In high mountains, much of the structural stability can be attributed to glaciers and permafrost. [104] As climate warms, permafrost thaws, decreasing slope stability and increasing stress through buildup of pore-water pressure, which may ultimately lead to slope failure and rockfalls . [105] [106] Over the past century, an increasing number of alpine rock slope failure events in mountain ranges around the world have been recorded, and some have been attributed to permafrost thaw induced by climate change. The 1987 Val Pola landslide that killed 22 people in the Italian Alps is considered one such example. [107] In 2002, massive rock and ice falls (up to 11.8 million m3), earthquakes (up to 3.9 Richter ), floods (up to 7.8 million m3 water), and rapid rock-ice flow to long distances (up to 7.5 km at 60 m/s) were attributed to slope instability in high mountain permafrost. [108] Thawing permafrost in Herschel Island , Canada, 2013. Permafrost thaw can also result in the formation of frozen debris lobes (FDLs), which are defined as "slow-moving landslides composed of soil, rocks, trees, and ice". [109] This is a notable issue in the Alaska 's southern Brooks Range , where some FDLs measured over 100 m (110 yd) in width, 20 m (22 yd) in height, and 1,000 m (1,100 yd) in length by 2012. [110] [111] As of December 2021, there were 43 frozen debris lobes identified in the southern Brooks Range, where they could potentially threaten both the Trans Alaska Pipeline System (TAPS) corridor and the Dalton Highway , which is the main transport link between the Interior Alaska and the Alaska North Slope . [112] Infrastructure[ edit ] Map of likely risk to infrastructure from permafrost thaw expected to occur by 2050. [113] As of 2021, there are 1162 settlements located directly atop the Arctic permafrost, which host an estimated 5 million people. By 2050, permafrost layer below 42% of these settlements is expected to thaw, affecting all their inhabitants (currently 3.3 million people). [114] Consequently, a wide range of infrastructure in permafrost areas is threatened by the thaw. [12] [115] : 236 By 2050, it's estimated that nearly 70% of global infrastructure located in the permafrost areas would be at high risk of permafrost thaw, including 30–50% of "critical" infrastructure. The associated costs could reach tens of billions of dollars by the second half of the century. [13] Reducing greenhouse gas emissions in line with the Paris Agreement is projected to stabilize the risk after mid-century; otherwise, it'll continue to worsen. [113] In Alaska alone, damages to infrastructure by the end of the century would amount to $4.6 billion (at 2015 dollar value) if RCP8.5 , the high-emission climate change scenario , were realized. Over half stems from the damage to buildings ($2.8 billion), but there's also damage to roads ($700 million), railroads ($620 million), airports ($360 million) and pipelines ($170 million). [116] Similar estimates were done for RCP4.5, a less intense scenario which leads to around 2.5 °C (4.5 °F) by 2100, a level of warming similar to the current projections. [117] In that case, total damages from permafrost thaw are reduced to $3 billion, while damages to roads and railroads are lessened by approximately two-thirds (from $700 and $620 million to $190 and $220 million) and damages to pipelines are reduced more than ten-fold, from $170 million to $16 million. Unlike the other costs stemming from climate change in Alaska, such as damages from increased precipitation and flooding, climate change adaptation is not a viable way to reduce damages from permafrost thaw, as it would cost more than the damage incurred under either scenario. [116] In Canada, Northwest Territories have a population of only 45,000 people in 33 communities, yet permafrost thaw is expected to cost them $1.3 billion over 75 years, or around $51 million a year. In 2006, the cost of adapting Inuvialuit homes to permafrost thaw was estimated at $208/m2 if they were built at pile foundations, and $1,000/m2 if they didn't. At the time, the average area of a residential building in the territory was around 100 m2. Thaw-induced damage is also unlikely to be covered by home insurance , and to address this reality, territorial government currently funds Contributing Assistance for Repairs and Enhancements (CARE) and Securing Assistance for Emergencies (SAFE) programs, which provide long- and short-term forgivable loans to help homeowners adapt. It is possible that in the future, mandatory relocation would instead take place as the cheaper option. However, it would effectively tear the local Inuit away from their ancestral homelands. Right now, their average personal income is only half that of the median NWT resident, meaning that adaptation costs are already disproportionate for them. [118] By 2022, up to 80% of buildings in some Northern Russia cities had already experienced damage. [13] By 2050, the damage to residential infrastructure may reach $15 billion, while total public infrastructure damages could amount to 132 billion. [119] This includes oil and gas extraction facilities, of which 45% are believed to be at risk. [113] Detailed map of Qinghai–Tibet Plateau infrastructure at risk from permafrost thaw under the SSP2-4.5 scenario. [120] Outside of the Arctic, Qinghai–Tibet Plateau (sometimes known as "the Third Pole"), also has an extensive permafrost area. It is warming at twice the global average rate, and 40% of it is already considered "warm" permafrost, making it particularly unstable. Qinghai–Tibet Plateau has a population of over 10 million people – double the population of permafrost regions in the Arctic – and over 1 million m2 of buildings are located in its permafrost area, as well as 2,631 km of power lines , and 580 km of railways. [120] There are also 9,389 km of roads, and around 30% are already sustaining damage from permafrost thaw. [13] Estimates suggest that under the scenario most similar to today, SSP2-4.5 , around 60% of the current infrastructure would be at high risk by 2090 and simply maintaining it would cost $6.31 billion, with adaptation reducing these costs by 20.9% at most. Holding the global warming to 2 °C (3.6 °F) would reduce these costs to $5.65 billion, and fulfilling the optimistic Paris Agreement target of 1.5 °C (2.7 °F) would save a further $1.32 billion. In particular, fewer than 20% of railways would be at high risk by 2100 under 1.5 °C (2.7 °F), yet this increases to 60% at 2 °C (3.6 °F), while under SSP5-8.5, this level of risk is met by mid-century. [120] Release of toxic pollutants[ edit ] Graphical representation of leaks from various toxic hazards caused by the thaw of formerly stable permafrost. [14] For much of the 20th century, it was believed that permafrost would "indefinitely" preserve anything buried there, and this made deep permafrost areas popular locations for hazardous waste disposal. In places like Canada's Prudhoe Bay oil field, procedures were developed documenting the "appropriate" way to inject waste beneath the permafrost. This means that as of 2023, there are ~4500 industrial facilities in the Arctic permafrost areas which either actively process or store hazardous chemicals. Additionally, there are between 13,000 and 20,000 sites which have been heavily contaminated, 70% of them in Russia, and their pollution is currently trapped in the permafrost. About a fifth of both the industrial and the polluted sites (1000 and 2200–4800) are expected to start thawing in the future even if the warming does not increase from its 2020 levels. Only about 3% more sites would start thawing between now and 2050 under the climate change scenario consistent with the Paris Agreement goals, RCP2.6 , but by 2100, about 1100 more industrial facilities and 3500 to 5200 contaminated sites are expected to start thawing even then. Under the very high emission scenario RCP8.5, 46% of industrial and contaminated sites would start thawing by 2050, and virtually all of them would be affected by the thaw by 2100. [14] Organochlorines and other persistent organic pollutants are of a particular concern, due to their potential to repeatedly reach local communities after their re-release through biomagnification in fish. At worst, future generations born in the Arctic would enter life with weakened immune systems due to pollutants accumulating across generations. [16] Distribution of toxic substances currently located at various permafrost sites in Alaska, by sector. The number of fish skeletons represents the toxicity of each substance. [14] A notable example of pollution risks associated with permafrost was the 2020 Norilsk oil spill , caused by the collapse of diesel fuel storage tank at Norilsk-Taimyr Energy's thermal power plant No. 3. It spilled 6,000 tonnes of fuel into the land and 15,000 into the water, polluting Ambarnaya , Daldykan and many smaller rivers on Taimyr Peninsula , even reaching lake Pyasino , which is a crucial water source in the area. State of emergency at the federal level was declared. [121] [122] The event has been described as the second-largest oil spill in modern Russian history. [123] [124] Another issue associated with permafrost thaw is the release of natural mercury deposits. An estimated 800,000 tons of mercury are frozen in the permafrost soil. According to observations, around 70% of it is simply taken up by vegetation after the thaw. [16] However, if the warming continues under RCP8.5, then permafrost emissions of mercury into the atmosphere would match the current global emissions from all human activities by 2200. Mercury-rich soils also pose a much greater threat to humans and the environment if they thaw near rivers. Under RCP8.5, enough mercury will enter the Yukon River basin by 2050 to make its fish unsafe to eat under the EPA guidelines. By 2100, mercury concentrations in the river will double. Contrastingly, even if mitigation is limited to RCP4.5 scenario, mercury levels will increase by about 14% by 2100, and will not breach the EPA guidelines even by 2300. [15] Revival of ancient organisms[ edit ] Main article: Pathogenic microorganisms in frozen environments Some of the ancient amoeba-eating viruses revived by the research team of Jean-Michel Claverie. Clockwise from the top: Pandoravirus yedoma; Pandoravirus mammoth and Megavirus mammoth; Cedratvirus lena; Pithovirus mammoth; Megavirus mammoth; Pacmanvirus lupus. [17] Bacteria are known for being able to remain dormant to survive adverse conditions, and viruses are not metabolically active outside of host cells in the first place. This has motivated concerns that permafrost thaw could free previously unknown microorganisms, which may be capable of infecting either humans or important livestock and crops , potentially resulting in damaging epidemics or pandemics . [17] [18] Further, some scientists argue that horizontal gene transfer could occur between the older, formerly frozen bacteria, and modern ones, and one outcome could be the introduction of novel antibiotic resistance genes into the genome of current pathogens, exacerbating what is already expected to become a difficult issue in the future. [125] [16] At the same time, notable pathogens like influenza and smallpox appear unable to survive being thawed, [20] and other scientists argue that the risk of ancient microorganisms being both able to survive the thaw and to threaten humans is not scientifically plausible. [19] Likewise, some research suggests that antimicrobial resistance capabilities of ancient bacteria would be comparable to, or even inferior to modern ones. [126] [21] Plants[ edit ] In 2012, Russian researchers proved that permafrost can serve as a natural repository for ancient life forms by reviving a sample of Silene stenophylla from 30,000-year-old tissue found in an Ice Age squirrel burrow in the Siberian permafrost. This is the oldest plant tissue ever revived. The resultant plant was fertile, producing white flowers and viable seeds. The study demonstrated that living tissue can survive ice preservation for tens of thousands of years. [127] History of scientific research[ edit ] The annual number of scientific research papers published on the subject of permafrost carbon has grown from next to nothing around 1990 to around 400 by 2020. [11] Between the middle of the 19th century and the middle of the 20th century, most of the literature on basic permafrost science and the engineering aspects of permafrost was written in Russian. One of the earliest written reports describing the existence of permafrost dates to 1684 , when well excavation efforts in Yakutsk were stumped by its presence. [76] : 25 A significant role in the initial permafrost research was played by Alexander von Middendorff (1815–1894) and Karl Ernst von Baer , a Baltic German scientist at the University of Königsberg , and a member of the St Petersburg Academy of Sciences . Baer began publishing works on permafrost starting from 1838, and is often considered the "founder of scientific permafrost research". Through his compilation and analysis of all available data on ground ice and permafrost, Baer laid the foundation for the modern permafrost terminology. [128] Baer is also known to have composed the world's first permafrost textbook in 1843, "materials for the study of the perennial ground-ice", written in his native language. However, it was not printed at the time, and a Russian translation wasn't ready until 1942. The original German textbook was believed to be lost until the typescript from 1843 was discovered in the library archives of the University of Giessen . The 234-page text was made available online, with additional maps, preface and comments. [128] Notably, Baer's southern limit of permafrost in Eurasia drawn in 1843 corresponds well with the actual southern limit verified by modern research. [27] [128] Beginning in 1942, Siemon William Muller delved into the relevant Russian literature held by the Library of Congress and the U.S. Geological Survey Library so that he was able to furnish the government an engineering field guide and a technical report about permafrost by 1943. [129] That report coined the English term as a contraction of permanently frozen ground, [130] in what was considered a direct translation of the Russian term vechnaia merzlota (Russian: вечная мерзлота). In 1953, this translation was criticized by another USGS researcher Inna Poiré, as she believed the term had created unrealistic expectations about its stability: [76] : 3 more recently, some researchers have argued that "perpetually refreezing" would be a more suitable translation. [131] The report itself was classified (as U.S. Army. Office of the Chief of Engineers, Strategic Engineering Study, no. 62, 1943), [130] [132] until a revised version was released in 1947, which is regarded as the first North American treatise on the subject. [129] [133] Between 11 and 15 November 1963, the First International Conference on Permafrost took place on the grounds of Purdue University in the American town of West Lafayette, Indiana . It involved 285 participants (including "engineers, manufacturers and builders" who attended alongside the researchers) from a range of countries ( Argentina , Austria , Canada, Germany, Great Britain, Japan, Norway , Poland , Sweden, Switzerland, the US and the USSR ). This marked the beginning of modern scientific collaboration on the subject. Conferences continue to take place every five years. During the Fourth conference in 1983, a special meeting between the "Big Four" participant countries (US, USSR, China and Canada) had officially created the International Permafrost Association . [134] In the recent decades, permafrost research has attracted more attention than ever due to the role it plays in climate change . Consequently, there has been a massive acceleration in published scientific literature . Around 1990, almost no papers were released containing the words "permafrost" and "carbon": by 2020, around 400 such papers were published every year. [11] Southern limit of permafrost in Eurasia according to Karl Ernst von Baer (1843), and other authors.
Toggle the table of contents Glaciology From Wikipedia, the free encyclopedia Scientific study of ice and natural phenomena involving ice This article includes a list of general references , but it lacks sufficient corresponding inline citations . Please help to improve this article by introducing more precise citations. (February 2011) ( Learn how and when to remove this template message ) Lateral moraine on a glacier joining the Gorner Glacier , Zermatt , Swiss Alps . The moraine is the high bank of debris in the top left hand quarter of the image. Glaciologist Erin Pettit in Antarctica, 2016 Glaciology (from Latin glacies 'frost, ice', and Ancient Greek λόγος ( logos ) 'subject matter'; lit. 'study of ice') is the scientific study of glaciers , or, more generally, ice and natural phenomena that involve ice. Overview[ edit ] A glacier is an extended mass of ice formed from snow falling and accumulating over a long period of time; glaciers move very slowly, either descending from high mountains, as in valley glaciers, or moving outward from centers of accumulation, as in continental glaciers . Areas of study within glaciology include glacial history and the reconstruction of past glaciation. A glaciologist is a person who studies glaciers. A glacial geologist studies glacial deposits and glacial erosive features on the landscape. Glaciology and glacial geology are key areas of polar research. A Bylot Island glacier, Sirmilik National Park , Nunavut . This mountain glacier is one of many coming down from the interior ice cap on top of the Byam Martin Mountains . Glaciers can be identified by their geometry and the relationship to the surrounding topography. There are two general categories of glaciation which glaciologists distinguish: alpine glaciation, accumulations or "rivers of ice" confined to valleys; and continental glaciation, unrestricted accumulations which once covered much of the northern continents. Alpine – ice flows down the valleys of mountainous areas and forms a tongue of ice moving towards the plains below. Alpine glaciers tend to make topography more rugged by adding and improving the scale of existing features. Various features include large ravines called cirques and arêtes , which are ridges where the rims of two cirques meet. Continental – an ice sheet found today, only in high latitudes ( Greenland / Antarctica ), thousands of square kilometers in area and thousands of meters thick. These tend to smooth out the landscapes. Zones of glaciers[ edit ] Accumulation zone – where the formation of ice is faster than its removal. Ablation (or wastage) zone – when the sum of melting, calving , and evaporation (sublimation) is greater than the amount of snow added each year. Glacier equilibrium line and ELA[ edit ] The glacier equilibrium line is the line separating the glacial accumulation area above from the ablation area below.  The equilibrium line altitude (ELA) and its change over the years is a key indicator of the health of a glacier. A long term monitoring of the ELA may be used as indication to climate change . Movement[ edit ] Khurdopin glacier and Shimshal River, Gilgit-Baltistan , northern Pakistan 2017. Several glaciers flow into the Shimshal Valley, and are prone to blocking the river.  Khurdopin glacier surged in 2016–17, creating a sizable lake. [2] Glaciers of Shimsal Valley from space, May 13, 2017. Khurdopin glacier has dammed the Shimshal River, forming a glacial lake . The river has started to carve a path through the toe of the glacier. By early August 2017, the lake had completely drained. When a glacier is experiencing an accumulation input by precipitation (snow or refreezing rain) that exceeds the output by ablation, the glacier shows a positive glacier mass balance and will advance. Conversely, if the loss of volume (from evaporation, sublimation, melting, and calving) exceeds the accumulation, the glacier shows a negative glacier mass balance and the glacier will melt back. During times in which the volume input to the glacier by precipitation is equivalent to the ice volume lost from calving, evaporation, and melting, the glacier has a steady-state condition. Some glaciers show periods where the glacier is advancing at an extreme rate, that is typically 100 times faster than what is considered normal, it is referred to as a surging glacier. Surge periods may occur at an interval of 10 to 15 years, e.g. on Svalbard . This is caused mainly due to a long lasting accumulation period on subpolar glaciers frozen to the ground in the accumulation area. When the stress due to the additional volume in the accumulation area increases, the pressure melting point of the ice at its base may be reached, the basal glacier ice will melt, and the glacier will surge on a film of meltwater. Rate of movement[ edit ] The movement of glaciers is usually slow. Its velocity varies from a few centimeters to a few meters per day. The rate of movement depends upon the factors listed below: Temperature of the ice. A polar glacier shows cold ice with temperatures well below the freezing point from its surface to its base. It is frozen to its bed. A temperate glacier is at a melting point temperature throughout the year, from its surface to its base. This allows the glacier to slide on a thin layer of meltwater. Most glaciers in alpine regions are temperate glaciers. Gradient of the slope. Thickness of the glacier [3] Subglacial water dynamics
Toggle the table of contents Ice sheet From Wikipedia, the free encyclopedia Large mass of glacial ice "Continental glacier" redirects here. For the glacier located in Wyoming, see Continental Glacier . One of Earth's two ice sheets: The Antarctic ice sheet covers about 98% of the Antarctic continent and is the largest single mass of ice on Earth, with an average thickness of over 2 kilometers. [1] In glaciology , an ice sheet, also known as a continental glacier, [2] is a mass of glacial ice that covers surrounding terrain and is greater than 50,000 km2 (19,000 sq mi). [3] The only current ice sheets are the Antarctic ice sheet and the Greenland ice sheet . Ice sheets are bigger than ice shelves or alpine glaciers . Masses of ice covering less than 50,000 km2 are termed an ice cap . An ice cap will typically feed a series of glaciers around its periphery. Although the surface is cold, the base of an ice sheet is generally warmer due to geothermal heat. In places, melting occurs and the melt-water lubricates the ice sheet so that it flows more rapidly. This process produces fast-flowing channels in the ice sheet — these are ice streams . "Ice flow" redirects here. For floating ice, see Ice floe . Glacial flow rate in the Antarctic ice sheet. The motion of ice in Antarctica An ice sheet is a body of ice which has formed over thousands of years of snow accumulation and covers land area of continental size - i.e. >50,000 km2. Even stable ice sheets are continually in motion as the ice gradually flows outward from the central plateau, which is the tallest point of the ice sheet, and towards the margins. The ice sheet slope is low around the plateau but increases steeply at the margins. [4] This difference in slope occurs due to an imbalance between high ice accumulation in the central plateau and lower accumulation, as well as higher ablation , at the margins. This imbalance increases the shear stress on a glacier until it begins to flow. The flow velocity and deformation will increase as the equilibrium line between these two processes is approached. [5] [6] This motion is driven by gravity but is controlled by temperature and the strength of individual glacier bases. A number of processes alter these two factors, resulting in cyclic surges of activity interspersed with longer periods of inactivity, on time scales ranging from hourly (i.e. tidal flows) to the centennial (Milankovich cycles). [6] The stress–strain relationship of plastic flow (teal section): a small increase in stress creates an exponentially greater increase in strain, which equates to deformation speed. When the amount of strain (deformation) is proportional to the stress being applied, ice will act as an elastic solid. Ice will not flow until it has reached a thickness of 30 meters (98 ft), but after 50 meters (164 ft), small amounts of stress can result in a large amount of strain, causing the deformation to become a plastic flow rather than elastic. At this point the glacier will begin to deform under its own weight and flow across the landscape. According to the Glen–Nye flow law , the relationship between stress and strain, and thus the rate of internal flow, can be modeled as follows: [5] [6] Σ {\displaystyle \Sigma =k\tau ^{n},\,} where: = a constant between 2–4 (typically 3 for most glaciers) that increases with lower temperature k {\displaystyle k\,} = a temperature-dependent constant The lowest velocities are near the base of the glacier and along valley sides where friction acts against flow, causing the most deformation. Velocity increases inward toward the center line and upward, as the amount of deformation decreases. The highest flow velocities are found at the surface, representing the sum of the velocities of all the layers below. [5] [6] Differential erosion enhances relief, as clear in this incredibly steep-sided Norwegian fjord . Glaciers may also move by basal sliding , where the base of the glacier is lubricated by meltwater , allowing the glacier to slide over the terrain on which it sits. Meltwater may be produced by pressure-induced melting, friction or geothermal heat . The more variable the amount of melting at surface of the glacier, the faster the ice will flow. [7] Because ice can flow faster where it is thicker, the rate of glacier-induced erosion is directly proportional to the thickness of overlying ice.  Consequently, pre-glacial low hollows will be deepened and pre-existing topography will be amplified by glacial action, while nunataks , which protrude above ice sheets, barely erode at all – erosion has been estimated as 5 m per 1.2 million years. [8] This explains, for example, the deep profile of fjords , which can reach a kilometer in depth as ice is topographically steered into them.  The extension of fjords inland increases the rate of ice sheet thinning since they are the principal conduits for draining ice sheets. It also makes the ice sheets more sensitive to changes in climate and the ocean. [8] On an hour-to-hour basis, surges of ice motion can be modulated by tidal activity. The influence of a 1 m tidal oscillation can be felt as much as 100 km from the sea. [9] During larger spring tides , an ice stream will remain almost stationary for hours at a time, before a surge of around a foot in under an hour, just after the peak high tide; a stationary period then takes hold until another surge towards the middle or end of the falling tide. [10] [11] At neap tides, this interaction is less pronounced, and surges instead occur approximately every 12 hours. [10] Subglacial processes[ edit ] A cross-section through a glacier. The base of the glacier is more transparent as a result of melting. Most of the important processes controlling glacial motion occur in the ice-bed contact—even though it is only a few meters thick. [9] Glaciers will move by sliding when the basal shear stress drops below the shear resulting from the glacier's weight.[ clarification needed ] τD = ρgh sin α where τD is the driving stress, and α the ice surface slope in radians. [9] τB is the basal shear stress, a function of bed temperature and softness. [9] τF, the shear stress, is the lower of τB and τD.  It controls the rate of plastic flow, as per the figure (inset, right). For a given glacier, the two variables are τD, which varies with h, the depth of the glacier, and τB, the basal shear stress.[ clarification needed ] Basal shear stress[ edit ] The basal shear stress is a function of three factors: the bed's temperature, roughness and softness. [9] Whether a bed is hard or soft depends on the porosity and pore pressure; higher porosity decreases the sediment strength (thus increases the shear stress τB). [9] If the sediment strength falls far below τD, movement of the glacier will be accommodated by motion in the sediments, as opposed to sliding. Porosity may vary through a range of methods. Movement of the overlying glacier may cause the bed to undergo dilatancy ; the resulting shape change reorganises blocks.  This reorganises closely packed blocks (a little like neatly folded, tightly packed clothes in a suitcase) into a messy jumble (just as clothes never fit back in when thrown in  in a disordered fashion).  This increases the porosity.  Unless water is added, this will necessarily reduce the pore pressure (as the pore fluids have more space to occupy). [9] Pressure may cause compaction and consolidation of underlying sediments. [9] Since water is relatively incompressible, this is easier when the pore space is filled with vapour; any water must be removed to permit compression. In soils, this is an irreversible process. [9] Sediment degradation by abrasion and fracture decreases the size of particles, which tends to decrease pore space. However, the motion of the particles may disorder the sediment, with the opposite effect. These processes also generate heat. [9] A soft bed, with high porosity and low pore fluid pressure, allows the glacier to move by sediment sliding: the base of the glacier may even remain frozen to the bed, where the underlying sediment slips underneath it like a tube of toothpaste.  A hard bed cannot deform in this way; therefore the only way for hard-based glaciers to move is by basal sliding, where meltwater forms between the ice and the bed itself. [12] Bed softness may vary in space or time, and changes dramatically from glacier to glacier.  An important factor is the underlying  geology; glacial speeds tend to differ more when they change bedrock than when the gradient changes. [12] Further, bed roughness can also act to slow glacial motion. The roughness of the bed is a measure of how many boulders and obstacles protrude into the overlying ice.  Ice flows around these obstacles by melting under the high pressure on their stoss side ; the resultant meltwater is then forced into the cavity arising in their lee side , where it re-freezes. [9] As well as affecting the sediment stress, fluid pressure (pw) can affect the friction between the glacier and the bed. High fluid pressure provides a buoyancy force upwards on the glacier, reducing the friction at its base.  The fluid pressure is compared to the ice overburden pressure, pi, given by ρgh.  Under fast-flowing ice streams, these two pressures will be approximately equal, with an effective pressure (pi – pw) of 30 kPa; i.e. all of the weight of the ice is supported by the underlying water, and the glacier is afloat. [9] Basal melting[ edit ] Location and diagram of Lake Vostok , a prominent subglacial lake beneath the East Antarctic Ice Sheet. The flow of water under the glacial surface can have a large effect on the motion of the glacier itself.  Subglacial lakes contain significant amounts of water, which can move fast: cubic kilometres can be transported between lakes over the course of a couple of years. [13] This motion is thought to occur in two main modes: pipe flow involves liquid water moving through pipe-like conduits, like a sub-glacial river; sheet flow involves motion of water in a thin layer. A switch between the two flow conditions may be associated with surging behaviour. Indeed, the loss of sub-glacial water supply has been linked with the shut-down of ice movement in the Kamb ice stream. [13] The subglacial motion of water is expressed in the surface topography of ice sheets, which slump down into vacated subglacial lakes. [13] The presence of basal meltwater depends on both bed temperature and other factors. For instance, the melting point of water decreases under pressure, meaning that water melts at a lower temperature under thicker glaciers. [9] This acts as a "double whammy", because thicker glaciers have a lower heat conductance, meaning that the basal temperature is also likely to be higher. [12] Bed temperature tends to vary in a cyclic fashion. A cool bed has a high strength, reducing the speed of the glacier.  This increases the rate of accumulation, since newly fallen snow is not transported away.  Consequently, the glacier thickens, with three consequences: firstly, the bed is better insulated, allowing greater retention of geothermal heat.  Secondly, the increased pressure can facilitate melting.  Most importantly, τD is increased.  These factors will combine to accelerate the glacier. As friction increases with the square of velocity, faster motion will greatly increase frictional heating, with ensuing melting – which causes a positive feedback, increasing ice speed to a faster flow rate still: west Antarctic glaciers are known to reach velocities of up to a kilometre per year. [9] Eventually, the ice will be surging fast enough that it begins to thin, as accumulation cannot keep up with the transport.  This thinning will increase the conductive heat loss, slowing the glacier and causing freezing.  This freezing will slow the glacier further, often until it is stationary, whence the cycle can begin again. [12] Increasing global air temperatures due to climate change take around 10,000 years to directly propagate through the ice before they influence bed temperatures, but may have an effect through increased surface melting, producing more supraglacial lakes . These lakes may feed warm water to glacial bases and facilitate glacial motion. [14] Lakes of a diameter greater than ~300 m are capable of creating a fluid-filled crevasse to the glacier/bed interface. When these crevasses form, the entirety of the lake's (relatively warm) contents can reach the base of the glacier in as little as 2–18 hours – lubricating the bed and causing the glacier to surge . [15] Water that reaches the bed of a glacier may freeze there, increasing the thickness of the glacier by pushing it up from below. [16] Boundary conditions[ edit ] The collapse of the Larsen B ice shelf had profound effects on the velocities of its feeder glaciers. Accelerated ice flows after the break-up of an ice shelf As the margins end at the marine boundary, excess ice is discharged through ice streams or outlet glaciers . Then, it either falls directly into the sea or is accumulated atop the floating ice shelves . [4] : 2234 Those ice shelves then calve icebergs at their periphery if they experience excess of ice. Ice shelves would also experience accelerated calving due to basal melting. In Antarctica, this is driven by heat fed to the shelf by the circumpolar deep water current, which is 3 °C above the ice's melting point. [17] The presence of ice shelves has a stabilizing influence on the glacier behind them, while an absence of an ice shelf becomes destabilizing. For instance, when Larsen B ice shelf in the Antarctic Peninsula had collapsed over three weeks in February 2002, the four glaciers behind it - Crane Glacier , Green Glacier , Hektoria Glacier and Jorum Glacier - all started to flow at a much faster rate, while the two glaciers (Flask and Leppard) stabilized by the remnants of the ice shelf did not accelerate. [18] The collapse of the Larsen B shelf was preceded by thinning of just 1 metre per year, while some other Antarctic ice shelves have displayed thinning of tens of metres per year. [14] Further, increased ocean temperatures of 1 °C may lead to up to 10 metres per year of basal melting. [14] Ice shelves are always stable under mean annual temperatures of −9 °C, but never stable above −5 °C; this places regional warming of 1.5 °C, as preceded the collapse of Larsen B, in context. [14] Marine ice sheet instability[ edit ] In the 1970s, Johannes Weertman proposed that because seawater is denser than ice, then any ice sheets grounded below sea level inherently become less stable as they melt due to Archimedes' principle . [19] Effectively, these marine ice sheets must have enough mass to exceed the mass of the seawater displaced by the ice, which requires excess thickness. As the ice sheet melts and becomes thinner, the weight of the overlying ice decreases. At a certain point, sea water could force itself into the gaps which form at the base of the ice sheet, and marine ice sheet instability (MISI) would occur. [19] [20] Even if the ice sheet is grounded below the sea level, MISI cannot occur as long as there is a stable ice shelf in front of it. [21] The boundary between the ice sheet and the ice shelf, known as the grounding line, is particularly stable if it is constrained in an embayment . [21] In that case, the ice sheet may not be thinning at all, as the amount of ice flowing over the grounding line would be likely to match the annual accumulation of ice from snow upstream. [20] Otherwise, ocean warming at the base of an ice shelf tends to thin it through basal melting. As the ice shelf becomes thinner, it exerts less of an buttressing effect on the ice sheet, the so-called back stress increases and the grounding line is pushed backwards. [20] The ice sheet is likely to start losing more ice from the new location of the grounding line and so become lighter and less capable of displacing seawater. This eventually pushes the grounding line back even further, creating a self-reinforcing mechanism . [20] [22] Vulnerable locations[ edit ] Distribution of meltwater hotspots caused by ice losses in Pine Island Bay , the location of both Thwaites (TEIS refers to Thwaites Eastern Ice Shelf) and Pine Island Glaciers. [23] Because the entire West Antarctic Ice Sheet is grounded below the sea level, it would be vulnerable to geologically rapid ice loss in this scenario. [24] [25] In particular, the Thwaites and Pine Island glaciers are most likely to be prone to MISI, and both glaciers have been rapidly thinning and accelerating in recent decades. [26] [27] [28] [29] As the result, sea level rise from the ice sheet could be accelerated by tens of centimeters within the 21st century alone. [30] The majority of the East Antarctic Ice Sheet would not be affected. Totten Glacier is the largest glacier there which is known to be subject to MISI - yet, its potential contribution to sea level rise is comparable to that of the entire West Antarctic Ice Sheet. [31] Totten Glacier has been losing mass nearly monotonically in recent decades, [32] suggesting rapid retreat is possible in the near future, although the dynamic behavior of Totten Ice Shelf is known to vary on seasonal to interannual timescales. [33] [34] [35] The Wilkes Basin is the only major submarine basin in Antarctica that is not thought to be sensitive to warming. [28] Ultimately, even geologically rapid sea level rise would still most likely require several millennia for the entirety of these ice masses (WAIS and the subglacial basins) to be lost. [36] [37] Marine Ice Cliff Instability[ edit ] A collage of footage and animation to explain the changes that are occurring on the West Antarctic Ice Sheet, narrated by glaciologist Eric Rignot A related process known as Marine Ice Cliff Instability (MICI) posits that due to the physical characteristics of ice, subaerial ice cliffs exceeding ~90 meters in height are likely to collapse under their own weight, and this could lead to runaway ice sheet retreat in a fashion similar to MISI. [20] For an ice sheet grounded below sea level with an inland-sloping bed, ice cliff failure removes peripheral ice, which then exposes taller, more unstable ice cliffs, further perpetuating the cycle of ice front failure and retreat. Surface melt can further enhance MICI through ponding and hydrofracture. [21] [38] However, this process is considered more speculative than MISI, as it has never been observed at any scale. Some of the more detailed modelling has ruled it out. [39] Earth's current two ice sheets[ edit ] Antarctic ice sheet[ edit ] This section is an excerpt from Antarctic ice sheet .[ edit ] The Antarctic ice sheet is a continental glacier covering 98% of the Antarctic continent , with an area of 14 million square kilometres (5.4 million square miles) and an average thickness of over 2 kilometres (1.2 mi). It is the largest of Earth's two current ice sheets , containing 26.5 million cubic kilometres (6,400,000 cubic miles) of ice, which is equivalent to 61% of all fresh water on Earth. Its surface is nearly continuous, and the only ice-free areas on the continent are the dry valleys, nunataks of the Antarctic mountain ranges , and sparse coastal bedrock . However, it is often subdivided into East Antarctic ice sheet (EAIS), West Antarctic ice sheet (WAIS), and Antarctic Peninsula (AP), due to the large differences in topography , ice flow , and glacier mass balance between the three regions. West Antarctic ice sheet[ edit ] West Antarctic ice sheet
Toggle the table of contents Sea level rise From Wikipedia, the free encyclopedia Rise in sea levels due to climate change This article is about the current and projected rise in the world's average sea level. For sea level rise in general, see Past sea level . "Rising seas" redirects here. For the song, see Rising Seas (song) . The global average sea level has risen about 250 millimetres (9.8 in) since 1880. [1] Between 1901 and 2018, average global sea level rose by 15–25 cm (6–10 in), an average of 1–2 mm (0.039–0.079 in) per year. [2] This rate accelerated to 4.62 mm (0.182 in)/yr for the decade 2013–2022. [3] Climate change due to human activities is the main cause. [4] : 5, 8 Between 1993 and 2018, thermal expansion of water accounted for 42% of sea level rise. Melting temperate glaciers accounted for 21%, while polar glaciers in Greenland accounted for 15% and those in Antarctica for 8%. [5] : 1576 Sea level rise lags changes in the Earth 's temperature, and sea level rise will therefore continue to accelerate between now and 2050 in response to warming that has already happened. [6] What happens after that depends on human greenhouse gas emissions . Sea level rise may slow down between 2050 and 2100 if there are deep cuts in emissions. It could then reach slightly over 30 cm (1 ft) from now by 2100. With high emissions it may accelerate. It could rise by 1 m (3+1⁄2 ft) or even 2 m (6+1⁄2 ft) by then. [4] [7] In the long run, sea level rise would amount to 2–3 m (7–10 ft) over the next 2000 years if warming amounts to 1.5 °C (2.7 °F). It would be 19–22 metres (62–72 ft) if warming peaks at 5 °C (9.0 °F). [4] : 21 Rising seas affect every coastal and island population on Earth. [8] [9] This can be through flooding, higher storm surges , king tides , and tsunamis . There are many knock-on effects. They lead to loss of coastal ecosystems like mangroves . Crop production falls because of salinization of irrigation water. Damage to ports disrupts sea trade. [10] [11] [12] The sea level rise projected by 2050 will expose places currently inhabited by tens of millions of people to annual flooding. Without a sharp reduction in greenhouse gas emissions, this may increase to hundreds of millions in the latter decades of the century. [13] Areas not directly exposed to rising sea levels could be vulnerable to large-scale migration and economic disruption. Local factors like tidal range or land subsidence will greatly affect the severity of impacts. The varying resilience and adaptive capacity of individual ecosystems, sectors, and countries are also factors. [14] For instance, sea level rise in the United States (particularly along the US East Coast ) is already higher than the global average. It is likely to be 2 to 3 times greater than the global average by the end of the century. [15] [16] Yet, of the 20 countries with the greatest exposure to sea level rise, 12 are in Asia . Eight of them collectively account for 70% of the global population exposed to sea level rise and land subsidence. These are Bangladesh , China , India , Indonesia , Japan , the Philippines , Thailand and Vietnam . [17] The greatest impact on human populations in the near term will occur in the low-lying Caribbean and Pacific islands . Sea level rise will make many of them uninhabitable later this century. [18] Societies can adapt to sea level rise in three ways. Managed retreat , accommodating coastal change , or protecting against sea level rise through hard-construction practices like seawalls [19] are hard approaches. There are also soft approaches such as dune rehabilitation and beach nourishment . Sometimes these adaptation strategies go hand in hand. At other times choices must be made among different strategies. [20] A managed retreat strategy is difficult if an area's population is increasing rapidly. This is a particularly acute problem for Africa . There, the population of low-lying coastal areas is likely to increase by around 100 million people within the next 40 years. [21] Poorer nations may also struggle to implement the same approaches to adapt to sea level rise as richer states. Sea level rise at some locations may be compounded by other environmental issues. One example is subsidence in sinking cities . [22] Coastal ecosystems typically adapt to rising sea levels by moving inland. Natural or artificial barriers may make that impossible. [23] Observations[ edit ] Sea surface height change from 1992 to 2019 – NASA The visualization is based on data collected from the TOPEX/Poseidon, Jason-1, Jason-2, and Jason-3 satellites. Blue regions are where sea level has gone down, and orange/red regions are where sea level has risen. [24] Between 1901 and 2018, the global mean sea level rose by about 20 cm (7.9 in). [4] More precise data gathered from satellite radar measurements found a rise of 7.5 cm (3.0 in) from 1993 to 2017 (average of 2.9 mm (0.11 in)/yr). [5] This accelerated to 4.62 mm (0.182 in)/yr for 2013–2022. [3] Further information: Relative sea level and Sinking cities Sea level rise is not uniform around the globe. Some land masses are moving up or down as a consequence of subsidence (land sinking or settling) or post-glacial rebound (land rising as melting ice reduces weight). Therefore, local relative sea level rise may be higher or lower than the global average. Changing ice masses also affect the distribution of sea water around the globe through gravity. [25] [26] When a glacier or ice sheet melts, it loses mass. This reduces its gravitational pull. In some places near current and former glaciers and ice sheets, this has caused water levels to drop. At the same time water levels will increase more than average further away from the ice sheet. Thus ice loss in Greenland affects regional sea level differently than the equivalent loss in Antarctica . [27] On the other hand, the Atlantic is warming at a faster pace than the Pacific. This has consequences for Europe and the U.S. East Coast . The East Coast sea level is rising at 3–4 times the global average. [28] Scientists have linked extreme regional sea level rise on the US Northeast Coast to the downturn of the Atlantic meridional overturning circulation (AMOC). [29] Many ports , urban conglomerations, and agricultural regions stand on river deltas . Here land subsidence contributes to much higher relative sea level rise. Unsustainable extraction of groundwater and oil and gas is one cause. Levees and other flood management practices are another. They prevent sediments from accumulating. These would otherwise compensate for the natural settling of deltaic soils. [30] : 638 [31] : 88 Estimates for total human-caused subsidence in the Rhine-Meuse-Scheldt delta (Netherlands) are 3–4 m (10–13 ft), over 3 m (10 ft) in urban areas of the Mississippi River Delta ( New Orleans ), and over 9 m (30 ft) in the Sacramento–San Joaquin River Delta . [31] : 81–90 On the other hand, relative sea level around the Hudson Bay in Canada and the northern Baltic is falling due to post-glacial isostatic rebound. [32] Projections[ edit ] NOAA predicts different levels of sea level rise through 2050 for several US coastlines. [16] There are two complementary ways to model sea level rise (SLR) and project the future. The first  uses process-based modeling. This combines all relevant and well-understood physical processes in a global physical model. This approach calculates the contributions of ice sheets with an ice-sheet model and computes rising sea temperature and expansion with a general circulation model . The processes are not fully understood. But this approach can predict non-linearities and long delays in the response, which studies of the recent past will miss. The other approach employs semi-empirical techniques. These use historical geological data to determine likely sea level responses to a warming world, and some basic physical modeling. [33] These semi-empirical sea level models rely on statistical techniques. They use relationships between observed past contributions to global mean sea level and temperature. [34] Scientists developed this type of modeling because most physical models in previous Intergovernmental Panel on Climate Change (IPCC) literature assessments had underestimated the amount of sea level rise compared to 20th century observations. [26] Projections for the 21st century[ edit ] Historical sea level reconstruction and projections up to 2100 published in 2017 by the U.S. Global Change Research Program . [35] RCPs are different scenarios for future concentrations of greenhouse gases. Intergovernmental Panel on Climate Change is the largest and most influential scientific organization on climate change, and since 1990, it provides several plausible scenarios of 21st century sea level rise in each of its major reports. The differences between scenarios are mainly due to uncertainty about future greenhouse gas emissions. These depend on future economic developments, and  also future political action which is hard to predict. Each scenario  provides an estimate for sea level rise as a range with a lower and upper limit to reflect the unknowns. The scenarios in the 2013-2014 Fifth Assessment Report (AR5) were called Representative Concentration Pathways , or RCPs and the scenarios in the IPCC Sixth Assessment Report (AR6) are known as Shared Socioeconomic Pathways , or SSPs. A large difference between the two was the addition of SSP1-1.9 to AR6, which represents meeting the best Paris climate agreement goal of 1.5 °C (2.7 °F). In that case, the likely range of sea level rise by 2100 is 28–55 cm (11–21+1⁄2 in). [7] The lowest scenario in AR5, RCP2.6, would see greenhouse gas emissions low enough to meet the goal of limiting warming by 2100 to 2 °C (36 °F). It shows sea level rise in 2100 of about 44 cm (17 in) with a range of 28–61 cm (11–24 in). The "moderate" scenario, where CO2 emissions take a decade or two to peak and its atmospheric concentration does not plateau until 2070s is called RCP 4.5. Its likely range of sea level rise is 36–71 cm (14–28 in). The highest scenario in RCP8.5 pathway sea level would rise between 52 and 98 cm (20+1⁄2 and 38+1⁄2 in). [26] [36] AR6 had equivalents for both scenarios, but it estimated larger sea level rise under both. In AR6, the SSP1-2.6 pathway results in a range of 32–62 cm (12+1⁄2–24+1⁄2 in) by 2100. The "moderate" SSP2-4.5 results in a 44–76 cm (17+1⁄2–30 in) range by 2100 and SSP5-8.5 led to 65–101 cm (25+1⁄2–40 in). [7] Sea level rise projections for the years 2030, 2050 and 2100 from 2007 to 2012 Further, AR5 was criticized by multiple researchers for excluding detailed estimates the impact of "low-confidence" processes like marine ice sheet and marine ice cliff instability, [37] [38] [39] which can substantially accelerate ice loss to potentially add "tens of centimeters" to sea level rise within this century. [26] AR6 includes a version of SSP5-8.5 where these processes take place, and in that case, sea level rise of over 2 m (6+1⁄2 ft) by 2100 could not be ruled out. [7] The general increase of projections in AR6 was caused by the observed ice-sheet erosion in Greenland and Antarctica matching the upper-end range of the AR5 projections by 2020, [40] [41] and the finding that AR5 projections were likely too slow next to an extrapolation of observed sea level rise trends, while the subsequent reports had improved in this regard. [42] Notably, some scientists believe that ice sheet processes may accelerate sea level rise even at temperatures below the highest possible scenario, though not as much. For instance, a 2017 study from the University of Melbourne researchers suggested that these processes increase RCP2.6 sea level rise by about one quarter, RCP4.5 sea level rise by one half and practically double RCP8.5 sea level rise. [43] [44] A 2016 study led by Jim Hansen hypothesized that vulnerable ice sheet section collapse can lead to near-term exponential sea level rise acceleration, with a doubling time of 10, 20, or 40 years. Such acceleration would lead to multi-meter sea level rise in 50, 100, or 200 years, respectively, [39] but it remains a minority view amongst the scientific community. [45] For comparison, a major scientific survey of 106 experts in 2020 found that even when accounting for instability processes they had estimated a median sea level rise of 45 cm (17+1⁄2 in) by 2100 for RCP2.6, with a 5%-95% range of 21–82 cm (8+1⁄2–32+1⁄2 in). For RCP8.5, the experts estimated a median of 93 cm (36+1⁄2 in) by 2100  and a 5%-95% range of 45–165 cm (17+1⁄2–65 in). [46] Similarly, NOAA in 2022 had suggested that there is a 50% probability of 0.5 m (19+1⁄2 in) sea level rise by 2100 under 2 °C (3.6 °F), which increases to >80% to >99% under 3–5 °C (5.4–9.0 °F). [16] Year 2019 elicitation of 22 ice sheet experts suggested a median SLR of 30 cm (12 in) by 2050 and 70 cm (27+1⁄2 in) by 2100 in the low emission scenario and the median of 34 cm (13+1⁄2 in) by 2050 and 110 cm (43+1⁄2 in) by 2100 in a high emission scenario. They also estimated a small chance of sea levels exceeding 1 meter by 2100 even in the low emission scenario and of going beyond 2 metres in the high emission scenario, with the latter causing the displacement of 187 million people. [47] Post-2100 sea level rise[ edit ] If countries cut greenhouse gas emissions significantly (lowest trace), sea level rise by 2100 will be limited to 0.3 to 0.6 meters (1–2 feet). [48] However, in a worst-case scenario (top trace), sea levels could rise 5 meters (16 feet) by the year 2300. [48] Map of the Earth with a long-term 6-metre (20 ft) sea level rise represented in red (uniform distribution, actual sea level rise will vary regionally and local adaptation measures will also have an effect on local sea levels). Even if the temperature stabilizes, significant sea-level rise (SLR) will continue for centuries. [49] This is what models consistent with paleo records of sea level rise. [26] : 1189 After 500 years, sea level rise from thermal expansion alone may have reached only half of its eventual level. Models suggest this may lie within ranges of 0.5–2 m (1+1⁄2–6+1⁄2 ft). [50] Additionally, tipping points of Greenland and Antarctica ice sheets are likely to play a larger role over such timescales. [51] Ice loss from Antarctica is likely to dominate very long-term SLR, especially if the warming exceeds 2 °C (3.6 °F). Continued carbon dioxide emissions from fossil fuel sources could cause additional tens of metres of sea level rise, over the next millennia. The available fossil fuel on Earth is enough to melt the entire Antarctic ice sheet, causing about 58 m (190 ft) of sea level rise. [52] In the next 2,000 years, sea level is predicted to rise by 2–3 m (6+1⁄2–10 ft) if the temperature increase peaks at its current 1.5 °C (2.7 °F), It would rise by 2–6 m (6+1⁄2–19+1⁄2 ft) if it peaks at 2 °C (3.6 °F) and by 19–22 m (62+1⁄2–72 ft) if it peaks at 5 °C (9.0 °F). [4] : SPM-28 If the temperature rise stops at 2 °C (3.6 °F) or at 5 °C (9.0 °F), the sea level would still continue to rise for about 10,000 years. In the first case it will reach 8–13 m (26–42+1⁄2 ft) above pre-industrial level, and in the second 28–37 m (92–121+1⁄2 ft). [53] With better models and observational records, several studies have attempted to project SLR for the centuries immediately after 2100. This remains largely speculative. An April 2019 expert elicitation asked 22 experts about total sea level rise projections for the years 2200 and 2300 under its high, 5 °C warming scenario. It ended up with 90% confidence intervals of −10 cm (4 in) to 740 cm (24+1⁄2 ft) and − 9 cm (3+1⁄2 in) to 970 cm (32 ft), respectively. Negative values represent the extremely low probability of very large increases in the ice sheet surface mass balance due to climate change-induced increase in precipitation . [47] An elicitation of 106 experts led by Stefan Rahmstorf also included 2300 for RCP2.6 and RCP8.5. The former had the median of 118 cm (46+1⁄2 in), and a 5%-95% range of 24–311 cm (9+1⁄2–122+1⁄2 in). The latter had the median of 329 cm (129+1⁄2 in),  and a 5%-95% range of 88–783 cm (34+1⁄2–308+1⁄2 in). [46] By 2021, AR6 was also able to provide estimates for sea level rise in 2150 alongside the 2100 estimates for the first time. This showed that keeping warming at 1.5 °C under the SSP1-1.9 scenario would result in sea level rise in the 17-83% range of 37–86 cm (14+1⁄2–34 in). In the SSP1-2.6 pathway the range would be 46–99 cm (18–39 in), for SSP2-4.5 a 66–133 cm (26–52+1⁄2 in) range by 2100 and for SSP5-8.5 a rise of 98–188 cm (38+1⁄2–74 in). It stated that a "low-confidence" projection of over 2 m (6+1⁄2 ft) by 2100, would accelerate further to potentially 5 m (16+1⁄2 ft) by 2150. AR6 also provided lower-confidence estimates for year 2300 sea level rise under SSP1-2.6 and SSP5-8.5. The former had a range between 0.5 m (1+1⁄2 ft) and 3.2 m (10+1⁄2 ft), while the latter ranged from just under 2 m (6+1⁄2 ft) to just under 7 m (23 ft). The low-confidence projections of SSP5-8.5 project sea level rise exceeding 15 m (49 ft) by then. [7] A 2018 paper estimated that sea level rise in 2300 would increase by a median of 20 cm (8 in) for every five years CO2 emissions increase before peaking. It shows a 5% likelihood of a 1 m (3+1⁄2 ft) increase due to the same. The same estimate found that if the temperature stabilized below 2 °C (3.6 °F), 2300 sea level rise would still exceed 1.5 m (5 ft). Early net zero and slowly falling temperatures could limit it to 70–120 cm (27+1⁄2–47 in). [54] Measurements[ edit ] Variations in the amount of water in the oceans, changes in its volume, or varying land elevation compared to the sea surface can drive sea level changes. Over a consistent time period, assessments can attribute contributions to sea level rise and provide early indications of change in trajectory. This helps to inform adaptation plans. [55] The different techniques used to measure changes in sea level do not measure exactly the same level. Tide gauges can only measure relative sea level. Satellites can also measure absolute sea level changes. [56] To get precise measurements for sea level, researchers studying the ice and oceans factor in ongoing deformations of the solid Earth . They look in particular at landmasses still rising from past ice masses retreating , and the Earth's gravity and rotation . [5] Satellites[ edit ] Jason-1 continued the sea surface measurements started by TOPEX/Poseidon. It was followed by the Ocean Surface Topography Mission on Jason-2 , and by Jason-3 . Since the launch of TOPEX/Poseidon in 1992, an overlapping series of altimetric satellites has been continuously recording the sea level and its changes. [57] These satellites can measure the hills and valleys in the sea caused by currents and detect trends in their height. To measure the distance to the sea surface, the satellites send a microwave pulse towards Earth and record the time it takes to return after reflecting off the ocean's surface. Microwave radiometers correct the additional delay caused by water vapor in the atmosphere . Combining these data with the location of the spacecraft determines the sea-surface height to within a few centimetres. [58] These satellite measurements have estimated rates of sea level rise for 1993–2017 at 3.0 ± 0.4 millimetres (1⁄8 ± 1⁄64 in) per year. [59] Satellites are useful for measuring regional variations in sea level. An example is the substantial rise between 1993 and 2012 in the western tropical Pacific. This sharp rise has been linked to increasing trade winds . These occur when the Pacific Decadal Oscillation (PDO) and the El Niño–Southern Oscillation (ENSO) change from one state to the other. [60] The PDO is a basin-wide climate pattern consisting of two phases, each commonly lasting 10 to 30 years. The ENSO has a shorter period of 2 to 7 years. [61] Tide gauges[ edit ] Between 1993 and 2018, the mean sea level has risen across most of the world ocean (blue colors). [62] The global network of tide gauges is the other important source of sea-level observations. Compared to the satellite record, this record has major spatial gaps but covers a much longer period. [63] Coverage of tide gauges started mainly in the Northern Hemisphere . Data for the Southern Hemisphere remained scarce up to the 1970s. [63] The longest running sea-level measurements, NAP or Amsterdam Ordnance Datum were established in 1675, in Amsterdam . [64] Record collection is also extensive in Australia . They including measurements by an amateur meteorologist beginning in 1837. They also include measurements taken from a sea-level benchmark struck on a small cliff on the Isle of the Dead near the Port Arthur convict settlement in 1841. [65] Together with satellite data for the period after 1992, this network established that global mean sea level rose 19.5 cm (7.7 in) between 1870 and 2004 at an average rate of about 1.44 mm/yr. (For the 20th century the average is 1.7 mm/yr.) [66] By 2018, data collected by Australia's Commonwealth Scientific and Industrial Research Organisation (CSIRO) had shown that the global mean sea level was rising by 3.2 mm (1⁄8 in) per year. This was double the average 20th century rate. [67] [68] The 2023 World Meteorological Organization report found further acceleration to 4.62 mm/yr over the 2013–2022 period. [3] These observations help to check and verify predictions from climate change simulations. Regional differences are also visible in the tide gauge data. Some are caused by local sea level differences. Others are due to vertical land movements. In Europe , only some land areas are rising while the others are sinking. Since 1970, most tidal stations have measured higher seas. However sea levels along the northern Baltic Sea have dropped due to post-glacial rebound . [69] Past sea level rise[ edit ] Main articles: Past sea level , Sea level § Change , and Marine transgression Changes in sea levels since the end of the last glacial episode An understanding of past sea level is an important guide to where current changes in sea level will end up. In the recent geological past, thermal expansion from increased temperatures and changes in land ice are the dominant reasons of sea level rise. The last time that the Earth was 2 °C (3.6 °F) warmer than pre-industrial temperatures was 120,000 years ago. This was when warming due to Milankovitch cycles (changes in the amount of sunlight due to slow changes in the Earth's orbit) caused the Eemian interglacial . Sea levels during that warmer interglacial were at least 5 m (16 ft) higher than now. [70] The Eemian warming was sustained over a period of thousands of years. The size of the rise in sea level implies a large contribution from the Antarctic and Greenland ice sheets. [26] : 1139 Levels of atmospheric carbon dioxide of around 400 parts per million (similar to 2000s) had increased temperature by over 2–3 °C (3.6–5.4 °F) around three million years ago. This temperature increase eventually melted one third of Antarctica's ice sheet, causing sea levels to rise 20 meters above the preindustrial levels. [71] Since the Last Glacial Maximum , about 20,000 years ago, sea level has risen by more than 125 metres (410 ft). Rates vary from less than 1 mm/year during the pre-industrial era to 40+ mm/year when major ice sheets over Canada and Eurasia melted. Meltwater pulses are periods of fast sea level rise caused by the rapid disintegration of these ice sheets. The rate of sea level rise started to slow down about 8,200 years before today. Sea level was almost constant for the last 2,500 years. The recent trend of rising sea level started at the end of the 19th or beginning of the 20th century. [72] Causes[ edit ] Earth lost 28 trillion tonnes of ice between 1994 and 2017: ice sheets and glaciers raised the global sea level by 34.6 ± 3.1 mm. The rate of ice loss has risen by 57% since the 1990s−from 0.8 to 1.2 trillion tonnes per year. [73] The three main reasons warming causes global sea level to rise are the expansion of oceans due to heating , water inflow from melting ice sheets and water inflow from glaciers. Glacier retreat and ocean expansion have dominated sea level rise since the start of the 20th century. [33] Some of the losses from glaciers are offset when precipitation falls as snow, accumulates and over time forms glacial ice. If precipitation, surface processes and ice loss at the edge balance each other, sea level remains the same. Because of this precipitation began as water vapor evaporated from the ocean surface, effects of climate change on the water cycle can even increase ice build-up. However, this effect is not enough to fully offset ice losses, and sea level rise continues to accelerate. [21] [74] [75] [76] The contributions of the two large ice sheets, in Greenland and Antarctica , are likely to increase in the 21st century. [33] They store most of the land ice (~99.5%) and have a sea-level equivalent (SLE) of 7.4 m (24 ft 3 in) for Greenland and 58.3 m (191 ft 3 in) for Antarctica. [5] Thus, melting of all the ice on Earth would result in about 70 m (229 ft 8 in) of sea level rise, [77] although this would require at least 10,000 years and up to 10 °C (18 °F) of global warming. [78] [79] Main article: Ocean heat content There has been an increase in ocean heat content during recent decades as the oceans absorb most of the excess heat created by human-induced global warming . [80] The oceans store more than 90% of the extra heat added to the climate system by Earth's energy imbalance and act as a buffer against its effects. [81] This means that the same amount of heat that would increase the average world ocean temperature by 0.01 °C (0.018 °F) would increase atmospheric temperature by approximately 10 °C (18 °F). [82] So a small change in the mean temperature of the ocean represents a very large change in the total heat content of the climate system. Winds and currents move heat into deeper parts of the ocean. Some of it reaches depths of more than 2,000 m (6,600 ft). [83] When the ocean gains heat, the water expands and sea level rises. Warmer water and water under great pressure (due to depth) expand more than cooler water and water under less pressure. [26] : 1161 Consequently, cold Arctic Ocean water will expand less than warm tropical water. Different climate models present slightly different patterns of ocean heating. So their projections do not agree fully on how much ocean heating contributes to sea level rise. [84] Antarctic ice loss[ edit ] Processes around an Antarctic ice shelf The Ross Ice Shelf is Antarctica's largest. It is about the size of France and up to several hundred metres thick. The large volume of ice on the Antarctic continent stores around 60% of the world's fresh water. Excluding groundwater this is 90%. [85] Antarctica is experiencing ice loss from coastal glaciers in the West Antarctica and some glaciers of East Antarctica . However it is gaining mass from the increased snow build-up inland, particularly in the East. This leads to contradicting trends. [76] [86] There are different satellite methods for measuring ice mass and change. Combining them helps to reconcile the differences. [87] However, there can still be variations between the studies. In 2018, a systematic review estimated average annual ice loss of 43 billion tons (Gt) across the entire continent between 1992 and 2002. This tripled to an annual average of 220 Gt from 2012 to 2017. [74] [88] However, a 2021 analysis of data from four different research satellite systems ( Envisat , European Remote-Sensing Satellite , GRACE and GRACE-FO and ICESat ) indicated annual mass loss of only about 12 Gt from 2012 to 2016. This was due to greater ice gain in East Antarctica than estimated earlier. [76] In the future, it is known that West Antarctica at least will continue to lose mass, and the likely future losses of sea ice and ice shelves , which block warmer currents from direct contact with the ice sheet, can accelerate declines even in the East. [89] [90] Altogether, Antarctica is the source of the largest uncertainty for future sea level projections. [91] By 2019, several studies attempted to estimate 2300 sea level rise caused by ice loss in Antarctica alone. They suggest a median rise of 16 cm (6+1⁄2 in) and maximum rise of 37 cm (14+1⁄2 in) under the low-emission scenario. The highest emission scenario results in a median rise of 1.46 m (5 ft) metres, with a minimum of 60 cm (2 ft) and a maximum of 2.89 m (9+1⁄2 ft)). [7] East Antarctica[ edit ] The world's largest potential source of sea level rise is the East Antarctic Ice Sheet (EAIS). It is 2.2 km thick on average and holds enough ice to raise global sea levels by 53.3 m (174 ft 10 in) [92] Its great thickness and high elevation make it more stable than the other ice sheets. [93] As of the early 2020s, most studies show that it is still gaining mass. [94] [74] [76] [86] Some analyses have suggested it began to lose mass in the 2000s. [95] [75] [90] However they over-extrapolated some observed losses on to the poorly observed areas. A more complete observational record shows continued mass gain. [76] Aerial view of ice flows at Denman Glacier, one of the less stable glaciers in the East Antarctica In spite of the net mass gain, some East Antarctica glaciers have lost ice in recent decades due to ocean warming and declining structural support from the local sea ice , [89] such as Denman Glacier , [96] [97] and Totten Glacier . [98] [99] Totten Glacier is particularly important because it stabilizes the Aurora Subglacial Basin . Subglacial basins like Aurora and Wilkes Basin are major ice reservoirs together holding as much ice as all of West Antarctica. [100] They are more vulnerable than the rest of East Antarctica. [38] Their collective tipping point probably lies at around 3 °C (5.4 °F) of global warming. It may be as high as 6 °C (11 °F) or as low as 2 °C (3.6 °F). Once this tipping point is crossed, the collapse of these subglacial basins could take place over as little as 500 or as much as 10,000 years. The median timeline is 2000 years. [78] [79] Depending on how many subglacial basins are vulnerable, this causes sea level rise of between 1.4 m (4 ft 7 in) and 6.4 m (21 ft 0 in). [101] On the other hand, the whole EAIS would not definitely collapse until global warming reaches 7.5 °C (13.5 °F), with a range between 5 °C (9.0 °F) and 10 °C (18 °F). It would take at least 10,000 years to disappear. [78] [79] Some scientists have estimated that warming would have to reach at least 6 °C (11 °F) to melt two thirds of its volume. [102] West Antarctica[ edit ] Thwaites Glacier, with its vulnerable bedrock topography visible. East Antarctica contains the largest potential source of sea level rise. However the West Antarctica ice sheet (WAIS) is substantially more vulnerable. Temperatures on West Antarctica have increased significantly, unlike East Antarctica and the Antarctic Peninsula . The trend is between 0.08 °C (0.14 °F) and 0.96 °C (1.73 °F) per decade between 1976 and 2012. [103] Satellite observations recorded a substantial increase in WAIS melting from 1992 to 2017. This resulted in 7.6 ± 3.9 mm (19⁄64 ± 5⁄32 in) of Antarctica sea level rise. Outflow glaciers in the Amundsen Sea Embayment played a disproportionate role. [104] Scientists estimated in 2021 that the median increase in sea level rise from Antarctica by 2100 is ~11 cm (5 in). There is no difference between scenarios, because the increased warming would intensify the water cycle and increase snowfall accumulation over the EAIS at about the same rate as it would increase ice loss from WAIS. [7] However, most of the bedrock underlying the WAIS lies well below sea level, and it has to be buttressed by the Thwaites and Pine Island glaciers. If these glaciers were to collapse, the entire ice sheet would as well. [38] Their disappearance would take at least several centuries, but is considered almost inevitable, as their bedrock topography deepens inland and becomes more vulnerable to meltwater. [105] [106] [107] The contribution of these glaciers to global sea levels has already accelerated since the beginning of the 21st century. The Thwaites Glacier now accounts for 4% of global sea level rise. [105] [108] [109] It could start to lose even more ice if the Thwaites Ice Shelf fails, potentially in mid-2020s. [110] This is due to marine ice sheet instability hypothesis, where warm water enters between the seafloor and the base of the ice sheet once it is no longer heavy enough to displace the flow, causing accelerated melting and collapse. [111] Marine ice cliff instability, when ice cliffs with heights greater than 100 m (330 ft) collapse under their own weight once they are no longer buttressed by ice shelves , may also occur, though it has never been observed, and more detailed modelling has ruled it out. [112] A graphical representation of how warm waters, and the Marine Ice Sheet Instability and Marine Ice Cliff Instability processes are affecting the West Antarctic Ice Sheet Other hard-to-model processes include hydrofracturing, where meltwater collects atop the ice sheet, pools into fractures and forces them open. [37] and changes in the ocean circulation at a smaller scale. [113] [114] [115] A combination of these processes could cause the WAIS to contribute up to 41 cm (16 in) by 2100 under the low-emission scenario and up to 57 cm (22 in) under the highest-emission one. [7] The melting of all the ice in West Antarctica would increase the total sea level rise to 4.3 m (14 ft 1 in). [116] However, mountain ice caps not in contact with water are less vulnerable than the majority of the ice sheet, which is located below the sea level. [117] Its collapse would cause ~3.3 m (10 ft 10 in) of sea level rise. [118] This collapse is now considered practically inevitable, as it appears to have already occurred during the Eemian period 125,000 years ago, when temperatures were similar to the early 21st century. [119] [120] [121] [122] [123] [115] [124] This disappearance would take an estimated 2000 years. The absolute minimum for the loss of West Antarctica ice is 500 years, and the potential maximum is 13,000 years. [78] [79] The only way to stop ice loss from West Antarctica once triggered is by lowering the global temperature to 1 °C (1.8 °F) below the preindustrial level. This would be 2 °C (3.6 °F) below the temperature of 2020. [102] Other researchers suggested that a climate engineering intervention to stabilize the ice sheet's glaciers may delay its loss by centuries and give more time to adapt. However this is an uncertain proposal, and would end up as one of the most expensive projects ever attempted. [125] [126] Isostatic rebound[ edit ] 2021 research indicates that isostatic rebound after the loss of the main portion of the West Antarctic ice sheet would ultimately add another 1.02 m (3 ft 4 in) to global sea levels. This effect would start to increase sea levels before 2100. However it would take 1000 years for it to cause 83 cm (2 ft 9 in) of sea level rise. At this point, West Antarctica itself would be 610 m (2,001 ft 4 in) higher than now. Estimates of isostatic rebound after the loss of East Antarctica's subglacial basins suggest increases of between 8 cm (3.1 in) and 57 cm (1 ft 10 in) [101] Greenland ice sheet loss[ edit ] Greenland 2007 melt, measured as the difference between the number of days on which melting occurred in 2007 compared to the average annual melting days from 1988 to 2006 [127] Most ice on Greenland is in the Greenland ice sheet which is 3 km (10,000 ft) at its thickest. The rest of Greenland ice forms isolated glaciers and ice caps. The average annual ice loss in Greenland more than doubled in the early 21st century compared to the 20th century. [128] Its  contribution to sea level rise correspondingly increased from 0.07 mm per year between 1992 and 1997 to 0.68 mm per year between 2012 and 2017. Total ice loss from the Greenland ice sheet between 1992 and 2018 amounted to 3,902 gigatons (Gt) of ice. This is equivalent to a SLR contribution of 10.8 mm. [129] The contribution for the 2012–2016 period was equivalent to 37% of sea level rise from land ice sources (excluding thermal expansion). [130] This observed rate of ice sheet melting is at the higher end of predictions from past IPCC assessment reports. [131] [41] In 2021, AR6 estimated that by 2100, the melting of Greenland ice sheet would most likely add around 6 cm (2+1⁄2 in) to sea levels under the low-emission scenario, and 13 cm (5 in) under the high-emission scenario. The first scenario, SSP1-2.6 , largely fulfils the Paris Agreement goals, while the other, SSP5-8.5, has the emissions accelerate throughout the century. The uncertainty about ice sheet dynamics can affect both pathways. In the best-case scenario, ice sheet under SSP1-2.6 gains enough mass by 2100 through surface mass balance feedbacks to reduce the sea levels by 2 cm (1 in). In the worst case, it adds 15 cm (6 in). For SSP5-8.5, the best-case scenario is adding 5 cm (2 in) to sea levels, and the worst-case is adding 23 cm (9 in). [7] Trends of Greenland ice loss between 2002 and 2019 [132] Greenland's peripheral glaciers and ice caps crossed an irreversible tipping point around 1997. Sea level rise from their loss is now unstoppable. [133] [134] [135] However the temperature changes in future, the warming of 2000–2019 had already damaged the ice sheet enough for it to eventually lose ~3.3% of its volume. This is leading to 27 cm (10+1⁄2 in) of future sea level rise. [136] At a certain level of global warming, the Greenland ice sheet will almost completely melt. Ice cores show this happened at least once during the last million years, when the temperatures have at most been 2.5 °C (4.5 °F) warmer than the preindustrial. [137] [138] 2012 research suggested that the tipping point of the ice sheet was between 0.8 °C (1.4 °F) and 3.2 °C (5.8 °F). [139] 2023 modelling has narrowed the tipping threshold to a 1.7 °C (3.1 °F)-2.3 °C (4.1 °F) range. If temperatures reach or exceed that level, reducing the global temperature to 1.5 °C (2.7 °F) above pre-industrial levels or lower would prevent the loss of the entire ice sheet. One way to do this in theory would be large-scale carbon dioxide removal . But it would also cause greater losses and sea level rise from Greenland than if the threshold was not breached in the first place. [140] Otherwise, the ice sheet would take between 10,000 and 15,000 years to disintegrate entirely once the tipping point had been crossed. The most likely estimate is 10,000 years. [78] [79] If climate change continues along its worst trajectory and temperatures continue to rise quickly over multiple centuries, it would only take 1,000 years. [141] Mountain glacier loss[ edit ] Based on national pledges to reduce greenhouse gas emissions, global mean temperature is projected to increase by 2.7 °C (4.9 °F), which would cause loss of about half of Earth's glaciers by 2100—causing a sea level rise of 115±40 millimeters. [142] There are roughly 200,000 glaciers on Earth, which are spread out across all continents. [143] Less than 1% of glacier ice is in mountain glaciers, compared to 99% in Greenland and Antarctica . However, this small size also makes mountain glaciers more vulnerable to melting than the larger ice sheets. This means they have had a disproportionate contribution to historical sea level rise and are set to contribute a smaller, but still significant fraction of sea level rise in the 21st century. [144] Observational and modelling studies of mass loss from glaciers and ice caps show they contribute 0.2-0.4 mm per year to sea level rise, averaged over the 20th century. [145] The contribution for the 2012–2016 period was nearly as large as that of Greenland. It was 0.63 mm of sea level rise per year, equivalent to 34% of sea level rise from land ice sources. [130] Glaciers contributed around 40% to sea level rise during the 20th century, with estimates for the 21st century of around 30%. [5] In 2023, a Science paper estimated that at 1.5 °C (2.7 °F), one quarter of mountain glacier mass would be lost by 2100 and nearly half would be lost at 4 °C (7.2 °F), contributing ~ 9 cm (3+1⁄2 in) and ~15 cm (6 in) to sea level rise, respectively. Glacier mass is disproportionately concentrated in the most resilient glaciers. So in practice this would remove 49-83% of glacier formations. It further estimated that the current likely trajectory of 2.7 °C (4.9 °F) would result in the SLR contribution of ~ 11 cm (4+1⁄2 in) by 2100. [146] Mountain glaciers are even more vulnerable over the longer term. In 2022, another Science paper estimated that almost no mountain glaciers could survive once warming crosses 2 °C (3.6 °F). Their complete loss is largely inevitable around 3 °C (5.4 °F). There is even a possibility of complete loss after 2100 at just 1.5 °C (2.7 °F). This could happen as early as 50 years after the tipping point is crossed, although 200 years is the most likely value, and the maximum is around 1000 years. [78] [79] Sea ice loss[ edit ] Sea ice loss contributes very slightly to global sea level rise. If the melt water from ice floating in the sea was exactly the same as sea water then, according to Archimedes' principle , no rise would occur. However melted sea ice contains less dissolved salt than sea water and is therefore less dense , with a slightly greater volume per unit of mass. If all floating ice shelves and icebergs were to melt sea level would only rise by about 4 cm (1+1⁄2 in). [147] Trends in land water storage from GRACE observations in gigatons per year, April 2002 to November 2014 (glaciers and ice sheets are excluded). Changes to land water storage[ edit ] See also: Groundwater-related subsidence Human activity impacts how much water is stored on land. Dams retain large quantities of water, which is stored on land rather than flowing into the sea, though the total quantity stored will vary from time to time. On the other hand, humans extract water from lakes, wetlands and underground reservoirs for food production . This often causes subsidence . Furthermore, the hydrological cycle is influenced by climate change and deforestation . This can increase or reduce contributions to sea level rise. In the 20th century, these processes roughly balanced, but dam building has slowed down and is expected to stay low for the 21st century. [148] [26] : 1155 Water redistribution caused by irrigation from 1993 to 2010 caused a drift of Earth's rotational pole by 78.48 centimetres (30.90 in). This caused groundwater depletion equivalent to a global sea level rise of 6.24 millimetres (0.246 in). [149] See also: Human impacts on coasts , Coastal development hazards , and Coastal erosion High tide flooding, also called tidal flooding, has become much more common in the past seven decades. [150] Sea-level rise has many impacts. They include higher and more frequent high-tide and storm-surge flooding and increased coastal erosion . Other impacts are inhibition of primary production processes, more extensive coastal inundation, and changes in surface water quality and groundwater . These can lead to a greater loss of property and coastal habitats, loss of life during floods and loss of cultural resources. There are also impacts on agriculture and aquaculture . There can also be loss of tourism, recreation, and transport-related functions. [10] : 356 Land use changes such as urbanisation or deforestation of low-lying coastal zones exacerbate coastal flooding impacts. Regions already vulnerable to rising sea level also struggle with coastal flooding. This washes away land and alters the landscape. [151] Changes in emissions are likely to have only a small effect on the extent of sea level rise by 2050. [6] So projected sea level rise could put tens of millions of people at risk by then. Scientists estimate that 2050 levels of sea level rise would result in about 150 million people under the water line during high tide. About 300 million would be in places flooded every year. This projection is based on the distribution of population in 2010. It does not take into account the effects of population growth and human migration . These figures are 40 million and 50 million more respectively than the numbers at risk in 2010. [13] [152] By 2100, there would be another 40 million people under the water line during high tide if sea level rise remains low. This figure would be 80 million for a high estimate of median sea level rise. [13] Ice sheet processes under the highest emission scenario would result in sea level rise of well over one metre (3+1⁄4 ft) by 2100. This could be as much as over two metres (6+1⁄2 ft), [16] [4] : TS-45 This could result in as many as 520 million additional people  ending up under the water line during high tide and 640 million in places flooded every year, compared to the 2010 population distribution. [13] Major cities threatened by sea level rise. The cities indicated are under threat of even a small sea level rise (of 1.6 feet/49 cm) compared to the level in 2010. Even moderate projections indicate that such a rise will have occurred by 2060. [153] [154] Over the longer term, coastal areas are particularly vulnerable to rising sea levels. They are also vulnerable to changes in the frequency and intensity of storms, increased precipitation, and rising ocean temperatures . Ten percent of the world's population live in coastal areas that are less than 10 metres (33 ft) above sea level. Two thirds of the world's cities with over five million people are located in these low-lying coastal areas. [155] About 600 million people live directly on the coast around the world. [156] Cities such as Miami , Rio de Janeiro , Osaka and Shanghai will be especially vulnerable later in the century under warming of 3 °C (5.4 °F). This is close to the current trajectory. [12] [36] LiDAR -based research had established in 2021 that 267 million people worldwide lived on land less than 2 m (6+1⁄2 ft) above sea level. With a 1 m (3+1⁄2 ft) sea level rise and zero population growth, that could increase to 410 million people. [157] [158] Potential disruption of sea trade and migrations could impact people living further inland. United Nations Secretary-General António Guterres warned in 2023 that sea level rise risks causing human migrations on a "biblical scale". [159] Sea level rise will inevitably affect ports , but there is limited research on this. There is insufficient knowledge about the investments necessary to protect  ports currently in use. This includes protecting current facilities before it becomes more reasonable to build new ports elsewhere. [160] [161] Some coastal regions are rich agricultural lands. Their loss to the sea could cause food shortages . This is a particularly acute issue for river deltas such as Nile Delta in Egypt and Red River and Mekong Deltas in Vietnam. Saltwater intrusion into the soil and irrigation water has a disproportionate effect on them. [162] [163] See also: Extinction risk from climate change Bramble Cay melomys , the first known mammal species to go extinct due to sea level rise. Flooding and soil/water salinization threaten the habitats of coastal plants, birds, and freshwater/ estuarine fish when seawater reaches inland. [164] When coastal forest areas become inundated with saltwater to the point no trees can survive the resulting habitats are called ghost forests . [165] [166] Starting around 2050, some nesting sites in Florida , Cuba , Ecuador and the island of Sint Eustatius for leatherback , loggerhead , hawksbill , green and olive ridley turtles are expected to be flooded. The proportion will increase over time. [167] In 2016, Bramble Cay islet in the Great Barrier Reef was inundated. This flooded the habitat of a rodent named Bramble Cay melomys . [168] It was officially declared extinct in 2019. [169] An example of mangrove pneumatophores. Some ecosystems can move inland with the high-water mark. But natural or artificial barriers prevent many from migrating. This coastal narrowing is sometimes called 'coastal squeeze' when it involves human-made barriers. It could result in the loss of habitats such as mudflats and tidal marshes . [23] [170] Mangrove ecosystems on the mudflats of tropical coasts nurture high biodiversity . They are particularly vulnerable due to mangrove plants' reliance on breathing roots or pneumatophores . These will be submerged if the rate is too rapid for them to migrate upward. This would result in the loss of an ecosystem. [171] [172] [173] [174] Both mangroves and tidal marshes protect against storm surges, waves and tsunamis, so their loss makes the effects of sea level rise worse. [175] [176] Human activities such as dam building may restrict sediment supplies to wetlands. This would prevent natural adaptation processes. The loss of some tidal marshes is unavoidable as a consequence. [177] Corals are important for bird and fish life. They need to grow vertically to remain close to the sea surface in order to get enough energy from sunlight. The corals have so far been able to keep up the vertical growth with the rising seas, but might not be able to do so in the future. [178]
Toggle the table of contents Water cycle From Wikipedia, the free encyclopedia Continuous movement water on, above and below the surface of the Earth A detailed diagram by the United States Geological Survey depicting the global water cycle. Inset on the lower right is the directionality of the movement of water between reservoirs. Directionality tends towards upwards movement through evapotranspiration and downward movement through gravity . The water cycle, also known as the hydrologic cycle or the hydrological cycle, is a biogeochemical cycle that involves the continuous movement of water on, above and below the surface of the Earth . The mass of water on Earth remains fairly constant over time but the partitioning of the water into the major reservoirs of ice , fresh water , saline water (salt water) and atmospheric water is variable depending on a wide range of climatic variables . The water moves from one reservoir to another, such as from river to ocean , or from the ocean to the atmosphere, by the physical processes of evaporation , sublimation , transpiration , condensation , precipitation , infiltration , surface runoff , and subsurface flow. In doing so, the water goes through different forms: liquid, solid ( ice ) and vapor . The ocean plays a key role in the water cycle as it is the source of 86% of global evaporation. [1] The water cycle involves the exchange of energy, which leads to temperature changes. When water evaporates, it takes up energy from its surroundings and cools the environment. When it condenses, it releases energy and warms the environment. These heat exchanges influence climate . The evaporative phase of the cycle purifies water, causing salts and other solids picked up during the cycle to be left behind, and then the condensation phase in the atmosphere replenishes the land with freshwater. The flow of liquid water and ice transports minerals across the globe. It is also involved in reshaping the geological features of the Earth, through processes including erosion and sedimentation . The water cycle is also essential for the maintenance of most life and ecosystems on the planet. Part of a series on Video of the Earth's water cycle (NASA) [2] Overall process Further information: Water distribution on Earth The water cycle is powered from the energy emitted by the sun. This energy heats water in the ocean and seas. Water evaporates as water vapor into the air . Some ice and snow sublimates directly into water vapor. Evapotranspiration is water transpired from plants and evaporated from the soil. The water molecule H2O has smaller molecular mass than the major components of the atmosphere, nitrogen (N2) and oxygen (O2) and hence is less dense. Due to the significant difference in density, buoyancy drives humid air higher. As altitude increases, air pressure decreases and the temperature drops (see Gas laws ). The lower temperature causes water vapor to condense into tiny liquid water droplets which are heavier than the air, and which fall unless supported by an updraft. A huge concentration of these droplets over a large area in the atmosphere becomes visible as cloud , while condensation near ground level is referred to as fog . Atmospheric circulation moves water vapor around the globe; cloud particles collide, grow, and fall out of the upper atmospheric layers as precipitation . Some precipitation falls as snow, hail, or sleet, and can accumulate in ice caps and glaciers , which can store frozen water for thousands of years. Most water falls as rain back into the ocean or onto land, where the water flows over the ground as surface runoff . A portion of this runoff enters rivers, with streamflow moving water towards the oceans. Runoff and water emerging from the ground ( groundwater ) may be stored as freshwater in lakes. Not all runoff flows into rivers; much of it soaks into the ground as infiltration . Some water infiltrates deep into the ground and replenishes aquifers , which can store freshwater for long periods of time. Some infiltration stays close to the land surface and can seep back into surface-water bodies (and the ocean) as groundwater discharge or be taken up by plants and transferred back to the atmosphere as water vapor by transpiration . Some groundwater finds openings in the land surface and emerges as freshwater springs. In river valleys and floodplains , there is often continuous water exchange between surface water and ground water in the hyporheic zone . Over time, the water returns to the ocean, to continue the water cycle. The ocean plays a key role in the water cycle. The ocean holds "97% of the total water on the planet; 78% of global precipitation occurs over the ocean, and it is the source of 86% of global evaporation". [1] Processes leading to movements and phase changes in water Important physical processes within the water cycle include the following (in alphabetical order): Advection : The movement of water through the atmosphere. [3] Without advection, water that evaporated over the oceans could not precipitate over land. Atmospheric rivers that move large volumes of water vapor over long distances are an example of advection. [4] Condensation : The transformation of water vapor to liquid water droplets in the air, creating clouds and fog. [5] Evaporation : The transformation of water from liquid to gas phases as it moves from the ground or bodies of water into the overlying atmosphere. [6] The source of energy for evaporation is primarily solar radiation . Evaporation often implicitly includes transpiration from plants , though together they are specifically referred to as evapotranspiration . Total annual evapotranspiration amounts to approximately 505,000 km3 (121,000 cu mi) of water, 434,000 km3 (104,000 cu mi) of which evaporates from the oceans. [7] 86% of global evaporation occurs over the ocean. [8] Infiltration : The flow of water from the ground surface into the ground. Once infiltrated, the water becomes soil moisture or groundwater. [9] A recent global study using water stable isotopes, however, shows that not all soil moisture is equally available for groundwater recharge or for plant transpiration. [10] Percolation : Water flows vertically through the soil and rocks under the influence of gravity . Precipitation : Condensed water vapor that falls to the Earth's surface. Most precipitation occurs as rain , but also includes snow , hail , fog drip , graupel , and sleet . [11] Approximately 505,000 km3 (121,000 cu mi) of water falls as precipitation each year, 398,000 km3 (95,000 cu mi) of it over the oceans. [7] [12] The rain on land contains 107,000 km3 (26,000 cu mi) of water per year and a snowing only 1,000 km3 (240 cu mi). [12] 78% of global precipitation occurs over the ocean. [8] Runoff : The variety of ways by which water moves across the land. This includes both surface runoff and channel runoff . As it flows, the water may seep into the ground, evaporate into the air, become stored in lakes or reservoirs, or be extracted for agricultural or other human uses. Subsurface flow : The flow of water underground, in the vadose zone and aquifers . Subsurface water may return to the surface (e.g. as a spring or by being pumped) or eventually seep into the oceans. Water returns to the land surface at lower elevation than where it infiltrated, under the force of gravity or gravity induced pressures. Groundwater tends to move slowly and is replenished slowly, so it can remain in aquifers for thousands of years. Transpiration : The release of water vapor from plants and soil into the air. Residence times Average reservoir residence times [13] Reservoir Atmosphere 9 days The residence time of a reservoir within the hydrologic cycle is the average time a water molecule will spend in that reservoir (see adjacent table). It is a measure of the average age of the water in that reservoir. Groundwater can spend over 10,000 years beneath Earth's surface before leaving. [14] Particularly old groundwater is called fossil water . Water stored in the soil remains there very briefly, because it is spread thinly across the Earth, and is readily lost by evaporation, transpiration, stream flow, or groundwater recharge. After evaporating, the residence time in the atmosphere is about 9 days before condensing and falling to the Earth as precipitation. The major ice sheets – Antarctica and Greenland – store ice for very long periods. Ice from Antarctica has been reliably dated to 800,000 years before present, though the average residence time is shorter. [15] In hydrology, residence times can be estimated in two ways.[ citation needed ] The more common method relies on the principle of conservation of mass ( water balance ) and assumes the amount of water in a given reservoir is roughly constant. With this method, residence times are estimated by dividing the volume of the reservoir by the rate by which water either enters or exits the reservoir. Conceptually, this is equivalent to timing how long it would take the reservoir to become filled from empty if no water were to leave (or how long it would take the reservoir to empty from full if no water were to enter). An alternative method to estimate residence times, which is gaining in popularity for dating groundwater, is the use of isotopic techniques. This is done in the subfield of isotope hydrology . Water in storage Further information: Water resources and Water distribution on Earth Water cycle showing human influences and major pools (storages) and fluxes. [16] The water cycle describes the processes that drive the movement of water throughout the hydrosphere . However, much more water is "in storage" (or in "pools") for long periods of time than is actually moving through the cycle. The storehouses for the vast majority of all water on Earth are the oceans. It is estimated that of the 1,386,000,000 km3 of the world's water supply, about 1,338,000,000 km3 is stored in oceans, or about 97%. It is also estimated that the oceans supply about 90% of the evaporated water that goes into the water cycle. [17] The Earth's ice caps, glaciers, and permanent snowpack stores another 24,064,000 km3 accounting for only 1.7% of the planet's total water volume. However, this quantity of water is 68.7% of all freshwater on the planet. [18] Changes caused by humans Extreme weather (heavy rains, droughts , heat waves ) is one consequence of a changing water cycle due to global warming . These events will be progressively more common as the Earth warms more and more. [19] : Figure SPM.6 Predicted changes in average soil moisture for a scenario of 2°C global warming. This can disrupt agriculture and ecosystems. A reduction in soil moisture by one standard deviation means that average soil moisture will approximately match the ninth driest year between 1850 and 1900 at that location. Water cycle intensification due to climate change Main articles: Effects of climate change on the water cycle and Effects of climate change on oceans Since the middle of the 20th century, human-caused climate change has resulted in observable changes in the global water cycle. [20] : 85 The IPCC Sixth Assessment Report in 2021 predicted that these changes will continue to grow significantly at the global and regional level. [20] : 85 These findings are a continuation of scientific consensus expressed in the IPCC Fifth Assessment Report from 2007 and other special reports by the Intergovernmental Panel on Climate Change which had already stated that the water cycle will continue to intensify throughout the 21st century. [21] This section is an excerpt from Effects of climate change on the water cycle .[ edit ] The effects of climate change on the water cycle are profound and have been described as an intensification or a strengthening of the water cycle (also called hydrologic cycle). [22] : 1079 This effect has been observed since at least 1980. [22] : 1079 One example is the intensification of heavy precipitation events. This has important negative effects on the availability of freshwater resources, as well as other water reservoirs such as oceans, ice sheets, atmosphere and land surface. The water cycle is essential to life on Earth and plays a large role in the global climate and the ocean circulation . The warming of our planet is expected to be accompanied by changes in the water cycle for various reasons. [23] For example, a warmer atmosphere can contain more water vapor which has effects on evaporation and rainfall . The underlying cause of the intensifying water cycle is the increased amount of greenhouse gases, which lead to a warmer atmosphere through the greenhouse effect . [23] Physics dictates that saturation vapor pressure increases by 7% when temperature rises by 1 °C (as described in the Clausius-Clapeyron equation ). [24] The strength of the water cycle and its changes over time are of considerable interest, especially as the climate changes. [25] The essence of the overall hydrological cycle is the evaporation of moisture in one place and the precipitation in other places. In particular, evaporation exceeds precipitation over the oceans, which allows moisture to be transported by the atmosphere from the oceans onto land where precipitation exceeds evapotranspiration , and the runoff flows into streams and rivers and discharges into the ocean, completing the cycle. [25] The water cycle is a key part of Earth's energy cycle through the evaporative cooling at the surface which provides latent heat to the atmosphere, as atmospheric systems play a primary role in moving heat upward. [25] Changes due to other human activities Relationship between impervious surfaces and surface runoff Human activities, other than those that lead to global warming from greenhouse gas emissions , can also alter the water cycle. The IPCC Sixth Assessment Report stated that there is "abundant evidence that changes in land use and land cover alter the water cycle globally, regionally and locally, by changing precipitation, evaporation, flooding, groundwater, and the availability of freshwater for a variety of uses". [26] : 1153 Examples for such land use changes are converting fields to urban areas or clearing forests . Such changes can affect the ability of soils to soak up surface water. Deforestation has local as well as regional effects. For example it reduces soil moisture, evaporation and rainfall at the local level. Furthermore, deforestation causes regional temperature changes that can affect rainfall patterns. [26] : 1153 Aquifer drawdown or overdrafting and the pumping of fossil water increase the total amount of water in the hydrosphere. This is because the water that was originally in the ground has now become available for evaporation as it is now in contact with the atmosphere. [26] : 1153 Related processes Biogeochemical cycling While the water cycle is itself a biogeochemical cycle , flow of water over and beneath the Earth is a key component of the cycling of other biogeochemicals. [27] Runoff is responsible for almost all of the transport of eroded sediment and phosphorus from land to waterbodies . [28] The salinity of the oceans is derived from erosion and transport of dissolved salts from the land. Cultural eutrophication of lakes is primarily due to phosphorus, applied in excess to agricultural fields in fertilizers , and then transported overland and down rivers. Both runoff and groundwater flow play significant roles in transporting nitrogen from the land to waterbodies. [29] The dead zone at the outlet of the Mississippi River is a consequence of nitrates from fertilizer being carried off agricultural fields and funnelled down the river system to the Gulf of Mexico . Runoff also plays a part in the carbon cycle , again through the transport of eroded rock and soil. [30] Slow loss over geologic time Main article: Atmospheric escape The hydrodynamic wind within the upper portion of a planet's atmosphere allows light chemical elements such as Hydrogen to move up to the exobase , the lower limit of the exosphere , where the gases can then reach escape velocity , entering outer space without impacting other particles of gas. This type of gas loss from a planet into space is known as planetary wind . [31] Planets with hot lower atmospheres could result in humid upper atmospheres that accelerate the loss of hydrogen. [32] Historical interpretations Floating land mass In ancient times, it was widely thought that the land mass floated on a body of water, and that most of the water in rivers has its origin under the earth. Examples of this belief can be found in the works of Homer ( c. 800 BCE). Hebrew Bible In the ancient Near East , Hebrew scholars observed that even though the rivers ran into the sea, the sea never became full. Some scholars conclude that the water cycle was described completely during this time in this passage: "The wind goeth toward the south, and turneth about unto the north; it whirleth about continually, and the wind returneth again according to its circuits. All the rivers run into the sea, yet the sea is not full; unto the place from whence the rivers come, thither they return again" ( Ecclesiastes 1:6-7 ). [33] Scholars are not in agreement as to the date of Ecclesiastes, though most scholars point to a date during the time of King Solomon , son of David and Bathsheba, "three thousand years ago, [33] there is some agreement that the time period is 962–922 BCE. [34] Furthermore, it was also observed that when the clouds were full, they emptied rain on the earth ( Ecclesiastes 11:3 ). In addition, during 793–740 BCE a Hebrew prophet, Amos, stated that water comes from the sea and is poured out on the earth ( Amos 5:8 ). [35] In the Biblical Book of Job , dated between 7th and 2nd centuries BCE, [34] there is a description of precipitation in the hydrologic cycle, [33] "For he maketh small the drops of water: they pour down rain according to the vapour thereof; which the clouds do drop and distil upon man abundantly" ( Job 36:27-28 ). Understanding of precipitation and percolation In the Adityahridayam (a devotional hymn to the Sun God) of Ramayana , a Hindu epic dated to the 4th century BCE, it is mentioned in the 22nd verse that the Sun heats up water and sends it down as rain. By roughly 500 BCE, Greek scholars were speculating that much of the water in rivers can be attributed to rain. The origin of rain was also known by then. These scholars maintained the belief, however, that water rising up through the earth contributed a great deal to rivers. Examples of this thinking included Anaximander (570 BCE) (who also speculated about the evolution of land animals from fish [36] ) and Xenophanes of Colophon (530 BCE). [37] Warring States period Chinese scholars such as Chi Ni Tzu (320 BCE) and Lu Shih Ch'un Ch'iu (239 BCE) had similar thoughts. [38] The idea that the water cycle is a closed cycle can be found in the works of Anaxagoras of Clazomenae (460 BCE) and Diogenes of Apollonia (460 BCE). Both Plato (390 BCE) and Aristotle (350 BCE) speculated about percolation as part of the water cycle. Aristotle correctly hypothesized that the sun played a role in the Earth's hydraulic cycle in his book Meteorology , writing "By it [the sun's] agency the finest and sweetest water is everyday carried up and is dissolved into vapor and rises to the upper regions, where it is condensed again by the cold and so returns to the earth.", and believed that clouds were composed of cooled and condensed water vapor. [39] [40] Much like the earlier Aristotle, the Eastern Han Chinese scientist Wang Chong (27–100 AD) accurately described the water cycle of Earth in his Lunheng but was dismissed by his contemporaries. [41] Up to the time of the Renaissance, it was wrongly assumed that precipitation alone was insufficient to feed rivers, for a complete water cycle, and that underground water pushing upwards from the oceans were the main contributors to river water. Bartholomew of England held this view (1240 CE), as did Leonardo da Vinci (1500 CE) and Athanasius Kircher (1644 CE). Discovery of the correct theory The first published thinker to assert that rainfall alone was sufficient for the maintenance of rivers was Bernard Palissy (1580 CE), who is often credited as the discoverer of the modern theory of the water cycle. Palissy's theories were not tested scientifically until 1674, in a study commonly attributed to Pierre Perrault . Even then, these beliefs were not accepted in mainstream science until the early nineteenth century. [42] See also
Toggle the table of contents Sustainable agriculture From Wikipedia, the free encyclopedia Farming approach that balances environmental, economic and social factors in the long term Shade-grown coffee , a form of polyculture (an example of sustainable agriculture) in imitation of natural ecosystems. Trees provide resources for the coffee plants such as shade, nutrients, and soil structure; the farmers harvest coffee and timber. Sustainable agriculture is farming in sustainable ways meeting society's present food  and textile needs, without compromising the ability for current or future generations to meet their needs. [1] It can be based on an understanding of ecosystem services . There are many methods to increase the sustainability of agriculture. When developing agriculture within sustainable food systems , it is important to develop flexible business process and farming practices. [2] Agriculture has an enormous environmental footprint , playing a significant role in causing climate change ( food systems are responsible for one third of the anthropogenic greenhouse gas emissions ), [3] [4] water scarcity , water pollution , land degradation , deforestation and other processes; [5] it is simultaneously causing environmental changes and being impacted by these changes. [6] Sustainable agriculture consists of environment friendly methods of farming that allow the production of crops or livestock without damage to human or natural systems. It involves preventing adverse effects to soil, water, biodiversity, surrounding or downstream resources—as well as to those working or living on the farm or in neighboring areas. Elements of sustainable agriculture can include permaculture , agroforestry , mixed farming , multiple cropping , and crop rotation . [7] Developing sustainable food systems contributes to the sustainability of the human population. For example, one of the best ways to mitigate climate change is to create sustainable food systems based on sustainable agriculture. Sustainable agriculture provides a potential solution to enable agricultural systems to feed a growing population within the changing environmental conditions. [6] Besides sustainable farming practices, dietary shifts to sustainable diets are an intertwined way to substantially reduce environmental impacts. [8] [9] [10] [11] Numerous sustainability standards and certification systems exist, including organic certification , Rainforest Alliance , Fair Trade , UTZ Certified , GlobalGAP , Bird Friendly, and the Common Code for the Coffee Community (4C). [12] The term "sustainable agriculture" was defined in 1977 by the USDA as an integrated system of plant and animal production practices having a site-specific application that will, over the long term: [13] satisfy human food and fiber needs enhance environmental quality and the natural resource base upon which the agriculture economy depends make the most efficient use of nonrenewable resources and on-farm resources and integrate, where appropriate, natural biological cycles and controls sustain the economic viability of farm operations enhance the quality of life for farmers and society as a whole. Yet the idea of having a sustainable relationship with the land has been prevalent in indigenous communities for centuries before the term was formally added to the lexicon. [14] Aims[ edit ] A common consensus is that sustainable farming is the most realistic way to feed growing populations. In order to successfully feed the population of the planet, farming practices must consider future costs–to both the environment and the communities they fuel. [15] The fear of not being able to provide enough resources for everyone led to the adoption of technology within the sustainability field to increase farm productivity. The ideal end result of this advancement is the ability to feed ever-growing populations across the world. The growing popularity of sustainable agriculture is connected to the wide-reaching fear that the planet's carrying capacity (or planetary boundaries ), in terms of the ability to feed humanity, has been reached or even exceeded. [16] Key principles[ edit ] There are several key principles associated with sustainability in agriculture: [17] The incorporation of biological and ecological processes such as nutrient cycling , soil regeneration , and nitrogen fixation into agricultural and food production practices. Using decreased amounts of non-renewable and unsustainable inputs, particularly environmentally harmful ones. Using the expertise of farmers to both productively work the land as well as to promote the self-reliance and self-sufficiency of farmers. Solving agricultural and natural resource problems through the cooperation and collaboration of people with different skills. The problems tackled include pest management and irrigation . It "considers long-term as well as short-term economics because sustainability is readily defined as forever, that is, agricultural environments that are designed to promote endless regeneration". [18] It balances the need for resource conservation with the needs of farmers pursuing their livelihood . [19] It is considered to be reconciliation ecology , accommodating biodiversity within human landscapes. [20] Oftentimes the execution of sustainable practices within farming comes through the adoption of technology and environmentally-focused appropriate technology . Traditional farming methods have a low carbon footprint .[ citation needed ] Practices that can cause long-term damage to soil include excessive tilling of the soil (leading to erosion ) and irrigation without adequate drainage (leading to salinization ). [21] [22] Conservation farming in Zambia The most important factors for a farming site are climate , soil, nutrients and water resources . Of the four, water and soil conservation are the most amenable to human intervention. When farmers grow and harvest crops, they remove some nutrients from the soil. Without replenishment, the land suffers from nutrient depletion and becomes either unusable or suffers from reduced yields . Sustainable agriculture depends on replenishing the soil while minimizing the use or need of non-renewable resources, such as natural gas or mineral ores. A farm that can "produce perpetually", yet has negative effects on environmental quality elsewhere is not sustainable agriculture. An example of a case in which a global view may be warranted is the application of fertilizer or manure , which can improve the productivity of a farm but can pollute nearby rivers and coastal waters ( eutrophication ). [23] The other extreme can also be undesirable, as the problem of low crop yields due to exhaustion of nutrients in the soil has been related to rainforest destruction. [24] In Asia, the specific amount of land needed for sustainable farming is about 12.5 acres which include land for animal fodder, cereal production as a cash crop, and other food crops. In some cases, a small unit of aquaculture is included (AARI-1996). Nitrates[ edit ] Nitrates are used widely in farming as fertilizer. Unfortunately, a major environmental problem associated with agriculture is the leaching of nitrates into the environment. [25] Possible sources of nitrates that would, in principle, be available indefinitely, include: growing legume crops and forages such as peanuts or alfalfa that form symbioses with nitrogen-fixing bacteria called rhizobia [27] industrial production of nitrogen by the Haber process uses hydrogen, which is currently derived from natural gas (but this hydrogen could instead be made by electrolysis of water using renewable electricity) genetically engineering (non-legume) crops to form nitrogen-fixing symbioses or fix nitrogen without microbial symbionts. [28] The last option was proposed in the 1970s, but is only gradually becoming feasible. [29] [30] Sustainable options for replacing other nutrient inputs such as phosphorus and potassium are more limited. Other options include long-term crop rotations , returning to natural cycles that annually flood cultivated lands (returning lost nutrients) such as the flooding of the Nile , the long-term use of biochar , and use of crop and livestock landraces that are adapted to less than ideal conditions such as pests, drought, or lack of nutrients. Crops that require high levels of soil nutrients can be cultivated in a more sustainable manner with appropriate fertilizer management practices. Phosphate[ edit ] Phosphate is a primary component in fertilizer . It is the second most important nutrient for plants after nitrogen, [31] and is often a limiting factor. [32] It is important for sustainable agriculture as it can improve soil fertility and crop yields. [33] Phosphorus is involved in all major metabolic processes including photosynthesis, energy transfer, signal transduction, macromolecular biosynthesis, and respiration. It is needed for root ramification and strength and seed formation, and can increase disease resistance. [34] Phosphorus is found in the soil in both inorganic and organic forms [31] and makes up approximately 0.05% of soil biomass. [34] Phosphorus fertilizers are the main input of inorganic phosphorus in agricultural soils and approximately 70%–80% of phosphorus in cultivated soils is inorganic. [35] Long-term use of phosphate-containing chemical fertilizers causes eutrophication and deplete soil microbial life, so people have looked to other sources. [34] Phosphorus fertilizers are manufactured from rock phosphate . [36] However, rock phosphate is a non-renewable resource and it is being depleted by mining for agricultural use: [33] [35] peak phosphorus will occur within the next few hundred years, [37] [38] [39] or perhaps earlier. [40] [41] [42] Potassium[ edit ] Potassium is a macronutrient very important for plant development and is commonly sought in fertilizers. [43] This nutrient is essential for agriculture because it improves water retention, nutrient value, yield, taste, color, texture and disease resistance of crops. It is often used in the cultivation of grains, fruits, vegetables, rice, wheat, millets, sugar, corn, soybeans, palm oil and coffee. [44] Potassium chloride (KCl) represents the most widely source of K used in agriculture, [45] accounting for 90% of all potassium produced for agricultural use. [46] The use of KCl leads to high concentrations of chloride (Clˉ) in soil harming its health due to the increase in soil salinity, imbalance in nutrient availability and this ion's biocidal effect for soil organisms. In consequences the development of plants and soil organisms is affected, putting at risk soil biodiversity and agricultural productivity. [47] [48] [49] [50] A sustainable option for replacing KCl are chloride-free fertilizers, its use should take into account plants' nutrition needs, and the promotion of soil health. [51] [52] Soil[ edit ] Walls built to avoid water run-off, Andhra Pradesh , India Land degradation is becoming a severe global problem. According to the Intergovernmental Panel on Climate Change : "About a quarter of the Earth's ice-free land area is subject to human-induced degradation (medium confidence). Soil erosion from agricultural fields is estimated to be currently 10 to 20 times (no tillage) to more than 100 times (conventional tillage) higher than the soil formation rate (medium confidence)." [53] Almost half of the land on earth is covered with dry land, which is susceptible to degradation. [54] Over a billion tonnes of southern Africa's soil are being lost to erosion annually, which if continued will result in halving of crop yields within thirty to fifty years. [55] Improper soil management is threatening the ability to grow sufficient food. Intensive agriculture reduces the carbon level in soil, impairing soil structure, crop growth and ecosystem functioning, [56] and accelerating climate change . [56] Modification of agricultural practices is a recognized method of carbon sequestration as soil can act as an effective carbon sink . [57] See also: Peak farmland As the global population increases and demand for food increases, there is pressure on land as a resource. In land-use planning and management, considering the impacts of land-use changes on factors such as soil erosion can support long-term agricultural sustainability, as shown by a study of Wadi Ziqlab, a dry area in the Middle East where farmers graze livestock and grow olives, vegetables, and grains. [60] Looking back over the 20th century shows that for people in poverty, following environmentally sound land practices has not always been a viable option due to many complex and challenging life circumstances. [61] Currently, increased land degradation in developing countries may be connected with rural poverty among smallholder farmers when forced into unsustainable agricultural practices out of necessity. [62] Converting big parts of the land surface to agriculture has severe environmental and health consequences. For example, it leads to rise in zoonotic disease (like the Coronavirus disease 2019 ) due to the degradation of natural buffers between humans and animals, reducing biodiversity and creating larger groups of genetically similar animals. [63] [64] Land is a finite resource on Earth. Although expansion of agricultural land can decrease biodiversity and contribute to deforestation , the picture is complex; for instance, a study examining the introduction of sheep by Norse settlers (Vikings) to the Faroe Islands of the North Atlantic concluded that, over time, the fine partitioning of land plots contributed more to soil erosion and degradation than grazing itself. [65] The Food and Agriculture Organization of the United Nations estimates that in coming decades, cropland will continue to be lost to industrial and urban development , along with reclamation of wetlands, and conversion of forest to cultivation, resulting in the loss of biodiversity and increased soil erosion. [66] Energy[ edit ] In modern agriculture, energy is used in on-farm mechanisation, food processing, storage, and transportation processes. [67] It has therefore been found that energy prices are closely linked to food prices . [68] Oil is also used as an input in agricultural chemicals . The International Energy Agency projects higher prices of non-renewable energy resources as a result of fossil fuel resources being depleted. It may therefore decrease global food security unless action is taken to 'decouple' fossil fuel energy from food production, with a move towards 'energy-smart' agricultural systems including renewable energy . [68] [69] [70] The use of solar powered irrigation in Pakistan is said to be a closed system for agricultural water irrigation. [71] The environmental cost of transportation could be avoided if people use local products. [72] Water[ edit ] In some areas sufficient rainfall is available for crop growth, but many other areas require irrigation . For irrigation systems to be sustainable, they require proper management (to avoid salinization ) and must not use more water from their source than is naturally replenishable. Otherwise, the water source effectively becomes a non-renewable resource . Improvements in water well drilling technology and submersible pumps , combined with the development of drip irrigation and low-pressure pivots, have made it possible to regularly achieve high crop yields in areas where reliance on rainfall alone had previously made successful agriculture unpredictable. However, this progress has come at a price. In many areas, such as the Ogallala Aquifer , the water is being used faster than it can be replenished. According to the UC Davis Agricultural Sustainability Institute, several steps must be taken to develop drought-resistant farming systems even in "normal" years with average rainfall. These measures include both policy and management actions: [73] improving water conservation and storage measures [73] providing incentives for selection of drought-tolerant crop species [73] using reduced-volume irrigation systems [73] managing crops to reduce water loss [73] not planting crops at all. [73] Indicators for sustainable water resource development include the average annual flow of rivers from rainfall, flows from outside a country, the percentage of water coming from outside a country, and gross water withdrawal. [74] It is estimated that agricultural practices consume 69% of the world's fresh water. [75] Social factors[ edit ] Rural economic development[ edit ] Sustainable agriculture attempts to solve multiple problems with one broad solution.  The goal of sustainable agricultural practices is to decrease environmental degradation due to farming while increasing crop–and thus food–output.  There are many varying strategies attempting to use sustainable farming practices in order to increase rural economic development within small-scale farming communities.  Two of the most popular and opposing strategies within the modern discourse are allowing unrestricted markets to determine food production and deeming food a human right .  Neither of these approaches have been proven to work without fail. A promising proposal to rural poverty reduction within agricultural communities is sustainable economic growth; the most important aspect of this policy is to regularly include the poorest farmers in the economy-wide development through the stabilization of small-scale agricultural economies. [76] In 2007, the United Nations reported on " Organic Agriculture and Food Security in Africa", stating that using sustainable agriculture could be a tool in reaching global food security without expanding land usage and reducing environmental impacts . [77] There has been evidence provided by developing nations from the early 2000s stating that when people in their communities are not factored into the agricultural process that serious harm is done. The social scientist Charles Kellogg has stated that, "In a final effort, exploited people pass their suffering to the land." [77] Sustainable agriculture mean the ability to permanently and continuously "feed its constituent populations". [77] There are a lot of opportunities that can increase farmers' profits, improve communities, and continue sustainable practices. For example, in Uganda , Genetically Modified Organisms were originally illegal. However, with the stress of banana crisis in Uganda, where Banana Bacterial Wilt had the potential to wipe out 90% of yield, they decided to explore GMOs as a possible solution. [78] The government issued the National Biotechnology and Biosafety bill, which will allow scientists that are part of the National Banana Research Program to start experimenting with genetically modified organisms. [79] This effort has the potential to help local communities because a significant portion live off the food they grow themselves , and it will be profitable because the yield of their main produce will remain stable. Not all regions are suitable for agriculture. [80] [81] The technological advancement of the past few decades has allowed agriculture to develop in some of these regions. For example, Nepal has built greenhouses to deal with its high altitude and mountainous regions. [31] Greenhouses allow for greater crop production and also use less water since they are closed systems. [82] Desalination techniques can turn salt water into fresh water which allows greater access to water for areas with a limited supply. [83] This allows the irrigation of crops without decreasing natural fresh water sources. [84] While desalination can be a tool to provide water to areas that need it to sustain agriculture, it requires money and resources. Regions of China have been considering large scale desalination in order to increase access to water, but the current cost of the desalination process makes it impractical. [85] Women[ edit ] Selling produce at an American farmers market Women working in sustainable agriculture come from numerous backgrounds, ranging from academia to labour. [86] From 1978-2007, in the United States , the number of women farm operators has tripled. [80] In 2007, women operated 14 percent of farms, compared to five percent in 1978. Much of the growth is due to women farming outside of the "male dominated field of conventional agriculture". [80] Growing your own food[ edit ] Main article: Urban agriculture The practice of growing food in the backyard of houses, schools, etc., by families or by communities became widespread in the US at the time of World War I , the Great Recession and World War II , so that in one point of time 40% of the vegetables of the USA was produced in this way. The practice became more popular again in the time of the COVID-19 pandemic . This method permits to grow food in a relatively sustainable way and at the same time can make it easier for poor people to obtain food. [87] Economic factors[ edit ] Costs, such as environmental problems, not covered in traditional accounting systems (which take into account only the direct costs of production incurred by the farmer) are known as externalities . [17] Netting studied sustainability and intensive agriculture in smallholder systems through history. [88] There are several studies incorporating externalities such as ecosystem services, biodiversity, land degradation, and sustainable land management in economic analysis. These include The Economics of Ecosystems and Biodiversity study and the Economics of Land Degradation Initiative which seek to establish an economic cost-benefit analysis on the practice of sustainable land management and sustainable agriculture. Triple bottom line frameworks include social and environmental alongside a financial bottom line. A sustainable future can be feasible if growth in material consumption and population is slowed down and if there is a drastic increase in the efficiency of material and energy use. To make that transition, long- and short-term goals will need to be balanced enhancing equity and quality of life. [89] Challenges and debates[ edit ] Barriers[ edit ] The barriers to sustainable agriculture can be broken down and understood through three different dimensions. These three dimensions are seen as the core pillars to sustainability : social, environmental, and economic pillars. [90] The social pillar addresses issues related to the conditions in which societies are born into, growing in, and learning from. [90] It deals with shifting away from traditional practices of agricultural and moving into new sustainable practices that will create better societies and conditions. [90] The environmental pillar addresses climate change and focuses on agricultural practices that protect the environment for future generations. [90] The economic pillar discovers ways in which sustainable agriculture can be practiced while fostering economic growth and stability, with minimal disruptions to livelihoods. [90] All three pillars must be addressed to determine and overcome the barriers preventing sustainable agricultural practices. [90] Social barriers to sustainable agriculture include cultural shifts, the need for collaboration, incentives, and new legislation. [90] The move from conventional to sustainable agriculture will require significant behavioural changes from both farmers and consumers. [91] Cooperation and collaboration between farmers is necessary to successfully transition to sustainable practices with minimal complications. [91] This can be seen as a challenge for farmers who care about competition and profitability. [92] There must also be an incentive for farmers to change their methods of agriculture. [93] The use of public policy, advertisements, and laws that make sustainable agriculture mandatory or desirable can be utilized to overcome these social barriers. [94] Pesticide use remains a common practice in agriculture. Environmental barriers prevent the ability to protect and conserve the natural ecosystem. [90] Examples of these barriers include the use of pesticides and the effects of climate change. [90] Pesticides are widely used to combat pests that can devastate production and plays a significant role in keeping food prices and production costs low. [95] To move toward sustainable agriculture, farmers are encouraged to utilize green pesticides, which cause less harm to both human health and habitats, but would entail a higher production cost. [96] Climate change is also a rapidly growing barrier, one that farmers have little control over, which can be seen through place-based barriers. [97] These place-based barriers include factors such as weather conditions, topography , and soil quality which can cause losses in production, resulting in the reluctance to switch from conventional practices. [97] Many environmental benefits are also not visible or immediately evident. [98] Significant changes such as lower rates of soil and nutrient loss, improved soil structure , and higher levels of beneficial microorganisms take time. [98] In conventional agriculture , the benefits are easily visible with no weeds, pests, etc..., but the long term costs to the soil and surrounding ecosystems are hidden and "externalized". [98] Conventional agricultural practices since the evolution of technology have caused significant damage to the environment through biodiversity loss , disrupted ecosystems, poor water quality, among other harms. [93] The economic obstacles to implementing sustainable agricultural practices include low financial return/profitability, lack of financial incentives, and negligible capital investments. [99] Financial incentives and circumstances play a large role in whether sustainable practices will be adopted. [90] [99] The human and material capital required to shift to sustainable methods of agriculture requires training of the workforce and making investments in new technology and products, which comes at a high cost. [90] [99] In addition to this, farmers practicing conventional agriculture can mass produce their crops, and therefore maximize their profitability. [90] This would be difficult to do in sustainable agriculture which encourages low production capacity. [90] Community gardening is a promising method of sustainable agriculture. The author James Howard Kunstler claims almost all modern technology is bad and that there cannot be sustainability unless agriculture is done in ancient traditional ways. [100] Efforts toward more sustainable agriculture are supported in the sustainability community, however, these are often viewed only as incremental steps and not as an end. [93] One promising method of encouraging sustainable agriculture is through local farming and community gardens . [93] Incorporating local produce and agricultural education into schools, communities, and institutions can promote the consumption of freshly grown produce which will drive consumer demand. [93] Some foresee a true sustainable steady state economy that may be very different from today's: greatly reduced energy usage, minimal ecological footprint , fewer consumer packaged goods , local purchasing with short food supply chains , little processed foods , more home and community gardens , etc. [101] Different viewpoints about the definition[ edit ] There is a debate on the definition of sustainability regarding agriculture. The definition could be characterized by two different approaches: an ecocentric approach and a technocentric approach . [102] The ecocentric approach emphasizes no- or low-growth levels of human development, and focuses on organic and biodynamic farming techniques with the goal of changing consumption patterns, and resource allocation and usage. The technocentric approach argues that sustainability can be attained through a variety of strategies, from the view that state-led modification of the industrial system like conservation-oriented farming systems should be implemented, to the argument that biotechnology is the best way to meet the increasing demand for food. [102] One can look at the topic of sustainable agriculture through two different lenses: multifunctional agriculture and ecosystem services . [103] Both of approaches are similar, but look at the function of agriculture differently. Those that employ the multifunctional agriculture philosophy focus on farm-centered approaches, and define function as being the outputs of agricultural activity. [103] The central argument of multifunctionality is that agriculture is a multifunctional enterprise with other functions aside from the production of food and fiber. These functions include renewable resource management, landscape conservation and biodiversity. [104] The ecosystem service-centered approach posits that individuals and society as a whole receive benefits from ecosystems , which are called "ecosystem services". [103] [105] In sustainable agriculture, the services that ecosystems provide include pollination , soil formation , and nutrient cycling , all of which are necessary functions for the production of food. [106] It is also claimed sustainable agriculture is best considered as an ecosystem approach to agriculture, called agroecology . [107] Ethics[ edit ] Most agricultural professionals agree that there is a "moral obligation to pursue [the] goal [of] sustainability." [77] The major debate comes from what system will provide a path to that goal because if an unsustainable method is used on a large scale it will have a massive negative effect on the environment and human population. Methods[ edit ] Countries' evaluation of trends in the use of selected management practices and approaches Other practices include polyculture , growing a diverse number of perennial crops in a single field, each of which would grow in separate seasons so as not to compete with each other for natural resources. [108] This system would result in increased resistance to diseases and decreased effects of erosion and loss of nutrients in the soil. Nitrogen fixation from legumes, for example, used in conjunction with plants that rely on nitrate from the soil for growth, helps to allow the land to be reused annually. Legumes will grow for a season and replenish the soil with ammonium and nitrate, and the next season other plants can be seeded and grown in the field in preparation for harvest. Sustainable methods of weed management may help reduce the development of herbicide-resistant weeds. [109] Crop rotation may also replenish nitrogen if legumes are used in the rotations and may also use resources more efficiently. [110] Rotational grazing with pasture divided into paddocks There are also many ways to practice sustainable animal husbandry . Some of the tools to grazing management include fencing off the grazing area into smaller areas called paddocks , lowering stock density, and moving the stock between paddocks frequently. [111] Main article: Intensive farming § Sustainability An increased production is a goal of intensification . Sustainable intensification encompasses specific agriculture methods that increase production and at the same time help improve environmental outcomes. The desired outcomes of the farm are achieved without the need for more land cultivation or destruction of natural habitat; the system performance is upgraded with no net environmental cost. Sustainable Intensification has become a priority for the United Nations. Sustainable intensification differs from prior intensification methods by specifically placing importance on broader environmental outcomes. By 2018; it was predicted in 100 nations a combined total of 163 million farms used sustainable intensification. The amount of agricultural land covered by this is 453 million ha of land. That amount of land is equal to 29% of farms worldwide. [112] In light of concerns about food security , human population growth and dwindling land suitable for agriculture, sustainable intensive farming practises are needed to maintain high crop yields , while maintaining soil health and ecosystem services . The capacity for ecosystem services to be strong enough to allow a reduction in use of non-renewable inputs whilst maintaining or boosting yields has been the subject of much debate. Recent work in irrigated rice production system of east Asia has suggested that – in relation to pest management at least – promoting the ecosystem service of biological control using nectar plants can reduce the need for insecticides by 70% whilst delivering a 5% yield advantage compared with standard practice. [113] Vertical farming is a concept with the potential advantages of year-round production, isolation from pests and diseases, controllable resource recycling and reduced transportation costs. [114] Water[ edit ] Water efficiency can be improved by reducing the need for irrigation and using alternative methods. Such methods include: researching on drought resistant crops, monitoring plant transpiration and reducing soil evaporation . [115] Drought resistant crops have been researched extensively as a means to overcome the issue of water shortage . They are modified genetically so they can adapt in an environment with little water. This is beneficial as it reduces the need for irrigation and helps conserve water. Although they have been extensively researched, significant results have not been achieved as most of the successful species will have no overall impact on water conservation. However, some grains like rice , for example, have been successfully genetically modified to be drought resistant. [116] Soil and nutrients[ edit ] Soil amendments include using compost from recycling centers. Using compost from yard and kitchen waste uses available resources in the area. Abstinence from soil tillage before planting and leaving the plant residue after harvesting reduces soil water evaporation; It also serves to prevent soil erosion. [117] Crop residues left covering the surface of the soil may result in reduced evaporation of water, a lower surface soil temperature, and reduction of wind effects. [117] A way to make rock phosphate more effective is to add microbial inoculates such as phosphate-solubilizing microorganisms, known as PSMs, to the soil. [32] [81] These solubilize phosphorus already in the soil and use processes like organic acid production and ion exchange reactions to make that phosphorus available for plants. [81] Experimentally, these PSMs have been shown to increase crop growth in terms of shoot height, dry biomass and grain yield. [81] Phosphorus uptake is even more efficient with the presence of mycorrhizae in the soil. [118] Mycorrhiza is a type of mutualistic symbiotic association between plants and fungi, [118] which are well-equipped to absorb nutrients, including phosphorus, in soil. [119] These fungi can increase nutrient uptake in soil where phosphorus has been fixed by aluminum, calcium, and iron. [119] Mycorrhizae can also release organic acids that solubilize otherwise unavailable phosphorus. [119] Pests and weeds[ edit ] Sheet steaming with a MSD/moeschle steam boiler (left side) Soil steaming can be used as an alternative to chemicals for soil sterilization. Different methods are available to induce steam into the soil to kill pests and increase soil health. Solarizing is based on the same principle, used to increase the temperature of the soil to kill pathogens and pests. [120] Certain plants can be cropped for use as biofumigants , "natural" fumigants , releasing pest suppressing compounds when crushed, ploughed into the soil, and covered in plastic for four weeks. Plants in the Brassicaceae family release large amounts of toxic compounds such as methyl isothiocyanates . [121] [122] Location[ edit ] Relocating current croplands to environmentally more optimal locations, whilst allowing ecosystems in then-abandoned areas to regenerate could substantially decrease the current carbon, biodiversity, and irrigation water footprint of global crop production, with relocation only within national borders also having substantial potential. [123] [124] Plants[ edit ] Sustainability may also involve crop rotation . [125] Crop rotation and cover crops prevent soil erosion , by protecting topsoil from wind and water. [31] Effective crop rotation can reduce pest pressure on crops, provides weed control, reduces disease build up, and improves the efficiency of soil nutrients and nutrient cycling. [126] This reduces the need for fertilizers and pesticides . [125] Increasing the diversity of crops by introducing new genetic resources can increase yields by 10 to 15 percent compared to when they are grown in monoculture. [126] [127] Perennial crops reduce the need for tillage and thus help mitigate soil erosion, and may sometimes tolerate drought better, increase water quality and help increase soil organic matter. There are research programs attempting to develop perennial substitutes for existing annual crops, such as replacing wheat with the wild grass Thinopyrum intermedium , or possible experimental hybrids of it and wheat. [128] Being able to do all of this without the use of chemicals is one of the main goals of sustainability which is why crop rotation is a very central method of sustainable agriculture. [126]
Toggle the table of contents Precision agriculture From Wikipedia, the free encyclopedia Farming management strategy False-color images demonstrate remote sensing applications in precision farming. [1] Yara N-Sensor ALS mounted on a tractor's canopy – a system that records light reflection of crops, calculates fertilisation recommendations and then varies the amount of fertilizer spread Precision Agriculture NDVI 4 cm / pixel GSD Precision agriculture (PA) is a farming management strategy based on observing, measuring and responding to temporal and spatial variability to improve agricultural production sustainability. [2] It is used in both crop and livestock production . Precision agriculture often employs technologies to automate agricultural operations , improving their diagnosis, decision-making or performing. [3] [4] The goal of precision agriculture research is to define a decision support system for whole farm management with the goal of optimizing returns on inputs while preserving resources. [5] [6] Among these many approaches is a phytogeomorphological approach which ties multi-year crop growth stability/characteristics to topological terrain attributes. The interest in the phytogeomorphological approach stems from the fact that the geomorphology component typically dictates the hydrology of the farm field. [7] [8] The practice of precision agriculture has been enabled by the advent of GPS and GNSS . The farmer's and/or researcher's ability to locate their precise position in a field allows for the creation of maps of the spatial variability of as many variables as can be measured (e.g. crop yield, terrain features/topography, organic matter content, moisture levels, nitrogen levels, pH, EC, Mg, K, and others). [9] Similar data is collected by sensor arrays mounted on GPS-equipped combine harvesters . These arrays consist of real-time sensors that measure everything from chlorophyll levels to plant water status, along with multispectral imagery. [10] This data is used in conjunction with satellite imagery by variable rate technology (VRT) including seeders, sprayers, etc. to optimally distribute resources. However, recent technological advances have enabled the use of real-time sensors directly in soil, which can wirelessly transmit data without the need of human presence. [11] [12] [13] Precision agriculture has also been enabled by unmanned aerial vehicles that are relatively inexpensive and can be operated by novice pilots.  These agricultural drones can be equipped with multispectral or RGB cameras to capture many images of a field that can be stitched together using photogrammetric methods to create orthophotos . These multispectral images contain multiple values per pixel in addition to the traditional red, green blue values such as near infrared and red-edge spectrum values used to process and analyze vegetative indexes such as NDVI maps. [14] These drones are capable of capturing imagery and providing additional geographical references such as elevation, which allows software to perform map algebra functions to build precise topography maps. These topographic maps can be used to correlate crop health with topography, the results of which can be used to optimize crop inputs such as water, fertilizer or chemicals such as herbicides and growth regulators through variable rate applications. See also: Timeline of agriculture and food technology Precision agriculture is a key component of the third wave of modern agricultural revolutions .  The first agricultural revolution was the increase of mechanized agriculture , from 1900 to 1930. Each farmer produced enough food to feed about 26 people during this time. [15] The 1960s prompted the Green Revolution with new methods of genetic modification, which led to each farmer feeding about 156 people. [15] It is expected that by 2050, the global population will reach about 9.6 billion, and food production must effectively double from current levels in order to feed every mouth. With new technological advancements in the agricultural revolution of precision farming, each farmer will be able to feed 265 people on the same acreage. [15] Overview[ edit ] The first wave of the precision agricultural revolution came in the forms of satellite and aerial imagery, weather prediction, variable rate fertilizer application, and crop health indicators. [16] The second wave aggregates the machine data for even more precise planting, topographical mapping, and soil data. [17] Precision agriculture aims to optimize field-level management with regard to: crop science : by matching farming practices more closely to crop needs (e.g. fertilizer inputs); environmental protection : by reducing environmental risks and footprint of farming (e.g. limiting leaching of nitrogen); economics : by boosting competitiveness through more efficient practices (e.g. improved management of fertilizer usage and other inputs). Precision agriculture also provides farmers with a wealth of information to: build up a record of their farm enhance marketing of farm products improve lease arrangements and relationship with landlords enhance the inherent quality of farm products (e.g. protein level in bread-flour wheat) Prescriptive planting[ edit ] Prescriptive planting is a type of farming system that delivers data-driven planting advice that can determine variable planting rates to accommodate varying conditions across a single field, in order to maximize yield. It has been described as " Big Data on the farm." Monsanto , DuPont and others are launching this technology in the US. [18] [19] Principles[ edit ] Precision agriculture uses many tools but here are some of the basics: tractors, combines, sprayers, planters, diggers, which are all considered auto-guidance systems. The small devices on the equipment that uses GIS (geographic information system) are what makes precision agriculture what it is. You can think of the GIS system as the “brain.” To be able to use precision agriculture the equipment needs to be wired with the right technology and data systems. More tools include Variable rate technology (VRT), Global positioning system and Geographical information system, Grid sampling, and remote sensors. [20] Geolocating[ edit ] Geolocating a field enables the farmer to overlay information gathered from analysis of soils and residual nitrogen, and information on previous crops and soil resistivity. Geolocation is done in two ways The field is delineated using an in-vehicle GPS receiver as the farmer drives a tractor around the field. The field is delineated on a basemap derived from aerial or satellite imagery. The base images must have the right level of resolution and geometric quality to ensure that geolocation is sufficiently accurate. Variables[ edit ] Intra and inter-field variability may result from a number of factors. These include climatic conditions ( hail , drought, rain, etc.), soils (texture, depth, nitrogen levels), cropping practices ( no-till farming ), weeds and disease. Permanent indicators—chiefly soil indicators—provide farmers with information about the main environmental constants. Point indicators allow them to track a crop's status, i.e., to see whether diseases are developing, if the crop is suffering from water stress , nitrogen stress, or lodging, whether it has been damaged by ice and so on. This information may come from weather stations and other sensors (soil electrical resistivity, detection with the naked eye, satellite imagery, etc.). Soil resistivity measurements combined with soil analysis make it possible to measure moisture content . Soil resistivity is also a relatively simple and cheap measurement. [21] NDVI image taken with small aerial system Stardust II in one flight (299 images mosaic) Using soil maps , farmers can pursue two strategies to adjust field inputs: Predictive approach: based on analysis of static indicators (soil, resistivity , field history, etc.) during the crop cycle . Control approach: information from static indicators is regularly updated during the crop cycle by: sampling: weighing biomass , measuring leaf chlorophyll content, weighing fruit, etc. remote sensing: measuring parameters like temperature (air/ soil ), humidity (air/ soil /leaf), wind or stem diameter is possible thanks to Wireless Sensor Networks [22] and Internet of things (IoT) proxy-detection: in-vehicle sensors measure leaf status; this requires the farmer to drive around the entire field. aerial or satellite remote sensing: multispectral imagery is acquired and processed to derive maps of crop biophysical parameters, including indicators of disease. [23] Airborne instruments are able to measure the amount of plant cover and to distinguish between crops and weeds. [24] Decisions may be based on decision-support models (crop simulation models and recommendation models) based on big data , but in the final analysis it is up to the farmer to decide in terms of business value and impacts on the environment - a role being takenover by artificial intelligence (AI) systems based on machine learning and artificial neural networks . It is important to realize why PA technology is or is not adopted, "for PA technology adoption to occur the farmer has to perceive the technology as useful and easy to use. It might be insufficient to have positive outside data on the economic benefits of PA technology as perceptions of farmers have to reflect these economic considerations." [25] Implementing practices[ edit ] New information and communication technologies make field level crop management more operational and easier to achieve for farmers. Application of crop management decisions calls for agricultural equipment that supports variable-rate technology ( VRT ), for example varying seed density along with variable-rate application (VRA) of nitrogen and phytosanitary products. [26] Precision agriculture uses technology on agricultural equipment (e.g. tractors, sprayers, harvesters, etc.): positioning system (e.g. GPS receivers that use satellite signals to precisely determine a position on the globe); geographic information systems (GIS), i.e., software that makes sense of all the available data; variable-rate farming equipment ( seeder , spreader ). Usage around the world[ edit ] Pteryx UAV , a civilian UAV for aerial photography and photo mapping with roll-stabilised camera head The concept of precision agriculture first emerged in the United States in the early 1980s. In 1985, researchers at the University of Minnesota varied lime inputs in crop fields. It was also at this time that the practice of grid sampling appeared (applying a fixed grid of one sample per hectare). Towards the end of the 1980s, this technique was used to derive the first input recommendation maps for fertilizers and pH corrections. The use of yield sensors developed from new technologies, combined with the advent of GPS receivers, has been gaining ground ever since. Today, such systems cover several million hectares. In the American Midwest (US), it is associated not with sustainable agriculture but with mainstream farmers who are trying to maximize profits by spending money only in areas that require fertilizer. This practice allows the farmer to vary the rate of fertilizer across the field according to the need identified by GPS guided Grid or Zone Sampling. Fertilizer that would have been spread in areas that don't need it can be placed in areas that do, thereby optimizing its use. Around the world, precision agriculture developed at a varying pace. Precursor nations were the United States, Canada and Australia. In Europe, the United Kingdom was the first to go down this path, followed closely by France, where it first appeared in 1997–1998. In Latin America the leading country is Argentina , where it was introduced in the middle 1990s with the support of the National Agricultural Technology Institute . Brazil established a state-owned enterprise, Embrapa , to research and develop sustainable agriculture. The development of GPS and variable-rate spreading techniques helped to anchor precision farming [27] management practices. Today, less than 10% of France's farmers are equipped with variable-rate systems. Uptake of GPS is more widespread, but this hasn't stopped them using precision agriculture services, which supplies field-level recommendation maps. [28] While digital technologies can transform the landscape of agricultural machinery, making mechanization both more precise and more accessible, non-mechanized production is still dominant in many low- and middle-income countries, especially in sub-Saharan Africa. [3] [4] Research on precision agriculture for non-mechanized production is increasing and so is its adoption. [29] [30] [31] Examples include the AgroCares hand-held soil scanner, uncrewed aerial vehicle (UAV) services (also known as drones), and GNSS to map field boundaries and establish land tenure. [32] However, it is not clear how many agricultural producers actually use digital technologies. [32] [33] Precision livestock farming supports farmers in real-time by continuously monitoring and controlling animal productivity, environmental impacts, and health and welfare parameters. [34] Sensors attached to animals or to barn equipment operate climate control and monitor animals’ health status, movement and needs. For example, cows can be tagged with the electronic identification (EID) that allows a milking robot to access a database of udder coordinates for specific cows. [35] Global automatic milking system sales have increased over recent years, [36] but adoption is likely mostly in Northern Europe, [37] and likely almost absent in low- and middle-income countries. [38] Automated feeding machines for both cows and poultry also exist, but data and evidence regarding their adoption trends and drivers is likewise scarce. [3] [4] The economic and environmental benefits of precision agriculture have also been confirmed in China, but China is lagging behind countries such as Europe and the United States because the Chinese agricultural system is characterized by small-scale family-run farms, which makes the adoption rate of precision agriculture lower than other countries. Therefore, China is trying to better introduce precision agriculture technology into its own country and reduce some risks, paving the way for China's technology to develop precision agriculture in the future. [39] In December 2014, the Russian President made an address to the Russian Parliament where he called for a National Technology Initiative (NTI). It is divided into subcomponents such as the FoodNet initiative. The FoodNet initiative contains a set of declared priorities, such as precision agriculture. This field is of special interest to Russia as an important tool in developing elements of the bioeconomy in Russia. [40] [41] Economic and environmental impacts[ edit ] Precision agriculture, as the name implies, means application of precise and correct amount of inputs like water, fertilizer, pesticides etc. at the correct time to the crop for increasing its productivity and maximizing its yields. Precision agriculture management practices can significantly reduce the amount of nutrient and other crop inputs used while boosting yields. [42] Farmers thus obtain a return on their investment by saving on water, pesticide, and fertilizer costs. The second, larger-scale benefit of targeting inputs concerns environmental impacts.  Applying the right amount of chemicals in the right place and at the right time benefits crops, soils and groundwater, and thus the entire crop cycle. [43] Consequently, precision agriculture has become a cornerstone of sustainable agriculture , since it respects crops, soils and farmers.  Sustainable agriculture seeks to assure a continued supply of food within the ecological, economic and social limits required to sustain production in the long term. A 2013 article tried to show that precision agriculture can help farmers in developing countries like India. [44] Precision agriculture reduces the pressure of agriculture on the environment by increasing the efficiency of machinery and putting it into use. For example, the use of remote management devices such as GPS reduces fuel consumption for agriculture, while variable rate application of nutrients or pesticides can potentially reduce the use of these inputs, thereby saving costs and reducing harmful runoff into the waterways. [45] GPS also reduces the amount of compaction to the ground by following previously made guidance lines. This will also allow for less time in the field and reduce the environmental impact of the equipment and chemicals. Precision agriculture produces large quantities of varied sensing data which creates an opportunity to adapt and reuse such data for archaeology and heritage work, enhancing  understanding of archaeology in contemporary agricultural landscapes. [46] Emerging technologies[ edit ] Precision agriculture is an application of breakthrough digital farming technologies. Over $4.6 billion has been invested in agriculture tech companies—sometimes called agtech. [15] Robots[ edit ] Self-steering tractors have existed for some time now, as John Deere equipment works like a plane on autopilot . The tractor does most of the work, with the farmer stepping in for emergencies. [43] Technology is advancing towards driverless machinery programmed by GPS to spread fertilizer or plow land. Autonomy of technology is driven by the demanding need of diagnoses, often difficult to accomplish solely by hands-on farmer-operated machinery. In many instances of high rates of production, manual adjustments cannot sustain. [47] Other innovations include, partly solar powered, machines/robots that identify weeds and precisely kill them with a dose of a herbicide or lasers . [43] [48] [49] Agricultural robots , also known as AgBots, already exist, but advanced harvesting robots are being developed to identify ripe fruits, adjust to their shape and size, and carefully pluck them from branches. [50] Drones and satellite imagery[ edit ] Drone and satellite technology are used in precision farming. This often occurs when drones take high quality images while satellites capture the bigger picture. Aerial photography from light aircraft can be combined with data from satellite records to predict future yields based on the current level of field biomass . Aggregated images can create contour maps to track where water flows, determine variable-rate seeding, and create yield maps of areas that were more or less productive. [43] The Internet of things[ edit ] The Internet of things is the network of physical objects outfitted with electronics that enable data collection and aggregation. IoT comes into play with the development of sensors [51] and farm-management software. For example, farmers can spectroscopically measure nitrogen, phosphorus, and potassium in liquid manure , which is notoriously inconsistent. [43] They can then scan the ground to see where cows have already urinated and apply fertilizer to only the spots that need it. This cuts fertilizer use by up to 30%. [50] Moisture sensors [52] in the soil determine the best times to remotely water plants. The irrigation systems can be programmed to switch which side of tree trunk they water based on the plant's need and rainfall. [43] Innovations are not just limited to plants—they can be used for the welfare of animals. Cattle can be outfitted with internal sensors to keep track of stomach acidity and digestive problems. External sensors track movement patterns to determine the cow's health and fitness, sense physical injuries, and identify the optimal times for breeding. [43] All this data from sensors can be aggregated and analyzed to detect trends and patterns. As another example, monitoring technology can be used to make beekeeping more efficient. Honeybees are of significant economic value and provide a vital service to agriculture by pollinating a variety of crops. Monitoring of a honeybee colony's health via wireless temperature, humidity and CO2 sensors helps to improve the productivity of bees, and to read early warnings in the data that might threaten the very survival of an entire hive. [53] Smartphone applications[ edit ] A possible configuration of a smartphone-integrated precision agriculture system Smartphone and tablet applications are becoming increasingly popular in precision agriculture. Smartphones come with many useful applications already installed, including the camera, microphone, GPS, and accelerometer. There are also applications made dedicated to various agriculture applications such as field mapping, tracking animals, obtaining weather and crop information, and more. They are easily portable, affordable, and have high computing power. [54] Machine learning[ edit ] Machine learning is commonly used in conjunction with drones, robots, and internet of things devices. It allows for the input of data from each of these sources. The computer then processes this information and sends the appropriate actions back to these devices. This allows for robots to deliver the perfect amount of fertilizer or for IoT devices to provide the perfect quantity of water directly to the soil. [55] Machine learning may also provide predictions to farmers at the point of need, such as the contents of plant-available nitrogen in soil , to guide fertilization planning. [56] As more agriculture becomes ever more digital, machine learning will underpin efficient and precise farming with less manual labour.
Toggle the table of contents Urban planning From Wikipedia, the free encyclopedia Technical and political process concerned with the use of land and design of the urban environment "Urban development" redirects here. For actual development, see urbanization . For negative effects, see urban sprawl . "Development planning" redirects here. For proposals setting out a local authority's policies and proposals for land use, see Development plan . For planning for personal development, see Personal development planning . Partizánske in Slovakia – an example of a typical planned European industrial city founded in 1938 together with a shoemaking factory in which practically all adult inhabitants of the city were employed Urban planning, also known as town planning, city planning, regional planning, or rural planning in specific contexts, is a technical and political process that is focused on the development and design of land use and the built environment, including air, water, and the infrastructure passing into and out of urban areas , such as transportation , communications , and distribution networks , and their accessibility . [1] Traditionally, urban planning followed a top-down approach in master planning the physical layout of human settlements . [2] The primary concern was the public welfare , [1] [2] which included considerations of efficiency, sanitation , protection and use of the environment, [1] as well as effects of the master plans on the social and economic activities. [3] Over time, urban planning has adopted a focus on the social and environmental bottom lines that focus on planning as a tool to improve the health and well-being of people, maintaining sustainability standards. Sustainable development was added as one of the main goals of all planning endeavors in the late 20th century when the detrimental economic and the environmental impacts of the previous models of planning had become apparent.[ citation needed ] Similarly, in the early 21st century, Jane Jacobs 's writings on legal and political perspectives to emphasize the interests of residents, businesses and communities effectively influenced urban planners to take into broader consideration of resident experiences and needs while planning. Urban planning answers questions about how people will live, work, and play in a given area and thus, guides orderly development in urban, suburban and rural areas . [4] Although predominantly concerned with the planning of settlements and communities, urban planners are also responsible for planning the efficient transportation of goods, resources, people, and waste; the distribution of basic necessities such as water and electricity; a sense of inclusion and opportunity for people of all kinds, culture and needs; economic growth or business development; improving health and conserving areas of natural environmental significance that actively contributes to reduction in CO2 emissions [5] as well as protecting heritage structures and built environments. Since most urban planning teams consist of highly educated individuals that work for city governments, [6] recent debates focus on how to involve more community members in city planning processes. Urban planning is an interdisciplinary field that includes civil engineering , architecture , human geography , politics , social science and design sciences . Practitioners of urban planning are concerned with research and analysis, strategic thinking, engineering architecture, urban design, public consultation , policy recommendations, implementation and management. [2] It is closely related to the field of urban design and some urban planners provide designs for streets, parks, buildings and other urban areas. [7] Urban planners work with the cognate fields of civil engineering, landscape architecture , architecture, and public administration to achieve strategic, policy and sustainability goals. Early urban planners were often members of these cognate fields though today, urban planning is a separate, independent professional discipline. The discipline of urban planning is the broader category that includes different sub-fields such as land-use planning , zoning , economic development , environmental planning , and transportation planning . [8] Creating the plans requires a thorough understanding of penal codes and zonal codes of planning. Another important aspect of urban planning is that the range of urban planning projects include the large-scale master planning of empty sites or Greenfield projects as well as small-scale interventions and refurbishments of existing structures, buildings and public spaces. Pierre Charles L'Enfant in Washington, D.C., Daniel Burnham in Chicago, Lúcio Costa in Brasília and Georges-Eugene Haussmann in Paris planned cities from scratch, and Robert Moses and Le Corbusier refurbished and transformed cities and neighborhoods to meet their ideas of urban planning. [9] 1852 city plan of Pori by G. T. von Chiewitz Berlin – Siegessäule. August 1963. Spacious and organized city planning in Germany was official government policy dating back to Nazi rule. [10] There is evidence of urban planning and designed communities dating back to the Mesopotamian , Indus Valley , Minoan , and Egyptian civilizations in the third millennium BCE . Archaeologists studying the ruins of cities in these areas find paved streets that were laid out at right angles in a grid pattern. [11] The idea of a planned out urban area evolved as different civilizations adopted it. Beginning in the 8th century BCE, Greek city states primarily used orthogonal (or grid-like) plans. [12] Hippodamus of Miletus (498–408 BC), the ancient Greek architect and urban planner, is considered to be "the father of European urban planning", and the namesake of the "Hippodamian plan" (grid plan) of city layout. [13] The ancient Romans also used orthogonal plans for their cities. City planning in the Roman world was developed for military defense and public convenience. The spread of the Roman Empire subsequently spread the ideas of urban planning. As the Roman Empire declined, these ideas slowly disappeared. However, many cities in Europe still held onto the planned Roman city center. Cities in Europe from the 9th to 14th centuries, often grew organically and sometimes chaotically. But in the following centuries with the coming of the Renaissance many new cities were enlarged with newly planned extensions. [14] From the 15th century on, much more is recorded of urban design and the people that were involved. In this period, theoretical treatises on architecture and urban planning start to appear in which theoretical questions around planning the main lines, ensuring plans meet the needs of the given population and so forth are addressed and designs of towns and cities are described and depicted. During the Enlightenment period , several European rulers ambitiously attempted to redesign capital cities. During the Second French Empire , Baron Georges-Eugène Haussmann , under the direction of Napoleon III , redesigned the city of Paris into a more modern capital, with long, straight, wide boulevards. [15] Planning and architecture went through a paradigm shift at the turn of the 20th century. The industrialized cities of the 19th century grew at a tremendous rate. The evils of urban life for the working poor were becoming increasingly evident as a matter of public concern. The laissez-faire style of government management of the economy, in fashion for most of the Victorian era , was starting to give way to a New Liberalism that championed intervention on the part of the poor and disadvantaged. Around 1900, theorists began developing urban planning models to mitigate the consequences of the industrial age , by providing citizens, especially factory workers, with healthier environments. The following century would therefore be globally dominated by a central planning approach to urban planning, not representing an increment in the overall quality of the urban realm. At the beginning of the 20th century, urban planning began to be recognized as a separate profession. The Town and Country Planning Association was founded in 1899 and the first academic course in Great Britain on urban planning was offered by the University of Liverpool in 1909. [16] In the 1920s, the ideas of modernism and uniformity began to surface in urban planning, and lasted until the 1970s. In 1933, Le Corbusier presented the Radiant City, a city that grows up in the form of towers, as a solution to the problem of pollution and over-crowding. But many planners started to believe that the ideas of modernism in urban planning led to higher crime rates and social problems. [3] [17] In the second half of the 20th century, urban planners gradually shifted their focus to individualism and diversity in urban centers. [18] 21st century practices[ edit ] See also: Mobility transition Urban planners studying the effects of increasing congestion in urban areas began to address the externalities, the negative impacts caused by induced demand from larger highway systems in western countries such as in the United States. The United Nations Department of Economic and Social Affairs predicted in 2018 that around 2.5 billion more people occupy urban areas by 2050 according to population elements of global migration. New planning theories have adopted non-traditional concepts such as Blue Zones and Innovation Districts to incorporate geographic areas within the city that allow for novel business development and the prioritization of infrastructure that would assist with improving the quality of life of citizens by extending their potential lifespan. Planning practices have incorporated policy changes to help address anthropocentric global climate change . London began to charge a congestion charge for cars trying to access already crowded places in the city. [19] Cities nowadays stress the importance of public transit and cycling by adopting such policies. See also: Planning cultures Street Hierarchy and Accessibility Planning theory is the body of scientific concepts, definitions, behavioral relationships, and assumptions that define the body of knowledge of urban planning. There are eight procedural theories of planning that remain the principal theories of planning procedure today: the rational-comprehensive approach, the incremental approach, the transactive approach, the communicative approach, the advocacy approach, the equity approach, the radical approach, and the humanist or phenomenological approach. [20] Some other conceptual planning theories include Ebenezer Howard 's The Three Magnets theory that he envisioned for the future of British settlement, also his Garden Cities , the Concentric Model Zone also called the Burgess Model by sociologist Ernest Burgess , the Radburn Superblock that encourages pedestrian movement, the Sector Model and the Multiple Nuclei Model among others. [21] Further information: Technical aspects of urban planning Technical aspects of urban planning involve the application of scientific, technical processes, considerations and features that are involved in planning for land use , urban design , natural resources , transportation , and infrastructure . Urban planning includes techniques such as: predicting population growth , zoning , geographic mapping and analysis, analyzing park space, surveying the water supply , identifying transportation patterns, recognizing food supply demands, allocating healthcare and social services, and analyzing the impact of land use. In order to predict how cities will develop and estimate the effects of their interventions, planners use various models. These models can be used to indicate relationships and patterns in demographic, geographic, and economic data. They might deal with short-term issues such as how people move through cities, or long-term issues such as land use and growth. [22] One such model is the Geographic Information System (GIS) that is used to create a model of the existing planning and then to project future impacts on the society, economy and environment. Building codes and other regulations dovetail with urban planning by governing how cities are constructed and used from the individual level. [23] Enforcement methodologies include governmental zoning , planning permissions , and building codes , [1] as well as private easements and restrictive covenants . [24] Further information: Urban planner An urban planner is a professional who works in the field of urban planning for the purpose of optimizing the effectiveness of a community's land use and infrastructure. They formulate plans for the development and management of urban and suburban areas. They typically analyze land use compatibility as well as economic, environmental, and social trends. In developing any plan for a community (whether commercial, residential, agricultural, natural or recreational), urban planners must consider a wide array of issues including sustainability , existing and potential pollution , transport including potential congestion , crime , land values, economic development, social equity, zoning codes, and other legislation. The importance of the urban planner is increasing in the 21st century, as modern society begins to face issues of increased population growth, climate change and unsustainable development. [25] [26] An urban planner could be considered a green collar professional. [27] Some researchers suggest that urban planners, globally, work in different " planning cultures ", adapted to their cities and cultures. [28] However, professionals have identified skills, abilities, and basic knowledge sets that are common to urban planners across regional and national boundaries. [29] [30] [31] Participatory urban planning[ edit ] Participatory planning is an urban planning paradigm that involves the entire community in the planning process. Participatory planning in the United States emerged during the 1960s and 1970s. [32] At the same time, participatory planning began to enter the development field, with similar characteristics and agendas. [33] There are many notable urban planners and activists whose work facilitated and shaped participatory planning movements. Jane Jacobs and her work is one of the most significant contributions to participatory planning because of the influence it had across the entire United States. There has also been a recent emergence in engaging youth in urban planning education .[ citation needed ] Criticisms and debates[ edit ] The school of neoclassical economics argues that planning is unnecessary, or even harmful, as it market efficiency allows for effective land use. [34] A pluralist strain of political thinking argues in a similar vein that the government should not intrude in the political competition between different interest groups which decides how land is used. [34] The traditional justification for urban planning has in response been that the planner does to the city what the engineer or architect does to the home, that is, make it more amenable to the needs and preferences of its inhabitants. [34] The widely adopted consensus-building model of planning, which seeks to accommodate different preferences within the community has been criticized for being based upon, rather than challenging, the power structures of the community. [35] Instead, agonism has been proposed as a framework for urban planning decision-making. [35] Another debate within the urban planning field is about who is included and excluded in the urban planning decision-making process. Most urban planning processes use a top-down approach which fails to include the residents of the places where urban planners and city officials are working. Sherry Arnstein 's "ladder of citizen participation" is oftentimes used by many urban planners and city governments to determine the degree of inclusivity or exclusivity of their urban planning. [36] One main source of engagement between city officials and residents are city council meetings that are open to the residents and that welcome public comments. Additionally, there are some federal requirements for citizen participation in government-funded infrastructure projects. [6] Many urban planners and planning agencies rely on community input for their policies and zoning plans. The effectiveness of community engagement can be determined by how members' voices are heard and implemented.
Toggle the table of contents Environmental impact assessment Wikimedia Commons From Wikipedia, the free encyclopedia This article may have confusing or ambiguous abbreviations . Please review the Manual of Style , help improve this article , and discuss this issue on the talk page . (October 2016) ( Assessment of the environmental consequences of a decision before action Part of a series on e Environmental Impact assessment (EIA) is the assessment of the environmental consequences of a plan, policy, program, or actual projects prior to the decision to move forward with the proposed action. In this context, the term "environmental impact assessment" is usually used when applied to actual projects by individuals or companies and the term " strategic environmental assessment " (SEA) applies to policies, plans and programmes most often proposed by organs of state. [1] [2] It is a tool of environmental management forming a part of project approval and decision-making. [3] Environmental assessments may be governed by rules of administrative procedure regarding public participation and documentation of decision making, and may be subject to judicial review. The purpose of the assessment is to ensure that decision-makers consider the environmental impacts when deciding whether or not to proceed with a project. The International Association for Impact Assessment (IAIA) defines an environmental impact assessment as "the process of identifying, predicting, evaluating and mitigating the biophysical , social, and other relevant effects of development proposals prior to major decisions being taken and commitments made". [4] EIAs are unique in that they do not require adherence to a predetermined environmental outcome, but rather they require decision-makers to account for environmental values in their decisions and to justify those decisions in light of detailed environmental studies and public comments on the potential environmental impacts. [5] History[ edit ] Environmental Impact Assessments commenced in the 1960s, as part of increasing environmental awareness . [6] An EIA is prepared to estimate the effects of a proposed development or construction project. EIA provides technical evaluations that are intended to contribute to more objective decision making. In the United States, EIA obtained formal status in 1969, with the enactment of the National Environmental Policy Act (NEPA). EIAs have been used increasingly around the world. The number of environmental assessments filed every year "has vastly overtaken the number of more rigorous Environmental Impact Statements (EIS)." [7] An environmental assessment is a "mini-Environmental Impact Statement (EIS) designed to provide sufficient information to allow the agency to decide whether the preparation of a full-blown Environmental Impact Statement (EIS) is necessary." [8] [9] Methods[ edit ] General and industry specific assessment methods are available including: Industrial products – Product environmental life cycle analysis (LCA) is used for identifying and measuring the impact of industrial products on the environment. These EIAs consider activities related to extraction of raw materials, ancillary materials, equipment; production, use, disposal and ancillary equipment. [10] Genetically modified plants – Specific methods available to perform EIAs of genetically modified organisms include GMP-RAM and INOVA. [11] Fuzzy logic – EIA methods need measurement data to estimate values of impact indicators. However, many of the environment impacts cannot be quantified, e.g. landscape quality, lifestyle quality and social acceptance . Instead, information from similar EIAs, expert judgment and community sentiment are employed. Approximate reasoning methods known as fuzzy logic can be used. [12] A fuzzy arithmetic approach has also been proposed [13] and implemented using a software tool (TDEIA). [14] Follow-up[ edit ] At the end of the project, an audit evaluates the accuracy of the EIA by comparing actual to predicted impacts. The objective is to make future EIAs more valid and effective. Two primary considerations are: Scientific – to examine the accuracy of predictions and explain errors[ citation needed ] Management – to assess the success of mitigation in reducing impacts[ citation needed ] Audits can be performed either as a rigorous assessment of the null hypothesis or with a simpler approach comparing what actually occurred against the predictions in the EIA document. [15] After an EIA, the precautionary and polluter pays principles may be applied to decide whether to reject, modify or require strict liability or insurance coverage to a project, based on predicted harms.[ citation needed ] The Hydropower Sustainability Assessment Protocol is a sector-specific method for checking the quality of Environmental and Social assessments and management plans.[ citation needed ] Around the world[ edit ] Australia[ edit ] The history of EIA in Australia could be linked to the enactment of the U.S. National Environment Policy Act (NEPA) in 1970, which made the preparation of environmental impact statements a requirement. In Australia, one might say that the EIA procedures were introduced at a State Level prior to that of the Commonwealth (Federal), with a majority of the states having divergent views to the Commonwealth. One of the pioneering states was New South Wales, whose State Pollution Control Commission issued EIA guidelines in 1974. At a Commonwealth (i.e. Federal) level, this was followed by passing of the Environment Protection (Impact of Proposals) Act 1974 (Cth) in 1974. The Environment Protection and Biodiversity Conservation Act 1999 (Cth) (EPBC Act) superseded the Environment Protection (Impact of Proposals) Act 1974 (Cth) and is the current central piece for EIA in Australia on a Commonwealth (i.e. Federal) level. An important point to note is that this federal legislation does not override the validity of the States or Territories environmental and development assessments and approvals; rather the EPBC Act runs as a parallel to the State/Territory Systems. [16] Overlap between federal and state requirements is addressed via bilateral agreements or one-off accreditation of state processes, as provided for in the EPBC Act.[ citation needed ] The Commonwealth Level[ edit ] The EPBC Act provides a legal framework to protect and manage nationally and internationally important flora, fauna, ecological communities and heritage places. It defines this as matters of "national environmental significance". The following are the nine matters of such significance: [17] World Heritage properties; Wetlands of international importance (listed under the Ramsar Convention); Listed threatened species and ecological communities; Migratory species protected under international agreements; Commonwealth marine areas; Nuclear actions (including uranium mining); and Water resources, in relation with coal seam gas development and large coal mining development. In addition to this, the EPBC Act aims at providing a streamlined national assessment and approval process for activities. These activities could be by the Commonwealth, or its agents, anywhere in the world or activities on Commonwealth land; and activities that are listed as having a 'significant impact' on matters of 'national environment significance'. [17] The EPBC Act comes into play when a person (a proponent) wants an action (often called "a proposal" or "a project") assessed for environmental impacts under the EPBC Act, he or she must refer the project to the Department of the Environment and Energy (Commonwealth). This referral is then released to the public and the relevant state, territory and Commonwealth ministers, for comment on whether the project is likely to have a significant impact on matters of national environmental significance. [17] The Department of the Environment and Energy assess the process and makes recommendation to the minister or the delegate for the feasibility. The final discretion on the decision remains of the minister, which is not solely based on matters of national environmental significance but also on the consideration of social and economic impact of the project. [17] The Australian Government Minister for the Environment and Energy cannot intervene in a proposal if it has no significant impact on one of the eight matters of national environmental significance, regardless of any other undesirable environmental impacts. [17] This is primarily due to the division of powers between the states and the Federal government, and the Australian Government environment minister not being able to overturn a state decision.[ citation needed ] There are strict civil and criminal penalties for the breach of EPBC Act. Depending on the kind of breach, civil penalty (maximum) may go up to $550,000 for an individual and $5.5 million for a body corporate, or for criminal penalty (maximum) of seven years imprisonment and/or penalty of $46,200. [17] The State and Territory Level[ edit ] Australian Capital Territory (ACT)[ edit ] EIA provisions within Ministerial Authorities in the ACT are found in the Chapters 7 and 8 of the Planning and Development Act 2007 (ACT). EIA in ACT was previously administered with the help of Part 4 of the Land (Planning and Environment) Act 1991 (Land Act) and Territory Plan (plan for land-use). [16] Note that some EIA may occur in the ACT on Commonwealth land under the EPBC Act (Cth). Further provisions of the Australian Capital Territory (Planning and Land Management) Act 1988 (Cth) may also be applicable particularly to national land and "designated areas".[ citation needed ] New South Wales (NSW)[ edit ] In New South Wales, the Environment Planning and Assessment Act 1979 (EP&A Act) establishes two pathways for EIA. The first is under Division 5.2 of the EP&A Act, which provides for EIA of 'State Significant Infrastructure' projects (from June 2011, this Part replaced the previous Part 3A, which previously covered EIA of major projects). The second is under Part 4 of the EP&A Act dealing with development assessments for local, regional, and State Significant Developments (other than State Significant Infrastructure). [16] Northern Territory (NT)[ edit ] The EIA process in Northern Territory is chiefly administered under the Environmental Assessment Act (EAA). [18] Although EAA is the primary tool for EIA in Northern Territory, there are further provisions for proposals in the Inquiries Act 1985 (NT). [16] Queensland (QLD)[ edit ] There are four main EIA processes in Queensland. [19] Firstly, under the Integrated Planning Act 1997 (IPA) for development projects other than mining. Secondly, under the Chapter 3 of the Environmental Protection Act 1994 (Qld) (EP Act) for some mining and petroleum activities. Thirdly, under the State Development and Public Works Organisation Act 1971 (Qld) (State Development Act) for 'significant projects'. Finally, under the Environment Protection and Biodiversity Conservation Act 1999 (Cth) for 'controlled actions'. [19] South Australia (SA) The local governing tool for EIA in South Australia is the Development Act 1993 (SA). There are three levels of assessment possible under the Act in the form of an environment impact statement (EIS), a public environmental report (PER) or a Development Report (DR). [16] Tasmania (TAS)[ edit ] In Tasmania, an integrated system of legislation is used to govern development and approval process, this system is a mixture of the Environmental Management and Pollution Control Act 1994 (Tas) (EMPC Act), Land Use Planning and Approvals Act 1993 (Tas) (LUPA Act), State Policies and Projects Act 1993 (Tas) (SPPA), and Resource Management and Planning Appeals Tribunal Act 1993 (Tas). [16] Victoria (VIC)[ edit ] The EIA process in Victoria is intertwined with the Environment Effects Act 1978 (Vic) and the Ministerial Guidelines for Assessment of Environmental Effects (made under the s 10 of the EE Act). [20] Western Australia (WA)[ edit ] Part 4 of the Environmental Protection Act 1986 (WA) provides the legislative framework for the EIA process in Western Australia. [21] The EPA Act oversees the planning and development proposals and assesses their likely impacts on the environment.[ citation needed ] Canada[ edit ] In Friends of the Oldman River Society v. Canada (Minister of Transportation),( SCC 1992 ) La Forest J of the Supreme Court of Canada described environmental impact assessment in terms of the proper scope of federal jurisdiction with respect to environments matters, "Environmental impact assessment is, in its simplest form, a planning tool that is now generally regarded as an integral component of sound decision-making." [22] Supreme Court Justice La Forest cited ( Cotton & Emond 1981 , p. 245), "The basic concepts behind environmental assessment are simply stated: (1) early identification and evaluation of all potential environmental consequences of a proposed undertaking; (2) decision making that both guarantees the adequacy of this process and reconciles, to the greatest extent possible, the proponent's development desires with environmental protection and preservation." [23] La Forest referred to ( Jeffery 1989 , 1.2,1.4) and ( Emond 1978 , p. 5) who described "...environmental assessments as a planning tool with both an information-gathering and a decision-making component" that provide "...an objective basis for granting or denying approval for a proposed development." [24] [25] Justice La Forest addressed his concerns about the implications of Bill C-45 regarding public navigation rights on lakes and rivers that would contradict previous cases.( La Forest 1973 , pp. 178–80) [26] The Canadian Environmental Assessment Act 2012 (CEAA 2012) [27] "and its regulations establish the legislative basis for the federal practice of environmental assessment in most regions of Canada." [28] [29] [30] CEAA 2012 came into force July 6, 2012 and replaces the former Canadian Environmental Assessment Act (1995). EA is defined as a planning tool to identify, understand, assess and mitigate, where possible, the environmental effects of a project. "The purposes of this Act are: (a) to protect the components of the environment that are within the legislative authority of Parliament from significant adverse environmental effects caused by a designated project; (b) to ensure that designated projects that require the exercise of a power or performance of a duty or function by a federal authority under any Act of Parliament other than this Act to be carried out, are considered in a careful and precautionary manner to avoid significant adverse environmental effects; (c) to promote cooperation and coordinated action between federal and provincial governments with respect to environmental assessments; (d) to promote communication and cooperation with aboriginal peoples with respect to environmental assessments; (e) to ensure that opportunities are provided for meaningful public participation during an environmental assessment; (f) to ensure that an environmental assessment is completed in a timely manner; (g) to ensure that projects, as defined in section 66, that are to be carried out on federal lands, or those that are outside Canada and that are to be carried out or financially supported by a federal authority, are considered in a careful and precautionary manner to avoid significant adverse environmental effects; (h) to encourage federal authorities to take actions that promote sustainable development in order to achieve or maintain a healthy environment and a healthy economy; and (i) to encourage the study of the cumulative effects of physical activities in a region and the consideration of those study results in environmental assessments." [31] Opposition[ edit ] Environmental Lawyer Dianne Saxe argued that the CEAA 2012 "allows the federal government to create mandatory timelines for assessments of even the largest and most important projects, regardless of public opposition." [32] "Now that federal environmental assessments are gone, the federal government will only assess very large, very important projects. But it's going to do them in a hurry." Marathon Platinum Group Metals and Copper Mine Project (JRP): [33] 13 months; Site C Clean Energy Project (JRP) 8.5 months; Deep Geologic Repository Project (JRP) 17 months; New Prosperity Gold-Copper Mine Project (JRP) 7.5 months; Frontier Oil Sands Mine Project (JRP) [35] [36] 8.5 months; EnCana / Cenovus Shallow Gas Infill Project (JRP) 5 months. [37] Saxe compares these timelines with environmental assessments for the Mackenzie Valley Pipeline . Thomas R. Berger , Royal Commissioner of the Mackenzie Valley Pipeline Inquiry (9 May 1977), worked extremely hard to ensure that industrial development on Aboriginal people 's land resulted in benefits to those indigenous people . [38] On 22 April 2013, NDP MP Megan Leslie issued a statement claiming that the Harper government 's recent changes to "fish habitat protection, the Navigable Waters Protection Act and the Canadian Environmental Assessment Act", along with gutting existing laws and making cuts to science and research, "will be disastrous, not only for the environment but also for Canadians' health and economic prosperity." [39] On 26 September 2012, Leslie argued that with the changes to the Canadian Environmental Assessment Act that came into effect 6 July 2012, " seismic testing , dams, wind farms and power plants" no longer required any federal environmental assessment. She also claimed that because the CEAA 2012—which she claimed was rushed through Parliament—dismantled the CEAA 1995, the Oshawa ethanol plant project would no longer have a full federal environmental assessment. [40] Mr. Peter Kent (Minister of the Environment) explained that the CEAA 2012 "provides for the Government of Canada and the Environmental Assessment Agency to focus on the large and most significant projects that are being proposed across the country." The 2,000 to 3,000-plus smaller screenings that were in effect under CEAA 1995 became the "responsibility of lower levels of government but are still subject to the same strict federal environmental laws ." [40] Anne Minh-Thu Quach, MP for Beauharnois—Salaberry, QC, argued that the mammoth budget bill dismantled 50 years of environmental protection without consulting Canadians about the "colossal changes they are making to environmental assessments." She claimed that the federal government is entering into "limited consultations, by invitation only, months after the damage was done." [40] China[ edit ] The Environmental Impact Assessment Law (EIA Law) requires that an environmental impact assessment be completed prior to project construction. However, if a developer completely ignores this requirement and builds a project without submitting an environmental impact statement, the only penalty is that the environmental protection bureau (EPB) may require the developer to do a make-up environmental assessment. If the developer does not complete this make-up assessment within the designated time, only then is the EPB authorized to fine the developer. Even so, the possible fine is capped at a maximum of about US$25,000, a fraction of the overall cost of most major projects. The lack of more stringent enforcement mechanisms has resulted in a significant percentage of projects not completing legally required environmental impact assessments prior to construction. [41] China's State Environmental Protection Administration (SEPA) used the legislation to halt 30 projects in 2004, including three hydro-power plants under the Three Gorges Project Company. Although one month later (Note as a point of reference, that the typical EIA for a major project in the USA takes one to two years.), most of the 30 halted projects resumed their construction, reportedly having passed the environmental assessment, the fact that these key projects' construction was ever suspended was notable.[ citation needed ] A joint investigation by SEPA and the Ministry of Land and Resources in 2004 showed that 30–40% of the mining construction projects went through the procedure of environment impact assessment as required, while in some areas only 6–7% did so. This partly explains why China has witnessed so many mining accidents in recent years.[ citation needed ] SEPA alone cannot guarantee the full enforcement of environmental laws and regulations, observed Professor Wang Canfa , director of the centre to help environmental victims at China University of Political Science and Law . In fact, according to Wang, the rate of China's environmental laws and regulations that are actually enforced is estimated at barely 10%. [42] Egypt[ edit ] Environmental Impact Assessment (EIA) EIA is implemented in Egypt under the umbrella of the Ministry of state for environmental affairs. The Egyptian Environmental Affairs Agency (EEAA) is responsible for the EIA services. [43] In June 1997, the responsibility of Egypt's first full-time Minister of State for Environmental Affairs was assigned as stated in the Presidential Decree no.275/1997. From thereon, the new ministry has focused, in close collaboration with the national and international development partners, on defining environmental policies, setting priorities and implementing initiatives within a context of sustainable development. [44] According to the Law 4/1994 for the Protection of the Environment, the Egyptian Environmental Affairs Agency (EEAA) was restructured with the new mandate to substitute the institution initially established in 1982. At the central level, EEAA represents the executive arm of the Ministry. [45] The purpose of EIA is to ensure the protection and conservation of the environment and natural resources including human health aspects against uncontrolled development. The long-term objective is to ensure a sustainable economic development that meets present needs without compromising future generations ability to meet their own needs. EIA is an important tool in the integrated environmental management approach. [46] EIA must be performed for new establishments or projects and for expansions or renovations of existing establishments according to the Law for the Environment. [47] EU[ edit ] A wide range of instruments exist in the Environmental policy of the European Union . Among them the European Union has established a mix of mandatory and discretionary procedures to assess environmental impacts. [48] Directive (85/337/EEC) on Environmental Impact Assessments (known as the EIA Directive) [49] was first introduced in 1985, amended in 1997, amended again in 2003 following EU signature of the 1998 Aarhus Convention , and once more in 2009. [50] The initial Directive of 1985 and its three amendments have been codified in Directive 2011/92/EU of 13 December 2011. [51] In 2001, the issue was enlarged to include the assessment of plans and programmes by the so-called Strategic Environmental Assessment (SEA) Directive (2001/42/EC), which was amended by Directive 2014/52/EU of 16 April 2014. [52] [48] Under the EU directive, a compliant EIA must provide certain information in seven key areas: [53] Description of the project Description of actual project and site description Break the project down into its key components, i.e. construction, operations, decommissioning For each component list all of the sources of environmental disturbance For each component all the inputs and outputs must be listed, e.g., air pollution , noise, hydrology Alternatives that have been considered Examine alternatives that have been considered Example: in a biomass power station, will the fuel be sourced locally or nationally? Description of the environment List of all aspects of the environment that may be affected by the development Example: populations, fauna , flora, air, soil, water, humans, landscape, cultural heritage This section is best carried out with the help of local experts, e.g. the RSPB in the UK Description of the significant effects on the environment The word significant is crucial here as the definition can vary 'Significant' must be defined The most frequent method used here is use of the Leopold matrix The matrix is a tool used in the systematic examination of potential interactions Example: in a windfarm development a significant impact may be collisions with birds Mitigation This is where EIA is most useful Once section 4 is complete, it is obvious where impacts are greatest Using this information in ways to avoid negative impacts should be developed Best working with the developer with this section as they know the project best Using the windfarm example again, construction might take place outside of bird nesting seasons, or removal of hardstanding on a potentially contaminated land site might take place outside of the rainy season . Non-technical summary (EIS) The EIA is in the public domain and be used in the decision-making process It is important that the information is available to the public This section is a summary that does not include jargon or complicated diagrams It should be understood by the informed lay-person Lack of know-how/technical difficulties This section is to advise any areas of weakness in knowledge It can be used to focus areas of future research Some developers see the EIA as a starting block for poor environmental management In 2021, ESG reporting requirements changed in the EU and UK. The EU started enforcing the Sustainable Finance Disclosures Regulation (SFDR), which was created with the purpose of unifying climate risk disclosures across the private sector by 2023. It also requires businesses to report on "principal adverse impacts" for society and the environment. [54] Annexed projects[ edit ] All projects are either classified as Annex 1 or Annex 2 projects. Those lying in Annex 1 are large scale developments such as motorways, chemical works, bridges, power stations, etc. These always require an EIA under the Environmental Impact Assessment Directive (85,337,EEC as amended). Annex 2 projects are smaller in scale than those referred to in Annex 1. Member States must determine whether these project shall be made subject to an assessment subject to a set of criteria set out in Annex 3 of codified Directive 2011/92/EU.[ citation needed ] The Netherlands[ edit ] EIA was implemented in Dutch legislation on September 1, 1987. The categories of projects which require an EIA are summarised in Dutch legislation, the Wet milieubeheer. The use of thresholds for activities makes sure that EIA is obligatory for those activities that may have considerable impacts on the environment.[ citation needed ] For projects and plans which fit these criteria, an EIA report is required. The EIA report defines a.o. the proposed initiative, it makes clear the impact of that initiative on the environment and compares this with the impact of possible alternatives with less a negative impact. [55] United Kingdom[ edit ] The EU Directives concerning environmental impact assessment are implemented in England through the Town and Country Planning (Environmental Impact Assessment) Regulations 2017, which also apply to projects serving national defence purposes in Northern Ireland , Scotland and Wales . [56] Hong Kong[ edit ] EIA in Hong Kong is regulated by the Environmental Impact Assessment Ordinance 1997, which became effective in 1998.[ citation needed ] The original proposal to construct the Lok Ma Chau Spur Line overground across the Long Valley failed to get through EIA, and the Kowloon–Canton Railway Corporation had to change its plan and build the railway underground. In April 2011, the EIA of the Hong Kong section of the Hong Kong-Zhuhai-Macau Bridge was found to have breached the ordinance, and was declared unlawful. The appeal by the government was allowed in September 2011. However, it was estimated that this EIA court case had increased the construction cost of the Hong Kong section of the bridge by HK$6.5 billion in money-of-the-day prices. [57] Iraq[ edit ] The Ministry of Environment (MOE) of the federal government of Iraq is in charge of issuing environmental compliance certificates based on an EIA report prepared by professional consultant and thoroughly reviewed by the MOE. Any project or activity prior to its establishment or even already existing project has to be approved and obtain such certificate from the MOE. Projects are classified into 3 categories; “A”, “B” and “C”. EIA reporting is usually obligatory for those projects and activities falling under categories “A” (large-scale) and “B” (small-scale) that may have considerable impacts on environment. [58] Examples of “A” category activities include dams and reservoirs, forestry production projects, industrial plants, irrigation, drainage and flood control, land clearance and leveling, port and harbor development, river basin development, thermal power and hydro-power development, manufacture, transportation and use of pesticides or other hazardous materials, hazardous waste management and disposal... etc. Examples of “B” category activities include agro-industries, electrical transmission, renewable energy, rural electrification, tourism, rehabilitation or maintenance of highway or rural roads, rehabilitation or modification of existing industrial facilities... etc. Preparation of an EIA report is usually exempt for projects falling under category “C” which may have low to no impact on environment, such as small fish breeding ponds, institutional development, most human resources projects...etc.[ citation needed ] The main environmental legislation in Iraq is: Law No.64 for cities and land use (1965), Law No.21 for noise prevention (1966), Law No.25 for system of rivers and other water resources protection (1967), Law No.99 for ionized radiation (1980), Law No.89 for public health (drinking water provision, sanitation and environmental monitoring (1981), Law No.79 for protection and improvement of environment (1986), Environmental criteria for agricultural, industrial and public service projects (1990), Law No.3 for protection and improvement of environment (1997), Law No.2 for water systems protection (2001), Law No.44 for creation of Ministry of Environment instead of the council of protection and improvement of environment (2003), Law No.27 for environmental protection and improvement (2009), [59] Law No.4 for protection of ambient air system (2012).[ citation needed ] Meanwhile, Environmental Protection and Improvement Board in the regional government of Kurdistan in the northern Iraq (Erbil, Duhok, Sulaimany and Garmyan) is responsible of issuing Environmental compliance certificate, the board was established according to law No.3 Environmental protection and improvement board in Iraqi Kurdistan Region (2010). [60] The board is responsible of issuing such certificate for all projects and activities except of petroleum operation which EIA process is organized and implemented by the Ministry of Natural Resources of Kurdistan Regional government. [61] The same Iraqi environmental legislation mentioned is adopted but the procedure for EIA in Iraqi-Kurdistan region government may differ from the one in the Federal government of Iraq.[ citation needed ] India[ edit ] The Ministry of Environment, Forests and Climate Change (MoEFCC) of India has been in a great effort in Environmental Impact Assessment in India. The main laws in action are the Water Act(1974), the Indian Wildlife (Protection) Act (1972) , the Air (Prevention and Control of Pollution) Act (1981) and the Environment (Protection) Act (1986), Biological Diversity Act(2002). [62] The responsible body for this is the Central Pollution Control Board.[ citation needed ] Environmental Impact Assessment (EIA) studies need a significant amount of primary and secondary environmental data. Primary data are those collected in the field to define the status of the environment (like air quality data, water quality data etc.). Secondary data are those collected over the years that can be used to understand the existing environmental scenario of the study area. The environmental impact assessment (EIA) studies are conducted over a short period of time and therefore the understanding of the environmental trends, based on a few months of primary data, has limitations. Ideally, the primary data must be considered along with the secondary data for complete understanding of the existing environmental status of the area. In many EIA studies, the secondary data needs could be as high as 80% of the total data requirement. EIC is the repository of one-stop secondary data source for environmental impact assessment in India.[ citation needed ] The Environmental Impact Assessment (EIA) experience in India indicates that the lack of timely availability of reliable and authentic environmental data has been a major bottleneck in achieving the full benefits of EIA. The environment being a multi-disciplinary subject, a multitude of agencies are involved in collection of environmental data. However, no single organization in India tracks available data from these agencies and makes it available in one place in a form required by environmental impact assessment practitioners. Further, environmental data is not available in enhanced forms that improve the quality of the EIA. This makes it harder and more time-consuming to generate environmental impact assessments and receive timely environmental clearances from regulators. With this background, the Environmental Information Centre (EIC) has been set up to serve as a professionally managed clearinghouse of environmental information that can be used by MoEF, project proponents, consultants, NGOs and other stakeholders involved in the process of environmental impact assessment in India. EIC caters to the need of creating and disseminating of organized environmental data for various developmental initiatives all over the country.[ citation needed ] EIC stores data in GIS format and makes it available to all environmental impact assessment studies and to EIA stakeholders.[ citation needed ] In 2020, the Government of India proposed a new EIA 2020 Draft, which was widely criticized for heavily diluting the EIA. [63] Many Environmental groups started a campaign demanding the withdrawal of the Draft, in face of these campaigns, the Government of India resorted to banning/blocking the websites of these groups. [64] Malaysia[ edit ] In Malaysia, Section 34A, Environmental Quality Act, 1974 [65] requires developments that have significant impact to the environment are required to conduct the Environmental impact assessment. [66] Nepal[ edit ] In Nepal, EIA has been integrated in major development projects since the early 1980s. In the planning history of Nepal, the sixth plan (1980–85), for the first time, recognized the need for EIA with the establishment of Environmental Impact Study Project (EISP) under the Department of Soil Conservation in 1982 to develop necessary instruments for integration of EIA in infrastructure development projects. However, the government of Nepal enunciated environment conservation-related policies in the seventh plan (NPC, 1985–1990). To enforce this policy and make necessary arrangements, a series of guidelines were developed, thereby incorporating the elements of environmental factors right from the project formulation stage of the development plans and projects and to avoid or minimize adverse effects on the ecological system. In addition, it has also emphasized that EIAs of industry, tourism, water resources, transportation, urbanization, agriculture, forest and other developmental projects be conducted.[ citation needed ] In Nepal, the government's Environmental Impact Assessment Guideline of 1993 inspired the enactment of the Environment Protection Act (EPA) of 1997 and the Environment Protection Rules (EPR) of 1997 (EPA and EPR have been enforced since 24 and 26 June 1997 respectively in Nepal) to internalizing the environmental assessment system. The process institutionalized the EIA process in development proposals and enactment, which makes the integration of IEE and EIA legally binding to the prescribed projects. The projects, requiring EIA or IEE, are included in Schedules 1 and 2 of the EPR, 1997 (GoN/MoLJPA 1997).[ citation needed ] New Zealand[ edit ] In New Zealand, EIA is usually referred to as Assessment of Environmental Effects (AEE). The first use of EIA's dates back to a Cabinet minute passed in 1974 called Environmental Protection and Enhancement Procedures. This had no legal force and only related to the activities of government departments. When the Resource Management Act was passed in 1991, an EIA was required as part of a resource consent application. Section 88 of the Act specifies that the AEE must include "such detail as corresponds with the scale and significance of the effects that the activity may have on the environment". While there is no duty to consult any person when making a resource consent application (Sections 36A and Schedule 4), proof of consultation is almost certain required by local councils when they decide whether or not to publicly notify the consent application under Section 93. [67] Pakistan[ edit ] The Pakistan Environmental Protection Agency is an executive agency of the Government of Pakistan managed by the Ministry of Climate Change . The agency is charged with protecting human health and the environment by writing and enforcing regulations based on laws passed by Parliament. The Directorate of Environmental Impact Assessment (EIA) or Initial Environmental Examination (IEE) is tasked with implementing the Pakistan Environment Protection Act (PEPA) - 1997, specifically Section 12 and Review of IEE/EIA Regulations 2000. This Directorate comprises two sections namely EIA or Monitoring and Environment Engineering And Technology Transfer. All public and private sector developmental projects that fall under any of the Schedules of Regulations have to obtain environmental approval in respect of their projects. The  EIA/Monitoring Section also conducts post-environmental approval monitoring to ascertain the compliance status of the Environment Management Plan (EMP). [68] Russian Federation[ edit ] As of 2004 [update] , the state authority responsible for conducting the State EIA in Russia has been split between two Federal bodies: 1) Federal service for monitoring the use of natural resources – a part of the Russian Ministry for Natural Resources and Environment and 2) Federal Service for Ecological, Technological and Nuclear Control. The two main pieces of environmental legislation in Russia are the Federal Law 'On Ecological Expertise', 1995 and the 'Regulations on Assessment of Impact from Intended Business and Other Activity on Environment in the Russian Federation', 2000. [69] Federal Service for monitoring the use of natural resources In 2006, the parliament committee on ecology in conjunction with the Ministry for Natural Resources and Environment, created a working group to prepare a number of amendments to existing legislation to cover such topics as stringent project documentation for building of potentially environmentally damaging objects as well as building of projects on the territory of protected areas. There has been some success in this area, as evidenced from abandonment of plans to construct a gas pipe-line through the only remaining habitat of the critically endangered Amur leopard in the Russian Far East .[ citation needed ] Federal Service for Ecological, Technological and Nuclear Control The government's decision to hand over control over several important procedures, including state EIA in the field of all types of energy projects, to the Federal Service for Ecological, Technological and Nuclear Control has caused major controversy and elicited criticism from environmental groups, which have blamed the government for giving nuclear power industry control over the state EIA.[ citation needed ] The main problem concerning State EIA in Russia is the clear differentiation of jurisdiction between the two above-mentioned Federal bodies.[ citation needed ] Sri Lanka[ edit ] The National Environmental Act, 1998 requires environmental impact assessment for large scale projects in sensitive areas. It is enforced by the Central Environmental Authority. [70] Ukraine[ edit ] The new law of Ukraine on evaluation of impact on surroundings prescribes the requirements of environmental safety, rational use of national resources, minimizing of harmful impact on surroundings in the process of making managerial decisions about planned activity. The designing of the conclusion of evaluation of impact is a result of its conducting. The key moment of the law on evaluation of impact on surroundings is a substitution of conclusion of state environmental expertise on the conclusion of evaluation of impact on surroundings. Business entity is forbidden to conduct or to start its planned activity without the conclusion of impact on surroundings. [71] Main article: National Environmental Policy Act The National Environmental Policy Act of 1969 (NEPA), enacted in 1970, established a policy of environmental impact assessment for federal agency actions, federally funded activities or federally permitted/licensed activities that in the U. S. is termed "environmental review" or simply "the NEPA process." [72] The law also created the Council on Environmental Quality , which promulgated regulations to codify the law's requirements. [73] Under United States environmental law an Environmental Assessment (EA) is compiled to determine the need for an Environmental Impact Statement (EIS). Federal or federalized actions expected to subject or be subject to significant environmental impacts will publish a Notice of Intent to Prepare an EIS as soon as significance is known. Certain actions of federal agencies must be preceded by the NEPA process. Contrary to a widespread misconception, NEPA does not prohibit the federal government or its licensees/permittees from harming the environment, nor does it specify any penalty if an environmental impact assessment turns out to be inaccurate, intentionally or otherwise. NEPA requires that plausible statements as to the prospective impacts be disclosed in advance. The purpose of NEPA process is to ensure that the decision maker is fully informed of the environmental aspects and consequences prior to making the final decision.[ citation needed ] Environmental assessment[ edit ] An environmental assessment (EA) is an environmental analysis prepared pursuant to the National Environmental Policy Act to determine whether a federal action would significantly affect the environment and thus require a more detailed Environmental Impact Statement (EIS). The certified release of an Environmental Assessment results in either a Finding of No Significant Impact (FONSI) or an EIS . [74] The Council on Environmental Quality (CEQ), which oversees the administration of NEPA, issued regulations for implementing the NEPA in 1979. Eccleston reports that the NEPA regulations barely mention preparation of EAs. This is because the EA was originally intended to be a simple document used in relatively rare instances where an agency was not sure if the potential significance of an action would be sufficient to trigger preparation of an EIS. But today, because EISs are so much longer and complicated to prepare, federal agencies are going to great effort to avoid preparing EISs by using EAs, even in cases where the use of EAs may be inappropriate. The ratio of EAs that are being issued compared to EISs is about 100 to 1. [75] In July 2020, President Donald Trump moved to significantly weaken NEPA. CEQ published a final rule which limits the duration of EAs to 1 year and EISs to 2 years. The rule also exempts a number of projects from review entirely and prevents the consideration of cumulative environmental impacts, including those caused by climate change. The rule went into effect on September 14, 2020 and is the first update to the CEQ regulations since their promulgation in 1978. [76] [77] Content[ edit ] The Environmental Assessment is a concise public document prepared by the federal action agency that serves to: briefly provide sufficient evidence and analysis for determining whether to prepare an EIS or a Finding of No Significant Impact (FONSI) Demonstrate compliance with the act when no EIS is required facilitate the preparation of an EIS when a FONSI cannot be demonstrated The Environmental Assessment includes a brief discussion of the purpose and need of the proposal and of its alternatives as required by NEPA 102(2)(E), and of the human environmental impacts resulting from and occurring to the proposed actions and alternatives considered practicable, plus a listing of studies conducted and agencies and stakeholders consulted to reach these conclusions.  The action agency must approve an EA before it is made available to the public. The EA is made public through notices of availability by local, state, or regional clearing houses, often triggered by the purchase of a public notice advertisement in a newspaper of general circulation in the proposed activity area.[ citation needed ] The structure of a generic Environmental Assessment is as follows: Summary
Toggle the table of contents Remote sensing software (Redirected from Remote sensing application ) Type of software application A remote sensing software is a software application that processes remote sensing data. Remote sensing applications are similar to graphics software , but they enable generating geographic information from satellite and airborne sensor data. Remote sensing applications read specialized file formats that contain sensor image data, georeferencing information, and sensor metadata . Some of the more popular remote sensing file formats include: GeoTIFF , NITF , JPEG 2000 , ECW (file format) , MrSID , HDF , and NetCDF . Remote sensing applications perform many features including: Change Detection — Determining the changes from images taken at different times of the same area Orthorectification — Warping an image to its location on the earth Spectral Analysis — For example, using non-visible parts of the electromagnetic spectrum to determine whether a forest is healthy Image Classification — Categorizing pixels based upon reflectance into different land cover classes (e.g. Supervised classification, Unsupervised classification and Object Oriented classification) Many remote sensing applications are built using common remote sensing toolkits. Examples of remote sensing software[ edit ]
Toggle the table of contents Geospatial intelligence From Wikipedia, the free encyclopedia Information on military opponents' location Graphical depiction of the definition of Geospatial Intelligence (GEOINT) In the United States, geospatial intelligence (GEOINT) is intelligence about the human activity on Earth derived from the exploitation and analysis of imagery, signals, or signatures with geospatial information . GEOINT describes, assesses, and visually depicts physical features and geographically referenced activities on the Earth. GEOINT, as defined in US Code, consists of imagery, imagery intelligence (IMINT) and geospatial information. [1] GEOINT knowledge and related tradecraft is no longer confined to the U.S. government , or even the world's leading military powers. Additionally, countries such as India are holding GEOINT-specific conferences. While other countries may define geospatial intelligence somewhat differently than does the U.S., the use of GEOINT data and services is the same. [2] Geospatial Intelligence can also be referred to as "Location Intelligence". Although GEOINT is inclusive, Hydrospatial is preferably used to refer and to focus on the aquatic and costal zones spatial elements. Amplified definition[ edit ] GEOINT encompasses all aspects of imagery (including capabilities formerly referred to as Advanced Geospatial Intelligence and imagery-derived Measurement and Signature Intelligence (MASINT) and geospatial information and services (GIS); formerly referred to as mapping, charting, and geodesy). It includes, but is not limited to, data ranging from the ultraviolet through the microwave portions of the electromagnetic spectrum, as well as information derived from the analysis of literal imagery; geospatial data; georeferenced social media; and information technically derived from the processing, exploitation, literal, and non-literal analysis of spectral, spatial, temporal, radiometric, phase history, polarimetric data, fused products (products created out of two or more data sources), and the ancillary data needed for data processing and exploitation, and signature information (to include development, validation, simulation, data archival, and dissemination). These types of data can be collected on stationary and moving targets by electro-optical (to include IR, MWIR, SWIR TIR, Spectral, MSI, HSI, HD), SAR (to include MTI), related sensor programs (both active and passive) and non-technical means (to include geospatial information acquired by personnel in the field). [3] Here Geospatial Intelligence, or the frequently used term GEOINT, is an intelligence discipline comprising the exploitation and analysis of geospatial data and information to describe, assess, and visually depict physical features (both natural and constructed) and geographically reference activities on the Earth.  Geospatial Intelligence data sources include imagery and mapping data, whether collected by commercial satellite, government satellite, aircraft (such as Unmanned Aerial Vehicles [UAV] or reconnaissance aircraft), or by other means, such as maps and commercial databases, census information, GPS waypoints, utility schematics, or any discrete data that have locations on earth. There is growing recognition that human geography, [4] socio-cultural intelligence, [5] and other aspects of the human domain [6] are a critical domain of GEOINT data due to the now pervasive geo-referencing of demographic, ethnographic, and political stability data. There is an emerging recognition that "this legal definition paints with a broad brushstroke an idea of the width and depth of GEOINT" [7] and "GEOINT must evolve even further to integrate forms of intelligence and information beyond the traditional sources of geospatial information and imagery, and must move from an emphasis on data and analysis to an emphasis on knowledge." [8] Principles[ edit ] Key terms, such as GEOINT and NGA, were developed for public policy purposes. The NIMA Act of 1996 establishing the National Imagery and Mapping Agency. This resulted in the integration of multiple sources of information, intelligence and trade crafts into NIMA, which subsequently became NGA. Then Director James Clapper (2001–2006) designated this discipline as GEOINT, in the ilk of IMINT, SIGINT, MASINT, HUMINT.[ citation needed ] The question as to how GEOINT is different from other geospatial analytic activities is occasionally asked. Bacastow [9] [10] suggested the following First Principles as markers that define the professional domain in terms of uniqueness and value. These are: GEOINT, rooted in the geospatial sciences, geospatial technologies and a tradecraft that seeks knowledge to achieve a decision advantage. Achieving a decision advantage may result in or require information denial and deception (D&D) . Analysis occurs as a human-machine team. GEOINT reveals how human action is constrained by the physical landscape and human perceptions of Earth. GEOINT seeks to anticipate patterns of life through time. The data and technical systems reflect human biases. Geospatial data, information, and knowledge[ edit ] The definitions and usage of the terms geospatial data, geospatial information, and geospatial knowledge are not consistent or unambiguous, further exacerbating the situation. Geospatial data can (usually) be applied to the output of a collector or collection system before it is processed, i.e., data that was sensed. Geospatial Information is geospatial data that has been processed or had value added to it by a human or machine process. Geospatial knowledge is a structuring of geospatial information, accompanied by an  interpretation or analysis. The terms Data, Information, Knowledge and Wisdom ( DIKW pyramid ) are difficult to define, but cannot be used interchangeably. Generally, geospatial intelligence can be more readily defined as, data, information, and knowledge gathered about entities that can be referenced to a particular location on, above, or below the Earth's surface. The intelligence gathering method can include imagery, signals, measurements and signatures, and human sources, i.e., IMINT, SIGINT, MASINT, and HUMINT, as long as a geo-location can be associated with the intelligence. Relationship to other "INT"[ edit ] Thus, rather than being a peer to the other "INT", geospatial intelligence might better be viewed as the unifying structure of the Earth's natural and constructed features (including elevations and depths)—whether as individual layers in a GIS or as composited into a map or chart, imagery representations of the Earth, AND, the presentation of the existence of data, information, and knowledge derived from analysis of IMINT , SIGINT , MASINT , HUMINT , and other intelligence sources and disciplines. Other factors[ edit ] It has been suggested that GEOINT is just a new term used to identify a broad range of outputs from intelligence organizations that use a variety of existing spatial skills and disciplines including photogrammetry , cartography , imagery analysis , remote sensing , and terrain analysis .  However, GEOINT is  more than the sum of these parts.  Spatial thinking as applied in Geospatial Intelligence can synthesize any intelligence or other data that can be conceptualized in a geographic spatial context. Geospatial Intelligence can be derived entirely independent of any satellite or aerial imagery and can be clearly differentiated from IMINT (imagery intelligence). Confusion and dissension is caused by Title 10 U.S. Code §467's separation of "imagery" or "satellite information" from "geospatial information" as imagery is generally considered just one of the forms which geospatial information might take or be derived from. It has also been suggested[ by whom? ] that geospatial intelligence can be described as a product occurring at the point of delivery, i.e., by the amount of analysis which occurs to resolve particular problems, not by the type of data used. For example, a database containing a list of measurements of bridges obtained from imagery is 'information' while the development of an output using analysis to determine those bridges that are able to be utilized for specific purposes could be termed 'intelligence'. Similarly, the simple measurement of beach profiles is a classical geographic information-gathering activity, while the process of selecting a beach that matches a certain profile for a specific purpose is an analytical activity, and the output could be termed an intelligence product. In this form it is considered to be generally used by agencies requiring definitions of their outputs for descriptive and capability development purposes (or, more cynically, as a marketing strategy). Geospatial intelligence analysis has been light-heartedly defined as "seeing what everybody has seen and thinking what nobody has thought" or as "anticipating a target's mental map." [11] However, these perspectives affirm that creating geospatial knowledge is an effortful cognitive process the analyst undertakes; it is an intellectual endeavor that arrives at a conclusion through reasoning. Geospatial reasoning creates the objective connection between a geospatial problem representation and geospatial evidence. Here one set of activities, information foraging , focuses around finding information while another set of activities, sensemaking , focuses on giving meaning to the information. The activities of foraging and sensemaking in geospatial analysis have been incorporated in the Structured Geospatial Analytic Method . [12] De facto definition[ edit ] A de facto definition of geospatial intelligence, which is more reflective of the broad international nature of the discipline,  is vastly different from the de jure definition expressed in U.S. Code. This de facto definition is: Geospatial Intelligence is a field of knowledge, a process, and a profession. As knowledge, it is information integrated in a coherent space-time context that supports descriptions, explanations, or forecasts of human activities with which decision makers take action. As a process, it is the means by which data and information are collected, manipulated, geospatially reasoned, and disseminated to decision-makers. The geospatial intelligence profession establishes the scope of activities, interdisciplinary associations, competencies, and standards in academe, government, and the private sectors. [13] This has been suggested as an operational definition of Geospatial Intelligence which might use the moniker of GeoIntel so as to distinguish it from the more restrictive definition offered in U.S. Code Title 10, §467.
Toggle the table of contents Land cover From Wikipedia, the free encyclopedia Physical material covering the surface of Earth Land cover surrounding Madison, WI. Fields are colored yellow and brown, water is colored blue, and urban surfaces are colored red. Land cover is the physical material at the surface of Earth. Land covers include grass , asphalt , trees , bare ground, water , etc. Earth cover is the expression used by ecologist Frederick Edward Clements that has its closest modern equivalent being vegetation . [1] : 52 The expression continues to be used by the United States Bureau of Land Management . [2] There are two primary methods for capturing information on land cover: field survey, and analysis of remotely sensed imagery . [3] Land change models can be built from these types of data to assess changes in land cover over time. One of the major land cover issues (as with all natural resource inventories) is that every survey defines similarly named categories in different ways. For instance, there are many definitions of " forest "—sometimes within the same organisation—that may or may not incorporate a number of different forest features (e.g., stand height, canopy cover, strip width, inclusion of grasses, and rates of growth for timber production ). [4] Areas without trees may be classified as forest cover "if the intention is to re-plant" ( UK and Ireland ), while areas with many trees may not be labelled as forest "if the trees are not growing fast enough" ( Norway and Finland ). Distinction from "land use"[ edit ] "Land cover" is distinct from " land use ", despite the two terms often being used interchangeably. Land use is a description of how people utilize the land and of socio-economic activity . Urban and agricultural land uses are two of the most commonly known land use classes. At any one point or place, there may be multiple and alternate land uses, the specification of which may have a political dimension. The origins of the "land cover/land use" couplet and the implications of their confusion are discussed in Fisher et al. (2005). [5]
From Wikipedia, the free encyclopedia Classification of land resources based on what can be built and on its use Not to be confused with Zoning . Global distribution of land used for agriculture Land use involves the management and modification of natural environment or wilderness into built environment such as settlements and semi-natural habitats such as arable fields , pastures , and managed woods . Land use by humans has a long history, first emerging more than 10,000 years ago. [1] [2] It has been defined as "the purposes and activities through which people interact with land and terrestrial ecosystems" [3] and as "the total of arrangements, activities, and inputs that people undertake in a certain land type." [4] Land use is one of the most important drivers of global environmental change . [3] [5] History[ edit ] Human tribes since prehistory have segregated land into territories to control the use of land. Today, the total arable land is 10.7% of the land surface, with 1.3% being permanent cropland. [6] [7] Regulation[ edit ] A land use map of Europe —major non-natural land uses include arable farmland (yellow) and pasture (light green). Land use practices vary considerably across the world. The United Nations ' Food and Agriculture Organization Water Development Division explains that "Land use concerns the products and/or benefits obtained from use of the land as well as the land management actions (activities) carried out by humans to produce those products and benefits." [8] As of the early 1990s, about 13% of the Earth was considered arable land, with 26% in pasture, 32% forests and woodland, and 1.5% urban areas. Land change modeling can be used to predict and assess future shifts in land use. As Albert Guttenberg (1959) wrote many years ago, "'Land use' is a key term in the language of city planning ." [9] Commonly, political jurisdictions will undertake land-use planning and regulate the use of land in an attempt to avoid land-use conflicts . Land use plans are implemented through land division and use ordinances and regulations, such as zoning regulations . Management consulting firms and non-governmental organizations will frequently seek to influence these regulations before they are codified. Habitat fragmentation caused by numerous roads near the Indiana Dunes National Lakeshore In colonial America, few regulations were originally put into place regarding the usage of land. As society shifted from rural to urban, public land regulation became important, especially to city governments trying to control industry, commerce, and housing within their boundaries. The first zoning ordinance was passed in New York City in 1916, [10] [11] and, by the 1930s, most states had adopted zoning laws. In the 1970s, concerns about the environment and historic preservation led to further regulation. Today, federal, state, and local governments regulate growth and development through statutory law . The majority of controls on land, however, stem from the actions of private developers and individuals. Three typical situations bringing such private entities into the court system are: suits brought by one neighbor against another; suits brought by a public official against a neighboring landowner on behalf of the public; and suits involving individuals who share ownership of a particular parcel of land. In these situations, judicial decisions and enforcement of private land-use arrangements can reinforce public regulation, and achieve forms and levels of control that regulatory zoning cannot. There is growing concern that land use regulation is a direct cause of housing segregation in the United States today. [12] Two major federal laws passed in the 1960s limit the use of land significantly. These are the National Historic Preservation Act of 1966 (today embodied in 16 U.S.C. 461 et seq.) and the National Environmental Policy Act of 1969 (42 U.S.C. 4321 et seq.). The US Department of Agriculture has identified six major types of land use in the US. Acreage statistics for each type of land use in the contiguous 48 states in 2017 were as follows: [13] US land use (2017) [13] Use
Toggle the table of contents Geostatistics From Wikipedia, the free encyclopedia Not to be confused with statistical geography . Branch of statistics focusing on spatial data sets Background[ edit ] Geostatistics is intimately related to interpolation methods, but extends far beyond simple interpolation problems. Geostatistical techniques rely on statistical models that are based on random function (or random variable ) theory to model the uncertainty associated with spatial estimation and simulation. A number of simpler interpolation methods/algorithms, such as inverse distance weighting , bilinear interpolation and nearest-neighbor interpolation , were already well known before geostatistics. [2] Geostatistics goes beyond the interpolation problem by considering the studied phenomenon at unknown locations as a set of correlated random variables. Let Z(x) be the value of the variable of interest at a certain location x. This value is unknown (e.g. temperature, rainfall, piezometric level , geological facies, etc.). Although there exists a value at location x that could be measured, geostatistics considers this value as random since it was not measured, or has not been measured yet. However, the randomness of Z(x) is not complete, but defined by a cumulative distribution function (CDF) that depends on certain information that is known about the value Z(x): F information . {\displaystyle F({\mathit {z}},\mathbf {x} )=\operatorname {Prob} \lbrace Z(\mathbf {x} )\leqslant {\mathit {z}}\mid {\text{information}}\rbrace .} Typically, if the value of Z is known at locations close to x (or in the neighborhood of x) one can constrain the CDF of Z(x) by this neighborhood: if a high spatial continuity is assumed, Z(x) can only have values similar to the ones found in the neighborhood. Conversely, in the absence of spatial continuity Z(x) can take any value. The spatial continuity of the random variables is described by a model of spatial continuity that can be either a parametric function in the case of variogram -based geostatistics, or have a non-parametric form when using other methods  such as multiple-point simulation [3] or pseudo-genetic techniques. By applying a single spatial model on an entire domain, one makes the assumption that Z is a stationary process . It means that the same statistical properties are applicable on the entire domain. Several geostatistical methods provide ways of relaxing this stationarity assumption. In this framework, one can distinguish two modeling goals: Estimating the value for Z(x), typically by the expectation , the median or the mode of the CDF f(z,x). This is usually denoted as an estimation problem. Sampling from the entire probability density function f(z,x) by actually considering each possible outcome of it at each location. This is generally done by creating several alternative maps of Z, called realizations. Consider a domain discretized in N grid nodes (or pixels). Each realization is a sample of the complete N-dimensional joint distribution function F N . {\displaystyle F(\mathbf {z} ,\mathbf {x} )=\operatorname {Prob} \lbrace Z(\mathbf {x} _{1})\leqslant z_{1},Z(\mathbf {x} _{2})\leqslant z_{2},...,Z(\mathbf {x} _{N})\leqslant z_{N}\rbrace .} In this approach, the presence of multiple solutions to the interpolation problem is acknowledged. Each realization is considered as a possible scenario of what the real variable could be. All associated workflows are then considering ensemble of realizations, and consequently ensemble of predictions that allow for probabilistic forecasting. Therefore, geostatistics is often used to generate or update spatial models when solving inverse problems . [4] [5] A number of methods exist for both geostatistical estimation and multiple realizations approaches. Several reference books provide a comprehensive overview of the discipline. [6] [2] [7] [8] [9] [10] [11] [12] [13] [14] [15] Main article: Kriging Kriging is a group of geostatistical techniques to interpolate the value of a random field (e.g., the elevation, z, of the landscape as a function of the geographic location) at an unobserved location from observations of its value at nearby locations. Main article: Bayesian inference Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update a probability model as more evidence or information becomes available. Bayesian inference is playing an increasingly important role in Geostatistics. [16] Bayesian estimation implements kriging through a spatial process, most commonly a Gaussian process , and updates the process using Bayes' Theorem to calculate its posterior. High-dimensional Bayesian Geostatistics [17] Finite difference method[ edit ] Considering the principle of conservation of probability, recurrent difference equations (finite difference equations) were used in conjunction with lattices to compute probabilities quantifying uncertainty about the geological structures. This procedure is a numerical alternative method to Markov chains and Bayesian models. [18] Finite difference method Related academic journals[ edit ] This article's use of external links may not follow Wikipedia's policies or guidelines. Please improve this article by removing excessive or inappropriate external links, and converting useful links where appropriate into footnote references . (January 2022) ( European Forum for Geography and Statistics (EFGS; formerly the European Forum for Geostatistics) GeoEnvia promotes the use of geostatistical methods in environmental applications Notes[ edit ] ^ Krige, Danie G. (1951). "A statistical approach to some basic mine valuation problems on the Witwatersrand". J. of the Chem., Metal. and Mining Soc. of South Africa 52 (6): 119–139 ^ a b Isaaks, E. H. and Srivastava, R. M. (1989), An Introduction to Applied Geostatistics, Oxford University Press, New York, USA. ^ Mariethoz, Gregoire, Caers, Jef (2014). Multiple-point geostatistics: modeling with training images.  Wiley-Blackwell, Chichester, UK, 364 p. ^ Hansen, T.M., Journel, A.G., Tarantola, A. and Mosegaard, K. (2006). "Linear inverse Gaussian theory and geostatistics", Geophysics 71 ^ Kitanidis, P.K. and Vomvoris, E.G. (1983). "A geostatistical approach to the inverse problem in groundwater modeling (steady state) and one-dimensional simulations", Water Resources Research 19(3):677-690 ^ Remy, N., et al. (2009), Applied Geostatistics with SGeMS: A User's Guide, 284 pp., Cambridge University Press, Cambridge. ^ Deutsch, C.V., Journel, A.G, (1997). GSLIB: Geostatistical Software Library and User's Guide (Applied Geostatistics Series), Second Edition, Oxford University Press, 369 pp., http://www.gslib.com/ ^ Chilès, J.-P., and P. Delfiner (1999), Geostatistics - Modeling Spatial Uncertainty, John Wiley & Sons, Inc., New York, USA. ^ Lantuéjoul, C. (2002), Geostatistical simulation: Models and algorithms, 232 pp., Springer, Berlin. ^ Journel, A. G. and Huijbregts, C.J. (1978) Mining Geostatistics, Academic Press. ^ Kitanidis, P.K.  (1997) Introduction to Geostatistics: Applications in Hydrogeology, Cambridge University Press. ^ Wackernagel, H. (2003). Multivariate geostatistics, Third edition, Springer-Verlag, Berlin, 387 pp. ^ Pyrcz, M. J. and Deutsch, C.V., (2014). Geostatistical Reservoir Modeling, 2nd Edition, Oxford University Press, 448 pp. ^ Tahmasebi, P., Hezarkhani, A., Sahimi, M., 2012, Multiple-point geostatistical modeling based on the cross-correlation functions, Computational Geosciences, 16(3):779-79742, Schnetzler, Manu. "Statios - WinGslib" . ^ Banerjee S., Carlin B.P., and Gelfand A.E. (2014). Hierarchical Modeling and Analysis for Spatial Data, Second Edition. Chapman & Hall/CRC Monographs on Statistics & Applied Probability. References[ edit ] Armstrong, M and Champigny, N, 1988, A Study on Kriging Small Blocks, CIM Bulletin, Vol 82, No 923 Armstrong, M, 1992, Freedom of Speech? De Geeostatisticis, July, No 14 Clark I, 1979, Practical Geostatistics , Applied Science Publishers, London David, M, 1977, Geostatistical Ore Reserve Estimation, Elsevier Scientific Publishing Company, Amsterdam Hald, A, 1952, Statistical Theory with Engineering Applications, John Wiley & Sons, New York Honarkhah, Mehrdad; Caers, Jef (2010). "Stochastic Simulation of Patterns Using Distance-Based Pattern Modeling". Mathematical Geosciences. 42 (5): 487–517. doi : 10.1007/s11004-010-9276-7 . S2CID 73657847 . (best paper award IAMG 09) ISO/DIS 11648-1 Statistical aspects of sampling from bulk materials-Part1: General principles Lipschutz, S, 1968, Theory and Problems of Probability, McCraw-Hill Book Company, New York. Matheron, G. 1962. Traité de géostatistique appliquée. Tome 1, Editions Technip, Paris, 334 pp. Matheron, G. 1989. Estimating and choosing, Springer-Verlag, Berlin. McGrew, J. Chapman, & Monroe, Charles B., 2000. An introduction to statistical problem solving in geography, second edition, McGraw-Hill, New York. Merks, J W, 1992, Geostatistics or voodoo science , The Northern Miner, May 18 Merks, J W, Abuse of statistics , CIM Bulletin, January 1993, Vol 86, No 966 Myers, Donald E.; "What Is Geostatistics? Philip, G M and Watson, D F, 1986, Matheronian Geostatistics; Quo Vadis?, Mathematical Geology, Vol 18, No 1 Pyrcz, M.J. and Deutsch, C.V., 2014, Geostatistical Reservoir Modeling, 2nd Edition, Oxford University Press, New York, p. 448 Sharov, A: Quantitative Population Ecology, 1996, https://web.archive.org/web/20020605050231/http://www.ento.vt.edu/~sharov/PopEcol/popecol.html Shine, J.A., Wakefield, G.I.: A comparison of supervised imagery classification using analyst-chosen and geostatistically-chosen training sets, 1999, https://web.archive.org/web/20020424165227/http://www.geovista.psu.edu/sites/geocomp99/Gc99/044/gc_044.htm Strahler, A. H., and Strahler A., 2006, Introducing Physical Geography, 4th Ed., Wiley. Tahmasebi, P., Hezarkhani, A., Sahimi, M., 2012, Multiple-point geostatistical modeling based on the cross-correlation functions , Computational Geosciences, 16(3):779-79742. Volk, W, 1980, Applied Statistics for Engineers, Krieger Publishing Company, Huntington, New York. Wikimedia Commons has media related to Geostatistics . GeoENVia promotes the use of geostatistical methods in environmental applications, and organizes bi-annual conferences. [1] , a resource on the internet about geostatistics and spatial statistics
Toggle the table of contents Atmospheric dispersion modeling From Wikipedia, the free encyclopedia Mathematical simulation of how air pollutants disperse in the ambient atmosphere Part of a series on e Atmospheric dispersion modeling is the mathematical simulation of how air pollutants disperse in the ambient atmosphere . It is performed with computer programs that include algorithms to solve the mathematical equations that govern the pollutant dispersion. The dispersion models are used to estimate the downwind ambient concentration of air pollutants or toxins emitted from sources such as industrial plants, vehicular traffic or accidental chemical releases. They can also be used to predict future concentrations under specific scenarios (i.e. changes in emission sources). Therefore, they are the dominant type of model used in air quality policy making. They are most useful for pollutants that are dispersed over large distances and that may react in the atmosphere. For pollutants that have a very high spatio-temporal variability (i.e. have very steep distance to source decay such as black carbon ) and for epidemiological studies statistical land-use regression models are also used. Dispersion models are important to governmental agencies tasked with protecting and managing the ambient air quality . The models are typically employed to determine whether existing or proposed new industrial facilities are or will be in compliance with the National Ambient Air Quality Standards (NAAQS) in the United States and other nations. The models also serve to assist in the design of effective control strategies to reduce emissions of harmful air pollutants. During the late 1960s, the Air Pollution Control Office of the U.S. EPA initiated research projects that would lead to the development of models for the use by urban and transportation planners. [1] A major and significant application of a roadway dispersion model that resulted from such research was applied to the Spadina Expressway of Canada in 1971. Air dispersion models are also used by public safety responders and emergency management personnel for emergency planning of accidental chemical releases. Models are used to determine the consequences of accidental releases of hazardous or toxic materials, Accidental releases may result in fires, spills or explosions that involve hazardous materials, such as chemicals or radionuclides. The results of dispersion modeling, using worst case accidental release source terms and meteorological conditions, can provide an estimate of location impacted areas, ambient concentrations, and be used to determine protective actions appropriate in the event a release occurs. Appropriate protective actions may include evacuation or shelter in place for persons in the downwind direction. At industrial facilities, this type of consequence assessment or emergency planning is required under the U.S. Clean Air Act (CAA) codified in Part 68 of Title 40 of the Code of Federal Regulations . The dispersion models vary depending on the mathematics used to develop the model, but all require the input of data that may include: Meteorological conditions such as wind speed and direction, the amount of atmospheric turbulence (as characterized by what is called the "stability class" ), the ambient air temperature, the height to the bottom of any inversion aloft that may be present, cloud cover and solar radiation. Source term (the concentration or quantity of toxins in emission or accidental release source terms ) and temperature of the material Emissions or release parameters such as source location and height, type of source (i.e., fire, pool or vent stack) and exit velocity , exit temperature and mass flow rate or release rate. Terrain elevations at the source location and at the receptor location(s), such as nearby homes, schools, businesses and hospitals. The location, height and width of any obstructions (such as buildings or other structures) in the path of the emitted gaseous plume, surface roughness or the use of a more generic parameter "rural" or "city" terrain. Many of the modern, advanced dispersion modeling programs include a pre-processor module for the input of meteorological and other data, and many also include a post-processor module for graphing the output data and/or plotting the area impacted by the air pollutants on maps. The plots of areas impacted may also include isopleths showing areas of minimal to high concentrations that define areas of the highest health risk. The isopleths plots are useful in determining protective actions for the public and responders. The atmospheric dispersion models are also known as atmospheric diffusion models, air dispersion models, air quality models, and air pollution dispersion models. Atmospheric layers[ edit ] Discussion of the layers in the Earth's atmosphere is needed to understand where airborne pollutants disperse in the atmosphere. The layer closest to the Earth's surface is known as the troposphere . It extends from sea-level to a height of about 18 km (11 mi) and contains about 80 percent of the mass of the overall atmosphere. The stratosphere is the next layer and extends from 18 km (11 mi) to about 50 km (31 mi). The third layer is the mesosphere which extends from 50 km (31 mi) to about 80 km (50 mi). There are other layers above 80 km, but they are insignificant with respect to atmospheric dispersion modeling. The lowest part of the troposphere is called the atmospheric boundary layer (ABL) or the planetary boundary layer (PBL) . The air temperature of the atmosphere decreases with increasing altitude until it reaches what is called an inversion layer (where the temperature increases with increasing altitude) that caps the Convective Boundary Layer , typically to about 1.5 to 2 km (0.93 to 1.24 mi) in height. The upper part of the troposphere (i.e., above the inversion layer) is called the free troposphere and it extends up to the tropopause (the boundary in the Earth's atmosphere between the troposphere and the stratosphere). In tropical and mid-latitudes during daytime, the Free convective layer can comprise the entire troposphere, which is up to 10 to 18 km (6.2 to 11.2 mi) in the Intertropical convergence zone . The ABL is of the most important with respect to the emission, transport and dispersion of airborne pollutants. The part of the ABL between the Earth's surface and the bottom of the inversion layer is known as the mixing layer. Almost all of the airborne pollutants emitted into the ambient atmosphere are transported and dispersed within the mixing layer. Some of the emissions penetrate the inversion layer and enter the free troposphere above the ABL. In summary, the layers of the Earth's atmosphere from the surface of the ground upwards are: the ABL made up of the mixing layer capped by the inversion layer; the free troposphere; the stratosphere; the mesosphere and others. Many atmospheric dispersion models are referred to as boundary layer models because they mainly model air pollutant dispersion within the ABL. To avoid confusion, models referred to as mesoscale models have dispersion modeling capabilities that extend horizontally up to a few hundred kilometres. It does not mean that they model dispersion in the mesosphere. Gaussian air pollutant dispersion equation[ edit ] The technical literature on air pollution dispersion is quite extensive and dates back to the 1930s and earlier. One of the early air pollutant plume dispersion equations was derived by Bosanquet and Pearson. [2] Their equation did not assume Gaussian distribution nor did it include the effect of ground reflection of the pollutant plume. Sir Graham Sutton derived an air pollutant plume dispersion equation in 1947 [3] which did include the assumption of Gaussian distribution for the vertical and crosswind dispersion of the plume and also included the effect of ground reflection of the plume. Under the stimulus provided by the advent of stringent environmental control regulations , there was an immense growth in the use of air pollutant plume dispersion calculations between the late 1960s and today. A great many computer programs for calculating the dispersion of air pollutant emissions were developed during that period of time and they were called "air dispersion models". The basis for most of those models was the Complete Equation For Gaussian Dispersion Modeling Of Continuous, Buoyant Air Pollution Plumes shown below: [4] [5] C
Toggle the table of contents Hydrological model (Redirected from Hydrological modelling ) Predicting and managing water resources A hydrologic model is a simplification of a real-world system (e.g., surface water, soil water, wetland, groundwater, estuary) that aids in understanding, predicting, and managing water resources. Both the flow and quality of water are commonly studied using hydrologic models. MODFLOW, a computational groundwater flow model based on methods developed by the US Geological Survey. Analog models[ edit ] Prior to the advent of computer models, hydrologic modeling used analog models to simulate flow and transport systems. Unlike mathematical models that use equations to describe, predict, and manage hydrologic systems, analog models use non-mathematical approaches to simulate hydrology. Two general categories of analog models are common; scale analogs that use miniaturized versions of the physical system and process analogs that use comparable physics (e.g., electricity, heat, diffusion) to mimic the system of interest. Detail of the Mississippi River Basin Model ( US Army Corps of Engineers , 2006) Scale models offer a useful approximation of physical or chemical processes at a size that allows for greater ease of visualization. [1] The model may be created in one (core, column), two (plan, profile), or three dimensions, and can be designed to represent a variety of specific initial and boundary conditions as needed to answer a question. Scale models commonly use physical properties that are similar to their natural counterparts (e.g., gravity, temperature). Yet, maintaining some properties at their natural values can lead to erroneous predictions. [2] Properties such as viscosity, friction, and surface area must be adjusted to maintain appropriate flow and transport behavior. This usually involves matching dimensionless ratios (e.g., Reynolds number , Froude number ). A two-dimensional scale model of an aquifer. Groundwater flow can be visualized using a scale model built of acrylic and filled with sand, silt, and clay. [3] Water and tracer dye may be pumped through this system to represent the flow of the simulated groundwater. Some physical aquifer models are between two and three dimensions, with simplified boundary conditions simulated using pumps and barriers. [4] Process analogs are used in hydrology to represent fluid flow using the similarity between Darcy's Law , Ohms Law , Fourier's Law , and Fick's Law . The analogs to fluid flow are the flux of electricity , heat , and solutes , respectively. [5] The corresponding analogs to fluid potential are voltage , temperature , and solute concentration (or chemical potential ). The analogs to hydraulic conductivity are electrical conductivity , thermal conductivity , and the solute diffusion coefficient . An early process analog model was an electrical network model of an aquifer composed of resistors in a grid. [6] Voltages were assigned along the outer boundary, and then measured within the domain. Electrical conductivity paper [7] can also be used instead of resistors. Statistical models[ edit ] Statistical models are a type of mathematical model that are commonly used in hydrology to describe data, as well as relationships between data. [8] Using statistical methods, hydrologists develop empirical relationships between observed variables, [9] find trends in historical data, [10] or forecast probable storm or drought events. [11] Moments[ edit ] Statistical moments (e.g., mean , standard deviation , skewness , kurtosis ) are used to describe the information content of data. These moments can then be used to determine an appropriate frequency distribution , [12] which can then be used as a probability model . [13] Two common techniques include L-moment ratios [14] and Moment-Ratio Diagrams. [15] The frequency of extremal events, such as severe droughts and storms, often requires the use of distributions that focus on the tail of the distribution, rather than the data nearest the mean. These techniques, collectively known as extreme value analysis , provide a methodology for identifying the likelihood and uncertainty of extreme events. [16] [17] Examples of extreme value distributions include the Gumbel , Pearson , and Generalized Extreme Value . The standard method for determining peak discharge uses the log-Pearson Type III (log-gamma) distribution and observed annual flow peaks. [18] Correlation analysis[ edit ] The degree and nature of correlation may be quantified, by using a method such as the Pearson correlation coefficient , autocorrelation , or the T-test . [19] The degree of randomness or uncertainty in the model may also be estimated using stochastics , [20] or residual analysis . [21] These techniques may be used in the identification of flood dynamics, [22] [23] storm characterization, [24] [25] and groundwater flow in karst systems. [26] Regression analysis is used in hydrology to determine whether a relationship may exist between independent and dependent variables . Bivariate diagrams are the most commonly used statistical regression model in the physical sciences, but there are a variety of models available from simplistic to complex. [27] In a bivariate diagram, a linear or higher-order model may be fitted to the data. Factor Analysis and Principal Component Analysis are multivariate statistical procedures used to identify relationships between hydrologic variables. [28] [29] Convolution is a mathematical operation on two different functions to produce a third function.  With respect to hydrologic modeling, convolution can be used to analyze stream discharge's relationship to precipitation.  Convolution is used to predict discharge downstream after a precipitation event.  This type of model would be considered a “lag convolution”, because of the predicting of the “lag time” as water moves through the watershed using this method of modeling. Time-series analysis is used to characterize temporal correlation within a data series as well as between different time series. Many hydrologic phenomena are studied within the context of historical probability. Within a temporal dataset, event frequencies, trends, and comparisons may be made by using the statistical techniques of time series analysis. [30] The questions that are answered through these techniques are often important for municipal planning, civil engineering, and risk assessments. Markov Chains are a mathematical technique for determine the probability of a state or event based on a previous state or event. [31] The event must be dependent, such as rainy weather. Markov Chains were first used to model rainfall event length in days in 1976, [32] and continues to be used for flood risk assessment and dam management. Data-driven models[ edit ] Data-driven models in hydrology emerged as an alternative approach to traditional statistical models, offering a more flexible and adaptable methodology for analysing and predicting various aspects of hydrological processes. While statistical models rely on rigorous assumptions about probability distributions, data-driven models leverage techniques from artificial intelligence, machine learning, and statistical analysis, including correlation analysis, time series analysis, and statistical moments, to learn complex patterns and dependencies from historical data. This allows them to make more accurate predictions and provide insights into the underlying processes. [33] Since their inception in the latter half of the 20th century, data-driven models have gained popularity in the water domain, as they help improve forecasting, decision-making, and management of water resources. A couple of notable publications that use data-driven models in hydrology include "Application of machine learning techniques for rainfall-runoff modelling" by Solomatine and Siek (2004), [34] and "Data-driven modelling approaches for hydrological forecasting and prediction" by Valipour et al. (2021). [35] These models are commonly used for predicting rainfall, runoff, groundwater levels, and water quality, and have proven to be valuable tools for optimizing water resource management strategies. Conceptual models[ edit ] The Nash Model uses a cascade of linear reservoirs to predict streamflow. [36] Conceptual models represent hydrologic systems using physical concepts . The conceptual model is used as the starting point for defining the important model components. The relationships between model components are then specified using algebraic equations , ordinary or partial differential equations , or integral equations . The model is then solved using analytical or numerical procedures. Conceptual models are commonly used to represent the important components (e.g., features, events, and processes ) that relate hydrologic inputs to outputs. [37] These components describe the important functions of the system of interest, and are often constructed using entities (stores of water) and relationships between these entitites (flows or fluxes between stores). The conceptual model is coupled with scenarios to describe specific events (either input or outcome scenarios). For example, a watershed model could be represented using tributaries as boxes with arrows pointing toward a box that represents the main river.  The conceptual model would then specify the important watershed features (e.g., land use, land cover, soils, subsoils, geology, wetlands, lakes), atmospheric exchanges (e.g., precipitation, evapotranspiration), human uses (e.g., agricultural, municipal, industrial, navigation, thermo- and hydro-electric power generation), flow processes (e.g., overland, interflow, baseflow, channel flow), transport processes (e.g.,  sediments, nutrients, pathogens), and events (e.g., low-, flood-, and mean-flow conditions). Model scope and complexity is dependent on modeling objectives, with greater detail required if human or environmental systems are subject to greater risk. Systems modeling can be used for building conceptual models that are then populated using mathematical relationships. Example 1 The linear-reservoir model (or Nash Model) is widely used for rainfall-runoff analysis. The model uses a cascade of linear reservoirs along with a constant first-order storage coefficient, K, to predict the outflow from each reservoir (which is then used as the input to the next in the series). The model combines continuity and storage-discharge equations, which yields an ordinary differential equation that describes outflow from each reservoir. The continuity equation for tank models is: d
Toggle the table of contents Climate model From Wikipedia, the free encyclopedia Quantitative methods used to simulate climate This article is about the theories and mathematics of climate modeling. For computer-driven prediction of Earth's climate, see General circulation model . For broader coverage of this topic, see Atmospheric model . Climate models divide the planet into a 3-dimensional grid and apply differential equations to each grid The equations are based on the basic laws of physics , fluid motion , and chemistry. Numerical climate models (or climate system models) are mathematical models that can simulate the interactions of important drivers of climate . These drivers are the atmosphere , oceans , land surface and ice . Scientists use climate models to study the dynamics of the climate system and to make projections of future climate and of climate change . Climate models can also be qualitative (i.e. not numerical) models and contain narratives, largely descriptive, of possible futures. [1] Climate models take account of incoming energy from the sun as well as outgoing energy from Earth. An imbalance results in a change in temperature . The incoming energy from the sun is in the form of short wave electromagnetic radiation , chiefly visible and short-wave (near) infrared . The outgoing energy is in the form of long wave (far) infrared electromagnetic energy. These processes are part of the greenhouse effect . Climate models vary in complexity. For example, a simple radiant heat transfer model treats the earth as a single point and averages outgoing energy. This can be expanded vertically (radiative-convective models) and horizontally. More complex models are the coupled atmosphere–ocean– sea ice global climate models . These types of models solve the full equations for mass transfer, energy transfer and radiant exchange. In addition, other types of models can be interlinked. For example Earth System Models include also land use as well as land use changes . This allows researchers to predict the interactions between climate and ecosystems . Climate models are systems of differential equations based on the basic laws of physics , fluid motion , and chemistry . Scientists divide the planet into a 3-dimensional grid and apply the basic equations to those grids. Atmospheric models calculate winds , heat transfer , radiation , relative humidity , and surface hydrology within each grid and evaluate interactions with neighboring points. There are three major types of institution where climate models are developed, implemented and used: National meteorological services: Most national weather services have a climatology section. Universities: Relevant departments include atmospheric sciences, meteorology, climatology, and geography. National and international research laboratories: Examples include the National Center for Atmospheric Research (NCAR, in Boulder, Colorado , US), the Geophysical Fluid Dynamics Laboratory (GFDL, in Princeton, New Jersey , US), Los Alamos National Laboratory , the Hadley Centre for Climate Prediction and Research (in Exeter , UK), the Max Planck Institute for Meteorology in Hamburg, Germany, or the Laboratoire des Sciences du Climat et de l'Environnement (LSCE), France. Big climate models are essential but they are not perfect. Attention still needs to be given to the real world (what is happening and why). The global models are essential to assimilate all the observations, especially from space (satellites) and produce comprehensive analyses of what is happening, and then they can be used to make predictions/projections. Simple models have a role to play that is widely abused and fails to recognize the simplifications such as not including a water cycle. [2] General circulation models (GCMs)[ edit ] This section is an excerpt from General circulation model .[ edit ] Climate models are systems of differential equations based on the basic laws of physics , fluid motion , and chemistry . To "run" a model, scientists divide the planet into a 3-dimensional grid, apply the basic equations, and evaluate the results. Atmospheric models calculate winds , heat transfer , radiation , relative humidity , and surface hydrology within each grid and evaluate interactions with neighboring points. [3] A general circulation model (GCM) is a type of climate model. It employs a mathematical model of the general circulation of a planetary atmosphere or ocean. It uses the Navier–Stokes equations on a rotating sphere with thermodynamic terms for various energy sources ( radiation , latent heat ). These equations are the basis for computer programs used to simulate the Earth's atmosphere or oceans. Atmospheric and oceanic GCMs (AGCM and OGCM ) are key components along with sea ice and land-surface components. GCMs and global climate models are used for weather forecasting , understanding the climate , and forecasting climate change . Atmospheric GCMs (AGCMs) model the atmosphere and impose sea surface temperatures as boundary conditions. Coupled atmosphere-ocean GCMs (AOGCMs, e.g. HadCM3 , EdGCM , GFDL CM2.X , ARPEGE-Climat) [4] combine the two models. The first general circulation climate model that combined both oceanic and atmospheric processes was developed in the late 1960s at the NOAA Geophysical Fluid Dynamics Laboratory [5] AOGCMs represent the pinnacle of complexity in climate models and internalise as many processes as possible. However, they are still under development and uncertainties remain.  They may be coupled to models of other processes, such as the carbon cycle , so as to better model feedback effects. Such integrated multi-system models are sometimes referred to as either "earth system models" or "global climate models." Versions designed for decade to century time scale climate applications were originally created by Syukuro Manabe and Kirk Bryan at the Geophysical Fluid Dynamics Laboratory (GFDL) in Princeton, New Jersey . [3] These models are based on the integration of a variety of fluid dynamical, chemical and sometimes biological equations. Energy balance models (EBMs)[ edit ] Simulation of the climate system in full 3-D space and time was impractical prior to the establishment of large computational facilities starting in the 1960s.  In order to begin to understand which factors may have changed Earth's paleoclimate states, the constituent and dimensional complexities of the system needed to be reduced.  A simple quantitative model that balanced incoming/outgoing energy was first developed for the atmosphere in the late 19th century. [6] Other EBMs similarly seek an economical description of surface temperatures by applying the conservation of energy constraint to individual columns of the Earth-atmosphere system. [7] Essential features of EBMs include their relative conceptual simplicity and their ability to sometimes produce analytical solutions . [8] : 19 Some models account for effects of ocean, land, or ice features on the surface budget.   Others include interactions with parts of the water cycle or carbon cycle .  A variety of these and other reduced system models can be useful for specialized tasks that supplement GCMs, particularly to bridge gaps between simulation and understanding. [9] [10] Zero-dimensional models[ edit ] Zero-dimensional models consider Earth as a point in space, analogous to the pale blue dot viewed by Voyager 1 or an astronomer's view of very distant objects.    This dimensionless view while highly limited is still useful in that the laws of physics are applicable in a bulk fashion to unknown objects,  or in an appropriate lumped manner if some major properties of the object are known.   For example, astronomers know that most planets in our own solar system feature some kind of solid/liquid surface surrounded by a gaseous atmosphere. Model with combined surface and atmosphere[ edit ] A very simple model of the radiative equilibrium of the Earth is (
Toggle the table of contents Group on Earth Observations Wikimedia Commons From Wikipedia, the free encyclopedia ) Logo The Group on Earth Observations (GEO) coordinates international efforts to build a Global Earth Observation System of Systems (GEOSS). It links existing and planned Earth observation systems and supports the development of new ones in cases of perceived gaps in the supply of environment-related information. It aims to construct a global public infrastructure for Earth observations consisting of a flexible and distributed network of systems and content providers. Concept[ edit ] Common Earth observation instruments include ocean buoys , meteorological stations and balloons, seismic and Global Positioning System (GPS) stations, remote-sensing satellites, computerized forecasting models and early warning systems . These instruments are used to measure and monitor specific aspects of Earth’s physical, chemical and biological systems. To be useful, the raw data collected must be processed, archived, interpreted, and made available via easy-to-use channels in the form of information comprehensible not only by remote sensing experts. Earth observations are vital for policymaking and assessment in many fields. GEO focuses on facilitating access to Earth observation data for nine priority areas: natural and human-induced disasters, environmental sources of health hazards, energy management, climate change and its impacts, freshwater resources, weather forecasting, ecosystem management, sustainable agriculture , and biodiversity conservation. History and structure[ edit ] GEO was a result of the first ever Earth Observing Summit held in Washington, D.C. in 2003.  The Summit event was the result of efforts by NOAA Administrator Conrad Lautenbacher who spearheaded the GEO efforts as part of the Bush '43 Administration. [1] GEO was established formally in February 2005 by the Third Earth Observation Summit in Brussels at the end of a process that started in 2003 with the First Earth Observation Summit in Washington, DC. [2] It was launched in response to calls for action by the 2002 World Summit on Sustainable Development [3] and the Group of Eight ( G8 ) leading industrialized countries. [4] These high-level meetings recognized that international collaboration is essential for exploiting the growing potential of Earth observations to support decision making in an increasingly complex and environmentally stressed world. GEO is a voluntary partnership of governments and international organizations. It provides a framework within which these partners can develop new projects and coordinate their strategies and investments. As of January 2016, GEO’s membership includes 102 governments including the European Commission . In addition, 92 intergovernmental, international and regional organizations with a mandate in Earth observation or related issues have been recognized as participating organizations (see lists below). Each member and participating organization is represented by a principal and a principal alternate. Members make financial contributions to GEO on a voluntary basis. GEO is constructing GEOSS on the basis of a 10-year strategic plan from 2016 to 2025. The plan defines a vision statement for GEOSS, its purpose and scope, expected benefits, and eight “Societal Benefit Areas” (disaster resilience, public health surveillance, energy and mineral resources management, sustainable urban development, water resources management, biodiversity and ecosystem sustainability, food security and sustainable agriculture and infrastructure and transport management - with climate as a cross-cutting issue), technical and capacity-building priorities, and the GEO governance structure. [5] GEO is governed by a plenary consisting of all members and participating organizations. GEO meets in plenary at least once a year at the level of senior officials and periodically at the ministerial level. Members make decisions at the plenary by consensus.
Toggle the table of contents Global Earth Observation System of Systems 8 languages "GEOSS" redirects here. The term may also refer to the Geometric Centre of the Republic of Slovenia . The Global Earth Observation System of Systems (GEOSS) was built by the Group on Earth Observations (GEO) on the basis of a 10-Year Implementation Plan running from 2005 to 2015. [1] GEOSS seeks to connect the producers of environmental data and decision-support tools with the end users of these products, with the aim of enhancing the relevance of Earth observations to global issues . GEOSS aims to produce a global public infrastructure that generates comprehensive, near-real-time environmental data, information and analyses for a wide range of users. The Secretariat Director of Geoss is Barbara Ryan. [2] Earth observation systems[ edit ] Earth observation systems consist of instruments and models designed to measure, monitor and predict the physical, chemical and biological aspects of the Earth system. Buoys floating in the oceans monitor temperature and salinity; meteorological stations and balloons record air quality and rainwater trends; sonar and radar systems estimate fish and bird populations; seismic and Global Positioning System (GPS) stations record movements in the Earth's crust and interior; some 60-plus high-tech environmental satellites scan the planet from space; powerful computerized models generate simulations and forecasts; and early warning systems issue alerts to vulnerable populations. These various systems have typically operated in isolation from one another. In recent years, however, sophisticated new technologies for gathering vast quantities of near-real-time and high-resolution Earth observation data have become operational. At the same time, improved forecasting models and decision-support tools are increasingly allowing decision makers and other users of Earth observations to fully exploit this widening stream of information. With investments in Earth observations now reaching a critical mass, it has become possible to link diverse observing systems together to paint a full picture of the Earth's condition. Because the costs and logistics of expanding Earth observations are daunting for any single nation, linking systems together through international cooperation also offers cost savings. Implementation[ edit ] As a networked system, GEOSS is owned by all of the GEO Members and Participating Organizations. Partners maintain full control of the components and activities that they contribute to the system of systems. [3] Implementation is being pursued through a Work Plan consisting of over 70 tasks. Each task supports one of the nine societal-benefit or four transverse areas and is carried out by interested Members and Participating Organizations. Governments and organizations have also advanced GEOSS by contributing a variety of “Early Achievements”; these “First 100 Steps to GEOSS” were presented to the 2007 Cape Town Ministerial Summit. Interlinking observation systems requires common standards for architecture and data sharing. The architecture of an Earth observation system refers to the way in which its components are designed so that they function as a whole. Each GEOSS component must be included in the GEOSS registry and configured so that it can communicate with the other participating systems. In addition, each contributor to GEOSS must subscribe to the GEO data-sharing principles, which aim to ensure the full and open exchange of data, metadata and products. These issues are fundamental to the successful operation of GEOSS. [4] [5] [6] [7] GEOSS will disseminate information and analyses directly to users. GEO is developing the GEOPortal as a single Internet gateway to the data produced by GEOSS. The purpose of GEOPortal is to make it easier to integrate diverse data sets, identify relevant data and portals of contributing systems, and access models and other decision-support tools. For users without good access to high-speed internet, GEO has established GEONETCast, a system of four communications satellites that transmit data to low-cost receiving stations maintained by the users. [8] At present, GEONETCast seems still in its infancy, yet some tools have already been worked out. The GEONETCast toolbox has been made available and contains tools to access some radar altimetry, vegetation, satellite prediction and maritime information. [9] Other useful information available through GEONETCast is vegetation and desert locust information provided under the DevCoCast project, which is a subproject of GEONETCast. [10] [11] User groups[ edit ] The growing demand for Earth observation data and information is the driving force behind GEOSS. The GEOSS Implementation Plan identifies nine distinct groups of users and uses, which it calls “ Societal Benefit Areas ”. The nine areas are disasters, health, energy, climate, water, weather, ecosystems, agriculture and biodiversity. Current and potential users include decision makers in the public and private sectors, resource managers, planners, emergency responders and scientists. [12] [13] [14] [15] Related initiatives[ edit ] GEOSS can be characterized as a contribution towards the establishment of a spatial data infrastructure .  It is one of three related initiatives that are the subject of the GIGAS (GEOSS, INSPIRE and GMES an Action in Support) harmonization project under the auspices of the EU 7th Framework Programme . [16]
Cygnus Commercial Resupply Services missions approaching International Space Station Commercial Resupply Services (CRS) are a contract solution to deliver cargo and supplies to the International Space Station (ISS) on a commercial basis. [35] NASA signed its first CRS contracts in 2008 and awarded $1.6 billion to SpaceX for twelve cargo Dragon and $1.9 billion to Orbital Sciences [note 1] for eight Cygnus flights, covering deliveries to 2016. Both companies evolved or created their launch vehicle products to support the solution (SpaceX with The Falcon 9 and Orbital with the Antares ). SpaceX flew its first operational resupply mission ( SpaceX CRS-1 ) in 2012. [36] Orbital Sciences followed in 2014 ( Cygnus CRS Orb-1 ). [37] In 2015, NASA extended CRS-1 to twenty flights for SpaceX and twelve flights for Orbital ATK . [note 1] [38] [39] A second phase of contracts (known as CRS-2) was solicited in 2014; contracts were awarded in January 2016 to Orbital ATK [note 1] Cygnus , Sierra Nevada Corporation Dream Chaser , and SpaceX Dragon 2 , for cargo transport flights beginning in 2019 and expected to last through 2024. In March 2022, NASA awarded an additional six CRS-2 missions each to both SpaceX and Northrop Grumman (formerly Orbital). [40] Northrop Grumman successfully delivered Cygnus NG-17 to the ISS in February 2022. [41] In July 2022, SpaceX launched its 25th CRS flight ( SpaceX CRS-25 ) and successfully delivered its cargo to the ISS. [42] In late 2022, Sierra Nevada continued to assemble their Dream Chaser CRS solution; current estimates put its first launch in early 2023. [43] Commercial Crew Program (2011–present) Further information: Commercial Crew Program The Crew Dragon (left) and Starliner (right) approaching the ISS on their respective missions The Commercial Crew Program (CCP) provides commercially operated crew transportation service to and from the International Space Station (ISS) under contract to NASA, conducting crew rotations between the expeditions of the International Space Station program . American space manufacturer SpaceX began providing service in 2020, using the Crew Dragon spacecraft, [44] and NASA plans to add Boeing when its Boeing Starliner spacecraft becomes operational some time after 2024. [45] NASA has contracted for six operational missions from Boeing and fourteen from SpaceX, ensuring sufficient support for ISS through 2030. [46] The spacecraft are owned and operated by the vendor, and crew transportation is provided to NASA as a commercial service. Each mission sends up to four astronauts to the ISS, with an option for a fifth passenger available. Operational flights occur approximately once every six months for missions that last for approximately six months. A spacecraft remains docked to the ISS during its mission, and missions usually overlap by at least a few days. Between the retirement of the Space Shuttle in 2011 and the first operational CCP mission in 2020, NASA relied on the Soyuz program to transport its astronauts to the ISS. A Crew Dragon spacecraft is launched to space atop a Falcon 9 Block 5 launch vehicle and the capsule returns to Earth via splashdown in the ocean near Florida. The program's first operational mission, SpaceX Crew-1 , launched on November 16, 2020. [47] Boeing Starliner operational flights will now commence after its final test flight which was launched atop an Atlas V N22 launch vehicle. Instead of a splashdown, a Starliner capsule returns on land with airbags at one of four designated sites in the western United States. [48] Artemis (2017–present) Further information: Artemis program SLS with Orion rolling to Launch Complex 39B for tests, Mar 2022 Since 2017, NASA's crewed spaceflight program has been the Artemis program , which involves the help of US commercial spaceflight companies and international partners such as ESA , JAXA , and CSA . [49] The goal of this program is to land "the first woman and the next man" on the lunar south pole region by 2025. Artemis would be the first step towards the long-term goal of establishing a sustainable presence on the Moon, laying the foundation for companies to build a lunar economy, and eventually sending humans to Mars . The Orion Crew Exploration Vehicle was held over from the canceled Constellation program for Artemis. Artemis 1 was the uncrewed initial launch of Space Launch System (SLS) that would also send an Orion spacecraft on a Distant Retrograde Orbit . [50] NASA's next major space initiative is to be the construction of the Lunar Gateway , a small space station in lunar orbit. [51] This space station will be designed primarily for non-continuous human habitation. The first tentative steps of returning to crewed lunar missions will be Artemis 2 , which is to include the Orion crew module, propelled by the SLS, and is to launch in 2025. [49] [52] This mission is to be a 10-day mission planned to briefly place a crew of four into a Lunar flyby . [53] The construction of the Gateway would begin with the proposed Artemis 3, which is planned to deliver a crew of four to Lunar orbit along with the first modules of the Gateway. This mission would last for up to 30 days. NASA plans to build full scale deep space habitats such as the Lunar Gateway and the Nautilus-X as part of its Next Space Technologies for Exploration Partnerships (NextSTEP) program. [54] In 2017, NASA was directed by the congressional NASA Transition Authorization Act of 2017 to get humans to Mars-orbit (or to the Martian surface) by the 2030s. [55] [56] In support of the Artemis missions, NASA has been funding private companies to land robotic probes on the lunar surface in a program known as the Commercial Lunar Payload Services . As of March 2022, NASA has awarded contracts for robotic lunar probes to companies such as Intuitive Machines , Firefly Space Systems , and Astrobotic . [57] On April 16, 2021, NASA announced they had selected the SpaceX Lunar Starship as its Human Landing System. The agency's Space Launch System rocket will launch four astronauts aboard the Orion spacecraft for their multi-day journey to lunar orbit where they will transfer to SpaceX's Starship for the final leg of their journey to the surface of the Moon. [58] In November 2021, it was announced that the goal of landing astronauts on the Moon by 2024 had slipped to no earlier than 2025 due to numerous factors. Artemis 1 launched on November 16, 2022, and returned to Earth safely on December 11, 2022. As of June 2022, NASA plans to launch Artemis 2 in May 2024 and Artemis 3 in December 2025. [59] [60] Additional Artemis missions, Artemis 4 and Artemis 5 , are planned to launch after 2025. [61] Commercial LEO Development (2021–present) Further information: Commercial LEO Destinations program The Commercial Low Earth Orbit Destinations program is an initiative by NASA to support work on commercial space stations that the agency hopes to have in place by the end of the current decade to replace the "International Space Station". The three selected companies are: Blue Origin (et al.) with their Orbital Reef station concept, Nanoracks (et al.) with their Starlab Space Station concept, and Northrop Grumman with a station concept based on the HALO-module for the Gateway station. [62] Robotic exploration Further information: List of NASA missions and List of uncrewed NASA missions Video of many of the uncrewed missions used to explore the outer reaches of space NASA has conducted many uncrewed and robotic spaceflight programs throughout its history. More than 1,000 uncrewed missions have been designed to explore the Earth and the Solar System. [63] Mission selection process NASA executes a mission development framework to plan, select, develop, and operate robotic missions. This framework defines cost, schedule and technical risk parameters to enable competitive selection of missions involving mission candidates that have been developed by principal investigators and their teams from across NASA, the broader U.S. Government research and development stakeholders, and industry. The mission development construct is defined by four umbrella programs. Explorer program Further information: Explorers Program The Explorer program derives its origin from the earliest days of the U.S. Space program. In current form, the program consists of three classes of systems – Small Explorers (SMEX) , Medium Explorers (MIDEX) , and University-Class Explorers (UNEX) missions. The NASA Explorer program office provides frequent flight opportunities for moderate cost innovative solutions from the heliophysics and astrophysics science areas. The Small Explorer missions are required to limit cost to NASA to below $150M (2022 dollars). Medium class explorer missions have typically involved NASA cost caps of $350M. The Explorer program office is based at NASA Goddard Space Flight Center. [64] Discovery program Further information: Discovery Program The NASA Discovery program develops and delivers robotic spacecraft solutions in the planetary science domain. Discovery enables scientists and engineers to assemble a team to deliver a solution against a defined set of objectives and competitively bid that solution against other candidate programs. Cost caps vary but recent mission selection processes were accomplished using a $500M cost cap for NASA. The Planetary Mission Program Office is based at the NASA Marshall Space Flight Center and manages both the Discovery and New Frontiers missions. The office is part of the Science Mission Directorate. [65] NASA Administrator Bill Nelson announced on June 2, 2021, that the DAVINCI + and VERITAS missions were selected to launch to Venus in the late 2020s, having beat out competing proposals for missions to Jupiter's volcanic moon Io and Neptune's large moon Triton that were also selected as Discovery program finalists in early 2020. Each mission has an estimated cost of $500 million, with launches expected between 2028 and 2030. Launch contracts will be awarded later in each mission's development. [66] New Frontiers program Further information: New Frontiers program The New Frontiers program focuses on specific Solar System exploration goals identified as top priorities by the planetary science community. Primary objectives include Solar System exploration employing medium class spacecraft missions to conduct high-science-return investigations. New Frontiers builds on the development approach employed by the Discovery program but provides for higher cost caps and schedule durations than are available with Discovery. Cost caps vary by opportunity; recent missions have been awarded based on a defined cap of $1 billion. The higher cost cap and projected longer mission durations result in a lower frequency of new opportunities for the program – typically one every several years. OSIRIS-REx and New Horizons are examples of New Frontiers missions. [67] NASA has determined that the next opportunity to propose for the fifth round of New Frontiers missions will occur no later than the fall of 2024. Missions in NASA's New Frontiers Program tackle specific Solar System exploration goals identified as top priorities by the planetary science community. Exploring the Solar System with medium-class spacecraft missions that conduct high-science-return investigations is NASA's strategy to further understand the Solar System. [68] Large strategic missions Further information: Large strategic science missions Large strategic missions (formerly called Flagship missions) are strategic missions that are typically developed and managed by large teams that may span several NASA centers. The individual missions become the program as opposed to being part of a larger effort (see Discovery, New Frontiers, etc.). The James Webb Space Telescope is a strategic mission that was developed over a period of more than 20 years. Strategic missions are developed on an ad-hoc basis as program objectives and priorities are established. Missions like Voyager, had they been developed today, would have been strategic missions. Three of the Great Observatories were strategic missions (the Chandra X-ray Observatory , the Compton Gamma Ray Observatory , and the Hubble Space Telescope ). Europa Clipper is the next large strategic mission in development by NASA. Planetary science missions NASA continues to play a material role in exploration of the Solar System as it has for decades. Ongoing missions have current science objectives with respect to more than five extraterrestrial bodies within the Solar System – Moon ( Lunar Reconnaissance Orbiter ), Mars ( Perseverance rover), Jupiter ( Juno ), asteroid Bennu ( OSIRIS-REx ), and Kuiper Belt Objects ( New Horizons ). The Juno extended mission will make multiple flybys of the Jovian moon Io in 2023 and 2024 after flybys of Ganymede in 2021 and Europa in 2022. Voyager 1 and Voyager 2 continue to provide science data back to Earth while continuing on their outward journeys into interstellar space. On November 26, 2011, NASA's Mars Science Laboratory mission was successfully launched for Mars. The Curiosity rover successfully landed on Mars on August 6, 2012, and subsequently began its search for evidence of past or present life on Mars. [69] [70] [71] In September 2014, NASA's MAVEN spacecraft, which is part of the Mars Scout Program , successfully entered Mars orbit and, as of October 2022, continues its study of the atmosphere of Mars . [72] [73] NASA's ongoing Mars investigations include in-depth surveys of Mars by the Perseverance rover and InSight ). NASA's Europa Clipper , planned for launch in October 2024, will study the Galilean moon Europa through a series of flybys while in orbit around Jupiter. Dragonfly will send a mobile robotic rotorcraft to Saturn's biggest moon, Titan . [74] As of May 2021, Dragonfly is scheduled for launch in June 2027. [75] [76] Astrophysics missions NASA astrophysics spacecraft fleet, credit NASA GSFC , 2022 The NASA Science Mission Directorate Astrophysics division manages the agency's astrophysics science portfolio. NASA has invested significant resources in the development, delivery, and operations of various forms of space telescopes. These telescopes have provided the means to study the cosmos over a large range of the electromagnetic spectrum. [77] The Great Observatories that were launched in the 1980s and 1990s have provided a wealth of observations for study by physicists across the planent. The first of them, the Hubble Space Telescope , was delivered to orbit in 1990 and continues to function, in part due to prior servicing missions performed by the Space Shuttle. [78] [79] The other remaining active great observatories include the Chandra X-ray Observatory (CXO), launched by STS-93 in July 1999 and is now in a 64-hour elliptical orbit studying X-ray sources that are not readily viewable from terrestrial observatories. [80] Chandra X-ray Observatory (rendering), 2015 The Imaging X-ray Polarimetry Explorer (IXPE) is a space observatory designed to improve the understanding of X-ray production in objects such as neutron stars and pulsar wind nebulae, as well as stellar and supermassive black holes. [81] IXPE launched in December 2021 and is an international collaboration between NASA and the Italian Space Agency (ASI). It is part of the NASA Small Explorers program (SMEX) which designs low-cost spacecraft to study heliophysics and astrophysics. [82] The Neil Gehrels Swift Observatory was launched in November 2004 and is Gamma-ray burst observatory that also monitors the afterglow in X-ray, and UV/Visible light at the location of a burst. [83] The mission was developed in a joint partnership between Goddard Space Flight Center (GSFC) and an international consortium from the United States, United Kingdom, and Italy. Pennsylvania State University operates the mission as part of NASA's Medium Explorer program (MIDEX). [84] The Fermi Gamma-ray Space Telescope (FGST) is another gamma-ray focused space observatory that was launched to low Earth orbit in June 2008 and is being used to perform gamma-ray astronomy observations. [85] In addition to NASA, the mission involves the United States Department of Energy , and government agencies in France, Germany, Italy, Japan, and Sweden. [86] The James Webb Space Telescope (JWST), launched in December 2021 on an Ariane 5 rocket, operates in a halo orbit circling the Sun-Earth L2 point. [87] [88] [89] JWST's high sensitivity in the infrared spectrum and its imaging resolution will allow it to view more distant, faint, or older objects than its predecessors, including Hubble. [90] Earth Sciences Program missions (1965–present) Further information: NASA Earth Science Schematic of NASA Earth Science Division operating satellite missions as of February 2015 NASA Earth Science is a large, umbrella program comprising a range of terrestrial and space-based collection systems in order to better understand the Earth system and its response to natural and human-caused changes. Numerous systems have been developed and fielded over several decades to provide improved prediction for weather, climate, and other changes in the natural environment. Several of the current operating spacecraft programs include: Aqua , [91] Aura , [92] Orbiting Carbon Observatory 2 (OCO-2), [93] Gravity Recovery and Climate Experiment Follow-on (GRACE FO) , [94] and Ice, Cloud, and land Elevation Satellite 2 (ICESat-2) . [95] In addition to systems already in orbit, NASA is designing a new set of Earth Observing Systems to study, assess, and generate responses for climate change, natural hazards, forest fires, and real-time agricultural processes. [96] The GOES-T satellite (designated GOES-18 after launch) joined the fleet of U.S. geostationary weather monitoring satellites in March 2022. [97] NASA also maintains the Earth Science Data Systems (ESDS) program to oversee the life cycle of NASA's Earth science data — from acquisition through processing and distribution. The primary goal of ESDS is to maximize the scientific return from NASA's missions and experiments for research and applied scientists, decision makers, and society at large. [98] The Earth Science program is managed by the Earth Science Division of the NASA Science Mission Directorate. Space operations architecture NASA invests in various ground and space-based infrastructures to support its science and exploration mandate. The agency maintains access to suborbital and orbital space launch capabilities and sustains ground station solutions to support its evolving fleet of spacecraft and remote systems. Deep Space Network (1963–present) Further information: NASA Deep Space Network The NASA Deep Space Network (DSN) serves as the primary ground station solution for NASA's interplanetary spacecraft and select Earth-orbiting missions. [99] The system employs ground station complexes near Barstow California in the United States, in Spain near Madrid, and in Australia near Canberra. The placement of these ground stations approximately 120 degrees apart around the planet provides the ability for communications to spacecraft throughout the Solar System even as the Earth rotates about its axis on a daily basis. The system is controlled at a 24x7 operations center at JPL in Pasadena California which manages recurring communications linkages with up to 40 spacecraft. [100] The system is managed by the Jet Propulsion Laboratory (JPL). [99] Near Space Network (1983–present) Further information: Near Earth Network and Tracking and Data Relay Satellite System Near Earth Network Ground Stations, 2021 The Near Space Network (NSN) provides telemetry, commanding, ground-based tracking, data and communications services to a wide range of customers with satellites in low earth orbit (LEO), geosynchronous orbit (GEO), highly elliptical orbits (HEO), and lunar orbits. The NSN accumulates ground station and antenna assets from the Near-Earth Network and the Tracking and Data Relay Satellite System (TDRS) which operates in geosynchronous orbit providing continuous real-time coverage for launch vehicles and low earth orbit NASA missions. [101] The NSN consists of 19 ground stations worldwide operated by the US Government and by contractors including Kongsberg Satellite Services (KSAT), Swedish Space Corporation (SSC), and South African National Space Agency (SANSA). [102] The ground network averages between 120 and 150 spacecraft contacts a day with TDRS engaging with systems on a near-continuous basis as needed; the system is managed and operated by the Goddard Space Flight Center. [103] Sounding Rocket Program (1959–present) NASA sounding rocket launch from the Wallops Flight Facility The NASA Sounding Rocket Program (NSRP) is located at the Wallops Flight Facility and provides launch capability, payload development and integration, and field operations support to execute suborbital missions. [104] The program has been in operation since 1959 and is managed by the Goddard Space Flight Center using a combined US Government and contractor team. [105] The NSRP team conducts approximately 20 missions per year from both Wallops and other launch locations worldwide to allow scientists to collect data "where it occurs". The program supports the strategic vision of the Science Mission Directorate collecting important scientific data for earth science, heliophysics, and astrophysics programs. [104] In June 2022, NASA conducted its first rocket launch from a commercial spaceport outside the US. It launched a Black Brant IX from the Arnhem Space Centre in Australia. [106] Launch Services Program (1990–present) Further information: NASA Launch Services Program The NASA Launch Services Program (LSP) is responsible for procurement of launch services for NASA uncrewed missions and oversight of launch integration and launch preparation activity, providing added quality and mission assurance to meet program objectives. [107] Since 1990, NASA has purchased expendable launch vehicle launch services directly from commercial providers, whenever possible, for its scientific and applications missions. Expendable launch vehicles can accommodate all types of orbit inclinations and altitudes and are ideal vehicles for launching Earth-orbit and interplanetary missions. LSP operates from Kennedy Space Center and falls under the NASA Space Operations Mission Directorate (SOMD). [108] [109] Aeronautics Research Further information: NASA research and Aeronautics Research Mission Directorate The Aeronautics Research Mission Directorate (ARMD) is one of five mission directorates within NASA, the other four being the Exploration Systems Development Mission Directorate, the Space Operations Mission Directorate, the Science Mission Directorate , and the Space Technology Mission Directorate. [110] The ARMD is responsible for NASA's aeronautical research, which benefits the commercial , military , and general aviation sectors. ARMD performs its aeronautics research at four NASA facilities: Ames Research Center and Armstrong Flight Research Center in California, Glenn Research Center in Ohio, and Langley Research Center in Virginia. [111] NASA X-57 Maxwell aircraft (2016–present) Further information: NASA X-57 Maxwell The NASA X-57 Maxwell is an experimental aircraft being developed by NASA to demonstrate the technologies required to deliver a highly efficient all-electric aircraft. [112] The primary goal of the program is to develop and deliver all-electric technology solutions that can also achieve airworthiness certification with regulators. The program involves development of the system in several phases, or modifications, to incrementally grow the capability and operability of the system. The initial configuration of the aircraft has now completed ground testing as it approaches its first flights. In mid-2022, the X-57 was scheduled to fly before the end of the year. [113] The development team includes staff from the NASA Armstrong, Glenn, and Langley centers along with number of industry partners from the United States and Italy. [114] Next Generation Air Transportation System (2007–present) Further information: Next Generation Air Transportation System NASA is collaborating with the Federal Aviation Administration and industry stakeholders to modernize the United States National Airspace System (NAS). Efforts began in 2007 with a goal to deliver major modernization components by 2025. [115] The modernization effort intends to increase the safety, efficiency, capacity, access, flexibility, predictability, and resilience of the NAS while reducing the environmental impact of aviation . [116] The Aviation Systems Division of NASA Ames operates the joint NASA/FAA North Texas Research Station. The station supports all phases of NextGen research, from concept development to prototype system field evaluation. This facility has already transitioned advanced NextGen concepts and technologies to use through technology transfers to the FAA. [115] NASA contributions also include development of advanced automation concepts and tools that provide air traffic controllers, pilots, and other airspace users with more accurate real-time information about the nation's traffic flow, weather, and routing. Ames' advanced airspace modeling and simulation tools have been used extensively to model the flow of air traffic flow across the U.S., and to evaluate new concepts in airspace design, traffic flow management, and optimization. [117] Technology research For technologies funded or otherwise supported by NASA, see NASA spinoff technologies . Nuclear in-space power and propulsion (ongoing) NASA has made use of technologies such as the multi-mission radioisotope thermoelectric generator (MMRTG), which is a type of radioisotope thermoelectric generator used to power spacecraft. [118] Shortages of the required plutonium-238 have curtailed deep space missions since the turn of the millennium. [119] An example of a spacecraft that was not developed because of a shortage of this material was New Horizons 2 . [119] In July 2021, NASA announced contract awards for development of nuclear thermal propulsion reactors. Three contractors will develop individual designs over 12 months for later evaluation by NASA and the U.S. Department of Energy . [120] NASA's space nuclear technologies portfolio are led and funded by its Space Technology Mission Directorate. In January 2023, NASA announced a partnership with Defense Advanced Research Projects Agency on the Demonstration Rocket for Agile Cislunar Operations (DRACO) program to demonstrate a NTR engine in space, an enabling capability for NASA missions to Mars. [121] In July 2023, NASA and DARPA jointly announced the award of $499 million to Lockheed Martin to design and build an experimental NTR rocket to be launched in 2027. [122] Other initiatives Free Space Optics. NASA contracted a third party to study the probability of using Free Space Optics (FSO) to communicate with Optical ( laser ) Stations on the Ground (OGS) called laser-com RF networks for satellite communications. [123] Water Extraction from Lunar Soil. On July 29, 2020, NASA requested American universities to propose new technologies for extracting water from the lunar soil and developing power systems. The idea will help the space agency conduct sustainable exploration of the Moon. [124] In 2024, NASA was tasked by the US Government to create a Time standard for the Moon . The standard is to be called Coordinated Lunar Time and is expected to be finalized in 2026. [125] Human Spaceflight Research (2005–present) SpaceX Crew-4 astronaut Samantha Cristoforetti operating the rHEALTH ONE on the ISS to address key health risks for space travel NASA's Human Research Program (HRP) is designed to study the effects of space on human health and also to provide countermeasures and technologies for human space exploration. The medical effects of space exploration are reasonably limited in low Earth orbit or in travel to the Moon. Travel to Mars is significantly longer and deeper into space, significant medical issues can result. These include bone density loss, radiation exposure, vision changes, circadian rhythm disturbances, heart remodeling, and immune alterations. In order to study and diagnose these ill-effects, HRP has been tasked with identifying or developing small portable instrumentation with low mass, volume, and power to monitor the health of astronauts. [126] To achieve this aim, on May 13, 2022, NASA and SpaceX Crew-4 astronauts successfully tested its rHEALTH ONE universal biomedical analyzer for its ability to identify and analyzer biomarkers, cells, microorganisms, and proteins in a spaceflight environment. [127] Planetary Defense (2016–present) Further information: Planetary Defense Coordination Office and Near Earth Objects NASA established the Planetary Defense Coordination Office (PDCO) in 2016 to catalog and track potentially hazardous near-Earth objects (NEO), such as asteroids and comets and develop potential responses and defenses against these threats. [128] The PDCO is chartered to provide timely and accurate information to the government and the public on close approaches by Potentially hazardous objects (PHOs) and any potential for impact. The office functions within the Science Mission Directorate Planetary Science division. [129] The PDCO augmented prior cooperative actions between the United States, the European Union, and other nations which had been scanning the sky for NEOs since 1998 in an effort called Spaceguard . [130] Near Earth object detection (1998–present) From the 1990s NASA has run many NEO detection programs from Earth bases observatories, greatly increasing the number of objects that have been detected. Many asteroids are very dark and those near the Sun are much harder to detect from Earth-based telescopes which observe at night, and thus face away from the Sun. NEOs inside Earth orbit only reflect a part of light also rather than potentially a "full Moon" when they are behind the Earth and fully lit by the Sun. In 1998, the United States Congress gave NASA a mandate to detect 90% of near-Earth asteroids over 1 km (0.62 mi) diameter (that threaten global devastation) by 2008. [131] This initial mandate was met by 2011. [132] In 2005, the original USA Spaceguard mandate was extended by the George E. Brown, Jr. Near-Earth Object Survey Act, which calls for NASA to detect 90% of NEOs with diameters of 140 m (460 ft) or greater, by 2020 (compare to the 20-meter Chelyabinsk meteor that hit Russia in 2013). [133] As of January 2020 [update] , it is estimated that less than half of these have been found, but objects of this size hit the Earth only about once in 2,000 years. [134] In January 2020, NASA officials estimated it would take 30 years to find all objects meeting the 140 m (460 ft) size criteria, more than twice the timeframe that was built into the 2005 mandate. [135] In June 2021, NASA authorized the development of the NEO Surveyor spacecraft to reduce that projected duration to achieve the mandate down to 10 years. [136] [137] Involvement in current robotic missions NASA has incorporated planetary defense objectives into several ongoing missions. In 1999, NASA visited 433 Eros with the NEAR Shoemaker spacecraft which entered its orbit in 2000, closely imaging the asteroid with various instruments at that time. [138] NEAR Shoemaker became the first spacecraft to successfully orbit and land on an asteroid, improving our understanding of these bodies and demonstrating our capacity to study them in greater detail. [139] OSIRIS-REx used its suite of instruments to transmit radio tracking signals and capture optical images of Bennu during its study of the asteroid that will help NASA scientists determine its precise position in the solar system and its exact orbital path. As Bennu has the potential for recurring approaches to the Earth-Moon system in the next 100–200 years, the precision gained from OSIRIS-REx will enable scientists to better predict the future gravitational interactions between Bennu and our planet and resultant changes in Bennu's onward flight path. [140] [141] The WISE/NEOWISE mission was launched by NASA JPL in 2009 as an infrared-wavelength astronomical space telescope. In 2013, NASA repurposed it as the NEOWISE mission to find potentially hazardous near-Earth asteroids and comets; its mission has been extended into 2023. [142] [143] NASA and Johns Hopkins Applied Physics Laboratory (JHAPL) jointly developed the first planetary defense purpose-built satellite, the Double Asteroid Redirection Test (DART) to test possible planetary defense concepts. [144] DART was launched in November 2021 by a SpaceX Falcon 9 from California on a trajectory designed to impact the Dimorphos asteroid. Scientists were seeking to determine whether an impact could alter the subsequent path of the asteroid; a concept that could be applied to future planetary defense. [145] On September 26, 2022, DART hit its target. In the weeks following impact, NASA declared DART a success, confirming it had shortened Dimorphos' orbital period around Didymos by about 32 minutes, surpassing the pre-defined success threshold of 73 seconds. [146] [147] NEO Surveyor , formerly called the Near-Earth Object Camera (NEOCam) mission, is a space-based infrared telescope under development to survey the Solar System for potentially hazardous asteroids . [148] The spacecraft is scheduled to launch in 2026. Study of Unidentified Aerial Phenomena (2022–present) In June 2022, the head of the NASA Science Mission Directorate , Thomas Zurbuchen , confirmed the start of NASA's UAP independent study team . [149] At a speech before the National Academies of Science, Engineering and Medicine, Zurbuchen said the space agency would bring a scientific perspective to efforts already underway by the Pentagon and intelligence agencies to make sense of dozens of such sightings. He said it was "high-risk, high-impact" research that the space agency should not shy away from, even if it is a controversial field of study. [150] Collaboration NASA Advisory Council In response to the Apollo 1 accident, which killed three astronauts in 1967, Congress directed NASA to form an Aerospace Safety Advisory Panel (ASAP) to advise the NASA Administrator on safety issues and hazards in NASA's air and space programs. In the aftermath of the Shuttle Columbia disaster , Congress required that the ASAP submit an annual report to the NASA Administrator and to Congress. [151] By 1971, NASA had also established the Space Program Advisory Council and the Research and Technology Advisory Council to provide the administrator with advisory committee support. In 1977, the latter two were combined to form the NASA Advisory Council (NAC). [152] The NASA Authorization Act of 2014 reaffirmed the importance of ASAP. National Oceanic and Atmospheric Administration (NOAA) Further information: National Oceanic and Atmospheric Administration NASA and NOAA have cooperated for decades on the development, delivery and operation of polar and geosynchronous weather satellites. [153] The relationship typically involves NASA developing the space systems, launch solutions, and ground control technology for the satellites and NOAA operating the systems and delivering weather forecasting products to users. Multiple generations of NOAA Polar orbiting platforms have operated to provide detailed imaging of weather from low altitude. [154] Geostationary Operational Environmental Satellites (GOES) provide near-real-time coverage of the western hemisphere to ensure accurate and timely understanding of developing weather phenomenon. [155] United States Space Force Further information: United States Space Force The United States Space Force (USSF) is the space service branch of the United States Armed Forces , while the National Aeronautics and Space Administration (NASA) is an independent agency of the United States government responsible for civil spaceflight. NASA and the Space Force's predecessors in the Air Force have a long-standing cooperative relationship, with the Space Force supporting NASA launches out of Kennedy Space Center , Cape Canaveral Space Force Station , and Vandenberg Space Force Base , to include range support and rescue operations from Task Force 45. [156] NASA and the Space Force also partner on matters such as defending Earth from asteroids. [157] Space Force members can be NASA astronauts, with Colonel Michael S. Hopkins , the commander of SpaceX Crew-1 , commissioned into the Space Force from the International Space Station on December 18, 2020. [158] [159] [160] In September 2020, the Space Force and NASA signed a memorandum of understanding formally acknowledging the joint role of both agencies. This new memorandum replaced a similar document signed in 2006 between NASA and Air Force Space Command. [161] [162] U.S. Geological Survey Further information: United States Geological Survey and Landsat 9 The Landsat program is the longest-running enterprise for acquisition of satellite imagery of Earth. It is a joint NASA / USGS program. [163] On July 23, 1972, the Earth Resources Technology Satellite was launched. This was eventually renamed to Landsat 1 in 1975. [164] The most recent satellite in the series, Landsat 9 , was launched on September 27, 2021. [165] The instruments on the Landsat satellites have acquired millions of images. The images, archived in the United States and at Landsat receiving stations around the world, are a unique resource for global change research and applications in agriculture , cartography , geology , forestry , regional planning , surveillance and education , and can be viewed through the U.S. Geological Survey (USGS) "EarthExplorer" website. The collaboration between NASA and USGS involves NASA designing and delivering the space system (satellite) solution, launching the satellite into orbit with the USGS operating the system once in orbit. [163] As of October 2022, nine satellites have been built with eight of them successfully operating in orbit. European Space Agency (ESA) Further information: European Space Agency NASA collaborates with the European Space Agency on a wide range of scientific and exploration requirements. [166] From participation with the Space Shuttle (the Spacelab missions) to major roles on the Artemis program (the Orion Service Module), ESA and NASA have supported the science and exploration missions of each agency. There are NASA payloads on ESA spacecraft and ESA payloads on NASA spacecraft. The agencies have developed joint missions in areas including heliophysics (e.g. Solar Orbiter ) [167] and astronomy ( Hubble Space Telescope , James Webb Space Telescope ). [168] Under the Artemis Gateway partnership, ESA will contribute habitation and refueling modules, along with enhanced lunar communications, to the Gateway. [169] [170] NASA and ESA continue to advance cooperation in relation to Earth Science including climate change with agreements to cooperate on various missions including the Sentinel-6 series of spacecraft [171] Japan Aerospace Exploration Agency (JAXA) Further information: Japan Aerospace Exploration Agency NASA and the Japan Aerospace Exploration Agency (JAXA) cooperate on a range of space projects. JAXA is a direct participant in the Artemis program, including the Lunar Gateway effort. JAXA's planned contributions to Gateway include I-Hab's environmental control and life support system, batteries, thermal control, and imagery components, which will be integrated into the module by the European Space Agency (ESA) prior to launch. These capabilities are critical for sustained Gateway operations during crewed and uncrewed time periods. [172] [173] JAXA and NASA have collaborated on numerous satellite programs, especially in areas of Earth science. NASA has contributed to JAXA satellites and vice versa. Japanese instruments are flying on NASA's Terra and Aqua satellites, and NASA sensors have flown on previous Japanese Earth-observation missions. The NASA-JAXA Global Precipitation Measurement mission was launched in 2014 and includes both NASA- and JAXA-supplied sensors on a NASA satellite launched on a JAXA rocket. The mission provides the frequent, accurate measurements of rainfall over the entire globe for use by scientists and weather forecasters. [174] Roscosmos Further information: Roscosmos NASA and Roscosmos have cooperated on the development and operation of the International Space Station since September 1993. [175] The agencies have used launch systems from both countries to deliver station elements to orbit. Astronauts and Cosmonauts jointly maintain various elements of the station. Both countries provide access to the station via launch systems noting Russia's unique role as the sole provider of delivery of crew and cargo upon retirement of the space shuttle in 2011 and prior to commencement of NASA COTS and crew flights. In July 2022, NASA and Roscosmos signed a deal to share space station flights enabling crew from each country to ride on the systems provided by the other. [176] Current geopolitical conditions in late 2022 make it unlikely that cooperation will be extended to other programs such as Artemis or lunar exploration. [177] Indian Space Research Organisation (ISRO) Further information: ISRO In September 2014, NASA and Indian Space Research Organisation (ISRO) signed a partnership to collaborate on and launch a joint radar mission, the NASA-ISRO Synthetic Aperature Radar ( NISAR ) mission. The mission is targeted to launch in 2024. NASA will provide the mission's L-band synthetic aperture radar, a high-rate communication subsystem for science data, GPS receivers, a solid-state recorder and payload data subsystem. ISRO provides the spacecraft bus, the S-band radar, the launch vehicle and associated launch services. [178] [179] Artemis Accords Further information: Artemis Accords The Artemis Accords have been established to define a framework for cooperating in the peaceful exploration and exploitation of the Moon , Mars , asteroids , and comets . The Accords were drafted by NASA and the U.S. State Department and are executed as a series of bilateral agreements between the United States and the participating countries. [180] [181] As of September 2022, 21 countries have signed the accords. They are Australia, Bahrain, Brazil, Canada, Colombia, France, Israel, Italy, Japan, the Republic of Korea, Luxembourg, Mexico, New Zealand, Poland, Romania, the Kingdom of Saudi Arabia, Singapore, Ukraine, the United Arab Emirates, the United Kingdom, and the United States. [182] [183] China National Space Administration Further information: Wolf Amendment and China National Space Administration The Wolf Amendment was passed by the U.S. Congress into law in 2011 and prevents NASA from engaging in direct, bilateral cooperation with the Chinese government and China-affiliated organizations such as the China National Space Administration without the explicit authorization from Congress and the Federal Bureau of Investigation. The law has been renewed annually since by inclusion in annual appropriations bills. [184] Management Administrator Bill Nelson (2021–present) The agency's administration is located at NASA Headquarters in Washington, DC, and provides overall guidance and direction. [185] Except under exceptional circumstances, NASA civil service employees are required to be US citizens . [186] NASA's administrator is nominated by the President of the United States subject to the approval of the US Senate , [187] and serves at the President's pleasure as a senior space science advisor. The current administrator is Bill Nelson , appointed by President Joe Biden , since May 3, 2021. [188] Strategic plan NASA operates with four FY2022 strategic goals. [189] Expand human knowledge through new scientific discoveries Extend human presence to the Moon and on towards Mars for sustainable long-term exploration, development, and utilization Catalyze economic growth and drive innovation to address national challenges Enhance capabilities and operations to catalyze current and future mission success Budget Further information: Budget of NASA NASA budget requests are developed by NASA and approved by the administration prior to submission to the U.S. Congress . Authorized budgets are those that have been included in enacted appropriations bills that are approved by both houses of Congress and enacted into law by the U.S. president. [190] NASA fiscal year budget requests and authorized budgets are provided below. Year Budget Request in bil. US$ Authorized Budget in bil. US$ U.S. GovernmentEmployees
Model of a first generation Meteosat geostationary satellite. EUMETSAT site, Darmstadt, Germany, 2006. Model of a second generation Meteosat geostationary satellite. EUMETSAT site, Darmstadt, Germany, 2006. Model of a MetOp polar satellite (MetOp refers to the Meteorological Operational Satellite Program of Europe, bottom up). EUMETSAT site, Darmstadt, Germany, 2006. There are two types of programmes: Geostationary satellites, providing a continuous view of the Earth disc from a stationary position in space. Polar-orbiting satellites, flying at a much lower altitude, sending back more precise details about atmospheric temperature and moisture profiles, although with less frequent global coverage. High-level, stationary in space (Geostationary satellites)[ edit ] Main article: Meteosat The current provision of geostationary satellite surveillance is enabled by the Meteosat series of satellites  operated by EUMETSAT, generating images of the full Earth disc and data for forecasting. The first generation of Meteosat, launched in 1977, provided continuous, reliable observations to a large user group. In response to demand for more frequent and comprehensive data, Meteostat Second Generation (MSG) was developed with key improvements in swift recognition and prediction of thunderstorms, fog, and the small depressions which can lead to dangerous wind storms. MSG was launched in 2004. To capture foreseeable user needs up to 2025, a Meteostat Third Generation (MTG) is in active preparation. Low-level orbiting (Polar satellites)[ edit ] EUMETSAT Polar System[ edit ] Main article: Metop This article duplicates the scope of other articles, specifically Metop . Please discuss this issue and help introduce a summary style to the article. (July 2014) The lack of observational coverage in certain parts of the globe, particularly the Pacific Ocean and continents of the southern hemisphere , has led to the increasingly important role for polar-orbiting satellite data in numerical weather prediction and climate monitoring. EUMETSAT Polar System (EPS) Metop mission consists of three polar orbiting Metop satellites, to be flown successively for more than 14 years. The first, Metop-A, was launched by a Russian Soyuz-2.1a rocket from Baikonur on October 19, 2006, at 22:28 Baikonur time (16:28 UTC). Metop-A was initially controlled by ESOC for the LEOP phase immediately following launch, with control handed over to EUMETSAT 72 hours after lift-off. EUMETSAT's first commands to the satellite were sent at 14:04 UTC on October 22, 2006. The second EPS satellite, Metop-B, was launched from Baikonur on 17 September 2012, [2] and the third, Metop-C, was launched from Centre Spatial Guyanais in Kourou , French Guiana on 7 November 2018 by Arianespace using a Soyuz ST-B launch vehicle with a Fregat-M upper stage. [3] Positioned at approximately 817 km (508 mi) above the Earth , special instruments on board Metop-A can deliver far more precise details about atmospheric temperature and moisture profiles than a geostationary satellite. The satellites also ensure that the more remote regions of the globe, particularly in Northern Europe as well as the oceans in the Southern hemisphere, are fully covered. The EPS programme is also the European half of a joint program with NOAA , called the International Joint Polar System (IJPS). NOAA has operated a continuous series of low earth orbiting meteorological satellite since April 1960. Many of the instruments on Metop are also operated on NOAA/ POES satellites, providing similar data types across the IJPS. Instruments on Metop[ edit ]
Toggle the table of contents International Space Station This is the latest accepted revision , reviewed on 8 April 2024. Inhabitated space station in low Earth orbit "ISS" redirects here. For other uses, see ISS (disambiguation) . International Space Station (ISS) International Space Station program insignia, with flags of the original signatory states. Station statistics 450,000 kg (990,000 lb) [3] Length 109 m (358 ft) (overall length), 94 m (310 ft) (truss length) [4] Width 25 years, 4 months, 19 days(8 April 2024) Days occupied 23 years, 5 months, 5 days(8 April 2024) No. of orbits Statistics as of 22 December 2022(unless noted otherwise)References: [4] [5] [8] [9] [10] Configuration Station elements as of December 2022 [update] ( exploded view ) The International Space Station (ISS) is a large space station assembled and maintained in low Earth orbit by a collaboration of five space agencies: NASA (United States), Roscosmos (Russia), JAXA (Japan), ESA (Europe), CSA (Canada), and their contractors. ISS is the largest space station ever built. Its primary purpose is performing microgravity and space environment experiments. Operationally the station is divided into two sections: the Russian Orbital Segment assembled by Roscosmos and the US Orbital Segment assembled by NASA, JAXA, ESA and CSA. A striking feature of the ISS is the Integrated Truss Structure , which connects the large solar panels and radiators to the pressurized modules. The pressurized modules are specialized for research, habitation, storage, spacecraft control and airlock functions. Visiting spacecraft dock to the station via its eight docking and berthing ports . The ISS maintains an orbit with an average altitude of 400 kilometres (250 mi) [11] and circles the Earth in roughly 93 minutes, completing 15.5 orbits per day. [12] The ISS programme combines two prior plans to construct crewed Earth-orbiting stations: Space Station Freedom planned by the United States, and  the Mir-2 station planned by the Soviet Union. The first ISS module was launched in 1998. Major modules have been launched by Proton and Soyuz rockets and by the Space Shuttle launch system. The first long-term residents, Expedition 1 , arrived on 2 November 2000. Since then the station has been continuously occupied for 23 years and 158 days, the longest continuous human presence in space. As of March 2024 [update] , 279 individuals from 22 countries have visited the space station. [13] The ISS is expected to have additional modules (the Axiom Orbital Segment , for example) before being de-orbited by a dedicated NASA spacecraft in January 2031. This section is an excerpt from International Space Station programme § History and conception .[ edit ] As the space race drew to a close in the early 1970s, the US and USSR began to contemplate a variety of potential collaborations in outer space. This culminated in the 1975 Apollo-Soyuz Test Project , the first docking of spacecraft from two different spacefaring nations. The ASTP was considered a success, and further joint missions were also contemplated. One such concept was International Skylab, which proposed launching the backup Skylab B space station for a mission that would see multiple visits by both Apollo and Soyuz crew vehicles. [14] More ambitious was the Skylab-Salyut Space Laboratory, which proposed docking the Skylab B to a Soviet Salyut space station. Falling budgets and rising cold war tensions in the late 1970s saw these concepts fall by the wayside, along with another plan to have the Space Shuttle dock with a Salyut space station. [15] In the early 1980s, NASA planned to launch a modular space station called Freedom as a counterpart to the Salyut and Mir space stations. In 1984 the ESA was invited to participate in Space Station Freedom, and the ESA approved the Columbus laboratory by 1987. [16] The Japanese Experiment Module (JEM), or Kibō, was announced in 1985, as part of the Freedom space station in response to a NASA request in 1982. In early 1985, science ministers from the European Space Agency (ESA) countries approved the Columbus programme, the most ambitious effort in space undertaken by that organization at the time. The plan spearheaded by Germany and Italy included a module which would be attached to Freedom, and with the capability to evolve into a full-fledged European orbital outpost before the end of the century. [17] Increasing costs threw these plans into doubt in the early 1990s. Congress was unwilling to provide enough money to build and operate Freedom, and demanded NASA increase international participation to defray the rising costs or they would cancel the entire project outright. [18] Simultaneously, the USSR was conducting planning for the Mir-2 space station, and had begun constructing modules for the new station by the mid 1980s. However the collapse of the Soviet Union required these plans to be greatly downscaled, and soon Mir-2 was in danger of never being launched at all. [19] With both space station projects in jeopardy, American and Russian officials met and proposed they be combined. [20] In September 1993, American Vice-President Al Gore and Russian Prime Minister Viktor Chernomyrdin announced plans for a new space station, which eventually became the International Space Station. [21] They also agreed, in preparation for this new project, that the United States would be involved in the Mir programme, including American Shuttles docking, in the Shuttle–Mir programme . [22] On 12 April 2021, at a meeting with Russian President Vladimir Putin , then-Deputy Prime Minister Yury Borisov announced he had decided that Russia might withdraw from the ISS programme in 2025. [23] [24] According to Russian authorities, the timeframe of the station's operations has expired and its condition leaves much to be desired. [23] On 26 July 2022, Borisov, who had become head of Roscosmos, submitted to Putin his plans for withdrawal from the programme after 2024. [25] However, Robyn Gatens, the NASA official in charge of space station operations, responded that NASA had not received any formal notices from Roscosmos concerning withdrawal plans. [26] On 21 September 2022, Borisov stated that Russia was "highly likely" to continue to participate in the ISS programme until 2028. [27] Purpose[ edit ] The ISS was originally intended to be a laboratory, observatory, and factory while providing transportation, maintenance, and a low Earth orbit staging base for possible future missions to the Moon, Mars, and asteroids. However, not all of the uses envisioned in the initial memorandum of understanding between NASA and Roscosmos have been realised. [28] In the 2010 United States National Space Policy , the ISS was given additional roles of serving commercial, diplomatic, [29] and educational purposes. [30] Expedition 8 Commander and Science Officer Michael Foale conducts an inspection of the Microgravity Science Glovebox Fisheye view of several labs and the Space Shuttle CubeSats are deployed by the NanoRacks CubeSat Deployer The ISS provides a platform to conduct scientific research, with power, data, cooling, and crew available to support experiments. Small uncrewed spacecraft can also provide platforms for experiments, especially those involving zero gravity and exposure to space, but space stations offer a long-term environment where studies can be performed potentially for decades, combined with ready access by human researchers. [31] [32] The ISS simplifies individual experiments by allowing groups of experiments to share the same launches and crew time. Research is conducted in a wide variety of fields, including astrobiology , astronomy , physical sciences , materials science , space weather , meteorology , and human research including space medicine and the life sciences . [33] [34] [35] [36] Scientists on Earth have timely access to the data and can suggest experimental modifications to the crew. If follow-on experiments are necessary, the routinely scheduled launches of resupply craft allows new hardware to be launched with relative ease. [32] Crews fly expeditions of several months' duration, providing approximately 160 person-hours per week of labour with a crew of six. However, a considerable amount of crew time is taken up by station maintenance. [37] Perhaps the most notable ISS experiment is the Alpha Magnetic Spectrometer (AMS), which is intended to detect dark matter and answer other fundamental questions about our universe. According to NASA, the AMS is as important as the Hubble Space Telescope . Currently docked on station, it could not have been easily accommodated on a free flying satellite platform because of its power and bandwidth needs. [38] [39] On 3 April 2013, scientists reported that hints of dark matter may have been detected by the AMS. [40] [41] [42] [43] [44] [45] According to the scientists, "The first results from the space-borne Alpha Magnetic Spectrometer confirm an unexplained excess of high-energy positrons in Earth-bound cosmic rays". The space environment is hostile to life. Unprotected presence in space is characterised by an intense radiation field (consisting primarily of protons and other subatomic charged particles from the solar wind , in addition to cosmic rays ), high vacuum, extreme temperatures, and microgravity. [46] Some simple forms of life called extremophiles , [47] as well as small invertebrates called tardigrades [48] can survive in this environment in an extremely dry state through desiccation . Medical research improves knowledge about the effects of long-term space exposure on the human body, including muscle atrophy , bone loss , and fluid shift. These data will be used to determine whether high duration human spaceflight and space colonisation are feasible. In 2006, data on bone loss and muscular atrophy suggested that there would be a significant risk of fractures and movement problems if astronauts landed on a planet after a lengthy interplanetary cruise, such as the six-month interval required to travel to Mars . [49] [50] Medical studies are conducted aboard the ISS on behalf of the National Space Biomedical Research Institute (NSBRI). Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity study in which astronauts perform ultrasound scans under the guidance of remote experts. The study considers the diagnosis and treatment of medical conditions in space. Usually, there is no physician on board the ISS and diagnosis of medical conditions is a challenge. It is anticipated that remotely guided ultrasound scans will have application on Earth in emergency and rural care situations where access to a trained physician is difficult. [51] [52] [53] In August 2020, scientists reported that bacteria from Earth, particularly Deinococcus radiodurans bacteria, which is highly resistant to environmental hazards , were found to survive for three years in outer space , based on studies conducted on the International Space Station. These findings supported the notion of panspermia , the hypothesis that life exists throughout the Universe , distributed in various ways, including space dust , meteoroids , asteroids , comets , planetoids or contaminated spacecraft . [54] [55] Freefall[ edit ] ISS crew member storing samples A comparison between the combustion of a candle on Earth (left) and in a free fall environment, such as that found on the ISS (right) Gravity at the altitude of the ISS is approximately 90% as strong as at Earth's surface, but objects in orbit are in a continuous state of freefall , resulting in an apparent state of weightlessness . [57] This perceived weightlessness is disturbed by five effects: [58] Drag from the residual atmosphere. Vibration from the movements of mechanical systems and the crew. Actuation of the on-board attitude control moment gyroscopes . Thruster firings for attitude or orbital changes. Gravity-gradient effects , also known as tidal effects. Items at different locations within the ISS would, if not attached to the station, follow slightly different orbits. Being mechanically connected, these items experience small forces that keep the station moving as a rigid body . Researchers are investigating the effect of the station's near-weightless environment on the evolution, development, growth and internal processes of plants and animals. In response to some of the data, NASA wants to investigate microgravity 's effects on the growth of three-dimensional, human-like tissues and the unusual protein crystals that can be formed in space. [33] Investigating the physics of fluids in microgravity will provide better models of the behaviour of fluids. Because fluids can be almost completely combined in microgravity, physicists investigate fluids that do not mix well on Earth. Examining reactions that are slowed by low gravity and low temperatures will improve our understanding of superconductivity . [33] The study of materials science is an important ISS research activity, with the objective of reaping economic benefits through the improvement of techniques used on the ground. [59] Other areas of interest include the effect of low gravity on combustion, through the study of the efficiency of burning and control of emissions and pollutants. These findings may improve knowledge about energy production and lead to economic and environmental benefits. [33] Exploration[ edit ] A 3D plan of the Russia-based MARS-500 complex, used for conducting ground-based experiments that complement ISS-based preparations for a human mission to Mars The ISS provides a location in the relative safety of low Earth orbit to test spacecraft systems that will be required for long-duration missions to the Moon and Mars. This provides experience in operations, maintenance, and repair and replacement activities on-orbit. This will help develop essential skills in operating spacecraft farther from Earth, reduce mission risks, and advance the capabilities of interplanetary spacecraft. [60] Referring to the MARS-500 experiment, a crew isolation experiment conducted on Earth, ESA states, "Whereas the ISS is essential for answering questions concerning the possible impact of weightlessness, radiation and other space-specific factors, aspects such as the effect of long-term isolation and confinement can be more appropriately addressed via ground-based simulations". [61] Sergey Krasnov, the head of human space flight programmes for Russia's space agency, Roscosmos, in 2011 suggested a "shorter version" of MARS-500 may be carried out on the ISS. [62] In 2009, noting the value of the partnership framework itself, Sergey Krasnov wrote, "When compared with partners acting separately, partners developing complementary abilities and resources could give us much more assurance of the success and safety of space exploration. The ISS is helping further advance near-Earth space exploration and realisation of prospective programmes of research and exploration of the Solar system, including the Moon and Mars." [63] A crewed mission to Mars may be a multinational effort involving space agencies and countries outside the current ISS partnership. In 2010, ESA Director-General Jean-Jacques Dordain stated his agency was ready to propose to the other four partners that China, India, and South Korea be invited to join the ISS partnership. [64] NASA chief Charles Bolden stated in February 2011, "Any mission to Mars is likely to be a global effort." [65] Currently, US federal legislation prevents NASA co-operation with China on space projects. [66] Education and cultural outreach[ edit ] Original Jules Verne manuscripts displayed by crew inside the Jules Verne ATV The ISS crew provides opportunities for students on Earth by running student-developed experiments, making educational demonstrations, allowing for student participation in classroom versions of ISS experiments, and directly engaging students using radio, and email. [67] [68] ESA offers a wide range of free teaching materials that can be downloaded for use in classrooms. [69] In one lesson, students can navigate a 3D model of the interior and exterior of the ISS, and face spontaneous challenges to solve in real time. [70] The Japanese Aerospace Exploration Agency (JAXA) aims to inspire children to "pursue craftsmanship" and to heighten their "awareness of the importance of life and their responsibilities in society". [71] Through a series of education guides, students develop a deeper understanding of the past and near-term future of crewed space flight, as well as that of Earth and life. [72] [73] In the JAXA "Seeds in Space" experiments, the mutation effects of spaceflight on plant seeds aboard the ISS are explored by growing sunflower seeds that have flown on the ISS for about nine months. In the first phase of Kibō utilisation from 2008 to mid-2010, researchers from more than a dozen Japanese universities conducted experiments in diverse fields. [74] Cultural activities are another major objective of the ISS programme. Tetsuo Tanaka, the director of JAXA's Space Environment and Utilization Center, has said: "There is something about space that touches even people who are not interested in science." [75] Amateur Radio on the ISS (ARISS) is a volunteer programme that encourages students worldwide to pursue careers in science, technology, engineering, and mathematics, through amateur radio communications opportunities with the ISS crew. ARISS is an international working group, consisting of delegations from nine countries including several in Europe, as well as Japan, Russia, Canada, and the United States. In areas where radio equipment cannot be used, speakerphones connect students to ground stations which then connect the calls to the space station. [76] Spoken voice recording by ESA astronaut Paolo Nespoli on the subject of the ISS, produced in November 2017 for Wikipedia First Orbit is a 2011 feature-length documentary film about Vostok 1 , the first crewed space flight around the Earth. By matching the orbit of the ISS to that of Vostok 1 as closely as possible, in terms of ground path and time of day, documentary filmmaker Christopher Riley and ESA astronaut Paolo Nespoli were able to film the view that Yuri Gagarin saw on his pioneering orbital space flight. This new footage was cut together with the original Vostok 1 mission audio recordings sourced from the Russian State Archive. Nespoli is credited as the director of photography for this documentary film, as he recorded the majority of the footage himself during Expedition 26 / 27 . [77] The film was streamed in a global YouTube premiere in 2011 under a free licence through the website firstorbit.org. [78] In May 2013, commander Chris Hadfield shot a music video of David Bowie 's " Space Oddity " on board the station, which was released on YouTube. [79] [80] It was the first music video ever to be filmed in space. [81] In November 2017, while participating in Expedition 52 / 53 on the ISS, Paolo Nespoli made two recordings of his spoken voice (one in English and the other in his native Italian), for use on Wikipedia articles. These were the first content made in space specifically for Wikipedia. [82] [83] In November 2021, a virtual reality exhibit called The Infinite featuring life aboard the ISS was announced. [84] ISS module Node 2 manufacturing and processing in the Space Station Processing Facility An MPLM module in the SSPF at Cape Canaveral Since the International Space Station is a multi-national collaborative project, the components for in-orbit assembly were manufactured in various countries around the world. Beginning in the mid-1990s, the U.S. components Destiny , Unity , the Integrated Truss Structure , and the solar arrays were fabricated at the Marshall Space Flight Center and the Michoud Assembly Facility . These modules were delivered to the Operations and Checkout Building and the Space Station Processing Facility (SSPF) for final assembly and processing for launch. [85] The Russian modules, including Zarya and Zvezda , were manufactured at the Khrunichev State Research and Production Space Center in Moscow . Zvezda was initially manufactured in 1985 as a component for Mir-2 , but Mir 2 was never launched and instead became the ISS Service Module. [86] The European Space Agency (ESA) Columbus module was manufactured at the EADS Astrium Space Transportation facilities in Bremen , Germany, along with many other contractors throughout Europe. [87] The other ESA-built modules – Harmony , Tranquility , the Leonardo MPLM , and the Cupola – were initially manufactured at the Thales Alenia Space factory in Turin, Italy. [88] The structural steel hulls of the modules were transported by aircraft to the Kennedy Space Center SSPF for launch processing. [89] The Japanese Experiment Module Kibō, was fabricated in various technology manufacturing facilities in Japan, at the NASDA (now JAXA) Tsukuba Space Center , and the Institute of Space and Astronautical Science . The Kibo module was transported by ship and flown by aircraft to the SSPF. [90] The Mobile Servicing System , consisting of the Canadarm2 and the Dextre grapple fixture, was manufactured at various factories in Canada (such as the David Florida Laboratory ) and the United States, under contract by the Canadian Space Agency . The mobile base system, a connecting framework for Canadarm2 mounted on rails, was built by Northrop Grumman . The ISS was slowly assembled over more than a decade of spaceflights and crews A view of the completed station as seen from Shuttle Atlantis during STS-132 , 23 May 2010 The assembly of the International Space Station, a major endeavour in space architecture , began in November 1998. [8] Russian modules launched and docked robotically, with the exception of Rassvet . All other modules were delivered by the Space Shuttle , which required installation by ISS and Shuttle crewmembers using the Canadarm2 (SSRMS) and extra-vehicular activities (EVAs); by 5 June 2011, they had added 159 components during more than 1,000 hours of EVA. 127 of these spacewalks originated from the station, and the remaining 32 were launched from the airlocks of docked Space Shuttles. [91] The beta angle of the station had to be considered at all times during construction. [92] The first module of the ISS, Zarya, was launched on 20 November 1998 on an autonomous Russian Proton rocket . It provided propulsion, attitude control , communications, and electrical power, but lacked long-term life support functions. A passive NASA module, Unity, was launched two weeks later aboard Space Shuttle flight STS-88 and attached to Zarya by astronauts during EVAs. The Unity module has two Pressurized Mating Adapters (PMAs): one connects permanently to Zarya and the other allowed the Space Shuttle to dock to the space station. At that time, the Russian (Soviet) station Mir was still inhabited, and the ISS remained uncrewed for two years. On 12 July 2000, the Zvezda module was launched into orbit. Onboard preprogrammed commands deployed its solar arrays and communications antenna. Zvezda then became the passive target for a rendezvous with Zarya and Unity, maintaining a station-keeping orbit while the Zarya–Unity vehicle performed the rendezvous and docking via ground control and the Russian automated rendezvous and docking system. Zarya's computer transferred control of the station to Zvezda's computer soon after docking. Zvezda added sleeping quarters, a toilet, kitchen, CO2 scrubbers, dehumidifier, oxygen generators, and exercise equipment, plus data, voice and television communications with mission control, enabling permanent habitation of the station. [93] [94] The first resident crew, Expedition 1 , arrived in November 2000 on Soyuz TM-31 . At the end of the first day on the station, astronaut Bill Shepherd requested the use of the radio call sign "Alpha", which he and cosmonaut Sergei Krikalev preferred to the more cumbersome "International Space Station". [95] The name "Alpha" had previously been used for the station in the early 1990s, [96] and its use was authorised for the whole of Expedition 1. [97] Shepherd had been advocating the use of a new name to project managers for some time. Referencing a naval tradition in a pre-launch news conference he had said: "For thousands of years, humans have been going to sea in ships. People have designed and built these vessels, launched them with a good feeling that a name will bring good fortune to the crew and success to their voyage." [98] Yuriy Semenov [ ru ], the President of Russian Space Corporation Energia at the time, disapproved of the name "Alpha" as he felt that Mir was the first modular space station, so the names "Beta" or "Mir 2" for the ISS would have been more fitting. [97] [99] [100] Expedition 1 arrived midway between the Space Shuttle flights of missions STS-92 and STS-97 . These two flights each added segments of the station's Integrated Truss Structure , which provided the station with Ku-band communication for US television, additional attitude support needed for the additional mass of the USOS, and substantial solar arrays to supplement the station's four existing arrays. [101] Over the next two years, the station continued to expand. A Soyuz-U rocket delivered the Pirs docking compartment . The Space Shuttles Discovery , Atlantis , and Endeavour delivered the Destiny laboratory and Quest airlock , in addition to the station's main robot arm, the Canadarm2, and several more segments of the Integrated Truss Structure. The expansion schedule was interrupted in 2003 by the Space Shuttle Columbia disaster and a resulting hiatus in flights. The Space Shuttle was grounded until 2005 with STS-114 flown by Discovery. [102] Assembly resumed in 2006 with the arrival of STS-115 with Atlantis, which delivered the station's second set of solar arrays. Several more truss segments and a third set of arrays were delivered on STS-116 , STS-117 , and STS-118 . As a result of the major expansion of the station's power-generating capabilities, more pressurised modules could be accommodated, and the Harmony node and Columbus European laboratory were added. These were soon followed by the first two components of Kibō . In March 2009, STS-119 completed the Integrated Truss Structure with the installation of the fourth and final set of solar arrays. The final section of Kibō was delivered in July 2009 on STS-127 , followed by the Russian Poisk module. The third node, Tranquility, was delivered in February 2010 during STS-130 by the Space Shuttle Endeavour, alongside the Cupola, followed by the penultimate Russian module, Rassvet, in May 2010. Rassvet was delivered by Space Shuttle Atlantis on STS-132 in exchange for the Russian Proton delivery of the US-funded Zarya module in 1998. [103] The last pressurised module of the USOS, Leonardo, was brought to the station in February 2011 on the final flight of Discovery, STS-133 . [104] The Alpha Magnetic Spectrometer was delivered by Endeavour on STS-134 the same year. [105] By June 2011, the station consisted of 15 pressurised modules and the Integrated Truss Structure. Two power modules called NEM-1 and NEM-2. [106] are still to be launched. Russia's new primary research module Nauka docked in July 2021, [107] along with the European Robotic Arm which will be able to relocate itself to different parts of the Russian modules of the station. [108] Russia's latest addition, the nodal module Prichal , docked in November 2021. [109] The gross mass of the station changes over time. The total launch mass of the modules on orbit is about 417,289 kg (919,965 lb) (as of 3 September 2011 [update] ). [91] The mass of experiments, spare parts, personal effects, crew, foodstuff, clothing, propellants, water supplies, gas supplies, docked spacecraft, and other items add to the total mass of the station. Hydrogen gas is constantly vented overboard by the oxygen generators. Structure[ edit ] The ISS functions as a modular space station, enabling the addition or removal of modules from its structure for increased adaptability. Overview blueprint of components The ISS exterior and steelwork taken on 8 November 2021, from the departing SpaceX Crew-2 capsule Diagram structure of International Space Station after installation of iROSA solar arrays (as of 2023) Below is a diagram of major station components. The blue areas are pressurised sections accessible by the crew without using spacesuits. The station's unpressurised superstructure is indicated in red. Planned components are shown in white, non installed, temporarily defunct or non-commissioned components are shown in brown and former ones in gray. Other unpressurised components are yellow. The Unity node joins directly to the Destiny laboratory. For clarity, they are shown apart. Similar cases are also seen in other parts of the structure. Pressurised modules[ edit ] This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (November 2015) ( 'Dawn' [b] ), also known as the Functional Cargo Block or FGB (from the Russian: "Функционально-грузовой блок", lit. 'Funktsionalno-gruzovoy blok' or ФГБ), is the first module of the ISS to have been launched. [110] The FGB provided electrical power, storage, propulsion, and guidance to the ISS during the initial stage of assembly. With the launch and assembly in orbit of other modules with more specialized functionality, Zarya, as of August 2021, is primarily used for storage, both inside the pressurized section and in the externally mounted fuel tanks. The Zarya is a descendant of the TKS spacecraft designed for the Russian Salyut program . The name Zarya ("Dawn") was given to the FGB because it signified the dawn of a new era of international cooperation in space. Although it was built by a Russian company, it is owned by the United States. [111] Main article: Unity (ISS module) The Unity connecting module, also known as Node 1, is the first U.S.-built component of the ISS. It connects the Russian and U.S. segments of the station, and is where crew eat meals together. [112] [113] The module is cylindrical in shape, with six berthing locations ( forward , aft , port , starboard , zenith , and nadir ) facilitating connections to other modules. Unity measures 4.57 metres (15.0 ft) in diameter, is 5.47 metres (17.9 ft) long, made of steel, and was built for NASA by Boeing in a manufacturing facility at the Marshall Space Flight Center in Huntsville, Alabama . Unity is the first of the three connecting modules; the other two are Harmony and Tranquility. [114] Main article: Zvezda (ISS module) Zvezda (Russian: Звезда, meaning "star"), Salyut DOS-8 , is also known as the Zvezda Service Module. It was the third module launched to the station, and provides all of the station's life support systems , some of which are supplemented in the USOS, as well as living quarters for two crew members. It is the structural and functional center of the Russian Orbital Segment , which is the Russian part of the ISS. Crew assemble here to deal with emergencies on the station. [115] [116] [117] The module was manufactured by RKK Energia , with major sub-contracting work by GKNPTs Khrunichev. [118] Zvezda was launched on a Proton rocket on 12 July 2000, and docked with the Zarya module on 26 July 2000. The Destiny module being installed on the ISS Main article: Destiny (ISS module) The Destiny module, also known as the U.S. Lab, is the primary operating facility for U.S. research payloads aboard the ISS. [119] [120] It was berthed to the Unity module and activated over a period of five days in February 2001. [121] Destiny is NASA's first permanent operating orbital research station since Skylab was vacated in February 1974. The Boeing Company began construction of the 14.5-tonne (32,000 lb) research laboratory in 1995 at the Michoud Assembly Facility and then the Marshall Space Flight Center in Huntsville, Alabama. [119] Destiny was shipped to the Kennedy Space Center in Florida in 1998, and was turned over to NASA for pre-launch preparations in August 2000. It launched on 7 February 2001, aboard the Space Shuttle Atlantis on STS-98 . [121] Astronauts work inside the pressurized facility to conduct research in numerous scientific fields. Scientists throughout the world would use the results to enhance their studies in medicine, engineering, biotechnology, physics, materials science, and Earth science. [120] Quest Joint Airlock Module Main article: Quest Joint Airlock The Joint Airlock (also known as "Quest") is provided by the U.S. and provides the capability for ISS-based Extravehicular Activity (EVA) using either a U.S. Extravehicular Mobility Unit (EMU) or Russian Orlan EVA suits. [122] Before the launch of this airlock, EVAs were performed from either the U.S. Space Shuttle (while docked) or from the Transfer Chamber on the Service Module. Due to a variety of system and design differences, only U.S. space suits could be used from the Shuttle and only Russian suits could be used from the Service Module. The Joint Airlock alleviates this short-term problem by allowing either (or both) spacesuit systems to be used. [123] The Joint Airlock was launched on ISS-7A / STS-104 in July 2001 and was attached to the right hand docking port of Node 1. [124] The Joint Airlock is 20 ft. long, 13 ft. in diameter, and weighs 6.5 tons. The Joint Airlock was built by Boeing at Marshall Space Flight Center. The Joint Airlock was launched with the High Pressure Gas Assembly. The High Pressure Gas Assembly was mounted on the external surface of the Joint Airlock and will support EVAs operations with breathing gases and augments the Service Module's gas resupply system. The Joint Airlock has two main components: a crew airlock from which astronauts and cosmonauts exit the ISS and an equipment airlock designed for storing EVA gear and for so-called overnight "campouts" wherein Nitrogen is purged from astronaut's bodies overnight as pressure is dropped in preparation for spacewalks the following day. This alleviates the bends as the astronauts are repressurized after their EVA. [123] The crew airlock was derived from the Space Shuttle's external airlock. It is equipped with lighting, external handrails, and an Umbilical Interface Assembly (UIA). The UIA is located on one wall of the crew airlock and provides a water supply line, a wastewater return line, and an oxygen supply line. The UIA also provides communication gear and spacesuit power interfaces and can support two spacesuits simultaneously. This can be either two American EMU spacesuits, two Russian ORLAN spacesuits, or one of each design. Poisk (Russian: По́иск, lit. 'Search') was launched on 10 November 2009 [125] [126] attached to a modified Progress spacecraft , called Progress M-MIM2 , on a Soyuz-U rocket from Launch Pad 1 at the Baikonur Cosmodrome in Kazakhstan . Poisk is used as the Russian airlock module, containing two identical EVA hatches. An outward-opening hatch on the Mir space station failed after it swung open too fast after unlatching, because of a small amount of air pressure remaining in the airlock. [127] All EVA hatches on the ISS open inwards and are pressure-sealing. Poisk is used to store, service, and refurbish Russian Orlan suits and provides contingency entry for crew using the slightly bulkier American suits. The outermost docking port on the module allows docking of Soyuz and Progress spacecraft, and the automatic transfer of propellants to and from storage on the ROS. [128] Since the departure of the identical Pirs module on 26 July 2021, Poisk has served as the only airlock on the ROS. Harmony shown connected to Columbus, Kibo, and Destiny. PMA-2 faces. The nadir and zenith locations are open. Main article: Harmony (ISS module) Harmony, also known as Node 2, is the "utility hub" of the ISS. It connects the laboratory modules of the United States, Europe and Japan, as well as providing electrical power and electronic data. Sleeping cabins for four of the crew are housed here. [129] Harmony was successfully launched into space aboard Space Shuttle flight STS-120 on 23 October 2007. [130] [131] After temporarily being attached to the port side of the Unity node, [132] [133] it was moved to its permanent location on the forward end of the Destiny laboratory on 14 November 2007. [134] Harmony added 75.5 m3 (2,666 cu ft) to the station's living volume, an increase of almost 20 per cent, from 424.8 to 500.2 m3 (15,000 to 17,666 cu ft). Its successful installation meant that from NASA's perspective, the station was considered to be "U.S. Core Complete". Tranquility in 2011 Main article: Tranquility (ISS module) Tranquility, also known as Node 3, is a module of the ISS. It contains environmental control systems, life support systems , a toilet, exercise equipment, and an observation cupola . The European Space Agency and the Italian Space Agency had Tranquility manufactured by Thales Alenia Space . A ceremony on 20 November 2009 transferred ownership of the module to NASA. [135] On 8 February 2010, NASA launched the module on the Space Shuttle's STS-130 mission. The Columbus module on the ISS Main article: Columbus (ISS module) Columbus is a science laboratory that is part of the ISS and is the largest single contribution to the station made by the European Space Agency. Like the Harmony and Tranquility modules, the Columbus laboratory was constructed in Turin , Italy by Thales Alenia Space . The functional equipment and software of the lab was designed by EADS in Bremen , Germany. It was also integrated in Bremen before being flown to the Kennedy Space Center in Florida in an Airbus Beluga . It was launched aboard Space Shuttle Atlantis on 7 February 2008, on flight STS-122 . It is designed for ten years of operation. The module is controlled by the Columbus Control Centre , located at the German Space Operations Center , part of the German Aerospace Center in Oberpfaffenhofen near Munich , Germany. The European Space Agency has spent € 1.4 billion (about US$ 2 billion) on building Columbus, including the experiments it carries and the ground control infrastructure necessary to operate them. [136] Main article: Kibō (ISS module) The Japanese Experiment Module (JEM), nicknamed Kibō (きぼう, Kibō, Hope), is a Japanese science module for the International Space Station (ISS) developed by JAXA. It is the largest single ISS module, and is attached to the Harmony module. The first two pieces of the module were launched on Space Shuttle missions STS-123 and STS-124 . The third and final components were launched on STS-127 . [137] The Cupola's windows with shutters open Main article: Cupola (ISS module) The Cupola is an ESA -built observatory module of the ISS. Its name derives from the Italian word cupola, which means " dome ". Its seven windows are used to conduct experiments, dockings and observations of Earth. It was launched aboard Space Shuttle mission STS-130 on 8 February 2010 and attached to the Tranquility (Node 3) module. With the Cupola attached, ISS assembly reached 85 per cent completion. The Cupola's central window has a diameter of 80 cm (31 in). [138] Rassvet module with MLM-outfitting equipment (consisting of experiment airlock, RTOd radiators, and ERA workpost) at KSC Main article: Rassvet (ISS module) Rassvet ( Russian : Рассвет; lit. "dawn"), also known as the Mini-Research Module 1 (MRM-1) ( Russian : Малый исследовательский модуль, МИМ 1) and formerly known as the Docking Cargo Module (DCM), is a component of the International Space Station (ISS). The module's design is similar to the Mir Docking Module launched on STS-74 in 1995. Rassvet is primarily used for cargo storage and as a docking port for visiting spacecraft. It was flown to the ISS aboard Space Shuttle Atlantis on the STS-132 mission on 14 May 2010, [139] and was connected to the ISS on 18 May 2010. [140] The hatch connecting Rassvet with the ISS was first opened on 20 May 2010. [141] On 28 June 2010, the Soyuz TMA-19 spacecraft performed the first docking with the module. [142] Science (or Experiment) Airlock[ edit ] Experiment airlock berthed to Nauka The airlock, ShK, is designed for a payload with dimensions up to 1,200 mm × 500 mm × 500 mm (47 in × 20 in × 20 in), has a volume of 2.1 m3, weight of 1050 kg and consumes 1.5 kW of power at the peak. Prior to berthing the MLM to the ISS, the airlock is stowed as part of MRM1 . [143] On 4 May 2023, 01:00 UTC, the chamber was moved by the ERA manipulator and berthed to the forward active docking port of the pressurized docking hub of the Nauka module during VKD-57 spacewalk. It is intended to be used: for extracting payloads and from the MLM docking adapter and placing them on the outer surface of the station; enable science investigations to be removed, exposed to the external microgravity environment, then returned inside while being maneuvered with the European robotic arm. for receiving payloads from the ERA manipulator and moving them into the internal volume of the airlock and further into the MLM pressurized adapter; for conducting scientific experiments in the internal volume of the airlock; for conducting scientific experiments outside the airlock chamber on an extended table and in a special organized place. [143] [144] for launching cubesats into space, with the aid of ERA – very similar to the Japanese airlock and Nanoracks Bishop Airlock on the U.S. segment of the station. [145] Leonardo Permanent Multipurpose Module Main article: Leonardo (ISS module) The Leonardo Permanent Multipurpose Module (PMM) is a module of the International Space Station. It was flown into space aboard the Space Shuttle on STS-133 on 24 February 2011 and installed on 1 March. Leonardo is primarily used for storage of spares, supplies and waste on the ISS, which was until then stored in many different places within the space station. It is also the personal hygiene area for the astronauts who live in the US Orbital Segment . The Leonardo PMM was a Multi-Purpose Logistics Module (MPLM) before 2011, but was modified into its current configuration. It was formerly one of two MPLM used for bringing cargo to and from the ISS with the Space Shuttle. The module was named for Italian polymath Leonardo da Vinci . Bigelow Expandable Activity Module[ edit ] Progression of the expansion of BEAM The Bigelow Expandable Activity Module (BEAM) is an experimental expandable space station module developed by Bigelow Aerospace , under contract to NASA, for testing as a temporary module on the International Space Station (ISS) from 2016 to at least 2020. It arrived at the ISS on 10 April 2016, [146] was berthed to the station on 16 April at Tranquility Node 3, and was expanded and pressurized on 28 May 2016. In December 2021, Bigelow Aerospace conveyed ownership of the module to NASA, as a result of Bigelow's cessation of activity. [147] IDA-1 upright International Docking Adapters[ edit ] The International Docking Adapter (IDA) is a spacecraft docking system adapter developed to convert APAS-95 to the NASA Docking System (NDS). An IDA is placed on each of the ISS's two open Pressurized Mating Adapters (PMAs), both of which are connected to the Harmony module. Two International Docking Adapters are currently installed aboard the Station. Originally, IDA-1 was planned to be installed on PMA-2, located at Harmony's forward port, and IDA-2 would be installed on PMA-3 at Harmony's zenith. After IDA 1 was destroyed in a launch incident , IDA-2 was installed on PMA-2 on 19 August 2016, [148] while IDA-3 was later installed on PMA-3 on 21 August 2019. [149] NanoRacks Bishop airlock module installed on the ISS Bishop Airlock Module[ edit ] Main article: Nanoracks Bishop Airlock The NanoRacks Bishop Airlock Module is a commercially funded airlock module launched to the ISS on SpaceX CRS-21 on 6 December 2020. [150] [151] The module was built by NanoRacks , Thales Alenia Space , and Boeing. [152] It will be used to deploy CubeSats , small satellites , and other external payloads for NASA, CASIS , and other commercial and governmental customers. [153] Nauka (Russian: Наука, lit. 'Science'), also known as the Multipurpose Laboratory Module-Upgrade (MLM-U), (Russian: Многоцелевой лабораторный модуль, усоверше́нствованный, or МЛМ-У), is a Roscosmos-funded component of the ISS that was launched on 21 July 2021, 14:58 UTC. In the original ISS plans, Nauka was to use the location of the Docking and Stowage Module (DSM), but the DSM was later replaced by the Rassvet module and moved to Zarya's nadir port. Nauka was successfully docked to Zvezda's nadir port on 29 July 2021, 13:29 UTC, replacing the Pirs module. Progress MS-17 undocking and taking the Nauka nadir temporary docking adapter with it [c] [d] It had a temporary docking adapter on its nadir port for crewed and uncrewed missions until Prichal arrival, where just before its arrival it was removed by a departing Progress spacecraft. [154] Nauka and Prichal docked to ISS Prichal, also known as Uzlovoy Module or UM (Russian: Узловой Модуль Причал, lit. 'Nodal Module Berth'), [155] is a 4-tonne (8,800 lb) [156] ball-shaped module that will provide the Russian segment additional docking ports to receive Soyuz MS and Progress MS spacecraft. UM was launched in November 2021. [157] It was integrated with a special version of the Progress cargo spacecraft and launched by a standard Soyuz rocket, docking to the nadir port of the Nauka module. One port is equipped with an active hybrid docking port, which enables docking with the MLM module. The remaining five ports are passive hybrids, enabling docking of Soyuz and Progress vehicles, as well as heavier modules and future spacecraft with modified docking systems. The node module was intended to serve as the only permanent element of the cancelled Orbital Piloted Assembly and Experiment Complex (OPSEK). [157] [158] [159] Unpressurised elements[ edit ] ISS Truss Components breakdown showing Trusses and all ORUs in situ The ISS has a large number of external components that do not require pressurisation. The largest of these is the Integrated Truss Structure (ITS), to which the station's main solar arrays and thermal radiators are mounted. [160] The ITS consists of ten separate segments forming a structure 108.5 metres (356 ft) long. [8] The station was intended to have several smaller external components, such as six robotic arms, three External Stowage Platforms (ESPs) and four ExPRESS Logistics Carriers (ELCs). [161] [162] While these platforms allow experiments (including MISSE , the STP-H3 and the Robotic Refueling Mission ) to be deployed and conducted in the vacuum of space by providing electricity and processing experimental data locally, their primary function is to store spare Orbital Replacement Units (ORUs). ORUs are parts that can be replaced when they fail or pass their design life, including pumps, storage tanks, antennas, and battery units. Such units are replaced either by astronauts during EVA or by robotic arms. [163] Several shuttle missions were dedicated to the delivery of ORUs, including STS-129 , [164] STS-133 [165] and STS-134. [166] As of January 2011 [update] , only one other mode of transportation of ORUs had been utilised – the Japanese cargo vessel HTV-2 – which delivered an FHRC and CTC-2 via its Exposed Pallet (EP). [167] [ needs update ] Construction of the Integrated Truss Structure over New Zealand There are also smaller exposure facilities mounted directly to laboratory modules; the Kibō Exposed Facility serves as an external " porch " for the Kibō complex, [168] and a facility on the European Columbus laboratory provides power and data connections for experiments such as the European Technology Exposure Facility [169] [170] and the Atomic Clock Ensemble in Space . [171] A remote sensing instrument, SAGE III-ISS , was delivered to the station in February 2017 aboard CRS-10 , [172] and the NICER experiment was delivered aboard CRS-11 in June 2017. [173] The largest scientific payload externally mounted to the ISS is the Alpha Magnetic Spectrometer (AMS), a particle physics experiment launched on STS-134 in May 2011, and mounted externally on the ITS. The AMS measures cosmic rays to look for evidence of dark matter and antimatter. [174] [175] The commercial Bartolomeo External Payload Hosting Platform, manufactured by Airbus, was launched on 6 March 2020 aboard CRS-20 and attached to the European Columbus module. It will provide an additional 12 external payload slots, supplementing the eight on the ExPRESS Logistics Carriers , ten on Kibō, and four on Columbus. The system is designed to be robotically serviced and will require no astronaut intervention. It is named after Christopher Columbus's younger brother. [176] [177] [178] MLM outfittings[ edit ] MLM outfittings on Rassvet A wide-angle view of the new module (behind Rassvet) attached to the ROS as seen from the cupola In May 2010, equipment for Nauka was launched on STS-132 (as part of an agreement with NASA) and delivered by Space Shuttle Atlantis. Weighing 1.4 metric tons, the equipment was attached to the outside of Rassvet (MRM-1). It included a spare elbow joint for the European Robotic Arm (ERA) (which was launched with Nauka) and an ERA-portable workpost used during EVAs, as well as RTOd add-on heat radiator and internal hardware alongside the pressurized experiment airlock. [145] The RTOd radiator adds additional cooling capability to Nauka, which enables the module to host more scientific experiments. [145] The ERA was used to remove the RTOd radiator from Rassvet and transferred over to Nauka during VKD-56 spacewalk. Later it was activated and fully deployed on VKD-58 spacewalk. [179] This process took several months. A portable work platform was also transferred over in August 2023 during VKD-60 spacewalk, which can attach to the end of the ERA to allow cosmonauts to "ride" on the end of the arm during spacewalks. [180] [181] However, even after several months of outfitting EVAs and RTOd heat radiator installation, six months later, the RTOd radiator malfunctioned before active use of Nauka (the purpose of RTOd installation is to radiate heat from Nauka experiments). The malfunction, a leak, rendered the RTOd radiator unusable for Nauka. This is the third ISS radiator leak after Soyuz MS-22 and Progress MS-21 radiator leaks. If a spare RTOd is not available, Nauka experiments will have to rely on Nauka's main launch radiator and the module could never be utilized to its full capacity. [182] [183] Another MLM outfitting is a 4 segment external payload interface called means of attachment of large payloads (Sredstva Krepleniya Krupnogabaritnykh Obyektov, SKKO). [184] Delivered in two parts to Nauka by Progress MS-18 (LCCS part) and Progress MS-21 (SCCCS part) as part of the module activation outfitting process. [185] [186] [187] [188] It was taken outside and installed on the ERA aft facing base point on Nauka during the VKD-55 spacewalk. [189] [190] [191] [192] Robotic arms and cargo cranes[ edit ] Commander Volkov stands on Pirs with his back to the Soyuz whilst operating the manual Strela crane (which is holding photographer Oleg Kononenko ) Dextre , like many of the station's experiments and robotic arms, can be operated from Earth, allowing tasks to be performed while the crew sleeps The Integrated Truss Structure serves as a base for the station's primary remote manipulator system, the Mobile Servicing System (MSS), which is composed of three main components: Canadarm2 , the largest robotic arm on the ISS, has a mass of 1,800 kilograms (4,000 lb) and is used to: dock and manipulate spacecraft and modules on the USOS; hold crew members and equipment in place during EVAs; and move Dextre around to perform tasks. [193] Dextre is a 1,560 kg (3,440 lb) robotic manipulator that has two arms and a rotating torso, with power tools, lights, and video for replacing orbital replacement units (ORUs) and performing other tasks requiring fine control. [194] The Mobile Base System (MBS) is a platform that rides on rails along the length of the station's main truss, which serves as a mobile base for Canadarm2 and Dextre, allowing the robotic arms to reach all parts of the USOS. [195] A grapple fixture was added to Zarya on STS-134 to enable Canadarm2 to inchworm itself onto the Russian Orbital Segment. [166] Also installed during STS-134 was the 15 m (50 ft) Orbiter Boom Sensor System (OBSS), which had been used to inspect heat shield tiles on Space Shuttle missions and which can be used on the station to increase the reach of the MSS. [166] Staff on Earth or the ISS can operate the MSS components using remote control, performing work outside the station without the need for space walks. Japan's Remote Manipulator System , which services the Kibō Exposed Facility, [196] was launched on STS-124 and is attached to the Kibō Pressurised Module. [197] The arm is similar to the Space Shuttle arm as it is permanently attached at one end and has a latching end effector for standard grapple fixtures at the other. The European Robotic Arm , which will service the Russian Orbital Segment, was launched alongside the Nauka module. [198] The ROS does not require spacecraft or modules to be manipulated, as all spacecraft and modules dock automatically and may be discarded the same way. Crew use the two Strela ( Russian : Стрела́, Main article: Pirs (ISS module) Pirs (Russian: Пирс, lit. 'Pier') was launched on 14 September 2001, as ISS Assembly Mission 4R, on a Russian Soyuz-U rocket, using a modified Progress spacecraft , Progress M-SO1 , as an upper stage.  Pirs was undocked by Progress MS-16 on 26 July 2021, 10:56 UTC, and deorbited on the same day at 14:51 UTC to make room for Nauka module to be attached to the space station. Prior to its departure, Pirs served as the primary Russian airlock on the station, being used to store and refurbish the Russian Orlan spacesuits. The Pirs module attached to the ISS ISS-65 Pirs docking compartment separates from the International Space Station Main article: Axiom Orbital Segment In January 2020, NASA awarded Axiom Space a contract to build a commercial module for the ISS. The contract is under the NextSTEP2 program. NASA negotiated with Axiom on a firm fixed-price contract basis to build and deliver the module, which will attach to the forward port of the space station's Harmony (Node 2) module. Although NASA has only commissioned one module, Axiom plans to build an entire segment consisting of five modules, including a node module, an orbital research and manufacturing facility, a crew habitat, and a "large-windowed Earth observatory". The Axiom segment is expected to greatly increase the capabilities and value of the space station, allowing for larger crews and private spaceflight by other organisations. Axiom plans to convert the segment into a stand-alone space station once the ISS is decommissioned, with the intention that this would act as a successor to the ISS. [199] [200] [201] Canadarm 2 will also help to berth the Axiom Space Station modules to the ISS and will continue its operations on the Axiom Space Station after the retirement of ISS in late 2020s. [202] As of December 2023, Axiom Space expects to launch the first module, Hab One, at the end of 2026. [203] Independence-1[ edit ] Nanoracks , after finalizing its contract with NASA, and after winning NextSTEPs Phase II award, is now developing its concept Independence-1 (previously known as Ixion), which would turn spent rocket tanks into a habitable living area to be tested in space. In Spring 2018, Nanoracks announced that Ixion is now known as the Independence-1, the first 'outpost' in Nanoracks' Space Outpost Program. Nautilus-X Centrifuge Demonstration[ edit ] Main article: Nautilus-X If produced, this centrifuge will be the first in-space demonstration of sufficient scale centrifuge for artificial partial-g effects. It will be designed to become a sleep module for the ISS crew. Cancelled components[ edit ] The cancelled Habitation module under construction at Michoud in 1997 Several modules planned for the station were cancelled over the course of the ISS programme. Reasons include budgetary constraints, the modules becoming unnecessary, and station redesigns after the 2003 Columbia disaster . The US Centrifuge Accommodations Module would have hosted science experiments in varying levels of artificial gravity . [204] The US Habitation Module would have served as the station's living quarters. Instead, the living quarters are now spread throughout the station. [205] The US Interim Control Module and ISS Propulsion Module would have replaced the functions of Zvezda in case of a launch failure. [206] Two Russian Research Modules were planned for scientific research. [207] They would have docked to a Russian Universal Docking Module . [208] The Russian Science Power Platform would have supplied power to the Russian Orbital Segment independent of the ITS solar arrays. Science Power Modules 1 and 2 (Repurposed Components)[ edit ] Science Power Module 1 (SPM-1, also known as NEM-1) and Science Power Module 2 (SPM-2, also known as NEM-2) are modules that were originally planned to arrive at the ISS no earlier than 2024, and dock to the Prichal module, which is currently docked to the Nauka module. [159] [209] In April 2021, Roscosmos announced that NEM-1 would be repurposed to function as the core module of the proposed Russian Orbital Service Station (ROSS), launching no earlier than 2027 [210] and docking to the free-flying Nauka module either before or after the ISS has been deorbited. [211] [212] NEM-2 may be converted into another core "base" module, which would be launched in 2028. [213] Main article: B330 Designed by Bigelow Aerospace . In August 2016, Bigelow negotiated an agreement with NASA to develop a full-size ground prototype Deep Space Habitation based on the B330 under the second phase of Next Space Technologies for Exploration Partnerships. The module was called the Expandable Bigelow Advanced Station Enhancement (XBASE), as Bigelow hoped to test the module by attaching it to the International Space Station. However, in March 2020, Bigelow laid off all 88 of its employees, and as of February 2024 [update] the company remains dormant and is currently considered defunct, [214] [215] making it appear unlikely that the XBASE module will ever be launched. Main articles: ISS ECLSS and Chemical oxygen generator The critical systems are the atmosphere control system, the water supply system, the food supply facilities, the sanitation and hygiene equipment, and fire detection and suppression equipment. The Russian Orbital Segment's life support systems are contained in the Zvezda service module. Some of these systems are supplemented by equipment in the USOS. The Nauka laboratory has a complete set of life support systems. Atmospheric control systems[ edit ] The interactions between the components of the ISS Environmental Control and Life Support System (ECLSS) The atmosphere on board the ISS is similar to that of Earth . [216] Normal air pressure on the ISS is 101.3 kPa (14.69 psi); [217] the same as at sea level on Earth. An Earth-like atmosphere offers benefits for crew comfort, and is much safer than a pure oxygen atmosphere, because of the increased risk of a fire such as that responsible for the deaths of the Apollo 1 crew. [218] [ better source needed ] Earth-like atmospheric conditions have been maintained on all Russian and Soviet spacecraft. [219] The Elektron system aboard Zvezda and a similar system in Destiny generate oxygen aboard the station. [220] The crew has a backup option in the form of bottled oxygen and Solid Fuel Oxygen Generation (SFOG) canisters, a chemical oxygen generator system. [221] Carbon dioxide is removed from the air by the Vozdukh system in Zvezda. Other by-products of human metabolism, such as methane from the intestines and ammonia from sweat, are removed by activated charcoal filters. [221] Part of the ROS atmosphere control system is the oxygen supply. Triple-redundancy is provided by the Elektron unit, solid fuel generators, and stored oxygen. The primary supply of oxygen is the Elektron unit which produces O2 and H2 by electrolysis of water and vents H2 overboard. The 1 kW (1.3 hp) system uses approximately one litre of water per crew member per day. This water is either brought from Earth or recycled from other systems. Mir was the first spacecraft to use recycled water for oxygen production. The secondary oxygen supply is provided by burning oxygen-producing Vika cartridges (see also ISS ECLSS ). Each 'candle' takes 5–20 minutes to decompose at 450–500 °C (842–932 °F), producing 600 litres (130 imp gal; 160 US gal) of O2. This unit is manually operated. [222] The US Orbital Segment has redundant supplies of oxygen, from a pressurised storage tank on the Quest airlock module delivered in 2001, supplemented ten years later by ESA-built Advanced Closed-Loop System (ACLS) in the Tranquility module (Node 3), which produces O2 by electrolysis. [223] Hydrogen produced is combined with carbon dioxide from the cabin atmosphere and converted to water and methane. Power and thermal control[ edit ] Russian solar arrays, backlit by sunset One of the eight truss mounted pairs of USOS solar arrays ISS new roll out solar array as seen from a zoom camera on the P6 Truss Double-sided solar arrays provide electrical power to the ISS. These bifacial cells collect direct sunlight on one side and light reflected off from the Earth on the other, and are more efficient and operate at a lower temperature than single-sided cells commonly used on Earth. [224] The Russian segment of the station, like most spacecraft, uses 28 V low voltage DC from two rotating solar arrays mounted on Zvezda. The USOS uses 130–180 V DC from the USOS PV array, power is stabilised and distributed at 160 V DC and converted to the user-required 124 V DC. The higher distribution voltage allows smaller, lighter conductors, at the expense of crew safety. The two station segments share power with converters. The USOS solar arrays are arranged as four wing pairs, for a total production of 75 to 90 kilowatts. [4] These arrays normally track the Sun to maximise power generation. Each array is about 375 m2 (4,036 sq ft) in area and 58 m (190 ft) long. In the complete configuration, the solar arrays track the Sun by rotating the alpha gimbal once per orbit; the beta gimbal follows slower changes in the angle of the Sun to the orbital plane. The Night Glider mode aligns the solar arrays parallel to the ground at night to reduce the significant aerodynamic drag at the station's relatively low orbital altitude. [225] The station originally used rechargeable nickel–hydrogen batteries ( NiH2) for continuous power during the 45 minutes of every 90-minute orbit that it is eclipsed by the Earth. The batteries are recharged on the day side of the orbit. They had a 6.5-year lifetime (over 37,000 charge/discharge cycles) and were regularly replaced over the anticipated 20-year life of the station. [226] Starting in 2016, the nickel–hydrogen batteries were replaced by lithium-ion batteries , which are expected to last until the end of the ISS program. [227] The station's large solar panels generate a high potential voltage difference between the station and the ionosphere. This could cause arcing through insulating surfaces and sputtering of conductive surfaces as ions are accelerated by the spacecraft plasma sheath. To mitigate this, plasma contactor units create current paths between the station and the ambient space plasma. [228] ISS External Active Thermal Control System (EATCS) diagram The station's systems and experiments consume a large amount of electrical power, almost all of which is converted to heat. To keep the internal temperature within workable limits, a passive thermal control system (PTCS) is made of external surface materials, insulation such as MLI, and heat pipes. If the PTCS cannot keep up with the heat load, an External Active Thermal Control System (EATCS) maintains the temperature. The EATCS consists of an internal, non-toxic, water coolant loop used to cool and dehumidify the atmosphere, which transfers collected heat into an external liquid ammonia loop. From the heat exchangers, ammonia is pumped into external radiators that emit heat as infrared radiation, then back to the station. [229] The EATCS provides cooling for all the US pressurised modules, including Kibō and Columbus, as well as the main power distribution electronics of the S0, S1 and P1 trusses. It can reject up to 70 kW. This is much more than the 14 kW of the Early External Active Thermal Control System (EEATCS) via the Early Ammonia Servicer (EAS), which was launched on STS-105 and installed onto the P6 Truss. [230] Communications and computers[ edit ] See also: ThinkPad § Use in space The communications systems used by the ISS.* Luch and the Space Shuttle are not in use as of 2020. Radio communications provide telemetry and scientific data links between the station and mission control centres . Radio links are also used during rendezvous and docking procedures and for audio and video communication between crew members, flight controllers and family members. As a result, the ISS is equipped with internal and external communication systems used for different purposes. [231] The Russian Orbital Segment communicates directly with the ground via the Lira antenna mounted to Zvezda. [67] [232] The Lira antenna also has the capability to use the Luch data relay satellite system. [67] This system fell into disrepair during the 1990s, and so was not used during the early years of the ISS, [67] [233] [234] although two new Luch satellites – Luch-5A and Luch-5B – were launched in 2011 and 2012 respectively to restore the operational capability of the system. [235] Another Russian communications system is the Voskhod-M , which enables internal telephone communications between Zvezda, Zarya, Pirs, Poisk, and the USOS and provides a VHF radio link to ground control centres via antennas on Zvezda's exterior. [236] The US Orbital Segment (USOS) makes use of two separate radio links: S band (audio, telemetry, commanding – located on the P1/S1 truss) and Ku band (audio, video and data – located on the Z1 truss ) systems. These transmissions are routed via the United States Tracking and Data Relay Satellite System (TDRSS) in geostationary orbit , allowing for almost continuous real-time communications with Christopher C. Kraft Jr. Mission Control Center (MCC-H) in Houston . [67] [237] [231] Data channels for the Canadarm2, European Columbus laboratory and Japanese Kibō modules were originally also routed via the S band and Ku band systems, with the European Data Relay System and a similar Japanese system intended to eventually complement the TDRSS in this role. [237] [238] Communications between modules are carried on an internal wireless network . [239] An array of laptops in the US lab Laptop computers surround the Canadarm2 console An error message displays a problem with a hard drive on a laptop aboard the ISS UHF radio is used by astronauts and cosmonauts conducting EVAs and other spacecraft that dock to or undock from the station. [67] Automated spacecraft are fitted with their own communications equipment; the ATV used a laser attached to the spacecraft and the Proximity Communications Equipment attached to Zvezda to accurately dock with the station. [240] [241] The ISS is equipped with about 100 IBM/Lenovo ThinkPad and HP ZBook 15 laptop computers. The laptops have run Windows 95 , Windows 2000 , Windows XP , Windows 7 , Windows 10 and Linux operating systems. [242] Each computer is a commercial off-the-shelf purchase which is then modified for safety and operation including updates to connectors, cooling and power to accommodate the station's 28V DC power system and weightless environment. Heat generated by the laptops does not rise but stagnates around the laptop, so additional forced ventilation is required. Portable Computer System (PCS) laptops connect to the Primary Command & Control computer (C&C MDM) as remote terminals via a USB to 1553 adapter. [243] Station Support Computer (SSC) laptops aboard the ISS are connected to the station's wireless LAN via Wi-Fi and ethernet, which connects to the ground via Ku band. While originally this provided speeds of 10 Mbit/s download and 3 Mbit/s upload from the station, [244] [245] NASA upgraded the system in late August 2019 and increased the speeds to 600 Mbit/s. [246] Laptop hard drives occasionally fail and must be replaced. [247] Other computer hardware failures include instances in 2001, 2007 and 2017; some of these failures have required EVAs to replace computer modules in externally mounted devices. [248] [249] [250] [251] The operating system used for key station functions is the Debian Linux distribution . [252] The migration from Microsoft Windows to Linux was made in May 2013 for reasons of reliability, stability and flexibility. [253] In 2017, an SG100 Cloud Computer was launched to the ISS as part of OA-7 mission. [254] It was manufactured by NCSIST of Taiwan and designed in collaboration with Academia Sinica , and National Central University under contract for NASA. [255] ISS crew members have access to the Internet , and thus the web . [256] [257] This was first enabled in 2010, [256] allowing NASA astronaut T.J. Creamer to make the first tweet from space. [258] Access is achieved via an Internet-enabled computer in Houston, using remote desktop mode , thereby protecting the ISS from virus infection and hacking attempts. [256] Zarya and Unity were entered for the first time on 10 December 1998 Soyuz TM-31 being prepared to bring the first resident crew to the station in October 2000 Each permanent crew is given an expedition number. Expeditions run up to six months, from launch until undocking, an 'increment' covers the same time period, but includes cargo spacecraft and all activities. Expeditions 1 to 6 consisted of three-person crews. Expeditions 7 to 12 were reduced to the safe minimum of two following the destruction of the NASA Shuttle Columbia. From Expedition 13 the crew gradually increased to six around 2010. [259] [260] With the arrival of crew on US commercial vehicles beginning in 2020, [261] NASA has indicated that expedition size may be increased to seven crew members, the number ISS was originally designed for. [262] [263] Gennady Padalka , member of Expeditions 9 , 19 / 20 , 31 / 32 , and 43 / 44 , and Commander of Expedition 11 , has spent more time in space than anyone else, a total of 878 days, 11 hours, and 29 minutes. [264] Peggy Whitson has spent the most time in space of any American, totalling 675 days, 3 hours and 48 minutes during her time on Expeditions 5 , 16 , and 50 / 51 / 52 and Axiom Mission 2 . [265] [266] See also: Space tourism Travellers who pay for their own passage into space are termed spaceflight participants by Roscosmos and NASA, and are sometimes referred to as "space tourists", a term they generally dislike. [e] As of June 2023 [update] , thirteen space tourists have visited the ISS; nine were transported to the ISS on Russian Soyuz spacecraft, and four were transported on American SpaceX Dragon 2 spacecraft. For one-tourist missions, when professional crews change over in numbers not divisible by the three seats in a Soyuz, and a short-stay crewmember is not sent, the spare seat is sold by MirCorp through Space Adventures. Space tourism was halted in 2011 when the Space Shuttle was retired and the station's crew size was reduced to six, as the partners relied on Russian transport seats for access to the station. Soyuz flight schedules increased after 2013, allowing five Soyuz flights (15 seats) with only two expeditions (12 seats) required. [274] The remaining seats were to be sold for around US$40 million to members of the public who could pass a medical exam. ESA and NASA criticised private spaceflight at the beginning of the ISS, and NASA initially resisted training Dennis Tito , the first person to pay for his own passage to the ISS. [f] Anousheh Ansari became the first self-funded woman to fly to the ISS as well as the first Iranian in space. Officials reported that her education and experience made her much more than a tourist, and her performance in training had been "excellent." [275] She did Russian and European studies involving medicine and microbiology during her 10-day stay. The 2009 documentary Space Tourists follows her journey to the station, where she fulfilled "an age-old dream of man: to leave our planet as a 'normal person' and travel into outer space." [276] In 2008, spaceflight participant Richard Garriott placed a geocache aboard the ISS during his flight. [277] This is currently the only non-terrestrial geocache in existence. [278] At the same time, the Immortality Drive , an electronic record of eight digitised human DNA sequences , was placed aboard the ISS. [279] After a 12-year hiatus, the first two wholly space tourism-dedicated private spaceflights to the ISS were undertaken. Soyuz MS-20 launched in December 2021, carrying visiting Roscosmos cosmonaut Alexander Misurkin and two Japanese space tourists under the aegis of the private company Space Adventures ; [280] [281] in April 2022, the company Axiom Space chartered a SpaceX Dragon 2 spacecraft and sent its own employee astronaut Michael Lopez-Alegria and three space tourists to the ISS for Axiom Mission 1 , [282] [283] [284] followed in May 2023 by one more tourist, John Shoffner , alongside employee astronaut Peggy Whitson and two Saudi astronauts for Axiom Mission 2 . [285] [286] Fleet operations[ edit ] Dragon and Cygnus cargo vessels were docked at the ISS together for the first time in April 2016 Japan's Kounotori 4 berthing Commercial Crew Program vehicles Starliner and Dragon A wide variety of crewed and uncrewed spacecraft have supported the station's activities. Flights to the ISS include 37 Space Shuttle missions, 83 Progress resupply spacecraft (including the modified M-MIM2 , M-SO1 and M-UM module transports), 63 crewed Soyuz spacecraft, 5 European ATVs , 9 Japanese HTVs , 1 Boeing Starliner , 30 SpaceX Dragon (both crewed and uncrewed) and 18 Cygnus missions. [287] There are currently eleven available docking ports for visiting spacecraft: [288] Harmony zenith (with IDA 3 ) Harmony nadir Uncrewed[ edit ] Uncrewed spaceflights to the ISS are made primarily to deliver cargo, however several Russian modules have also docked to the outpost following uncrewed launches. Resupply missions typically use the Russian Progress spacecraft, former European ATVs , Japanese Kounotori vehicles, and the American Dragon and Cygnus spacecraft. The primary docking system for Progress spacecraft is the automated Kurs system, with the manual TORU system as a backup. ATVs also used Kurs, however they were not equipped with TORU. Progress and former ATV can remain docked for up to six months. [290] [291] The other spacecraft – the Japanese HTV, the SpaceX Dragon (under CRS phase 1), and the Northrop Grumman [292] Cygnus – rendezvous with the station before being grappled using Canadarm2 and berthed at the nadir port of the Harmony or Unity module for one to two months. Under CRS phase 2, Cargo Dragon docks autonomously at IDA-2 or IDA-3. As of December 2020 [update] , Progress spacecraft have flown most of the uncrewed missions to the ISS. Soyuz MS-22 was launched in 2022. A micro-meteorite impact in December 2022 caused a coolant leak in its external radiator and it was considered risky for human landing. Thus MS-22 reentered uncrewed on 28 March 2023 and Soyuz MS-23 was launched uncrewed on 24 February 2023, and it returned the MS-22 crew. [293] [294] [295] [1] Rendering of the ISS and visiting vehicles. Live link at nasa.gov. Spacecraft 3 December 2023 [296] [297] 2024 1 February 2024 [296] [297] Q1 2024 17 February 2023 [296] [297] 2024 23 March 2024 [296] [297] April 2024 Soyuz MS No. 756 Kazbek Crewed Scheduled missions[ edit ] All dates are UTC . Dates are the earliest possible dates and may change. Forward ports are at the front of the station according to its normal direction of travel and orientation ( attitude ). Aft is at the rear of the station, used by spacecraft boosting the station's orbit. Nadir is closest the Earth, zenith is on top. Port is to the left if pointing one's feet towards the Earth and looking in the direction of travel; starboard to the right. Mission See also: Docking and berthing of spacecraft The Progress M-14M resupply vehicle approaching the ISS in 2012. More than 50 unpiloted Progress spacecraft have delivered supplies during the lifetime of the station. Space Shuttle Endeavour , ATV-2 , Soyuz TMA-21 , and Progress M-10M docked to the ISS, as seen from the departing Soyuz TMA-20 All Russian spacecraft and self-propelled modules are able to rendezvous and dock to the space station without human intervention using the Kurs radar docking system from over 200 kilometres away. The European ATV uses star sensors and GPS to determine its intercept course. When it catches up it uses laser equipment to optically recognise Zvezda, along with the Kurs system for redundancy.  Crew supervise these craft, but do not intervene except to send abort commands in emergencies. Progress and ATV supply craft can remain at the ISS for six months, [290] [291] allowing great flexibility in crew time for loading and unloading of supplies and trash. From the initial station programs, the Russians pursued an automated docking methodology that used the crew in override or monitoring roles. Although the initial development costs were high, the system has become very reliable with standardisations that provide significant cost benefits in repetitive operations. [300] Soyuz spacecraft used for crew rotation also serve as lifeboats for emergency evacuation; they are replaced every six months and were used after the Columbia disaster to return stranded crew from the ISS. [301] The average expedition requires 2,722 kg of supplies, and by 9 March 2011, crews had consumed a total of around 22,000 meals. [91] Soyuz crew rotation flights and Progress resupply flights visit the station on average two and three times respectively each year. [302] Other vehicles berth instead of docking. The Japanese H-II Transfer Vehicle parked itself in progressively closer orbits to the station, and then awaited 'approach' commands from the crew, until it was close enough for a robotic arm to grapple and berth the vehicle to the USOS. Berthed craft can transfer International Standard Payload Racks . Japanese spacecraft berth for one to two months. [303] The berthing Cygnus and SpaceX Dragon were contracted to fly cargo to the station under phase 1 of the Commercial Resupply Services program. [304] [305] From 26 February 2011 to 7 March 2011 four of the governmental partners (United States, ESA, Japan and Russia) had their spacecraft (NASA Shuttle, ATV, HTV, Progress and Soyuz) docked at the ISS, the only time this has happened to date. [306] On 25 May 2012, SpaceX delivered the first commercial cargo with a Dragon spacecraft. [307] Launch and docking windows[ edit ] Prior to a spacecraft's docking to the ISS, navigation and attitude control ( GNC ) is handed over to the ground control of the spacecraft's country of origin. GNC is set to allow the station to drift in space, rather than fire its thrusters or turn using gyroscopes. The solar panels of the station are turned edge-on to the incoming spacecraft, so residue from its thrusters does not damage the cells. Before its retirement, Shuttle launches were often given priority over Soyuz, with occasional priority given to Soyuz arrivals carrying crew and time-critical cargoes, such as biological experiment materials. [308] Main article: Maintenance of the International Space Station Spare parts are called ORUs ; some are externally stored on pallets called ELCs and ESPs While anchored on the end of the OBSS during STS-120 , astronaut Scott Parazynski performs makeshift repairs to a US solar array that damaged itself when unfolding Mike Hopkins during a spacewalk Orbital Replacement Units (ORUs) are spare parts that can be readily replaced when a unit either passes its design life or fails. Examples of ORUs are pumps, storage tanks, controller boxes, antennas, and battery units. Some units can be replaced using robotic arms. Most are stored outside the station, either on small pallets called ExPRESS Logistics Carriers (ELCs) or share larger platforms called External Stowage Platforms which also hold science experiments. Both kinds of pallets provide electricity for many parts that could be damaged by the cold of space and require heating. The larger logistics carriers also have local area network (LAN) connections for telemetry to connect experiments. A heavy emphasis on stocking the USOS with ORU's occurred around 2011, before the end of the NASA shuttle programme, as its commercial replacements, Cygnus and Dragon, carry one tenth to one quarter the payload. Unexpected problems and failures have impacted the station's assembly time-line and work schedules leading to periods of reduced capabilities and, in some cases, could have forced abandonment of the station for safety reasons. Serious problems include an air leak from the USOS in 2004, [309] the venting of fumes from an Elektron oxygen generator in 2006, [310] and the failure of the computers in the ROS in 2007 during STS-117 that left the station without thruster, Elektron, Vozdukh and other environmental control system operations. In the latter case, the root cause was found to be condensation inside electrical connectors leading to a short circuit. [311] During STS-120 in 2007 and following the relocation of the P6 truss and solar arrays, it was noted during unfurling that the solar array had torn and was not deploying properly. [312] An EVA was carried out by Scott Parazynski , assisted by Douglas Wheelock . Extra precautions were taken to reduce the risk of electric shock, as the repairs were carried out with the solar array exposed to sunlight. [313] The issues with the array were followed in the same year by problems with the starboard Solar Alpha Rotary Joint (SARJ), which rotates the arrays on the starboard side of the station. Excessive vibration and high-current spikes in the array drive motor were noted, resulting in a decision to substantially curtail motion of the starboard SARJ until the cause was understood. Inspections during EVAs on STS-120 and STS-123 showed extensive contamination from metallic shavings and debris in the large drive gear and confirmed damage to the large metallic bearing surfaces, so the joint was locked to prevent further damage. [314] [315] Repairs to the joints were carried out during STS-126 with lubrication and the replacement of 11 out of 12 trundle bearings on the joint. [316] [317] In September 2008, damage to the S1 radiator was first noticed in Soyuz imagery. The problem was initially not thought to be serious. [318] The imagery showed that the surface of one sub-panel has peeled back from the underlying central structure, possibly because of micro-meteoroid or debris impact. On 15 May 2009 the damaged radiator panel's ammonia tubing was mechanically shut off from the rest of the cooling system by the computer-controlled closure of a valve. The same valve was then used to vent the ammonia from the damaged panel, eliminating the possibility of an ammonia leak. [318] It is also known that a Service Module thruster cover struck the S1 radiator after being jettisoned during an EVA in 2008, but its effect, if any, has not been determined. In the early hours of 1 August 2010, a failure in cooling Loop A (starboard side), one of two external cooling loops, left the station with only half of its normal cooling capacity and zero redundancy in some systems. [319] [320] [321] The problem appeared to be in the ammonia pump module that circulates the ammonia cooling fluid. Several subsystems, including two of the four CMGs, were shut down. Planned operations on the ISS were interrupted through a series of EVAs to address the cooling system issue. A first EVA on 7 August 2010, to replace the failed pump module, was not fully completed because of an ammonia leak in one of four quick-disconnects. A second EVA on 11 August successfully removed the failed pump module. [322] [323] A third EVA was required to restore Loop A to normal functionality. [324] [325] The USOS's cooling system is largely built by the US company Boeing, [326] which is also the manufacturer of the failed pump. [319] The four Main Bus Switching Units (MBSUs, located in the S0 truss), control the routing of power from the four solar array wings to the rest of the ISS. Each MBSU has two power channels that feed 160V DC from the arrays to two DC-to-DC power converters (DDCUs) that supply the 124V power used in the station. In late 2011 MBSU-1 ceased responding to commands or sending data confirming its health. While still routing power correctly, it was scheduled to be swapped out at the next available EVA. A spare MBSU was already on board, but a 30 August 2012 EVA failed to be completed when a bolt being tightened to finish installation of the spare unit jammed before the electrical connection was secured. [327] The loss of MBSU-1 limited the station to 75% of its normal power capacity, requiring minor limitations in normal operations until the problem could be addressed. On 5 September 2012, in a second six-hour EVA, astronauts Sunita Williams and Akihiko Hoshide successfully replaced MBSU-1 and restored the ISS to 100% power. [328] On 24 December 2013, astronauts installed a new ammonia pump for the station's cooling system. The faulty cooling system had failed earlier in the month, halting many of the station's science experiments. Astronauts had to brave a "mini blizzard" of ammonia while installing the new pump. It was only the second Christmas Eve spacewalk in NASA history. [329] Mission control centres[ edit ] Living quarters[ edit ] The living and working space on the International Space Station is larger than a six-bedroom house (complete with six sleeping quarters, two bathrooms, a gym, and a 360-degree view bay window). [4] Crew activities[ edit ] Engineer Gregory Chamitoff peering out of a window A typical day for the crew begins with a wake-up at 06:00, followed by post-sleep activities and a morning inspection of the station. The crew then eats breakfast and takes part in a daily planning conference with Mission Control before starting work at around 08:10. The first scheduled exercise of the day follows, after which the crew continues work until 13:05. Following a one-hour lunch break, the afternoon consists of more exercise and work before the crew carries out its pre-sleep activities beginning at 19:30, including dinner and a crew conference. The scheduled sleep period begins at 21:30. In general, the crew works ten hours per day on a weekday, and five hours on Saturdays, with the rest of the time their own for relaxation or work catch-up. [330] STS-122 mission specialists working on robotic equipment in the US lab The time zone used aboard the ISS is Coordinated Universal Time (UTC). [331] The windows are covered during night hours to give the impression of darkness because the station experiences 16 sunrises and sunsets per day. During visiting Space Shuttle missions, the ISS crew mostly followed the shuttle's Mission Elapsed Time (MET), which was a flexible time zone based on the launch time of the Space Shuttle mission. [332] [333] [334] The station provides crew quarters for each member of the expedition's crew, with two "sleep stations" in the Zvezda, one in Nauka and four more installed in Harmony. [335] [336] [337] [338] The USOS quarters are private, approximately person-sized soundproof booths. The ROS crew quarters in Zvezda include a small window, but provide less ventilation and sound proofing. A crew member can sleep in a crew quarter in a tethered sleeping bag, listen to music, use a laptop, and store personal items in a large drawer or in nets attached to the module's walls. The module also provides a reading lamp, a shelf and a desktop. [339] [340] [341] Visiting crews have no allocated sleep module, and attach a sleeping bag to an available space on a wall. It is possible to sleep floating freely through the station, but this is generally avoided because of the possibility of bumping into sensitive equipment. [342] It is important that crew accommodations be well ventilated; otherwise, astronauts can wake up oxygen-deprived and gasping for air, because a bubble of their own exhaled carbon dioxide has formed around their heads. [339] During various station activities and crew rest times, the lights in the ISS can be dimmed, switched off, and colour temperatures adjusted. [343] [344] Food and personal hygiene[ edit ] The crews of Expedition 20 and STS-127 enjoy a meal inside Unity Main dining desk in Node 1 Fresh fruits and vegetables are grown in the ISS On the USOS, most of the food aboard is vacuum sealed in plastic bags; cans are rare because they are heavy and expensive to transport. Preserved food is not highly regarded by the crew and taste is reduced in microgravity, [339] so efforts are taken to make the food more palatable, including using more spices than in regular cooking. The crew looks forward to the arrival of any spacecraft from Earth as they bring fresh fruit and vegetables. Care is taken that foods do not create crumbs, and liquid condiments are preferred over solid to avoid contaminating station equipment. Each crew member has individual food packages and cooks them using the galley , which has two food warmers, a refrigerator (added in November 2008), and a water dispenser that provides heated and unheated water. [340] Drinks are provided as dehydrated powder that is mixed with water before consumption. [340] [341] Drinks and soups are sipped from plastic bags with straws, while solid food is eaten with a knife and fork attached to a tray with magnets to prevent them from floating away. Any food that floats away, including crumbs, must be collected to prevent it from clogging the station's air filters and other equipment. [341] Showers on space stations were introduced in the early 1970s on Skylab and Salyut 3. [345] : 139 By Salyut 6, in the early 1980s, the crew complained of the complexity of showering in space, which was a monthly activity. [346] The ISS does not feature a shower; instead, crewmembers wash using a water jet and wet wipes, with soap dispensed from a toothpaste tube-like container. Crews are also provided with rinseless shampoo and edible toothpaste to save water. [342] [347] There are two space toilets on the ISS, both of Russian design, located in Zvezda and Tranquility. [340] These Waste and Hygiene Compartments use a fan-driven suction system similar to the Space Shuttle Waste Collection System. Astronauts first fasten themselves to the toilet seat, which is equipped with spring-loaded restraining bars to ensure a good seal. [339] A lever operates a powerful fan and a suction hole slides open: the air stream carries the waste away. Solid waste is collected in individual bags which are stored in an aluminium container. Full containers are transferred to Progress spacecraft for disposal. [340] [348] Liquid waste is evacuated by a hose connected to the front of the toilet, with anatomically correct "urine funnel adapters" attached to the tube so that men and women can use the same toilet. The diverted urine is collected and transferred to the Water Recovery System, where it is recycled into drinking water. [341] In 2021, the arrival of the Nauka module also brought a third toilet to the ISS. [349] The space toilet in the Zvezda module in the Russian segment The main toilet in the US Segment inside the Tranquility module * Both toilets are a Russian design. Crew health and safety[ edit ] Overall[ edit ] On 12 April 2019, NASA reported medical results from the Astronaut Twin Study . Astronaut Scott Kelly spent a year in space on the ISS, while his twin spent the year on Earth. Several long-lasting changes were observed, including those related to alterations in DNA and cognition , when one twin was compared with the other. [350] [351] In November 2019, researchers reported that astronauts experienced serious blood flow and clot problems while on board the ISS, based on a six-month study of 11 healthy astronauts. The results may influence long-term spaceflight, including a mission to the planet Mars, according to the researchers. [352] [353] See also: Coronal mass ejection Video of the Aurora Australis , taken by the crew of Expedition 28 on an ascending pass from south of Madagascar to just north of Australia over the Indian Ocean The ISS is partially protected from the space environment by Earth's magnetic field . From an average distance of about 70,000 km (43,000 mi) from the Earth's surface, depending on Solar activity, the magnetosphere begins to deflect solar wind around Earth and the space station. Solar flares are still a hazard to the crew, who may receive only a few minutes warning. In 2005, during the initial "proton storm" of an X-3 class solar flare, the crew of Expedition 10 took shelter in a more heavily shielded part of the ROS designed for this purpose. [354] [355] Subatomic charged particles, primarily protons from cosmic rays and solar wind, are normally absorbed by Earth's atmosphere. When they interact in sufficient quantity, their effect is visible to the naked eye in a phenomenon called an aurora . Outside Earth's atmosphere, ISS crews are exposed to approximately one millisievert each day (about a year's worth of natural exposure on Earth), resulting in a higher risk of cancer. Radiation can penetrate living tissue and damage the DNA and chromosomes of lymphocytes ; being central to the immune system , any damage to these cells could contribute to the lower immunity experienced by astronauts. Radiation has also been linked to a higher incidence of cataracts in astronauts. Protective shielding and medications may lower the risks to an acceptable level. [49] Radiation levels on the ISS are between 12 and 28.8 milli rads per day, [356] about five times greater than those experienced by airline passengers and crew, as Earth's electromagnetic field provides almost the same level of protection against solar and other types of radiation in low Earth orbit as in the stratosphere. For example, on a 12-hour flight, an airline passenger would experience 0.1 millisieverts of radiation, or a rate of 0.2 millisieverts per day; this is only one fifth the rate experienced by an astronaut in LEO. Additionally, airline passengers experience this level of radiation for a few hours of flight, while the ISS crew are exposed for their whole stay on board the station. [357] Stress[ edit ] Cosmonaut Nikolai Budarin at work inside the Zvezda service module crew quarters There is considerable evidence that psychosocial stressors are among the most important impediments to optimal crew morale and performance. [358] Cosmonaut Valery Ryumin wrote in his journal during a particularly difficult period on board the Salyut 6 space station: "All the conditions necessary for murder are met if you shut two men in a cabin measuring 18 feet by 20 [5.5 m × 6 m] and leave them together for two months." NASA's interest in psychological stress caused by space travel, initially studied when their crewed missions began, was rekindled when astronauts joined cosmonauts on the Russian space station Mir. Common sources of stress in early US missions included maintaining high performance under public scrutiny and isolation from peers and family. The latter is still often a cause of stress on the ISS, such as when the mother of NASA astronaut Daniel Tani died in a car accident, and when Michael Fincke was forced to miss the birth of his second child. A study of the longest spaceflight concluded that the first three weeks are a critical period where attention is adversely affected because of the demand to adjust to the extreme change of environment. [359] ISS crew flights typically last about five to six months. The ISS working environment includes further stress caused by living and working in cramped conditions with people from very different cultures who speak a different language. First-generation space stations had crews who spoke a single language; second- and third-generation stations have crew from many cultures who speak many languages. Astronauts must speak English and Russian, and knowing additional languages is even better. [360] Due to the lack of gravity, confusion often occurs. Even though there is no up and down in space, some crew members feel like they are oriented upside down. They may also have difficulty measuring distances. This can cause problems like getting lost inside the space station, pulling switches in the wrong direction or misjudging the speed of an approaching vehicle during docking. [361] Astronaut Frank De Winne , attached to the TVIS treadmill with bungee cords aboard the ISS The physiological effects of long-term weightlessness include muscle atrophy , deterioration of the skeleton ( osteopenia ), fluid redistribution, a slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, and puffiness of the face. [49] Sleep is regularly disturbed on the ISS because of mission demands, such as incoming or departing spacecraft. Sound levels in the station are unavoidably high. The atmosphere is unable to thermosiphon naturally, so fans are required at all times to process the air which would stagnate in the freefall (zero-G) environment. To prevent some of the adverse effects on the body, the station is equipped with: two TVIS treadmills (including the COLBERT); the ARED (Advanced Resistive Exercise Device), which enables various weightlifting exercises that add muscle without raising (or compensating for) the astronauts' reduced bone density; [362] and a stationary bicycle. Each astronaut spends at least two hours per day exercising on the equipment. [339] [340] Astronauts use bungee cords to strap themselves to the treadmill. [363] [364] Microbiological environmental hazards[ edit ] See also: Microbiological environmental hazards on the Mir space station Hazardous molds that can foul air and water filters may develop aboard space stations. They can produce acids that degrade metal, glass, and rubber. They can also be harmful to the crew's health. Microbiological hazards have led to a development of the LOCAD-PTS which identifies common bacteria and molds faster than standard methods of culturing , which may require a sample to be sent back to Earth. [365] Researchers in 2018 reported, after detecting the presence of five Enterobacter bugandensis bacterial strains on the ISS (none of which are pathogenic to humans), that microorganisms on the ISS should be carefully monitored to continue assuring a medically healthy environment for astronauts. [366] [367] Contamination on space stations can be prevented by reduced humidity, and by using paint that contains mold-killing chemicals, as well as the use of antiseptic solutions. All materials used in the ISS are tested for resistance against fungi . [368] Since 2016, a series of ESA-sponsored experiments have been done to test the anti-bacterial properties of various materials, with the goal of developing "smart surfaces" that mitigate bacterial growth in multiple ways, using the best method for a particular circumstance. Dubbed "Microbial Aerosol Tethering on Innovative Surfaces" (MATISS), the programme involves deployment of small plaques containing an array of glass squares covered with different test coatings. They remain on the station for six months before being returned to earth for analysis. [369] The most recent and final experiment of the series was launched on 5 June 2023 aboard the SpaceX CRS-28 cargo mission to ISS, comprising four plaques. Whereas previous experiments in the series were limited to analysis by light microsocopy , the present experiment uses quartz glass made of pure silica, which will allow spectrographic analysis . Two of the plaques will be returned after eight months and the remaining two after 16 months. [370] In April 2019, NASA reported that a comprehensive study had been conducted into the microorganisms and fungi present on the ISS. The experiment was performed over a period of 14 months on three different flight missions, and involved taking samples from 8 predefined locations inside the station, then returning them to earth for analysis. In prior experiments, analysis was limited to culture-based methods, thus overlooking microbes which cannot be grown in culture. The present study utilized molecular -based methods in addition to culturing, resulting in a more complete catalog. The results may be useful in improving the health and safety conditions for astronauts, as well as better understanding other closed-in environments on earth such as clean rooms used by the pharmaceutical and medical industries. [371] [372] Noise[ edit ] Space flight is not inherently quiet, with noise levels exceeding acoustic standards as far back as the Apollo missions . [373] [374] For this reason, NASA and the International Space Station international partners have developed noise control and hearing loss prevention goals as part of the health program for crew members. Specifically, these goals have been the primary focus of the ISS Multilateral Medical Operations Panel (MMOP) Acoustics Subgroup since the first days of ISS assembly and operations. [375] [376] The effort includes contributions from acoustical engineers , audiologists , industrial hygienists , and physicians who comprise the subgroup's membership from NASA, Roscosmos, the European Space Agency (ESA), the Japanese Aerospace Exploration Agency (JAXA), and the Canadian Space Agency (CSA). When compared to terrestrial environments, the noise levels incurred by astronauts and cosmonauts on the ISS may seem insignificant and typically occur at levels that would not be of major concern to the Occupational Safety and Health Administration – rarely reaching 85 dBA. But crew members are exposed to these levels 24 hours a day, seven days a week, with current missions averaging six months in duration. These levels of noise also impose risks to crew health and performance in the form of sleep interference and communication, as well as reduced alarm audibility . Over the 19 plus year history of the ISS, significant efforts have been put forth to limit and reduce noise levels on the ISS. During design and pre-flight activities, members of the Acoustic Subgroup have written acoustic limits and verification requirements, consulted to design and choose quietest available payloads, and then conducted acoustic verification tests prior to launch. [375] : 5.7.3 During spaceflights, the Acoustics Subgroup has assessed each ISS module's in flight sound levels, produced by a large number of vehicle and science experiment noise sources, to assure compliance with strict acoustic standards. The acoustic environment on ISS changed when additional modules were added during its construction, and as additional spacecraft arrive at the ISS. The Acoustics Subgroup has responded to this dynamic operations schedule by successfully designing and employing acoustic covers, absorptive materials, noise barriers , and vibration isolators to reduce noise levels. Moreover, when pumps, fans, and ventilation systems age and show increased noise levels, this Acoustics Subgroup has guided ISS managers to replace the older, noisier instruments with quiet fan and pump technologies, significantly reducing ambient noise levels . NASA has adopted most-conservative damage risk criteria (based on recommendations from the National Institute for Occupational Safety and Health and the World Health Organization ), in order to protect all crew members. The MMOP Acoustics Subgroup has adjusted its approach to managing noise risks in this unique environment by applying, or modifying, terrestrial approaches for hearing loss prevention to set these conservative limits. One innovative approach has been NASA's Noise Exposure Estimation Tool (NEET), in which noise exposures are calculated in a task-based approach to determine the need for hearing protection devices (HPDs). Guidance for use of HPDs, either mandatory use or recommended, is then documented in the Noise Hazard Inventory, and posted for crew reference during their missions. The Acoustics Subgroup also tracks spacecraft noise exceedances, applies engineering controls , and recommends hearing protective devices to reduce crew noise exposures. Finally, hearing thresholds are monitored on-orbit, during missions. There have been no persistent mission-related hearing threshold shifts among US Orbital Segment crewmembers (JAXA, CSA, ESA, NASA) during what is approaching 20 years of ISS mission operations, or nearly 175,000 work hours. In 2020, the MMOP Acoustics Subgroup received the Safe-In-Sound Award for Innovation for their combined efforts to mitigate any health effects of noise. [377] Fire and toxic gases[ edit ] An onboard fire or a toxic gas leak are other potential hazards. Ammonia is used in the external radiators of the station and could potentially leak into the pressurised modules. [378] Altitude and orbital inclination[ edit ] Graph showing the changing altitude of the ISS from November 1998 until November 2018 Animation of ISS orbit from 14 September 2018 to 14 November 2018. Earth is not shown. The ISS is currently maintained in a nearly circular orbit with a minimum mean altitude of 370 km (230 mi) and a maximum of 460 km (290 mi), [379] in the centre of the thermosphere , at an inclination of 51.6 degrees to Earth's equator with an eccentricity of 0.007.[ citation needed ] This orbit was selected because it is the lowest inclination that can be directly reached by Russian Soyuz and Progress spacecraft launched from Baikonur Cosmodrome at 46° N latitude without overflying China or dropping spent rocket stages in inhabited areas. [380] [381] It travels at an average speed of 28,000 kilometres per hour (17,000 mph), and completes 15.5 orbits per day (93 minutes per orbit). [5] [382] The station's altitude was allowed to fall around the time of each NASA shuttle flight to permit heavier loads to be transferred to the station. After the retirement of the shuttle, the nominal orbit of the space station was raised in altitude (from about 350 km to about 400 km). [383] [384] Other, more frequent supply spacecraft do not require this adjustment as they are substantially higher performance vehicles. [32] [385] Atmospheric drag reduces the altitude by about 2 km a month on average. Orbital boosting can be performed by the station's two main engines on the Zvezda service module, or Russian or European spacecraft docked to Zvezda's aft port. The Automated Transfer Vehicle is constructed with the possibility of adding a second docking port to its aft end, allowing other craft to dock and boost the station. It takes approximately two orbits (three hours) for the boost to a higher altitude to be completed. [385] Maintaining ISS altitude uses about 7.5 tonnes of chemical fuel per annum [386] at an annual cost of about $210 million. [387] Orbits of the ISS, shown in April 2013 The Russian Orbital Segment contains the Data Management System, which handles Guidance, Navigation and Control (ROS GNC) for the entire station. [388] Initially, Zarya, the first module of the station, controlled the station until a short time after the Russian service module Zvezda docked and was transferred control. Zvezda contains the ESA built DMS-R Data Management System. [389] Using two fault-tolerant computers (FTC), Zvezda computes the station's position and orbital trajectory using redundant Earth horizon sensors, Solar horizon sensors as well as Sun and star trackers. The FTCs each contain three identical processing units working in parallel and provide advanced fault-masking by majority voting. Orientation[ edit ] Zvezda uses gyroscopes ( reaction wheels ) and thrusters to turn itself around. Gyroscopes do not require propellant; instead they use electricity to 'store' momentum in flywheels by turning in the opposite direction to the station's movement. The USOS has its own computer-controlled gyroscopes to handle its extra mass. When gyroscopes 'saturate' , thrusters are used to cancel out the stored momentum. In February 2005, during Expedition 10, an incorrect command was sent to the station's computer, using about 14 kilograms of propellant before the fault was noticed and fixed. When attitude control computers in the ROS and USOS fail to communicate properly, this can result in a rare 'force fight' where the ROS GNC computer must ignore the USOS counterpart, which itself has no thrusters. [390] [391] [392] Docked spacecraft can also be used to maintain station attitude, such as for troubleshooting or during the installation of the S3/S4 truss , which provides electrical power and data interfaces for the station's electronics. [393] Orbital debris threats[ edit ] Main article: Space debris The low altitudes at which the ISS orbits are also home to a variety of space debris, [394] including spent rocket stages, defunct satellites, explosion fragments (including materials from anti-satellite weapon tests), paint flakes, slag from solid rocket motors, and coolant released by US-A nuclear-powered satellites. These objects, in addition to natural micrometeoroids , [395] are a significant threat. Objects large enough to destroy the station can be tracked, and are not as dangerous as smaller debris. [396] [397] Objects too small to be detected by optical and radar instruments, from approximately 1 cm down to microscopic size, number in the trillions. Despite their small size, some of these objects are a threat because of their kinetic energy and direction in relation to the station. Spacewalking crew in spacesuits are also at risk of suit damage and consequent exposure to vacuum . [398] Ballistic panels, also called micrometeorite shielding, are incorporated into the station to protect pressurised sections and critical systems. The type and thickness of these panels depend on their predicted exposure to damage. The station's shields and structure have different designs on the ROS and the USOS. On the USOS, Whipple Shields are used. The US segment modules consist of an inner layer made from 1.5–5.0 cm-thick (0.59–1.97 in) aluminium , a 10 cm-thick (3.9 in) intermediate layers of Kevlar and Nextel (a ceramic fabric), [399] and an outer layer of stainless steel , which causes objects to shatter into a cloud before hitting the hull, thereby spreading the energy of impact. On the ROS, a carbon fibre reinforced polymer honeycomb screen is spaced from the hull, an aluminium honeycomb screen is spaced from that, with a screen-vacuum thermal insulation covering, and glass cloth over the top. [400] Space debris is tracked remotely from the ground, and the station crew can be notified. [401] If necessary, thrusters on the Russian Orbital Segment can alter the station's orbital altitude, avoiding the debris. These Debris Avoidance Manoeuvres (DAMs) are not uncommon, taking place if computational models show the debris will approach within a certain threat distance. Ten DAMs had been performed by the end of 2009. [402] [403] [404] Usually, an increase in orbital velocity of the order of 1 m/s is used to raise the orbit by one or two kilometres. If necessary, the altitude can also be lowered, although such a manoeuvre wastes propellant. [403] [405] If a threat from orbital debris is identified too late for a DAM to be safely conducted, the station crew close all the hatches aboard the station and retreat into their spacecraft in order to be able to evacuate in the event the station was seriously damaged by the debris. This partial station evacuation has occurred on 13 March 2009, 28 June 2011, 24 March 2012 and 16 June 2015. [406] [407] In November 2021, a debris cloud from the destruction of Kosmos 1408 by an anti-satellite weapons test threatened the ISS, leading to the announcement of a yellow alert, leading to crew sheltering in the crew capsules. [408] A couple of weeks later, it had to perform an unscheduled maneuver to drop the station by 310 meters to avoid a collision with hazardous space debris. [409] A 7-gram object (shown in centre) shot at 7 km/s (23,000 ft/s), the orbital velocity of the ISS, made this 15 cm (5.9 in) crater in a solid block of aluminium Radar -trackable objects, including debris, with distinct ring of geostationary satellites Example of risk management : A NASA model showing areas at high risk from impact for the International Space Station Sightings from Earth[ edit ] Further information: Satellite watching and Satellite flare The ISS is visible to the naked eye as a slow-moving, bright white dot because of reflected sunlight, and can be seen in the hours after sunset and before sunrise, when the station remains sunlit but the ground and sky are dark. [410] The ISS takes about 10 minutes to pass from one horizon to another, and will only be visible part of that time because of moving into or out of the Earth's shadow . Because of the size of its reflective surface area, the ISS is the brightest artificial object in the sky (excluding other satellite flares ), with an approximate maximum magnitude of −4 when in sunlight and overhead (similar to Venus ), and a maximum angular size of 63 arcseconds. [411] Tools are provided by a number of websites such as Heavens-Above (see Live viewing below) as well as smartphone applications that use orbital data and the observer's longitude and latitude to indicate when the ISS will be visible (weather permitting), where the station will appear to rise, the altitude above the horizon it will reach and the duration of the pass before the station disappears either by setting below the horizon or entering into Earth's shadow. [412] [413] [414] [415] In November 2012 NASA launched its "Spot the Station" service, which sends people text and email alerts when the station is due to fly above their town. [416] The station is visible from 95% of the inhabited land on Earth, but is not visible from extreme northern or southern latitudes. [380] Under specific conditions, the ISS can be observed at night on five consecutive orbits. Those conditions are 1) a mid-latitude observer location, 2) near the time of the solstice with 3) the ISS passing in the direction of the pole from the observer near midnight local time. The three photos show the first, middle and last of the five passes on 5–6 June 2014. Skytrack long duration exposure of the ISS The ISS on its first pass of the night passing nearly overhead shortly after sunset in June 2014 The ISS passing north on its third pass of the night near local midnight in June 2014 The ISS passing west on its fifth pass of the night before sunrise in June 2014 Astrophotography[ edit ] The ISS and HTV photographed from Earth by Ralf Vandebergh Using a telescope-mounted camera to photograph the station is a popular hobby for astronomers, [417] while using a mounted camera to photograph the Earth and stars is a popular hobby for crew. [418] The use of a telescope or binoculars allows viewing of the ISS during daylight hours. [419] Composite of six photos of the ISS transiting the gibbous Moon Transits of the ISS in front of the Sun, particularly during an eclipse (and so the Earth, Sun, Moon, and ISS are all positioned approximately in a single line) are of particular interest for amateur astronomers. [420] [421] Main articles: Politics of the International Space Station and International Space Station programme A Commemorative Plaque honouring Space Station Intergovernmental Agreement signed on 28 January 1998 Involving five space programs and fifteen countries, [422] the International Space Station is the most politically and legally complex space exploration programme in history. [422] The 1998 Space Station Intergovernmental Agreement sets forth the primary framework for international cooperation among the parties. A series of subsequent agreements govern other aspects of the station, ranging from jurisdictional issues to a code of conduct among visiting astronauts. [423] Following the 2022 Russian invasion of Ukraine , continued cooperation between Russia and other countries on the International Space Station has been put into question.  Roscosmos Director General Dmitry Rogozin insinuated that Russian withdrawal could cause the International Space Station to de-orbit due to lack of reboost capabilities, writing in a series of tweets, "If you block cooperation with us, who will save the ISS from an unguided de-orbit to impact on the territory of the US or Europe? There's also the chance of impact of the 500-ton construction in India or China. Do you want to threaten them with such a prospect? The ISS doesn't fly over Russia, so all the risk is yours. Are you ready for it?" [424] (This latter claim is untrue: the ISS flies over all parts of the Earth between 51.6 degrees latitude north and south, approximately the latitude of Saratov .) Rogozin later tweeted that normal relations between ISS partners could only be restored once sanctions have been lifted, and indicated that Roscosmos would submit proposals to the Russian government on ending cooperation. [425] NASA stated that, if necessary, US corporation Northrop Grumman has offered a reboost capability that would keep the ISS in orbit. [426] On 26 July 2022, Yury Borisov , Rogozin's successor as head of Roscosmos, submitted to Russian President Putin plans for withdrawal from the programme after 2024. [427] However, Robyn Gatens, the NASA official in charge of the space station, responded that NASA had not received any formal notices from Roscosmos concerning withdrawal plans. [428]
Main article: SpaceX launch vehicles The landing of a Falcon 9 Block 5 first stage at Cape Canaveral in July 2019. VTVL technologies are used in many of SpaceX's launch vehicles. SpaceX has developed three launch vehicles. The small-lift Falcon 1 was the first launch vehicle developed and was retired in 2009. The medium-lift Falcon 9 and the heavy-lift Falcon Heavy are both operational. Falcon 1 was a small rocket capable of placing several hundred kilograms into low Earth orbit . It launched five times between 2006 and 2009, of which 2 were successful. [142] The Falcon 1 was the first privately funded, liquid-fueled rocket to reach orbit. [124] Falcon 9 is a medium-lift launch vehicle capable of delivering up to 22,800 kilograms (50,265 lb) to orbit, competing with the Delta IV and the Atlas V rockets, as well as other launch providers around the world. It has nine Merlin engines in its first stage. The Falcon 9 v1.0 rocket successfully reached orbit on its first attempt on 4 June 2010. Its third flight, COTS Demo Flight 2 , launched on 22 May 2012 and launched the first commercial spacecraft to reach and dock with the International Space Station (ISS). [52] The vehicle was upgraded to Falcon 9 v1.1 in 2013, Falcon 9 Full Thrust in 2015, and finally to Falcon 9 Block 5 in 2018. The first stage of Falcon 9 is designed to retro propulsively land, be recovered, and reflown. [143] Falcon Heavy is a heavy-lift launch vehicle capable of delivering up to 63,800 kg (140,700 lb) to Low Earth orbit (LEO) or 26,700 kg (58,900 lb) to geosynchronous transfer orbit (GTO). It uses three slightly modified Falcon 9 first-stage cores with a total of 27 Merlin 1D engines. [144] [145] The Falcon Heavy successfully flew its inaugural mission on 6 February 2018, launching Musk's personal Tesla Roadster into heliocentric orbit [146] Both the Falcon 9 and Falcon Heavy are certified to conduct launches for the National Security Space Launch (NSSL). [147] [148] As of 7 April 2024, the Falcon 9 and Falcon Heavy have been launched 329 times, resulting in 327 full mission successes, one partial success, and one in-flight failure . In addition, a Falcon 9 experienced a pre-flight failure before a static fire test in 2016. [149] [150] SpaceX is developing a fully reusable super-heavy lift launch system known as Starship . It comprises a reusable first stage, called Super Heavy , and the reusable Starship second stage space vehicle. The system is intended to supersede the company's existing launch vehicle hardware by the early 2020s. [151] [152] Main article: SpaceX rocket engines Merlin 1D engine undergoes a test at SpaceX's Rocket Development and Test Facility in McGregor, Texas Since the founding of SpaceX in 2002, the company has developed several rocket engines – Merlin , Kestrel , and Raptor – for use in launch vehicles , [153] [154] Draco for the reaction control system of the Dragon series of spacecraft, [155] and SuperDraco for abort capability in Crew Dragon . [156] Merlin is a family of rocket engines that uses liquid oxygen (LOX) and RP-1 propellants. Merlin was first used to power the Falcon 1's first stage and is now used on both stages of the Falcon 9 and Falcon Heavy vehicles. [157] Kestrel uses the same propellants and was used as the Falcon 1 rocket's second-stage main engine. [154] [158] Draco and SuperDraco are hypergolic liquid-propellant rocket engines. Draco engines are used on the reaction control system of the Dragon and Dragon 2 spacecraft. [155] The SuperDraco engine is more powerful, and eight SuperDraco engines provide launch escape capability for crewed Dragon 2 spacecraft during an abort scenario. [159] Raptor is a new family of liquid oxygen and liquid methane -fueled full-flow staged combustion cycle engines to power the first and second stages of the in-development Starship launch system. [153] Development versions were test-fired in late 2016, [160] and the engine flew for the first time in 2019, powering the Starhopper vehicle to an altitude of 20 m (66 ft). [161] Main article: SpaceX Dragon The SpaceX's Crew Dragon spacecraft, designed to deliver crew to and from the International Space Station as part of the Commercial Crew Development program SpaceX has developed the Dragon spacecraft to transport cargo and crew to the International Space Station . The first version of Dragon , used only for cargo, was first launched in 2010. [45] The currently operational second generation Dragon spacecraft, known as Dragon 2 , conducted its first flight , without crew, to the ISS in early 2019, followed by a crewed flight of Dragon 2 in 2020. [117] The cargo variant of Dragon 2 flew for the first time in December 2020, for a resupply to the Space Station as part of the CRS contract with NASA. [162] In March 2020 SpaceX revealed the Dragon XL, designed as a resupply spacecraft for NASA's planned Lunar Gateway space station under a Gateway Logistics Services (GLS) contract. [163] Dragon XL is planned to launch on the Falcon Heavy , and is able to transport over 5,000 kg (11,000 lb) to the Gateway. Dragon XL will be docked at the Gateway for six to twelve months at a time. [164] SpaceX designed a spacesuit to be worn inside the Dragon spacecraft to protect from possible depressurization. Autonomous spaceport drone ships[ edit ] Autonomous spaceport drone ship in position prior to CRS-6 mission SpaceX routinely returns the first stage of Falcon 9 and Falcon Heavy rockets after orbital launches. The rocket flights and lands at a predetermined landing site using only its propulsion systems. [165] When propellant margins do not permit a return to launch site (RTLS), rockets return to a floating landing platform in the ocean, called autonomous spaceport drone ships (ASDS). [166] SpaceX also plans to introduce floating launch platforms . These are modified oil rigs to use in the 2020s to provide a sea launch option for their second-generation launch vehicle: the heavy-lift Starship system, consisting of the Super Heavy booster and Starship second stage. Main article: Starlink Sixty Starlink satellites stacked together before deployment Starlink is an internet satellite constellation under development by Starlink Services, LLC, a wholly-owned subsidiary of SpaceX, [7] [167] that consists of thousands of cross-linked communications satellites in ~550 km orbits. Its goal is to address the significant unmet demand worldwide for low-cost broadband capabilities. [168] Development began in 2015, and initial prototype test-flight satellites were launched on the SpaceX Paz satellite mission in 2017. In May 2019, SpaceX launched the first batch of 60 satellites aboard a Falcon 9. [169] Initial test operation of the constellation began in late 2020 [170] and first orders were taken in early 2021. [171] Customers were told to expect internet service speeds of 50 Mbit/s to 150 Mbit/s and latency from 20 ms to 40 ms. [172] In December 2022, Starlink reached over 1 million subscribers worldwide. [173] The planned large number of Starlink satellites has been criticized by astronomers due to concerns over light pollution , [174] [175] [176] with the brightness of Starlink satellites in both optical and radio wavelengths interfering with scientific observations. [177] In response, SpaceX has implemented several upgrades to Starlink satellites aimed at reducing their brightness. [178] The large number of satellites employed by Starlink also creates long-term dangers of space debris collisions . [179] [180] However, the satellites are equipped with krypton -fueled Hall thrusters which allow them to de-orbit at the end of their life. They are also designed to avoid collisions based on uplinked tracking data autonomously. [181] In December 2022, SpaceX announced Starshield , a program to incorporate military or government entity payloads on board a Starlink-derived satellite bus. The Space Development Agency is a key customer procuring satellites for a space-based missile defense system. [182] [183] Main article: Hyperloop pod competition In June 2015, SpaceX announced that they would sponsor a Hyperloop competition , and would build a 1.6 km (0.99 mi) long subscale test track near SpaceX's headquarters for the competitive events. [184] [185] The company has held the annual competition since 2017. [186] In collaboration with doctors and academic researchers, SpaceX invited all employees to participate in the creation of a COVID-19 antibody-testing program in 2020. As such, 4300 employees volunteered to provide blood samples resulting in a peer-reviewed scientific paper crediting eight SpaceX employees as coauthors and suggesting that a certain level of COVID-19 antibodies may provide lasting protection against the virus. [187] [188] In July 2018, Musk arranged for his employees to build a mini-submarine to assist the rescue of children stuck in a flooded cavern in Thailand . [189] Richard Stanton , leader of the international rescue diving team, urged Musk to facilitate the construction of the vehicle as a back-up, in case flooding worsened. [190] [191] Engineers at SpaceX and The Boring Company built the mini-submarine from a Falcon 9 liquid oxygen transfer tube in eight hours and personally delivered it to Thailand. [192] [193] By this time, however, eight of the 12 children had already been rescued using full face masks and oxygen under anesthesia ; consequently Thai authorities declined to use the submarine. [189] NASA's PACE (plankton, Aerosol, Cloud, ocean Ecosystem) satellite launched on a SpaceX Falcon 9 rocket from the Cape Canaveral Space Force Stattion's Space Launch Complex 40 at 12.03 PM IST on Thursday, 8 February 2024. The spacecraft was separated from the rocket's second stage a few minutes after that and entered a sun-synchronous orbit. The Falcon 9 rocket also stuck its landing completing the 4th completed flight for this particular Falcon 9 rocket. [194] Facilities[ edit ] SpaceX is headquartered in Hawthorne, California , which also serves as its primary manufacturing plant. [195] The company operates a research and major operation in Redmond, Washington , owns a test site in Texas [196] and operates three launch sites, with another under development. SpaceX also operates regional offices in Texas, Virginia, and Washington, D.C. [76] SpaceX was incorporated in the state of Delaware . [197] Headquarters, mission control, manufacturing, and refurbishment facilities[ edit ] SpaceX Headquarters in Hawthorne, California at night during a Falcon 9 launch from Vandenberg Space Force Base SpaceX Headquarters is located in the Los Angeles suburb of Hawthorne, California . The large three-story facility, originally built by Northrop Corporation to build Boeing 747 fuselages, [195] houses SpaceX's office space, mission control, and Falcon 9 manufacturing facilities. [198] The area has one of the largest concentrations of space sector headquarters, facilities, and/or subsidiaries in the U.S., including Boeing / McDonnell Douglas main satellite building campuses, The Aerospace Corporation , Raytheon , NASA's Jet Propulsion Laboratory , United States Space Force 's Space Systems Command at Los Angeles Air Force Base , Lockheed Martin , BAE Systems , Northrop Grumman , and AECOM , etc., with a large pool of aerospace engineers and recent college engineering graduates. [195] SpaceX uses a high degree of vertical integration in the production of its rockets and rocket engines. [16] SpaceX builds its rocket engines , rocket stages , spacecraft , principal avionics and all software in-house in their Hawthorne facility, which is unusual for the space industry. [16] In January 2015, SpaceX announced it would be entering the satellite production business and global satellite internet business. The first satellite facility is a 30,000 sq ft (2,800 m2) office building located in Redmond, Washington . As of January 2017, a second facility in Redmond was acquired with 40,625 sq ft (3,774.2 m2) and has become a research and development laboratory for the satellites. [199] In July 2016, SpaceX acquired an additional 8,000 sq ft (740 m2) office space in Irvine, California to focus on satellite communications. [200] Development and test facilities[ edit ] Main article: SpaceX Rocket Development and Test Facility Aerial view of the SpaceX McGregor engine testing facility, 2008 SpaceX operates its Rocket Development and Test Facility in McGregor, Texas . All SpaceX rocket engines are tested on rocket test stands , [196] and low-altitude VTVL flight testing of the Falcon 9 Grasshopper in 2012–2013 were carried out at McGregor. [201] Testing of the much larger Starship prototypes is conducted at the SpaceX Starbase near Brownsville, Texas . [198] The company purchased the McGregor facilities from Beal Aerospace , where it refitted the largest test stand for Falcon 9 engine testing. SpaceX has made many improvements to the facility since its purchase and has also extended the acreage by purchasing several pieces of adjacent farmland. As of October 2012 [update] , the McGregor facility had seven test stands that are operated "18 hours a day, six days a week" [202] and is building more test stands because production is ramping up and the company has a large manifest in the next several years. [203] In addition to routine testing, Dragon capsules (following recovery after an orbital mission), are shipped to McGregor for de-fueling, cleanup, and refurbishment for reuse in future missions. [204] Main article: SpaceX launch facilities Falcon Heavy Side Boosters landing on LZ1 and LZ2 at Cape Canaveral SpaceX currently operates four orbital launch sites, at Cape Canaveral Space Force Station and Kennedy Space Center in Florida and Vandenberg Space Force Base in California for Falcon rockets, and Starbase near Brownsville, Texas for Starship. SpaceX has indicated that they see a niche for each of the four orbital facilities and that they have sufficient launch business to fill each pad. [205] The Vandenberg launch site enables highly inclined orbits (66–145°), while Cape Canaveral and Kennedy enable orbits of medium inclination (28.5–55°). [206] Larger inclinations, including SSO , are possible from Florida by overflying Cuba. [207] Before it was retired, all Falcon 1 launches took place at the Ronald Reagan Ballistic Missile Defense Test Site on Omelek Island of the Marshall Islands . [208] In April 2007, the Pentagon approved the use of Cape Canaveral Space Launch Complex 40 (SLC-40) by SpaceX. [209] The site has been used since 2010 for Falcon 9 launches, mainly to low Earth and geostationary orbits. The former Launch Complex 13 at Cape Canaveral, now renamed Landing Zones 1 and 2 , has since 2015 been used for Falcon 9 first-stage booster landings . [210] SpaceX west coast launch facility at Vandenberg Space Force Base , during the launch of CASSIOPE Vandenberg Space Launch Complex 4 (SLC-4E) was leased from the military in 2011 and is used for payloads to polar orbits. The Vandenberg site can launch both Falcon 9 and Falcon Heavy vehicles, [211] but cannot launch to low inclination orbits. The neighboring SLC-4W was converted to Landing Zone 4 in 2015 for booster landings. [212] On 14 April 2014, SpaceX signed a 20-year lease for Kennedy Space Center Launch Complex 39A . [213] The pad was subsequently modified to support Falcon 9 and Falcon Heavy launches. As of 2024 [update] it is the only pad that supports Falcon Heavy launches. SpaceX launched its first crewed mission to the ISS from Launch Pad 39A on 30 May 2020. [214] Pad 39A has been prepared since 2019 to eventually accommodate Starship launches. With delays in launch FAA permits for Boca Chica, the 39A Starship preparation was accelerated in 2022. [215] The Starship assembly building at SpaceX Starbase in Texas SpaceX manufactures and flies Starship test vehicles from the SpaceX Starbase in Boca Chica near Brownsville, Texas , having announced first plans for the launch facility in August 2014. [216] [217] The Federal Aviation Administration (FAA) issued the permit in July 2014. [218] SpaceX broke ground on the new launch facility in 2014 with construction ramping up in the latter half of 2015, [219] with the first suborbital launches from the facility in 2019 [198] and orbital launches starting in 2023. Some residents of Boca Chica Village , Brownsville , and environmental activists criticized the site along with Starship development program in various aspects. [220] [221] Further information on SpaceX launches: Falcon 1 § Launches , List of Falcon 9 and Falcon Heavy launches , and SpaceX Starship flight tests SpaceX won demonstration and actual supply contracts from NASA for the International Space Station (ISS) with technology the company developed. SpaceX is also certified for U.S. military launches of Evolved Expendable Launch Vehicle -class (EELV) payloads. With approximately thirty missions on the manifest for 2018 alone, SpaceX represents over $12 billion under contract. [76] Cargo to ISS[ edit ] Main articles: Commercial Orbital Transportation Services , Commercial Resupply Services , and Gateway Logistics Services The COTS 2 Dragon is berthed to the International Space Station (ISS) by Canadarm2 In 2006, SpaceX won a NASA Commercial Orbital Transportation Services (COTS) Phase 1 contract to demonstrate cargo delivery to the International Space Station (ISS), with a possible contract option for crew transport. [222] Through this contract, designed by NASA to provide "seed money" through Space Act Agreements for developing new capabilities, NASA paid SpaceX $396 million to develop the cargo configuration of the Dragon spacecraft, while SpaceX developed the Falcon 9 launch vehicle with their resources. [223] These Space Act Agreements have been shown to have saved NASA millions of dollars in development costs, making rocket development 4–10 times cheaper than if produced by NASA alone. [224] In December 2010 the launch of the SpaceX COTS Demo Flight 1 mission, SpaceX became the first private company to successfully launch, orbit, and recover a spacecraft. [225] Dragon successfully berthed with the ISS during SpaceX COTS Demo Flight 2 in May 2012, a first for a private spacecraft . [226] Commercial Resupply Services (CRS) is a series of contracts awarded by NASA from 2008 to 2016 for the delivery of cargo and supplies to the International Space Station on commercially operated spacecraft. The first CRS contracts were signed in 2008 and awarded $1.6 billion to SpaceX for 12 cargo transport missions, covering deliveries to 2016. [227] SpaceX CRS-1 , the first of the 12 planned resupply missions, launched in October 2012, achieved orbit, berthed, and remained on station for 20 days, before re-entering the atmosphere and splashing down in the Pacific Ocean. [228] CRS missions have flown approximately twice a year to the ISS since then. In 2015, NASA extended the Phase 1 contracts by ordering an additional three resupply flights from SpaceX, and then extended the contract further for a total of twenty cargo missions to the ISS. [229] [227] [230] The final Dragon 1 mission, SpaceX CRS-20 , departed the ISS in April 2020, and Dragon was subsequently retired from service. A second phase of contracts was awarded in January 2016 with SpaceX as one of the awardees. SpaceX will fly up to nine additional CRS flights with the upgraded Dragon 2 spacecraft. [231] [232] In March 2020, NASA contracted SpaceX to develop the Dragon XL spacecraft to send supplies to the Lunar Gateway space station. Dragon XL will be launched on a Falcon Heavy. [233] See also: Commercial Crew Program NASA astronauts inside the Dragon spacecraft during the Crew-1 mission rendezvous with the International Space Station SpaceX is responsible for the transportation of NASA astronauts to and from the ISS. The NASA contracts started as part of the Commercial Crew Development (CCDev) program, aimed at developing commercially operated spacecraft capable of delivering astronauts to the ISS. The first contract was awarded to SpaceX in 2011, [234] [235] followed by another in 2012 to continue development and testing of its Dragon 2 spacecraft. [236] In September 2014, NASA chose SpaceX and Boeing as the two companies that would be funded to develop systems to transport U.S. crews to and from the ISS. [237] SpaceX won $2.6 billion to complete and certify Dragon 2 by 2017. The contracts called for at least one crewed flight test with at least one NASA astronaut aboard. Once Crew Dragon received NASA human-spaceflight certification, the contract required SpaceX to conduct at least two, and as many as six, crewed missions to the space station. [237] SpaceX completed the first key flight test of its Crew Dragon spacecraft, a Pad Abort Test , in May 2015, [238] and successfully conducted a full uncrewed test flight in early 2019. The capsule docked to the ISS and then splashed down in the Atlantic Ocean. [239] In January 2020, SpaceX conducted an in-flight abort test , the last test flight before flying crew, in which the Dragon spacecraft fired its launch escape engines in a simulated abort scenario. [240] On 30 May 2020, the Crew Dragon Demo-2 mission was launched to the International Space Station with NASA astronauts Bob Behnken and Doug Hurley , the first time a crewed vehicle had launched from the U.S. since 2011, and the first commercial crewed launch to the ISS . [241] The Crew-1 mission was successfully launched to the International Space Station on 16 November 2020, with NASA astronauts Michael Hopkins , Victor Glover and Shannon Walker along with JAXA astronaut Soichi Noguchi , [242] all members of the Expedition 64 crew. [243] On 23 April 2021, Crew-2 was launched to the International Space Station with NASA astronauts Shane Kimbrough and K. Megan McArthur , JAXA astronaut Akihiko Hoshide , and ESA astronaut Thomas Pesquet . [244] The Crew-2 mission successfully docked on 24 April 2021. [245] Resilience after splashdown SpaceX also offers paid crewed spaceflights for private individuals. The first of these missions, Inspiration4 , launched in 2021 on behalf of Shift4 Payments CEO Jared Isaacman . The mission launched the Crew Dragon Resilience from the Florida Kennedy Space Center 's Launch Complex 39A atop a Falcon 9 launch vehicle, placed the Dragon capsule into low Earth orbit , and ended successfully about three days later when the Resilience splashed down in the Atlantic Ocean. All four crew members received commercial astronaut training from SpaceX. The training included lessons in orbital mechanics, operating in a microgravity environment, stress testing, emergency-preparedness training, and mission simulations. [246] National defense[ edit ] Launch of the STP-2 mission on a Falcon Heavy in June 2019 In 2005, SpaceX announced that it had been awarded an Indefinite Delivery/Indefinite Quantity (IDIQ) contract, allowing the United States Air Force to purchase up to $100 million worth of launches from the company. [247] Three years later, NASA announced that it had awarded an IDIQ Launch Services contract to SpaceX for up to $1 billion, depending on the number of missions awarded. [248] In December 2012, SpaceX announced its first two launch contracts with the United States Department of Defense (DoD). The United States Air Force Space and Missile Systems Center awarded SpaceX two EELV-class missions: Deep Space Climate Observatory (DSCOVR) and Space Test Program 2 (STP-2). DSCOVR was launched on a Falcon 9 launch vehicle in 2015, while STP-2 was launched on a Falcon Heavy on 25 June 2019. [249] The Falcon 9 v1.1 was certified for National Security Space Launch (NSSL) in 2015, allowing SpaceX to contract launch services to the Air Force for any payloads classified under national security. [147] This broke the monopoly held since 2006 by United Launch Alliance (ULA) over U.S. Air Force launches of classified payloads. [250] In April 2016, the U.S. Air Force awarded the first such national security launch to SpaceX to launch the second GPS III satellite for $82.7 million. [251] This was approximately 40% less than the estimated cost for similar previous missions. [252] SpaceX also launched the third GPS III launch on 20 June 2020. [253] In March 2018, SpaceX secured an additional $290 million contract from the U.S. Air Force to launch another three GPS III satellites. [254] The U.S. National Reconnaissance Office (NRO) also purchased launches from SpaceX, with the first taking place on 1 May 2017. [255] In February 2019, SpaceX secured a $297 million contract from the U.S. Air Force to launch another three national security missions, all slated to launch no earlier than FY 2021. [256] In August 2020, the U.S. Space Force awarded its National Security Space Launch (NSSL) contracts for the following 5–7 years. SpaceX won a contract for $316 million for one launch. In addition, SpaceX will handle 40% of the U.S. military's satellite launch requirements over the period. [257] SpaceX also designs and launches custom military satellites for the Space Development Agency as part of a new missile defense system in low Earth orbit. [258] The constellation would give the United States capabilities to sense, target and potentially intercept nuclear missiles and hypersonic weapons launched from anywhere on Earth. [259] Both China and Russia brought concerns to the United Nations about the program, [260] and various organizations warn it could be destabilizing and trigger an arms race in space. [261] [262] In March 2024, Reuters reported that, as part of a $1.8-billion contract signed with the National Reconnaissance Office in 2021, SpaceX is building a network of hundreds of spy satellites . This new network, Reuters reported, would be able to operate as a swarm in low orbits. [263] Launch market competition and pricing pressure[ edit ] Main article: Space launch market competition SpaceX's low launch prices, especially for communications satellites flying to geostationary transfer orbit (GTO), have resulted in market pressure on its competitors to lower their own prices. [16] Prior to 2013, the openly competed comsat launch market had been dominated by Arianespace (flying the Ariane 5 ) and International Launch Services (flying the Proton ). [264] With a published price of $56.5 million per launch to low Earth orbit , Falcon 9 rockets were the cheapest in the industry. [265] European satellite operators are pushing the ESA to reduce launch prices of the Ariane 5 and the future Ariane 6 rockets as a result of competition from SpaceX. [266] SpaceX ended the United Launch Alliance (ULA) monopoly of U.S. military payloads when it began to compete for national security launches. In 2015, anticipating a slump in domestic, military, and spy launches, ULA stated that it would go out of business unless it won commercial satellite launch orders. [267] To that end, ULA announced a major restructuring of processes and workforce to decrease launch costs by half. [268] [269] Congressional testimony by SpaceX in 2017 suggested that the NASA Space Act Agreement process of "setting only a high-level requirement for cargo transport to the space station [while] leaving the details to industry" had allowed SpaceX to design and develop the Falcon 9 rocket on its own at a substantially lower cost. According to NASA's own independently verified numbers, SpaceX's total development cost for the Falcon 9 rocket, including the Falcon 1 rocket, was estimated at $390 million. In 2011, NASA estimated that it would have cost the agency about $4 billion to develop a rocket like the Falcon 9 booster based upon NASA's traditional contracting processes, about ten times more. [224] In May 2020, NASA administrator Jim Bridenstine remarked that thanks to NASA's investments into SpaceX, the United States has 70% of the commercial launch market, a major improvement since 2012 when there were no commercial launches from the country. [270] As of 2024, SpaceX operates a Rideshare and Bandwagon (mid inclination) programs. This provides additional competition for small satellite launchers. [271]
Toggle the table of contents Blue Origin From Wikipedia, the free encyclopedia American aerospace company "BE-1" and "BE-2" redirect here. For other uses, see Be-1 (disambiguation) and BE2 (disambiguation) . Blue Origin Enterprises, L.P. September 8, 2000; 23 years ago (September 8, 2000) Founder 10 (5 production facilities & 5 field offices) Area served Spacecrafts, rockets, heavy-lift launch vehicles and lunar growth technology Owner Blue Origin Enterprises, L.P., [2] commonly referred to as Blue Origin [3] is an American aerospace manufacturer , defense contractor , [4] [5] launch service provider and space technologies [6] company headquartered in Kent, Washington, United States . The company makes rocket engines for United Launch Alliance (ULA) 's Vulcan rocket and manufactures their own rockets , spacecraft , satellites , [7] and heavy-lift launch vehicles . The company is the second provider of lunar lander services for NASA 's Artemis program and was awarded a $3.4 billion contract. [8] The four rocket engines the company has in production are the BE-3U , BE-3PM , BE-4 and the BE-7. [9] The organization was awarded the Robert J. Collier Trophy in 2016 for demonstrating rocket booster reusability with their New Shepard Rocket Program. [10] The award is administered by the U.S. National Aeronautic Association (NAA) and is presented to those who have made "the greatest achievement in aeronautics or astronautics in America, with respect to improving the performance, efficiency, and safety of air or space vehicles, the value of which has been thoroughly demonstrated by actual use during the preceding year." [11] History[ edit ] The company was founded in 2000 by Jeff Bezos , the founder of Amazon . [12] [13] Rob Meyerson joined the company in 2003 and served as the CEO before leaving the company in 2018. [14] Bob Smith served as CEO from 2018 to 2023. [15] The current CEO is Dave Limp. [16] Little is known about the company's activities in its early years. In 2006, the company purchased land for its New Shepard missions 30 miles North of Van Horn, Texas, United States called Launch Site One (LS1). In November 2006, the first test vehicle was launched, the Goddard rocket, which reached an altitude of 285 feet. [17] After initiating the development of an orbital rocket system prior to 2012, and stating in 2013 on their website that the first stage would perform a powered vertical landing and be reusable, the company publicly announced their orbital launch vehicle intentions in September 2015. In January 2016, the company indicated that the new rocket would be many times larger than New Shepard . The company publicly released the high-level design of the vehicle and announced its name in September 2016 as " New Glenn ". The New Glenn heavy-lift launch vehicle can be configured in both two-stage and three-stage variants. New Glenn is planned to launch in Q3 of 2024. [18] New Shepard is a fully reusable suborbital launch vehicle developed for space tourism . The vehicle is named after Alan Shepard , the first American astronaut in space. The vehicle is capable of vertical takeoff and landings and can carry humans and customer payloads to the edge of space . [22] The New Shepard launch vehicle is a rocket that consists of a booster rocket and a crew capsule. The capsule can be configured to house up to six passengers , cargo , or a combination of both. The booster rocket is powered by one BE-3PM engine, which sends the capsule to an apogee ( Sub-Orbital ) of 100.5 kilometres (62.4 mi) and flies above the Kármán line , where passengers and cargo can experience a few minutes of weightlessness before the capsule returns to Earth. [23] [24] The launch vehicle is designed to be fully reusable, with the capsule returning to Earth via three parachutes and a solid rocket motor . The booster lands vertically on the same launchpad it took off from. The company has successfully launched and landed the New Shepard launch vehicle 22 times with 1 partial failure deemed successful and 1 failure. The launch vehicle has a length of 15.0 metres (49.2 ft), a diameter of 3.7 metres (12 ft) and a launch mass of 75 short tons (150,000 lb; 68,000 kg). The BE-3PM engine produces 490 kN of thrust at takeoff . New Shepard allows the company to significantly reduce the cost of space tourism , making the experience more accessible to the general public . [25] [26] Main article: New Glenn New Glenn is a heavy-lift launch vehicle in development stage, and is expected to be ready for Launch in Q3 of 2024. The launch date has been set back because of numerous delays.  Named after NASA astronaut John Glenn , design work on the vehicle began in early 2012. Illustrations of the vehicle, and the high-level specifications, were initially publicly unveiled in September 2016. The full vehicle was first unveiled on a launch pad on February 21, 2024. [27] The rocket will have a diameter of 7 meters (23 ft), and its first stage will be powered by seven BE-4 engines. The 7 meter-diameter fairing is claimed to have twice the payload volume of "any commercial launch system" and to be the biggest payload fairing in the world. [28] Like the New Shepard , New Glenn's first stage is also designed to be reusable . In 2021, the company initiated conceptual design work on approaches to potentially make the second stage reusable as well, with the project codenamed " Project Jarvis ". [29] NASA announced on February 9, 2023, that it had selected the New Glenn heavy-lift launch vehicle for the launch of two Escape and Plasma Acceleration and Dynamics Explorers (ESCAPADE) spacecraft . The New Glenn heavy-lift launch vehicle will launch ESCAPADE [30] [31] in Q3 of 2024 with the ESCAPADE spacecraft entering Mars's orbit approximately one year after launch. In 2024, Blue Origin received funding from the USSF to assess New Glenn's ability to launch national security payloads. [32] Main article: Blue Moon (spacecraft) In May 2019, Jeff Bezos announced plans for a crew-carrying lunar lander known as Blue Moon . [33] The standard version of the lander is intended to transport 3.6 t (7,900 lb) to the lunar surface , whereas a stretched tank variant could land up to 6.5 t (14,000 lb) on the Moon , both are vehicles designed to make a soft landing on the Moon's surface . The lander will use the BE-7 hydrolox engine. [34] On May 19, 2023 NASA contracted Blue Origin to develop, test and deploy its Blue Moon landing system for the agency's Artemis V mission, which explores the Moon and prepares future manned missions to Mars . The project includes an unmanned test mission followed by a manned Moon landing in 2029. The contract value is $3.4 billion. [35] [36] BE-1[ edit ] Blue Origin's first engine was a "simple, single-propellant engine" called the Blue Engine-1 (BE-1) which used peroxide propellant and generated only 8.9 kN (2,000 lbf) of thrust . [37] BE-2[ edit ] The Blue Engine-2 (BE-2) which was a bipropellant engine using kerosene and peroxide, producing 140 kN (31,000 lbf) thrust. [37] BE-3 (BE-3U and BE-3PM)[ edit ] Main article: BE-3 The BE-3 is a family of rocket engines made by Blue Origin with two variants, the BE-3U and BE-3PM. The rocket engine is a liquid hydrogen / liquid oxygen (LH2/LOX) cryogenic engine that can produce 490 kN (110,000 lbf) and 710 kN (160,000 lbf) of thrust, respectively. Early thrust chamber testing began at NASA Stennis [38] in 2013. [39] By late 2013, the BE-3 had been successfully tested on a full-duration sub-orbital burn, with simulated coast phases and engine relights, "demonstrating deep throttle, full power, long-duration and reliable restart all in a single-test sequence." [40] NASA has released a video of the test. [39] As of December 2013 [update] , the engine had demonstrated more than 160 starts and 9,100 seconds (2.5 h) of operation at the company's test facility near Van Horn, Texas . [40] [41] The BE-3U is an open expander cycle variant of the BE-3. Two of these engines will be used to power the New Glenn heavy-lift launch vehicle's second stage. The amount of thrust the BE-3U produces is 710 kilonewtons (160,000 lbf). [42] The BE-3PM , uses a pump-fed engine design, with a combustion tap-off cycle to take a small amount of combustion gases from the main combustion chamber to power the engine's turbopumps . One engine is used to power the Propulsive Module (PM) of New Shepard . The amount of thrust the BE-3PM produces is 490 kilonewtons (110,000 lbf). [42] The rocket engine can be throttled down to as low as 110 kN (25,000 lbf) for use in controlled vertical landings. Main article: BE-4 The BE-4 is a liquid oxygen /liquified natural gas (LOX/LNG) rocket engine that can produce 2,400 kN (550,000 lbf) of thrust. [43] In late 2014, the company signed an agreement with United Launch Alliance (ULA) to develop the BE-4 engine, for ULA's upgraded Atlas V and Vulcan Centaur rockets replacing the RD-180 Russian-made rocket engine. The newly developed heavy-lift launch vehicle will use two of the 2,400 kN (550,000 lbf) BE-4 engines on each first stage . The engine development program for the BE-4 began in 2011. [44] On October 31, 2022, a Twitter post by the official Blue Origin account announced that the first two BE-4 engines had been delivered to ULA and were in the process of being integrated on a Vulcan rocket. In a later tweet, ULA CEO Tory Bruno said that one of the engines had already been installed on the booster, and that the other would be joining it momentarily. [45] On June 7, 2023, the two BE-4 rocket engines performed as expected when ULA performed a Flight Readiness Firing of the Vulcan Rocket at launch pad 41 at the Cape Canaveral Space Force Station in Cape Canaveral, Florida . [46] [47] Vulcan Centaur launched for the first time on January 8, 2024, successfully carrying Astrobotic Technology 's Peregrine lunar lander , the first mission on NASA's Commercial Lunar Payload Services (CLPS) program using the BE-4 engine. [48] BE-7[ edit ] The BE-7 engine is a liquid oxygen/liquid hydrogen dual expander cycle engine currently under development, designed for use on Blue Moon . [49] The engine produces 44 kN (10,000 lbf) of thrust. Its first ignition tests were performed in June 2019, with thrust chamber assembly testing continuing through 2023. [50] Pusher escape motor[ edit ] Facilities[ edit ] The company has facilities across the United States which include five main locations and five field offices: [53] Washington, DC The company’s headquarters is in Kent, Washington . Rocket development takes place at its Headquarters. The company has continued to expand its Seattle-area offices and rocket production facilities since 2016, purchasing an adjacent 11,000 m2 (120,000 sq ft)-building. [54] In 2017, the company filed permits to build a new 21,900 m2 (236,000 sq ft) warehouse complex and an additional 9,560 m2 (102,900 sq ft) of office space. [55] The company established a new headquarters and R&D facility, called the O'Neill Building on June 6, 2020. [56] [57] Launch Site One (LSO)[ edit ] Corn Ranch, commonly referred to as Launch Site One (LSO) is the company's launch site 30 miles north of Van Horn, Texas. [58] The launch facility is located at 31.422927°N 104.757152°W. [59] In addition to the sub-orbital launch pad, Launch Site One (LSO) includes a number of rocket engine test stands and engine test cells are at the site to support the hydrolox , methalox and storable propellant engines that are used. There are three test cells for testing the BE-3 and BE-4 engines. The test cells  support full-thrust and full-duration burns, and one supports short-duration, high-pressure preburner tests. Blue Engine[ edit ] Engine production is located in Huntsville, Alabama, at a 600,000sqft facility called, "Blue Engine". The companies website states that, "The world-class engine manufacturing facility in The Rocket City conduct[s] high rate production of the BE-4 and BE-3U engines. The company is planning a third major expansion in Huntsville and the company was approved for the sale of 14.83 acres adjacent to its already sprawling campus at the price of $1.427 million. [60] Orbital Launch Site (OLS)[ edit ] The Orbital Launch Site (OLS) at the Cape Canaveral Space Force Station , develops rockets and does extensive testing. The company converted Launch Complex 36 (LC-36) to launch its New Glenn into orbit [61] at the Cape Canaveral Space Force Station . The facility was initially completed in 2020 and is being used for the construction of New Glenn prototypes, rocket testing, and designs. [62] The companies facility is situated on 306 acres of land assembled from former Launch Complexes 11, 36A, and 36B. The land parcel used to build a rocket engine test stand for the BE-4 engine, a launch mount , called the Orbital Launch Site, (hence its name) and a reusable booster refurbishment facility for the New Glenn launch vehicle, which is expected to land on a drone ship and return to Port Canaveral for refurbishment. Manufacturing of "large elements, such as New Glenn's first and second stages as well as the payload fairings and other large components will be made nearby in Exploration Park ,  which is near the entrance to the Kennedy Space Center Visitor Complex on Merritt Island, Florida . [63] Main article: Blue Ring The Blue Ring vehicle was announced in October 2023 by Blue Origin. It will have its own engine and is meant to handle orbital logistics and delivery. In March 2024, in partnership with the United States Space Force , it was announced that the Blue Ring’s capabilities will be tested soon on a mission called DarkSky-1. [64] Orbital Reef (commercial space station)[ edit ] Main article: Orbital Reef The company and its partners Sierra Space , Boeing , Redwire Space and Genesis Engineering Solutions won a $130 million award to jump-start the design of their Orbital Reef commercial space station. The project is envisioned as an expandable business park, with Boeing's Starliner and Sierra Space's Dream Chaser transporting passengers to and from low Earth orbit (LEO) for tourism , research and in-space manufacturing projects. [65] Orbital Reef’s design will be modular in nature, to provide the greatest amount of customization and compatibility. It will reportedly be designed to accept docking from almost every in operation spacecraft like SpaceX Dragon 2 , Soyuz (spacecraft) , Dream Chaser , and Boeing Starliner . The initial modules will be: Life, Node, Core, and Research Modules. [66] Nuclear rocket program[ edit ] NASA plans to test spacecraft , engines and other propellent systems powered by nuclear fission no later than 2027 as part of the agency's effort to demonstrate more efficient methods of traveling through outer space for space exploration . [67] One benefit to using nuclear fission as a propellent for spacecraft is that nuclear-based systems can have less mass than solar cells which means a spacecraft could be much smaller while absorbing and using the same amount of energy more efficiently. Nuclear fission concepts that can power both life support and propulsion systems could greatly reduce the cost and flight time during space exploration. [68] The Defense Advanced Research Projects Agency awarded General Atomics , Lockheed Martin and Blue Origin contracts to fund and build nuclear spacecraft under the agency's Demonstration Rocket for Agile Cislunar Operations program or DRACO program. The company was awarded $2.9 million to develop spacecraft component designs. [69] In partnership with Blue Origin, Ultra Safe Nuclear Corp., GE Hitachi Nuclear Energy , GE Research , Framatome and Materion , USNC-Tech won a $5 million contract from NASA and the U.S. Department of Energy (DOE) to develop a long range nuclear propulsion system called the Power Adjusted Demonstration Mars Engine, or PADME. [70] Space Technology[ edit ] NASA awarded $35 million to the company in 2023 for the company's work on lunar regrowth to be used for solar powered systems on the moon. The company's website states that "Blue Alchemist is a proposed end-to-end, scalable, autonomous, and commercial solution that produces solar cells from lunar regolith, which is the dust and crushed rock abundant on the surface of the Moon. Based on a process called molten regolith electrolysis, the breakthrough would bootstrap unlimited electricity and power transmission cables anywhere on the surface of the Moon. This process also produces oxygen as a useful byproduct for propulsion and life support." Gary Lai, chief architect of the New Shepard rocket said during the pathfinder awards at the Seattle Museum of Flight that [The company] "aims to be the first company that harvests natural resources from the moon to use here on Earth,” He also mentioned that the company is building a novel approach to extract outer space's vast resources. Blue Origin flight data[ edit ] 1 Timeline of Space­Ship­One, Space­Ship­Two, CSXT and New Shepard sub-orbital flights. Where booster and capsule achieved different altitudes, the higher is plotted. In the SVG file, hover over a point to show details. In the chart below, ♺ means "Flight Proven Booster" New Shepard and test vehicle flight data Flight No. First rocket-powered test flight [71] 3 Pad escape test flight [75] 8 Flight to altitude 93.5 km, capsule recovered , booster crashed on landing [76] 9 Sub-orbital spaceflight and landing [77] 10 Sub-orbital spaceflight and landing of a reused booster [78] 11 Sub-orbital spaceflight and landing of a reused booster [79] 12 331,501 ft (63 mi) Success Sub-orbital spaceflight and landing of a reused booster: The fourth launch and landing of the same rocket. The company published a live webcast of the takeoff and landing. [80] 13 Capsule:23,269 ft (4 mi) Success Sub-orbital spaceflight and landing of a reused booster. Successful test of the in-flight abort system. The fifth and final launch and landing of the same rocket (NS2). [81] 14 Capsule:322,405 ft(61 mi) Success Flight to just under 100 km and landing. The first launch of NS3 and a new Crew Capsule 2.0. [82] 15 Sub-orbital spaceflight and landing of a reused booster. [83] 16 389,846 ft (74 mi) Success Sub-orbital spaceflight and landing of a reused booster, with the Crew Capsule 2.0–1 RSS H.G.Wells, carrying a mannequin . Successful test of the in-flight abort system at high altitude. Flight duration was 11 minutes. [84] 17 351,000 ft (66 mi) Success Sub-orbital flight, delayed from December 18, 2018. Eight NASA research and technology payloads were flown. [85] [86] 18 346,000 ft (65 mi) Success Sub-orbital flight. Max Ascent Velocity: 2,217 mph (3,568 km/h), [87] duration: 10 minutes, 10 seconds. Payload: 38 microgravity research payloads (nine sponsored by NASA). 19 343,000 ft (64 mi) Success Sub-orbital flight, Payload: Multiple commercial, research (8 sponsored by NASA) and educational payloads, including postcards from Club for the Future . [88] [89] [90] 20 346,000 ft (65 mi) Success 7th flight of the same capsule/booster. Onboard 12 payloads include Space Lab Technologies, Southwest Research Institute, postcards and seeds for Club for the Future, and multiple payloads for NASA including SPLICE to test future lunar landing technologies in support of the Artemis program [91] 21 350,858 ft (66 mi) Success Uncrewed qualification flight for NS4 rocket and "RSS First Step" capsule and maiden flight for NS4. [92] 22 348,753 ft (66 mi) Success 2nd flight of NS4 with Astronaut Rehearsal. Gary Lai , Susan Knapp, Clay Mowry, and Audrey Powers, all Blue Origin personnel, are "stand-in astronauts". Lai and Powers briefly get in. [93] 23 347,434 ft (66 mi) Success Payload mission consisting of 18 commercial payloads inside the crew capsule, a NASA lunar landing technology demonstration installed on the exterior of the booster and an art installation installed on the exterior of the crew capsule. [96] 25 341,434 ft (66 mi) Success 351,050 ft (66 mi) Success Third crewed flight ( NS-19 ). Crew: Laura Shepard Churchley, Michael Strahan , Dylan Taylor , Evan Dick, Lane Bess , and Cameron Bess. [98] 27 351,050 ft (66 mi) Success Fourth crewed flight ( NS-20 ). Crew: Marty Allen, Sharon Hagle, Marc Hagle, Jim Kitchen , George Nield, and Gary Lai . [99] 28 351,050 ft (66 mi) Success Fifth crewed flight ( NS-21 ). Crew: Evan Dick, Katya Echazarreta , Hamish Harding , Victor Correa Hespanha, Jaison Robinson , and Victor Vescovo . [100] 29 351,050 ft (66 mi) Success Sixth crewed flight ( NS-22 ). Crew: Coby Cotton , Mário Ferreira , Vanessa O'Brien , Clint Kelly III, Sara Sabry, and Steve Young. [101] 30 37,402 ft (7 mi) Failure Uncrewed flight with commercial payloads onboard ( NS-23 ). A booster failure triggered the launch escape system during flight, and the capsule landed successfully. The Blue Origin incident investigation found that a thermal-structural failure occurred on the BE-3 nozzle leading to the launch failure. [102] 31 107.060 km (66.5242 mi) Success Successful Return to Flight mission ( NS-24 ) following failure of NS-23 more than a year prior. 33 payloads and 38,000 Club for the Future postcards from students around the world. [103] NASA partnerships and funding[ edit ] The company has contracted to do work for NASA on several development efforts. The company was awarded $3.7 million in funding by NASA in 2009 via a Space Act Agreement [104] [105] under the first Commercial Crew Development (CCDev) program for development of concepts and technologies to support future human spaceflight operations. [106] [107] NASA co-funded risk-mitigation activities related to ground testing of (1) an innovative 'pusher' escape system, that lowers cost by being reusable and enhances safety by avoiding the jettison event of a traditional 'tractor' Launch Escape System, and (2) an innovative composite pressure vessel cabin that both reduces weight and increases safety of astronauts. [104] This was later revealed to be a part of a larger system, designed for a bionic capsule, that would be launched atop an Atlas V rocket. [108] On November 8, 2010, it was announced that the company had completed all milestones under its CCDev Space Act Agreement. [109] In April 2011, The company received a commitment from NASA for $22 million of funding under the CCDev phase 2 program . [110] Milestones included (1) performing a Mission Concept Review (MCR) and System Requirements Review (SRR) on the orbital Space Vehicle, which utilizes a bionic shape to optimize its launch profile and atmospheric reentry, (2) further maturing the pusher escape system, including ground and flight tests, and (3) accelerating development of its BE-3 LOX/LH2 440 kN (100,000 lbf ) engine through full-scale thrust chamber testing. [111] In 2012, NASA 's Commercial Crew Program released its follow-on CCiCap solicitation for the development of crew delivery to ISS by 2017. The company did not submit a proposal for CCiCap, but reportedly continued work on its development program with private funding. [112] The company had a failed attempt to lease a different part of the Space Coast , when they submitted a bid in 2013 to lease Launch Complex 39A (LC39A) at the Kennedy Space Center – on land to the north of, and adjacent to, Cape Canaveral AFS – following NASA's decision to lease the unused complex out as part of a bid to reduce annual operation and maintenance costs. The companies bid was for shared and non-exclusive use of the LC39A complex such that the launchpad was to have been able to interface with multiple vehicles , and costs for using the launch pad were to have been shared across multiple companies over the term of the lease. One potential shared user in the companies proposed plan was United Launch Alliance (ULA). Commercial use of the LC39A launch complex was awarded to SpaceX , which submitted a bid for exclusive use of the launch complex to support their crewed missions . [113] The company completed work for NASA on several small development contracts, receiving total funding of $25.7 million by 2013. [104] [110] In September 2013 – before completion of the bid period, and before any public announcement by NASA of the results of the process – Florida Today reported that the company had filed a protest with the U.S. General Accounting Office (GAO) "over what it says is a plan by NASA to award an exclusive commercial lease to SpaceX for use of mothballed space shuttle launch pad 39A". [114] NASA had originally planned to complete the bid award and have the pad transferred by October 1, 2013, but the protest delayed a decision until the U.S. General Accounting Office (GAO) reached a decision on the protest. [114] [115] SpaceX said that they would be willing to support a multi-user arrangement for pad 39A. [116] In December 2013, the U.S. General Accounting Office (GAO) denied the companies protest and sided with NASA , which argued that the solicitation contained no preference on the use of the facility as either multi-use or single-use. "The [solicitation] document merely [asked] bidders to explain their reasons for selecting one approach instead of the other and how they would manage the facility". [115] NASA selected the SpaceX proposal in late 2013 and signed a 20-year lease contract for Launch Pad 39A to SpaceX in April 2014. [117] The company placed their first bid via the NASA Sustaining Lunar Development (SLD) competition to fund and develop a lunar lander capable of transporting astronauts to and from the lunar surface. The Blue Origin led team called the "National Team" included, Lockheed Martin , Northrop Grumman , and Draper . On April 30, 2020, the company and its partners won a $579 million contract to start developing and testing an integrated Human Landing System (HLS) for the Artemis program to return humans to the Moon . [118] [119] However, the Blue Origin led team lost their first bid to work for NASA's Artemis program and on April 16, 2021, NASA officially selected the Space Exploration Technologies Corp. (SpaceX) to develop, test and build their version of the Human Landing System (HLS) for Artemis missions 2 (II), 3 (III) and 4 (IV). In early 2021, the company received over $275 million from NASA for lunar lander projects and sub-orbital research flights. [120] The company then announced on December 6, 2022, that it had submitted a second bid via the NASA Sustaining Lunar Development (SLD) competition to fund and develop a second lunar lander capable for transporting astronauts to and from the lunar surface. The announcement fell within NASA's deadline for Sustaining Lunar Development (SLD) proposals. As with their first bid, the company is leading another team called the "National Team" which includes Draper , Boeing , Lockheed Martin , Astrobotic , Honeybee Robotics and Blue Origin. [121] On May 19, 2023 NASA contracted the company to develop, test and deploy its Blue Moon landing system for the agency's Artemis V mission, which explores the Moon and prepares future manned missions to Mars . The project includes an unmanned test mission followed by a manned Moon landing in 2029. The contract value is $3.4 billion. [35] [36] Internal and additional U.S Government funding[ edit ] By July 2014, Jeff Bezos had invested over $500 million into the company. [122] and the vast majority of further funding into 2016 was to support technology development and operations where a majority of funding came from Jeff Bezos' private investment fund . In April 2017, an annual amount was published showing that Jeff Bezos was selling approximately $1 billion in Amazon stock per year to invest in the company. [123] Jeff Bezos has been criticized for spending excessive amounts of his fortune on spaceflight . [124] The company received $181 million from the United States Air Force for launch vehicle development in 2019. The company was also eligible to benefit from further grants totaling $500M as part of the U.S. Space Force Launch Services Agreement competition . [125] On November 18, 2022, the U.S. Space Systems Command announced that an agreement with the company that "paves the way" for the company's New Glenn rocket to compete for national security launch contracts once it completes its required flight certifications for Top Secret military payloads. In an interview with the Bob Smith by the financial Times in 2023, Smith said that the company had "hundreds of millions in revenue as well as billions of dollars in orders". [126] Early test vehicles[ edit ] Charon[ edit ] Charon on display at the Museum of Flight in Seattle, Washington. The companies first flight test vehicle, called Charon after Pluto 's moon, [127] was powered by four vertically mounted Rolls-Royce Viper Mk. 301 jet engines rather than rockets. The low-altitude vehicle was developed to test autonomous guidance and control technologies, and the processes that the company would use to develop its later rockets. Charon made its only test flight at Moses Lake, Washington on March 5, 2005. It flew to an altitude of 96 m (316 ft) before returning for a controlled landing near the liftoff point. [128] [129] As of 2016, Charon is on display at the Museum of Flight in Seattle, Washington. [130] Goddard[ edit ] The next test vehicle, named Goddard (also known as PM1), first flew on November 13, 2006. The flight was successful. A test flight for December 2 never launched. [131] [132] According to Federal Aviation Administration records, two further flights were performed by Goddard. [133] Blue Engine 1, or BE-1, was the first rocket engine developed by the company and was used in the company's Goddard development vehicle. PM2[ edit ] Another early suborbital test vehicle, PM2, had two flight tests in 2011 in west Texas. The vehicle designation may be short for "Propulsion Module". [134] The first flight was a short hop (low altitude, VTVL takeoff and landing mission) flown on May 6, 2011. The second flight, August 24, 2011, failed when ground personnel lost contact and control of the vehicle. The company released its analysis of the failure nine days later. As the vehicle reached a speed of Mach 1.2 and 14 km (46,000 ft) altitude, a "flight instability drove an angle of attack that triggered [the] range safety system to terminate thrust on the vehicle". [135] Blue Engine 2, or BE-2, was a pump-fed bipropellant engine burning kerosene and peroxide which produced 140 kN (31,000 lbf) of thrust. [136] [137] Five BE-2 engines powered the companies PM-2 development vehicle on two test flights in 2011. [138]
Toggle the table of contents Commercial use of space From Wikipedia, the free encyclopedia General space-related commerce This article is about general space-related commerce. For entrepreneurial space ventures and colonization, see Private spaceflight . Starlink user terminals used in Ukraine Commercial use of space is the provision of goods or services of commercial value by using equipment sent into Earth orbit or outer space . The first commercial use of outer space occurred in 1962, when the Telstar 1 satellite was launched to transmit television signals over the Atlantic Ocean. By 2004, global investment in all space sectors was estimated to be US$50.8 billion. [1] As of 2010, 31% of all space launches were commercial. [2] See also: Timeline of private spaceflight The first commercial use of satellites may have been the Telstar 1 satellite, launched in 1962, which was the first privately sponsored space launch, funded by AT&T and Bell Telephone Laboratories . Telstar 1 was capable of relaying television signals across the Atlantic Ocean , and was the first satellite to transmit live television, telephone , fax , and other data signals. [3] [4] Two years later, the Hughes Aircraft Company developed the Syncom 3 satellite, a geosynchronous communications satellite , leased to the Department of Defense . Commercial possibilities of satellites were further realized when the Syncom 3, orbiting near the International Date Line , was used to telecast the 1964 Olympic Games from Tokyo to the United States . [5] [6] Between 1960 and 1966, the U.S. National Aeronautics and Space Administration (NASA) launched a series of early weather satellites known as Television Infrared Observation Satellites (TIROS). These satellites greatly advanced meteorology worldwide, as satellite imagery was used for better forecasting, for both public and commercial interests. [7] [8] On April 6, 1965, the Hughes Aircraft Company placed the Intelsat I communications satellite in geosynchronous orbit over the Atlantic Ocean. Intelsat I was built for the Communications Satellite Corporation (COMSAT), and demonstrated that satellite-based communication was commercially feasible. Intelsat I allowed for near-instantaneous contact between Europe and North America by handling television, telephone and fax transmissions. [9] [10] Two years later, the Soviet Union launched the Orbita satellite, which provided television signals across Russia , and started the first national satellite television network. [11] [12] Similarly, the 1972 Anik A satellite, launched by Telesat Canada , allowed the Canadian Broadcasting Corporation to reach northern Canada for the first time. [13] [14] In 1980, Europe's Arianespace became the world's first commercial launch service provider . [15] [16] Beginning in 1997, Iridium Communications began launching a series of satellites known as the Iridium satellite constellation , which provided the first satellites for direct satellite telephone service. [17] [18] See also: Private spaceflight Delta IV Medium launch carrying DSCS III-B6 The commercial spaceflight industry derives the bulk of its revenue from the launching of satellites into the Earth's orbit. Commercial launch providers typically place private and government satellites into low Earth orbit (LEO) and geosynchronous Earth orbit (GEO). The Federal Aviation Administration (FAA) has licensed six commercial spaceports in the United States: Wallops Flight Facility , Kodiak Launch Complex , Spaceport Florida , Kennedy Space Center , Cape Canaveral Space Force Station , and the Vandenberg Air Force Base . Launch sites within Russia , France , and China have added to the global commercial launch capacity. The Delta IV , Atlas V , and Falcon family of launch vehicles are made available for commercial ventures for the United States, while Russia promotes eight families of vehicles.[ citation needed ] Between 1996 and 2002, 245 launches were made for commercial ventures while government (non-classified) launches only totaled 167 for the same period.[ citation needed ] Commercial space flight has spurred investment into the development of an efficient reusable launch vehicle (RLV) which can place larger payloads into orbit. Several companies such as SpaceX and Blue Origin are currently developing new RLV designs. In the United States , the Office of Commercial Space Transportation (generally referred to as FAA/AST or simply AST) is the branch of the Federal Aviation Administration (FAA) that approves any commercial rocket launch operations—that is, any launches that are not classified as model , amateur, or "by and for the government ." [19] In fiscal year 2022, there were 74 FAA-licensed commercial space operations, which includes both launches and reentries. [20] In 2023, the FAA predicted that commercial launches it licenses could more than double in the next several years. [20] Satellites and equipment[ edit ] ESTCube-1, a low-cost CubeSat for education. Satellite manufacturing Commercial satellite manufacturing is defined by the United States government as satellites manufactured for civilian, government, or non-profit use.[ citation needed ] Not included are satellites constructed for military use, nor for activities associated with any human space flight program. Between the years of 1996 and 2002, satellite manufacturing within the United States experienced an annual growth of 11%.[ citation needed ] The rest of the world experienced higher growth levels of around 13%.[ citation needed ] Ground equipment manufacturing Operating satellites communicate via receivers and transmitters on Earth. The manufacturing of satellite ground station communication terminals (including VSATs ), mobile satellite telephones , and home television receivers are a part of the ground equipment manufacturing sector. This sector grew through the latter half of the 1990s as it manufactured equipment for the satellite services sector. Between 1996 and 2002, this industry saw a 14% annual increase.[ citation needed ] Transponder leasing Businesses that operate satellites often lease or sell access to their satellites to data relay and telecommunication firms. This service is often referred to as transponder leasing. Between 1996 and 2002, this industry experienced a 15% annual growth. The United States accounts for about 32% of the world's transponder market.[ citation needed ] Subscription satellite services[ edit ] In 1994, DirecTV debuted direct broadcast satellite by introducing a signal receiving dish 18 inches in diameter. In 1996, Astro started in Malaysia with the launch of the MEASAT satellite. In November 1999, the Satellite Home Viewer Improvement Act became law, and local stations were then made available in satellite channel packages, fueling the industry's growth in the years that followed. By the end of 2000, DTH subscriptions totaled over 67 million.[ citation needed ] Satellite radio was pioneered by XM Satellite Radio and Sirius Satellite Radio . XM's first satellite was launched on March 18, 2001 and its second on May 8, 2001. [21] Its first broadcast occurred on September 25, 2001, nearly four months before Sirius. [22] Sirius launched the initial phase of its service in four cities on February 14, 2002, [23] expanding to the rest of the contiguous United States on July 1, 2002. [22] The two companies spent over $3 billion combined to develop satellite radio technology, build and launch the satellites, and for various other business expenses. [24] Main article: Satellite imagery Satellite imagery (also Earth observation imagery or spaceborne photography) are images of Earth or other planets collected by imaging satellites operated by governments and businesses around the world. Satellite imaging companies sell images by licensing them to governments and businesses such as Apple Maps and Google Maps . Main article: Satellite internet Satellites can be used to transmit and receive Internet services from space to any place in the planet Earth. [25] Main article: Satellite navigation Magellan GPS receiver in a marine application. A satellite navigation or satnav system is a system that uses satellites to provide autonomous geo-spatial positioning. It allows small electronic receivers to determine their location ( longitude , latitude , and altitude / elevation ) to high precision (within a few centimeters to metres) using time signals transmitted along a line of sight by radio from satellites. The system can be used for providing position, navigation or for tracking the position of something fitted with a receiver (satellite tracking). The signals also allow the electronic receiver to calculate the current local time to high precision, which allows time synchronization. These uses are collectively known as Positioning, Navigation and Timing (PNT). Satnav systems operate independently of any telephonic or internet reception, though these technologies can enhance the usefulness of the positioning information generated.
Toggle the table of contents Space tourism From Wikipedia, the free encyclopedia Human space travel for recreation This article is about paying space travellers. For other commercial spacefarers, see Commercial astronaut . For entrepreneurial space ventures and colonization, see Private spaceflight . "Space tourist" redirects here. For persons in space other than government employees or Axiom Space astronauts, see Space flight participant . For the 2009 film, see Space Tourists . e Space tourism is human space travel for recreational purposes. [1] There are several different types of space tourism, including orbital, suborbital and lunar space tourism. During the period from 2001 to 2009, seven space tourists made eight space flights aboard a Russian Soyuz spacecraft to the International Space Station , brokered by Space Adventures in conjunction with Roscosmos and RSC Energia . The publicized price was in the range of US$20–25 million per trip. Some space tourists have signed contracts with third parties to conduct certain research activities while in orbit. By 2007, space tourism was thought to be one of the earliest markets that would emerge for commercial spaceflight. [2] : 11 Russia halted orbital space tourism in 2010 due to the increase in the International Space Station crew size, using the seats for expedition crews that would previously have been sold to paying spaceflight participants. [3] [4] Orbital tourist flights were set to resume in 2015 but the planned flight was postponed indefinitely. [5] Russian orbital tourism eventually resumed with the launch of Soyuz MS-20 in 2021. [6] On June 7, 2019, NASA announced that starting in 2020, the organization aims to start allowing private astronauts to go on the International Space Station, with the use of the SpaceX Crew Dragon spacecraft and the Boeing Starliner spacecraft for public astronauts, which is planned to be priced at 35,000 USD per day for one astronaut, [7] and an estimated 50 million USD for the ride there and back. [8] See also: Space Race The Soviet space program was successful in broadening the pool of cosmonauts . The Soviet Intercosmos program included cosmonauts selected from Warsaw Pact member countries ( Czechoslovakia , Poland, East Germany, Bulgaria, Hungary, Romania) and later from allies of the USSR (Cuba, Mongolia, Vietnam) and non-aligned countries (India, Syria, Afghanistan). Most of these cosmonauts received full training for their missions and were treated as equals, but were generally given shorter flights than Soviet cosmonauts. The European Space Agency (ESA) also took advantage of the program.[ citation needed ] [11] The US Space Shuttle program included payload specialist positions which were usually filled by representatives of companies or institutions managing a specific payload on that mission. These payload specialists did not receive the same training as professional NASA astronauts and were not employed by NASA. In 1983, Ulf Merbold from the ESA and Byron Lichtenberg from MIT (engineer and Air Force fighter pilot) were the first payload specialists to fly on the Space Shuttle , on mission STS-9 . [12] [13] In 1984, Charles D. Walker became the first non-government astronaut to fly, with his employer McDonnell Douglas paying US$40,000 (equivalent to $117,309 in 2023) for his flight. [14] : 74–75 During the 1970s, Shuttle prime contractor Rockwell International studied a $200–300 million removable cabin that could fit into the Shuttle's cargo bay. The cabin could carry up to 74 passengers into orbit for up to three days. Space Habitation Design Associates proposed, in 1983, a cabin for 72 passengers in the bay. Passengers were located in six sections, each with windows and its own loading ramp, and with seats in different configurations for launch and landing. Another proposal was based on the Spacelab habitation modules, which provided 32 seats in the payload bay in addition to those in the cockpit area. A 1985 presentation to the National Space Society stated that, although flying tourists in the cabin would cost $1 million to $1.5 million per passenger without government subsidy, within 15 years, 30,000 people a year would pay US$25,000 (equivalent to $70,823 in 2023) each to fly in space on new spacecraft. The presentation also forecast flights to lunar orbit within 30 years and visits to the lunar surface within 50 years. [15] As the shuttle program expanded in the early 1980s, NASA began a Space Flight Participant program to allow citizens without scientific or governmental roles to fly. Christa McAuliffe was chosen as the first Teacher in Space in July 1985 from 11,400 applicants. 1,700 applied for the Journalist in Space program. An Artist in Space program was considered, and NASA expected that after McAuliffe's flight two to three civilians a year would fly on the shuttle. After McAuliffe was killed in the Challenger disaster in January 1986, the programs were canceled. McAuliffe's backup, Barbara Morgan , eventually got hired in 1998 as a professional astronaut and flew on STS-118 as a mission specialist . [14] : 84–85 A second journalist-in-space program, in which NASA green-lighted Miles O'Brien to fly on the Space Shuttle, was scheduled to be announced in 2003. That program was canceled in the wake of the Columbia disaster on STS-107 and subsequent emphasis on finishing the International Space Station before retiring the Space Shuttle.[ citation needed ] Initially, senior figures at NASA strongly opposed space tourism on principle; from the beginning of the ISS expeditions, NASA stated it was not interested in accommodating paying guests. [16] The Subcommittee on Space and Aeronautics Committee on Science of the House of Representatives held in June 2001 revealed the shifting attitude of NASA towards paying space tourists wanting to travel to the ISS in its statement on the hearing's purpose: "Review the issues and opportunities for flying nonprofessional astronauts in space, the appropriate government role for supporting the nascent space tourism industry, use of the Shuttle and Space Station for Tourism, safety and training criteria for space tourists, and the potential commercial market for space tourism." The subcommittee report was interested in evaluating Dennis Tito 's extensive training and his experience in space as a nonprofessional astronaut.[ citation needed ] With the realities of the post- Perestroika economy in Russia, its space industry was especially starved for cash. The Tokyo Broadcasting System (TBS) offered to pay for one of its reporters to fly on a mission. Toyohiro Akiyama was flown in 1990 to Mir with the eighth crew and returned a week later with the seventh crew. Cost estimates vary from $10 million up to $37 million. [17] [18] Akiyama gave a daily TV broadcast from orbit and also performed scientific experiments for Russian and Japanese companies. In 1991, British chemist Helen Sharman was selected from a pool of 13,000 applicants to be the first Briton in space. [19] The program was known as Project Juno and was a cooperative arrangement between the Soviet Union and a group of British companies. The Project Juno consortium failed to raise the funds required, and the program was almost canceled. Reportedly Mikhail Gorbachev ordered it to proceed under Soviet expense in the interests of international relations, but in the absence of Western underwriting, less expensive experiments were substituted for those in the original plans. Sharman flew aboard Soyuz TM-12 to Mir and returned aboard Soyuz TM-11 . [20] In April 1999, the Russian space agency announced that 51-year-old British billionaire Peter Llewellyn would be sent to the aging Mir space station in return for a payment of $100 million by Llewellyn. [21] Llewellyn, however, denied agreeing to pay that sum, his refusal to pay which prompted his flight's cancellation a month later. [22] Sub-orbital space tourism[ edit ] Successful projects[ edit ] Scaled Composites won the $10 million X Prize in October 2004 with SpaceShipOne , as the first private company to reach and surpass an altitude of 100 km (62 mi) twice within two weeks. The altitude is beyond the Kármán Line , the arbitrarily-defined boundary of space. [23] The first flight was flown by Michael Melvill in June 2004, to a height of 100 km (62 mi), making him the first commercial astronaut. [24] The prize-winning flight was flown by Brian Binnie , which reached a height of 112.0 km (69.6 mi), breaking the X-15 record. [25] There were no space tourists on the flights even though the vehicle has seats for three passengers. Instead there was additional weight to make up for the weight of passengers. [26] In 2005, Virgin Galactic was founded as a joint venture between Scaled Composites and Richard Branson's Virgin Group. [27] Eventually Virgin Group owned the entire project. [28] Virgin Galactic began building SpaceShipTwo -class spaceplanes. The first of these spaceplanes , VSS Enterprise , was intended to commence its first commercial flights in 2015, and tickets were on sale at a price of $200,000 (later raised to $250,000). However, the company suffered a considerable setback when the Enterprise broke up over the Mojave Desert during a test flight in October 2014. Over 700 tickets had been sold prior to the accident. [29] A second spaceplane, VSS Unity , completed a successful test flight with four passengers on July 11, 2021 to an altitude of nearly 90 km (56 mi). [30] Galactic 01 became the company's first commercial spaceflight on June 29, 2023. [31] Blue Origin developed the New Shepard reusable suborbital launch system specifically to enable short-duration space tourism. Blue Origin plans to ferry a maximum of six persons on a brief journey to space on board the New Shepard. The capsule is attached to the top portion of an 18-meter (59-foot) rocket. The rocket successfully launched with four passengers on July 20, 2021, and reached an altitude of 107 km (66 mi). [32] Canceled projects[ edit ] Armadillo Aerospace was developing a two-seat vertical takeoff and landing ( VTOL ) rocket called Hyperion, which will be marketed by Space Adventures. [33] Hyperion uses a capsule similar in shape to the Gemini capsule. The vehicle will use a parachute for descent but will probably use retrorockets for final touchdown, according to remarks made by Armadillo Aerospace at the Next Generation Suborbital Researchers Conference in February 2012. The assets of Armadillo Aerospace were sold to Exos Aerospace and while SARGE is continuing to be developed, it is unclear whether Hyperion is still being developed. XCOR Aerospace was developing a suborbital vehicle called Lynx until development was halted in May 2016. [34] The Lynx would take off from a runway under rocket power. Unlike SpaceShipOne and SpaceShipTwo, Lynx would not require a mothership. Lynx was designed for rapid turnaround, which would enable it to fly up to four times per day. Because of this rapid flight rate, Lynx had fewer seats than SpaceShipTwo, carrying only one pilot and one spaceflight participant on each flight. XCOR expected to roll out the first Lynx prototype and begin flight tests in 2015, but as of late 2017, XCOR was unable to complete their prototype development and filed for bankruptcy. [35] Citizens in Space, formerly the Teacher in Space Project , is a project of the United States Rocket Academy . Citizens in Space combines citizen science with citizen space exploration. The goal is to fly citizen-science experiments and citizen explorers (who travel free) who will act as payload operators on suborbital space missions. By 2012, Citizens in Space had acquired a contract for 10 suborbital flights with XCOR Aerospace and expected to acquire additional flights from XCOR and other suborbital spaceflight providers in the future. In 2012, Citizens in Space reported they had begun training three citizen astronaut candidates and would select seven additional candidates over the next 12 to 14 months. [36] [ needs update ] Space Expedition Corporation was preparing to use the Lynx for " Space Expedition Curaçao ", a commercial flight from Hato Airport on Curaçao , and planned to start commercial flights in 2014. The costs were $95,000 each. [37] [38] Axe Apollo Space Academy promotion by Unilever which planned to provide 23 people suborbital spaceflights on board the Lynx. EADS Astrium , a subsidiary of European aerospace giant EADS , announced its space tourism project in June 2007. [39] Orbital space tourism[ edit ] See also: Orbital spaceflight As of 2021, Space Adventures and SpaceX are the only companies to have coordinated tourism flights to Earth's orbit. Virginia-based Space Adventures has worked with Russia to use its Soyuz spacecraft to fly ultra-wealthy individuals to the International Space Station. The tourists included entrepreneur and space investor Anousheh Ansari and Cirque du Soleil co-founder Guy Laliberté . Those missions were priced at around $20 million each. The space industry could soon be headed for a tourism revolution if SpaceX and Boeing make good on their plans to take tourists to orbit. [40]
Toggle the table of contents Space debris From Wikipedia, the free encyclopedia Pollution around Earth by defunct artificial objects "Space Junk" redirects here. For other uses, see Space Junk (disambiguation) . "Cosmic debris" redirects here. For the Frank Zappa song, see Cosmik Debris . Infographic showing the space debris situation in different kinds of orbits around Earth Part of a series on e Space debris (also known as space junk, space pollution, [1] space waste, space trash, space garbage, or cosmic debris [2] ) are defunct human-made objects in space – principally in Earth orbit – which no longer serve a useful function. These include derelict spacecraft (nonfunctional spacecraft and abandoned launch vehicle stages), mission-related debris, and particularly-numerous in-Earth orbit fragmentation debris from the breakup of derelict rocket bodies and spacecraft. In addition to derelict human-made objects left in orbit, space debris includes fragments from disintegration, erosion , or collisions ; solidified liquids expelled from spacecraft; unburned particles from solid rocket motors; and even paint flecks. Space debris represents a risk to spacecraft. [3] Space debris is typically a negative externality . It creates an external cost on others from the initial action to launch or use a spacecraft in near-Earth orbit, a cost that is typically not taken into account nor fully accounted for [4] [5] by the launcher or payload owner. [6] [1] [7] Several spacecraft, both crewed and un-crewed, have been damaged or destroyed by space debris. The measurement, mitigation, and potential removal of debris is conducted by some participants in the space industry . [8] As of November 2022 [update] , the US Space Surveillance Network reported 25,857 artificial objects in orbit above the Earth, [9] including 5,465 operational satellites. [10] However, these are just the objects large enough to be tracked and in an orbit that makes tracking possible. Satellite debris that is in a Molniya orbit , such as the Kosmos Oko series, might be too high above the Northern Hemisphere to be tracked. [11] As of January 2019 [update] , more than 128 million pieces of debris smaller than 1 cm (0.4 in), about 900,000 pieces of debris 1–10 cm, and around 34,000 of pieces larger than 10 cm (3.9 in) were estimated to be in orbit around the Earth. [8] When the smallest objects of artificial space debris (paint flecks, solid rocket exhaust particles, etc.) are grouped with micrometeoroids , they are together sometimes referred to by space agencies as MMOD (Micrometeoroid and Orbital Debris). Collisions with debris have become a hazard to spacecraft. The smallest objects cause damage akin to sandblasting , especially to solar panels and optics like telescopes or star trackers that cannot easily be protected by a ballistic shield . [12] Below 2,000 km (1,200 mi), pieces of debris are denser than meteoroids . Most are dust from solid rocket motors, surface erosion debris like paint flakes, and frozen coolant from Soviet nuclear-powered satellites . [13] [14] [15] For comparison, the International Space Station orbits in the 300–400 kilometres (190–250 mi) range, while the two most recent large debris events, the 2007 Chinese antisatellite weapon test and the 2009 satellite collision , occurred at 800 to 900 kilometres (500 to 560 mi) altitude. [16] The ISS has Whipple shielding to resist damage from small MMOD. However, known debris with a collision chance over 1/10,000 are avoided by maneuvering the station. History[ edit ] Space debris began to accumulate in Earth orbit with the launch of the first artificial satellite , Sputnik 1 , into orbit in October, 1957. But even before this event, humans might have produced ejecta that became space debris, as in the August 1957 Pascal B test . [17] [18] Going back further, natural ejecta from Earth has entered orbit. After the launch of Sputnik, the North American Aerospace Defense Command (NORAD) began compiling a database (the Space Object Catalog ) of all known rocket launches and objects reaching orbit, including satellites, protective shields and upper-stages of launch vehicles . NASA later published modified versions of the database in two-line element sets , [19] and beginning in the early 1980s, they were republished in the CelesTrak bulletin board system . [20] Gabbard diagram of almost 300 pieces of debris from the disintegration of the five-month-old third stage of the Chinese Long March 4 booster on 11 March 2000 NORAD trackers who fed the database were aware of other objects in orbit, many of which were the result of in-orbit explosions. [21] Some were deliberately caused during anti-satellite weapon (ASAT) testing in the 1960s, and others were the result of rocket stages blowing up in orbit as leftover propellant expanded and ruptured their tanks. More detailed databases and tracking systems were gradually developed, including Gabbard diagrams, to improve the modeling of orbital evolution and decay. [22] [23] When the NORAD database became publicly available during the 1970s,[ clarification needed ] techniques developed for the asteroid-belt were applied to the study[ by whom? ] of known artificial satellite objects.[ citation needed ] Baker-Nunn cameras were widely used to study space debris. Time and natural gravitational/atmospheric effects help to clear space debris. A variety of technological approaches have also been proposed, though most have not been implemented. A number of scholars have observed that systemic factors, political, legal, economic, and cultural, are the greatest impediment to the cleanup of near-Earth space. There has been little commercial incentive to reduce space debris since the associated cost does not accrue to the entity producing it. Rather, the cost falls to all users of the space environment who benefit from space technology and knowledge. A number of suggestions for increasing incentives to reduce space debris have been made. These encouraging companies to see the economic benefit of reducing debris more aggressively than existing government mandates require. [24] In 1979, NASA founded the Orbital Debris Program to research mitigation measures for space debris in Earth orbit. [25] [26] Debris growth[ edit ] During the 1980s, NASA and other U.S. groups attempted to limit the growth of debris. One trial solution was implemented by McDonnell Douglas in 1981 for the Delta launch vehicle by having the booster move away from its payload and vent any propellant remaining in its tanks. [27] This eliminated one source for pressure buildup in the tanks which had previously caused them to explode and create additional orbital debris. [28] Other countries were slower to adopt this measure and, due especially to a number of launches by the Soviet Union , the problem grew throughout the decade. [29] A new battery of studies followed as NASA, NORAD, and others attempted to better understand the orbital environment, with each adjusting the number of pieces of debris in the critical-mass zone upward. Although in 1981 (when Schefter's article was published) the number of objects was estimated at 5,000, [21] new detectors in the Ground-based Electro-Optical Deep Space Surveillance system found new objects. By the late 1990s, it was thought that most of the 28,000 launched objects had already decayed and about 8,500 remained in orbit. [30] By 2005 this was adjusted upward to 13,000 objects, [31] and a 2006 study increased the number to 19,000 as a result of an ASAT and a satellite collision. [32] In 2011, NASA said that 22,000 objects were being tracked. [33] A 2006 NASA model suggested that if no new launches took place the environment would retain the then-known population until about 2055, when it would increase on its own. [34] [35] Richard Crowther of Britain's Defence Evaluation and Research Agency said in 2002 that he believed the cascade would begin about 2015. [36] The National Academy of Sciences, summarizing the professional view, noted widespread agreement that two bands of LEO space – 900 to 1,000 km (620 mi) and 1,500 km (930 mi) – were already past critical density. [37] In the 2009 CEAS European Air and Space Conference, University of Southampton researcher Hugh Lewis predicted that the threat from space debris would rise 50 percent in the next decade and quadruple in the next 50 years. As of 2009 [update] , more than 13,000 close calls were tracked weekly. [38] A 2011 report by the U.S. National Research Council warned NASA that the amount of orbiting space debris was at a critical level. According to some computer models, the amount of space debris "has reached a tipping point, with enough currently in orbit to continually collide and create even more debris, raising the risk of spacecraft failures." The report called for international regulations limiting debris and research of disposal methods. [39] Objects in Earth orbit including fragmentation debris. November 2020 NASA:ODPO Debris history in particular years[ edit ] By mid-1994 there had been 68 breakups or debris "anomalous events" involving satellites launched by the former Soviet Union/Russia and 18 similar events had been discovered involving rocket bodies and other propulsion-related operational debris. [40] As of 2009 [update] , 19,000 debris over 5 cm (2 in) were tracked by United States Space Surveillance Network . [16] As of July 2013 [update] , estimates of more than 170 million debris smaller than 1 cm (0.4 in), about 670,000 debris 1–10 cm, and approximately 29,000 larger pieces of debris are in orbit. [41] As of July 2016 [update] , nearly 18,000 artificial objects are orbiting above Earth, [42] including 1,419 operational satellites. [43] As of October 2019 [update] , nearly 20,000 artificial objects in orbit above the Earth, [9] including 2,218 operational satellites. [10] Characterization[ edit ] Size and numbers[ edit ] As of January 2019 [update] there were estimated to be over 128 million pieces of debris smaller than 1 cm (0.39 in), and approximately 900,000 pieces between 1 and 10 cm. The count of large debris (defined as 10 cm across or larger [44] ) was 34,000 in 2019, [8] and at least 37,000 by June 2023. [45] The technical measurement cut-off[ clarification needed ] is c. 3 mm (0.12 in). [46] As of 2020 [update] , there were 8,000 metric tons of debris in orbit, a figure that is expected to increase. [47] Low Earth orbit[ edit ] Debris density in low Earth orbit In the orbits nearest to Earth – less than 2,000 km (1,200 mi) orbital altitude , referred to as low-Earth orbit (LEO) – there have traditionally been few "universal orbits" that keep a number of spacecraft in particular rings (in contrast to GEO , a single orbit that is widely used by over 500 satellites ). This is beginning to change in 2019, and several companies have begun to deploy the early phases of satellite internet constellations , which will have many universal orbits in LEO with 30 to 50 satellites per orbital plane and altitude. Traditionally, the most populated LEO orbits have been a number of Sun-synchronous satellites that keep a constant angle between the Sun and the orbital plane , making Earth observation easier with consistent sun angle and lighting. Sun-synchronous orbits are polar , meaning they cross over the polar regions. LEO satellites orbit in many planes, typically up to 15 times a day, causing frequent approaches between objects. The density of satellites – both active and derelict – is much higher in LEO. [48] Orbits are affected by gravitational perturbations (which in LEO include unevenness of the Earth's gravitational field due to variations in the density of the planet), and collisions can occur from any direction. The average impact speed of collisions in Low Earth Orbit is 10 km/s with maximums reaching above 14 km/s due to orbital eccentricity . [49] The 2009 satellite collision occurred at a closing speed of 11.7 km/s (26,000 mph), [50] creating over 2,000 large debris fragments. [51] These debris cross many other orbits and increase debris collision risk. It is theorized that a sufficiently large collision of spacecraft could potentially lead to a cascade effect, or even make some particular low Earth orbits effectively unusable for long term use by orbiting satellites, a phenomenon known as the Kessler syndrome . [52] The theoretical effect is projected to be a theoretical runaway chain reaction of collisions that could occur, exponentially increasing the number and density of space debris in low-Earth orbit, and has been hypothesized to ensue beyond some critical density. [53] Crewed space missions are mostly at 400 km (250 mi) altitude and below, where air drag helps clear zones of fragments. The upper atmosphere is not a fixed density at any particular orbital altitude; it varies as a result of atmospheric tides and expands or contracts over longer time periods as a result of space weather . [54] These longer-term effects can increase drag at lower altitudes; the 1990s expansion was a factor in reduced debris density. [55] Another factor was fewer launches by Russia; the Soviet Union made most of their launches in the 1970s and 1980s. [56] : 7 Higher altitudes[ edit ] At higher altitudes, where air drag is less significant, orbital decay takes longer. Slight atmospheric drag , lunar perturbations , Earth's gravity perturbations, solar wind , and solar radiation pressure can gradually bring debris down to lower altitudes (where it decays), but at very high altitudes this may take centuries. [57] Although high-altitude orbits are less commonly used than LEO and the onset of the problem is slower, the numbers progress toward the critical threshold more quickly.[ contradictory ][ page needed ] [58] Many communications satellites are in geostationary orbits (GEO), clustering over specific targets and sharing the same orbital path. Although velocities are low between GEO objects, when a satellite becomes derelict (such as Telstar 401 ) it assumes a geosynchronous orbit; its orbital inclination increases about .8° and its speed increases about 160 km/h (99 mph) per year. Impact velocity peaks at about 1.5 km/s (0.93 mi/s). Orbital perturbations cause longitude drift of the inoperable spacecraft and precession of the orbital plane. Close approaches (within 50 meters) are estimated at one per year. [59] The collision debris pose less short-term risk than from an LEO collision, but the satellite would likely become inoperable. Large objects, such as solar-power satellites , are especially vulnerable to collisions. [60] Although the ITU now requires proof a satellite can be moved out of its orbital slot at the end of its lifespan, studies suggest this is insufficient. [61] Since GEO orbit is too distant to accurately measure objects under 1 m (3 ft 3 in), the nature of the problem is not well known. [62] Satellites could be moved to empty spots in GEO, requiring less maneuvering and making it easier to predict future motion. [63] Satellites or boosters in other orbits, especially stranded in geostationary transfer orbit , are an additional concern due to their typically high crossing velocity. Despite efforts to reduce risk, spacecraft collisions have occurred. The European Space Agency telecom satellite Olympus-1 was struck by a meteoroid on 11 August 1993 and eventually moved to a graveyard orbit . [64] On 29 March 2006, the Russian Express-AM11 communications satellite was struck by an unknown object and rendered inoperable; [65] its engineers had enough contact time with the satellite to send it into a graveyard orbit. Main category: Spacecraft that broke apart in space Vanguard 1 is expected to remain in orbit for 240 years. [66] [67] In 1958, the United States of America had launched Vanguard I into a medium Earth orbit (MEO). As of October 2009 [update] , it, the upper stage of Vanguard 1's launch rocket and associated piece of debris, are the oldest surviving artificial space objects still in orbit and are expected to be until after the year 2250. [68] [69] As of May 2022 [update] , the Union of Concerned Scientists listed 5,465 operational satellites from a known population of 27,000 pieces of orbital debris tracked by NORAD. [70] [71] Occasionally satellites are left in orbit when they're no longer useful, many countries require that satellites go through passivation at the end of their life. The satellites are then either boosted into a higher, "graveyard" orbit or a lower, short-term orbit. Nonetheless, satellites that have been properly moved to a higher orbit have an eight-percent probability of puncture and coolant release over a 50-year period. The coolant freezes into droplets of solid sodium-potassium alloy, creating more debris. [13] [72] Despite the use of passivization, or prior to its standardization, many satellites and rocket bodies have exploded or broken apart on orbit. In February 2015, for example, the USAF Defense Meteorological Satellite Program Flight 13 (DMSP-F13) exploded on orbit, creating at least 149 debris objects, which were expected to remain in orbit for decades. [73] Later that same year, NOAA-16 which had been decommissioned after an anomaly in June 2014, broke apart on orbit into at least 275 pieces. [74] For older programs, such as the Soviet-era Meteor 2 and Kosmos satellites, design flaws resulted in numerous break-ups – at least 68 by 1994 – following decommissioning, resulting in more debris. [40] In addition to the accidental creation of debris, some has been made intentionally through the deliberate destruction of satellites. This has been done as a test of anti-satellite or anti-ballistic missile technology, or to prevent a sensitive satellite from being examined by a foreign power. [40] The United States has conducted over 30 anti-satellite weapons tests (ASATs), the Soviet Union / Russia has performed at least 27, China has performed 10 and India has performed at least one. [75] [76] The most recent ASATs were the Chinese interception of FY-1C , Russian trials of its PL-19 Nudol , the American interception of USA-193 and India's interception of an unstated live satellite . [76] A drifting thermal blanket photographed in 1998 during STS-88 . Space debris includes a glove lost by astronaut Ed White on the first American space-walk (EVA), a camera lost by Michael Collins near Gemini 10 , a thermal blanket lost during STS-88, garbage bags jettisoned by Soviet cosmonauts during Mir 's 15-year life, [77] a wrench, and a toothbrush. [78] Sunita Williams of STS-116 lost a camera during an EVA. During an STS-120 EVA to reinforce a torn solar panel, a pair of pliers was lost, and in an STS-126 EVA, Heidemarie Stefanyshyn-Piper lost a briefcase-sized tool bag. [79] Main category: Spacecraft that broke apart in space Spent upper stage of a Delta II rocket, photographed by the XSS 10 satellite A significant portion of debris is due to rocket upper stages (e.g. the Inertial Upper Stage ) breaking up due to decomposition of unvented fuel. [80] The first such instance involved the launch of the Transit-4a satellite in 1961. Two hours after insertion the Ablestar upper stage exploded. But even boosters that don't break apart can be a problem. A major known impact event involved an (intact) Ariane booster. [56] : 2 Although NASA and the United States Air Force now require upper-stage passivation, other launchers – such as the Chinese and Russian space agencies – do not. Lower stages, like the Space Shuttle's solid rocket boosters or the Apollo program 's Saturn IB launch vehicles, do not reach orbit. [81] Examples: Two Japanese H-2A rockets rockets broke up in 2006. [82] A Russian Briz-M booster stage exploded in orbit over South Australia on 19 February 2007. Launched on 28 February 2006 carrying an Arabsat-4A communications satellite , it malfunctioned before it could use up its propellant. Although the explosion was captured on film by astronomers, due to the orbit path the debris cloud has been difficult to measure with radar. By 21 February 2007, over 1,000 fragments were identified. [83] [84] A 14 February 2007 breakup was recorded by Celestrak. [85] Another Briz-M broke up on 16 October 2012 after a failed 6 August Proton-M launch. The amount and size of the debris was unknown. [86] The second stage of the Zenit-2 , called the SL-16 by western governments, along with the second stages of the Vostok and Kosmos launch vehicles, make up about 20% of the total mass of launch debris in Low Earth Orbit (LEO). [87] An analysis that determined the 50 "statistically most concerning" debris objects in low Earth orbit determined that the top 20 were all Zenit-2 upper stages. [88] a Delta II rocket used to launch NASA's 1989 COBE spacecraft exploded on December 3, 2006. This occurred even though its residual fuel had already been vented to space. [82] In 2018–2019, three different Atlas V Centaur second stages broke up. [89] [90] [91] In December 2020, scientists confirmed that a previously detected near-Earth object, 2020 SO, was rocket booster space junk launched in 1966 orbiting Earth and the Sun. [92] At least eight Delta rockets have contributed orbital debris in the Sun-synchronous low Earth orbit environment. The variant of the Delta upper stage that was used in the 1970s was found to be prone to in-orbit explosions. Starting in 1981, depletion burns – to get rid of excess propellant – became standard and no Delta Rocket Bodies launched after 1981 experience severe fragmentations afterward, but some of those launched prior to 1981 continued to explode. In 1991, the Delta 1975-052B fragmented, 16 years after launch, demonstrating the resilience of the propellent. [93] Main category: Intentionally destroyed artificial satellites A former source of debris was anti-satellite weapons (ASATs) testing by the U.S. and Soviet Union during the 1960s and 1970s. North American Aerospace Defense Command (NORAD) only collected data for Soviet tests, and debris from U.S. tests were identified subsequently. [94] By the time the debris problem was understood, widespread ASAT testing had ended. The U.S. Program 437 was shut down in 1975. [95] The U.S. restarted their ASAT programs in the 1980s with the Vought ASM-135 ASAT . A 1985 test destroyed a 1-tonne (2,200 lb) satellite orbiting at 525 km (326 mi), creating thousands of debris larger than 1 cm (0.39 in). At this altitude, atmospheric drag decayed the orbit of most debris within a decade. A de facto moratorium followed the test. [96] Known orbit planes of Fengyun -1C debris one month after the weather satellite's disintegration by the Chinese ASAT China's government was condemned for the military implications and the amount of debris from the 2007 anti-satellite missile test, [97] the largest single space debris incident in history (creating over 2,300 pieces golf-ball size or larger, over 35,000 1 cm (0.4 in) or larger, and one million pieces 1 mm (0.04 in) or larger). The target satellite orbited between 850 km (530 mi) and 882 km (548 mi), the portion of near-Earth space most densely populated with satellites. [98] Since atmospheric drag is low at that altitude, the debris is slow to return to Earth, and in June 2007 NASA's Terra environmental spacecraft maneuvered to avoid impact from the debris. [99] Brian Weeden, U.S. Air Force officer and Secure World Foundation staff member, noted that the 2007 Chinese satellite explosion created an orbital debris of more than 3,000 separate objects that then required tracking. [100] On 20 February 2008, the U.S. launched an SM-3 missile from the USS Lake Erie to destroy a defective U.S. spy satellite thought to be carrying 450 kg (1,000 lb) of toxic hydrazine propellant. The event occurred at about 250 km (155 mi), and the resulting debris has a perigee of 250 km (155 mi) or lower. [101] The missile was aimed to minimize the amount of debris, which (according to Pentagon Strategic Command chief Kevin Chilton) had decayed by early 2009. [102] On 27 March 2019, Indian Prime Minister Narendra Modi announced that India shot down one of its own LEO satellites with a ground-based missile. He stated that the operation, part of Mission Shakti, would defend the country's interests in space. Afterwards, US Air Force Space Command announced they were tracking 270 new pieces of debris but expected the number to grow as data collection continues. [103] On 15 November 2021, the Russian Defense Ministry destroyed Kosmos 1408 [104] orbiting at around 450 km, creating "more than 1,500 pieces of trackable debris and hundreds of thousands of pieces of un-trackable debris" according to the US State Department. [105] The vulnerability of satellites to debris and the possibility of attacking LEO satellites to create debris clouds has triggered speculation that it is possible for countries unable to make a precision attack.[ clarification needed ] An attack on a satellite of 10 t (22,000 lb) or more would heavily damage the LEO environment. [96] Hazards[ edit ] A micrometeoroid left this crater on the surface of Space Shuttle Challenger 's front window on STS-7 . To spacecraft[ edit ] Space junk can be a hazard to active satellites and spacecraft. It has been suggested that Earth orbit could even become impassable if the risk of collision becomes too great. [106] [ failed verification ] However, since the risk to spacecraft increases with exposure to high debris densities, it is more accurate to say that LEO would be rendered unusable by orbiting craft. The threat to craft passing through LEO to reach a higher orbit would be much lower owing to the very short time span of the crossing. Uncrewed spacecraft[ edit ] Although spacecraft are typically protected by Whipple shields , solar panels, which are exposed to the Sun, wear from low-mass impacts. Even small impacts can produce a cloud of plasma which is an electrical risk to the panels. [107] Satellites are believed to have been destroyed by micrometeorites and (small) orbital debris (MMOD). The earliest suspected loss was of Kosmos 1275, which disappeared on 24 July 1981 (a month after launch). Kosmos contained no volatile fuel, therefore, there appeared to be nothing internal to the satellite which could have caused the destructive explosion which took place. However, the case has not been proven and another hypothesis forwarded is that the battery exploded. Tracking showed it broke up, into 300 new objects. [108] Many impacts have been confirmed since. For example, on 24 July 1996, the French microsatellite Cerise was hit by fragments of an Ariane-1 H-10 upper-stage booster which exploded in November 1986. [56] : 2 On 29 March 2006, the Russian Ekspress-AM11 communications satellite was struck by an unknown object and rendered inoperable. [65] On 13 October 2009, Terra suffered a single battery cell failure anomaly and a battery heater control anomaly which were subsequently considered likely the result of an MMOD strike. [109] On 12 March 2010, Aura lost power from one-half of one of its 11 solar panels and this was also attributed to an MMOD strike. [110] On 22 May 2013, GOES 13 was hit by an MMOD which caused it to lose track of the stars that it used to maintain an operational attitude. It took nearly a month for the spacecraft to return to operation. [111] The first major satellite collision occurred on 10 February 2009. The 950 kg (2,090 lb) derelict satellite Kosmos 2251 and the operational 560 kg (1,230 lb) Iridium 33 collided, 500 mi (800 km) [112] over northern Siberia. The relative speed of impact was about 11.7 km/s (7.3 mi/s), or about 42,120 km/h (26,170 mph). [113] Both satellites were destroyed, creating thousands of pieces of new smaller debris, with legal and political liability issues unresolved even years later. [114] [115] [116] On 22 January 2013, BLITS (a Russian laser-ranging satellite) was struck by debris suspected to be from the 2007 Chinese anti-satellite missile test , changing both its orbit and rotation rate. [117] Satellites sometimes[ clarification needed ] perform Collision Avoidance Maneuvers and satellite operators may monitor space debris as part of maneuver planning. For example, in January 2017, the European Space Agency made the decision to alter orbit of one of its three [118] Swarm mission spacecraft, based on data from the US Joint Space Operations Center , to lower the risk of collision from Cosmos-375, a derelict Russian satellite. [119] Crewed spacecraft[ edit ] Crewed flights are particularly vulnerable to space debris conjunctions in the orbital path of the spacecraft. Occasional avoidance maneuvers or longer-term space debris wear have affected the space shuttle, the MIR space station, and the International Space Station. Space Shuttle missions[ edit ] Space Shuttle Endeavour had a major impact on its radiator during STS-118 . The entry hole is about 5.5 mm (0.22 in), and the exit hole is twice as large. From the early shuttle missions, NASA used NORAD space monitoring capabilities to assess the shuttle's orbital path for debris. In the 1980s, this consumed a large proportion of NORAD capacity. [28] The first collision-avoidance maneuver occurred during STS-48 , in September,1991, [120] a seven-second thruster burn to avoid debris from the derelict satellite Kosmos 955 . [121] Similar maneuvers were executed on missions 53, 72 and 82. [120] One of the earliest events to publicize the debris problem occurred on Space Shuttle Challenger 's second flight, STS-7. A fleck of paint struck its front window, creating a pit over 1 mm (0.04 in) wide. On STS-59 in 1994, Endeavour 's front window was pitted about half its depth. Minor debris impacts increased from 1998. [122] Window chipping and minor damage to thermal protection system tiles (TPS) were already common by the 1990s. The Shuttle was later flown tail-first to take a greater proportion of the debris load on the engines and rear cargo bay, which are not used in orbit or during descent, and thus are less critical for post-launch operation. When flying attached to the ISS , a shuttle was flipped around so the better-armoured station shielded the orbiter. [123] A NASA 2005 study concluded that debris accounted for approximately half of the overall risk to the Shuttle. [123] [124] Executive-level decision to proceed was required if the catastrophic impact was likelier than 1 in 200. On a normal (low-orbit) mission to the ISS, the risk was approximately 1 in 300, but the Hubble telescope repair mission was flown at the higher orbital altitude of 560 km (350 mi) where the risk was initially calculated at a 1-in-185 (due in part to the 2009 satellite collision). A re-analysis with better debris numbers reduced the estimated risk to 1 in 221, and the mission went ahead. [125] Debris incidents continued on later Shuttle missions. During STS-115 in 2006, a fragment of circuit board bored a small hole through the radiator panels in Atlantis's cargo bay. [126] On STS-118 in 2007, debris blew a bullet-like hole through Endeavour's radiator panel. [127] Mir[ edit ] Debris impacts on Mir's solar panels degraded their performance. The damage is most noticeable on the panel on the right, which is facing the camera with a high degree of contrast. Extensive damage to the smaller panel below is due to impact with a Progress spacecraft. Impact wear was notable on the Soviet space station Mir, since it remained in space for long periods with its original solar module panels. [128] [129] International Space Station[ edit ] The ISS also uses Whipple shielding to protect its interior from minor debris. [130] However, exterior portions (notably its solar panels ) cannot be protected easily. In 1989, the ISS panels were predicted to degrade approximately 0.23% in four years due to the "sandblasting" effect of impacts with small orbital debris. [131] An avoidance maneuver is typically performed for the ISS if "there is a greater than one-in-10,000 chance of a debris strike". [132] As of January 2014 [update] , there have been sixteen maneuvers in the fifteen years the ISS had been in orbit. [132] By 2019, over 1,400 meteoroid and orbital debris (MMOD) impacts had been recorded on the ISS. [133] As another method to reduce the risk to humans on board, ISS operational management asked the crew to shelter in the Soyuz on three occasions due to late debris-proximity warnings. In addition to the sixteen thruster firings and three Soyuz-capsule shelter orders, one attempted maneuver was not completed due to not having the several days' warning necessary to upload the maneuver timeline to the station's computer. [132] [134] [135] A March 2009 event involved debris believed to be a 10 cm (3.9 in) piece of the Kosmos 1275 satellite. [136] In 2013, the ISS operations management did not make a maneuver to avoid any debris, after making a record four debris maneuvers the previous year. [132] Main article: Kessler syndrome Growth of tracked objects in orbit and related events; [137] efforts to manage outer space global commons have so far not reduced the debris or the growth of objects in orbit The Kessler syndrome, [138] [139] proposed by NASA scientist Donald J. Kessler in 1978, is a theoretical scenario in which the density of objects in low Earth orbit (LEO) is high enough that collisions between objects could cause a cascade effect where each collision generates space debris that increases the likelihood of further collisions. [140] He further theorized that one implication, if this were to occur, is that the distribution of debris in orbit could render space activities and the use of satellites in specific orbital ranges economically impractical for many generations. [140] The growth in the number of objects as a result of the late-1990s studies sparked debate in the space community on the nature of the problem and the earlier dire warnings. According to Kessler's 1991 derivation and 2001 updates, [141] the LEO environment in the 1,000 km (620 mi) altitude range should be cascading. However, only one major satellite collision incident occurred: the 2009 satellite collision between Iridium 33 and Cosmos 2251. The lack of obvious short-term cascading has led to speculation that the original estimates overstated the problem. [142] According to Kessler in 2010 however, a cascade may not be obvious until it is well advanced, which might take years. [143] Main article: List of space debris fall incidents Saudi officials inspect a crashed PAM-D module in January 2001. Although most debris burns up in the atmosphere, larger debris objects can reach the ground intact. According to NASA, an average of one cataloged piece of debris has fallen back to Earth each day for the past 50 years. Despite their size, there has been no significant property damage from the debris. [144] Burning up in the atmosphere contributes to air pollution. [145] Numerous small cylindrical tanks from space objects have been found, designed to hold fuel or gasses. [146] Tracking and measurement[ edit ] See also: Satellite tracking Tracking from the ground[ edit ] Radar and optical detectors such as lidar are the main tools for tracking space debris. Although objects under 10 cm (4 in) have reduced orbital stability, debris as small as 1 cm can be tracked, [147] [148] however determining orbits to allow re-acquisition is difficult. Most debris remain unobserved. The NASA Orbital Debris Observatory tracked space debris with a 3 m (10 ft) liquid mirror transit telescope . [149] FM Radio waves can detect debris, after reflecting off them onto a receiver. [150] Optical tracking may be a useful early-warning system on spacecraft. [151] The U.S. Strategic Command keeps a catalog of known orbital objects, using ground-based radar and telescopes, and a space-based telescope (originally to distinguish from hostile missiles). The 2009 edition listed about 19,000 objects. [152] Other data come from the ESA Space Debris Telescope , TIRA , [153] the Goldstone , Haystack , [154] and EISCAT radars and the Cobra Dane phased array radar, [155] to be used in debris-environment models like the ESA Meteoroid and Space Debris Terrestrial Environment Reference (MASTER). Measurement in space[ edit ] The Long Duration Exposure Facility (LDEF) is an important source of information on small-particle space debris. Returned space hardware is a valuable source of information on the directional distribution and composition of the (sub-millimetre) debris flux. The LDEF satellite deployed by mission STS-41-C Challenger and retrieved by STS-32 Columbia spent 68 months in orbit to gather debris data. The EURECA satellite, deployed by STS-46 Atlantis in 1992 and retrieved by STS-57 Endeavour in 1993, was also used for debris study. [156] The solar arrays of Hubble were returned by missions STS-61 Endeavour and STS-109 Columbia, and the impact craters studied by the ESA to validate its models. Materials returned from Mir were also studied, notably the Mir Environmental Effects Payload (which also tested materials intended for the ISS [157] ). [158] [159] Gabbard diagrams[ edit ] A debris cloud resulting from a single event is studied with scatter plots known as Gabbard diagrams, where the perigee and apogee of fragments are plotted with respect to their orbital period . Gabbard diagrams of the early debris cloud prior to the effects of perturbations, if the data were available, are reconstructed. They often include data on newly observed, as yet uncatalogued fragments. Gabbard diagrams can provide important insights into the features of the fragmentation, the direction and point of impact. [23] [160] Dealing with debris[ edit ] An average of about one tracked object per day has been dropping out of orbit for the past 50 years, [161] averaging almost three objects per day at solar maximum (due to the heating and expansion of the Earth's atmosphere), but one about every three days at solar minimum , usually five and a half years later. [161] In addition to natural atmospheric effects, corporations, academics and government agencies have proposed plans and technology to deal with space debris, but as of November 2014 [update] , most of these are theoretical, and there is no extant business plan for debris reduction. [24] A number of scholars have also observed that institutional factors – political, legal, economic, and cultural "rules of the game" – are the greatest impediment to the cleanup of near-Earth space. There is little commercial incentive to act, since costs are not assigned to polluters , though a number of technological solutions have been suggested. [24] However, effects to date are limited. In the US, governmental bodies have been accused of backsliding on previous commitments to limit debris growth, "let alone tackling the more complex issues of removing orbital debris." [162] The different methods for removal of space debris have been evaluated by the Space Generation Advisory Council , including French astrophysicist Fatoumata Kébé . [163] National and international regulation[ edit ] There is no international treaty minimizing space debris. However, the United Nations Committee on the Peaceful Uses of Outer Space (COPUOS) published voluntary guidelines in 2007, [164] using a variety of earlier national regulatory attempts at developing standards for debris mitigation. As of 2008, the committee was discussing international "rules of the road" to prevent collisions between satellites. [165] By 2013, a number of national legal regimes existed, [166] [167] [168] typically instantiated in the launch licenses that are required for a launch in all spacefaring nations . [169] The U.S. issued a set of standard practices for civilian (NASA) and military ( DoD and USAF) orbital-debris mitigation in 2001. [170] [171] [167] The standard envisioned disposal for final mission orbits in one of three ways: 1) atmospheric reentry where even with "conservative projections for solar activity, atmospheric drag will limit the lifetime to no longer than 25 years after completion of mission;" 2) maneuver to a "storage orbit:" move the spacecraft to one of four very broad parking orbit ranges (2,000–19,700 km (1,200–12,200 mi), 20,700–35,300 km (12,900–21,900 mi), above 36,100 km (22,400 mi), or out of Earth orbit completely and into any heliocentric orbit ; 3) "Direct retrieval: Retrieve the structure and remove it from orbit as soon as practicable after completion of mission." [166] The standard articulated in option 1, which is the standard applicable to most satellites and derelict upper stages launched, has come to be known as the "25-year rule". [172] The US updated the Orbital Debris Mitigation Standard Practices (ODMSP) in December 2019, but made no change to the 25-year rule even though "[m]any in the space community believe that the timeframe should be less than 25 years." [173] There is no consensus however on what any new timeframe might be. [173] In 2002, the European Space Agency (ESA) worked with an international group to promulgate a similar set of standards, also with a "25-year rule" applying to most Earth-orbit satellites and upper stages. Space agencies in Europe began to develop technical guidelines in the mid-1990s, and ASI , UKSA , CNES , DLR and ESA signed a "European Code of Conduct" in 2006, [168] which was a predecessor standard to the ISO international standard work that would begin the following year. In 2008, ESA further developed "its own "Requirements on Space Debris Mitigation for Agency Projects" which "came into force on 1 April 2008." [168] Germany and France have posted bonds to safeguard the property from debris damage.[ clarification needed ] [174] The "direct retrieval" option (option no. 3 in the US "standard practices" above) has rarely been done by any spacefaring nation (exception, USAF X-37 ) or commercial actor since the earliest days of spaceflight due to the cost and complexity of achieving direct retrieval, but the ESA has scheduled a 2025 demonstration mission (Clearspace-1) to do this with a single small 100 kg (220 lb) derelict upper stage at a projected cost of €120 million not including the launch costs. [175] By 2006, the Indian Space Research Organization (ISRO) had developed a number of technical means of debris mitigation (upper stage passivation, propellant reserves for movement to graveyard orbits, etc.) for ISRO launch vehicles and satellites, and was actively contributing to inter-agency debris coordination and the efforts of the UN COPUOS committee. [176] In 2007, the ISO began preparing an international standard for space-debris mitigation. [177] By 2010, ISO had published "a comprehensive set of space system engineering standards aimed at mitigating space debris. [with primary requirements] defined in the top-level standard, ISO 24113." By 2017, the standards were nearly complete. However, these standards are not binding on any party by ISO or any international jurisdiction. They are simply available for use in any of a variety of voluntary ways. They "can be adopted voluntarily by a spacecraft manufacturer or operator, or brought into effect through a commercial contract between a customer and supplier, or used as the basis for establishing a set of national regulations on space debris mitigation." [172] The voluntary ISO standard also adopted the "25-year rule" for the "LEO protected region" below 2,000 km (1,200 mi) altitude that has been previously (and still is, as of 2019 [update] ) used by the US, ESA, and UN mitigation standards, and identifies it as "an upper limit for the amount of time that a space system shall remain in orbit after its mission is completed. Ideally, the time to deorbit should be as short as possible (i.e., much shorter than 25 years)". [172] Holger Krag of the European Space Agency states that as of 2017 there is no binding international regulatory framework with no progress occurring at the respective UN body in Vienna. [106]
Toggle the table of contents Space weather From Wikipedia, the free encyclopedia Branch of space physics and aeronomy Aurora australis observed from Space Shuttle Discovery , May 1991 Space weather is a branch of space physics and aeronomy , or heliophysics , concerned with the varying conditions within the Solar System and its heliosphere . This includes the effects of the solar wind , especially on the Earth's magnetosphere , ionosphere , thermosphere , and exosphere . [1] Though physically distinct, space weather is analogous to the terrestrial weather of Earth's atmosphere ( troposphere and stratosphere ). The term "space weather" was first used in the 1950s and popularized in the 1990s. [2] Later, it prompted research into " space climate ", the large-scale and long-term patterns of space weather. History[ edit ] For many centuries, the effects of space weather were noticed, but not understood. Displays of auroral light have long been observed at high latitudes. Beginnings[ edit ] In 1724, George Graham reported that the needle of a magnetic compass was regularly deflected from magnetic north over the course of each day. This effect was eventually attributed to overhead electric currents flowing in the ionosphere and magnetosphere by Balfour Stewart in 1882, and confirmed by Arthur Schuster in 1889 from analysis of magnetic observatory data. In 1852, astronomer and British Major General Edward Sabine showed that the probability of the occurrence of geomagnetic storms on Earth was correlated with the number of sunspots , demonstrating a novel solar-terrestrial interaction. The solar storm of 1859 caused brilliant auroral displays and disrupted global telegraph operations. Richard Carrington correctly connected the storm with a solar flare that he had observed the day before near a large sunspot group, demonstrating that specific solar events could affect the Earth. Kristian Birkeland explained the physics of aurorae by creating artificial ones in his laboratory, and predicted the solar wind. The introduction of radio revealed that solar weather could cause extreme static or noise. Radar jamming during a large solar event in 1942 led to the discovery of solar radio bursts, radio waves over a broad frequency range created by a solar flare. The 20th century[ edit ] In the 20th century, the interest in space weather expanded as military and commercial systems came to depend on systems affected by space weather. Communications satellites are a vital part of global commerce. Weather satellite systems provide information about terrestrial weather. The signals from satellites of a global positioning system (GPS) are used in a wide variety of applications. Space weather phenomena can interfere with or damage these satellites or interfere with the radio signals with which they operate. Space weather phenomena can cause damaging surges in long-distance transmission lines and expose passengers and crew of aircraft travel to radiation , [3] [4] especially on polar routes. The International Geophysical Year increased research into space weather. Ground-based data obtained during IGY demonstrated that the aurorae occurred in an auroral oval, a permanent region of luminescence 15 to 25° in latitude from the magnetic poles and 5 to 20° wide. [5] In 1958, the Explorer I satellite discovered the Van Allen belts , [6] regions of radiation particles trapped by the Earth's magnetic field. In January 1959, the Soviet satellite Luna 1 first directly observed the solar wind and measured its strength. A smaller International Heliophysical Year (IHY) occurred in 2007–2008. In 1969, INJUN-5 (or Explorer 40 [7] ) made the first direct observation of the electric field impressed on the Earth's high-latitude ionosphere by the solar wind. [8] In the early 1970s, Triad data demonstrated that permanent electric currents flowed between the auroral oval and the magnetosphere. [9] The term "space weather" came into usage in the late 1950s as the space age began and satellites began to measure the space environment . [2] The term regained popularity in the 1990s along with the belief that space's impact on human systems demanded a more coordinated research and application framework. [10] US National Space Weather Program[ edit ] The purpose of the US National Space Weather Program is to focus research on the needs of the affected commercial and military communities, to connect the research and user communities, to create coordination between operational data centers, and to better define user community needs. NOAA operates the National Weather Service's Space Weather Prediction Center . [11] The concept was turned into an action plan in 2000, [12] an implementation plan in 2002, an assessment in 2006 [13] and a revised strategic plan in 2010. [14] A revised action plan was scheduled to be released in 2011 followed by a revised implementation plan in 2012. Phenomena[ edit ] Within the Solar System , space weather is influenced by the solar wind and the interplanetary magnetic field carried by the solar wind plasma . A variety of physical phenomena is associated with space weather, including geomagnetic storms and substorms , energization of the Van Allen radiation belts , ionospheric disturbances and scintillation of satellite-to-ground radio signals and long-range radar signals, aurorae , and geomagnetically induced currents at Earth's surface. Coronal mass ejections are also important drivers of space weather, as they can compress the magnetosphere and trigger geomagnetic storms. Solar energetic particles (SEP) accelerated by coronal mass ejections or solar flares can trigger solar particle events , a critical driver of human impact space weather, as they can damage electronics onboard spacecraft (e.g. Galaxy 15 failure), and threaten the lives of astronauts , as well as increase radiation hazards to high-altitude, high-latitude aviation. GOES-11 and GOES-12 monitored space weather conditions during the October 2003 solar activity [15] Some spacecraft failures can be directly attributed to space weather; many more are thought to have a space weather component. For example, 46 of the 70 failures reported in 2003 occurred during the October 2003 geomagnetic storm. The two most common adverse space weather effects on spacecraft are radiation damage and spacecraft charging . Radiation (high-energy particles) passes through the skin of the spacecraft and into the electronic components. In most cases, the radiation causes an erroneous signal or changes one bit in memory of a spacecraft's electronics ( single event upsets ). In a few cases, the radiation destroys a section of the electronics ( single-event latchup ). Spacecraft charging is the accumulation of an electrostatic charge on a nonconducting material on the spacecraft's surface by low-energy particles. If enough charge is built up, a discharge (spark) occurs. This can cause an erroneous signal to be detected and acted on by the spacecraft computer. A recent study indicated that spacecraft charging is the predominant space weather effect on spacecraft in geosynchronous orbit . [16] Spacecraft orbit changes[ edit ] The orbits of spacecraft in low Earth orbit (LEO) decay to lower and lower altitudes due to the resistance from the friction between the spacecraft's surface (i.e. , drag) and the outer layer of the Earth's atmosphere (or the thermosphere and exosphere). Eventually, a LEO spacecraft falls out of orbit and towards the Earth's surface. Many spacecraft launched in the past few decades have the ability to fire a small rocket to manage their orbits. The rocket can increase altitude to extend lifetime, to direct the re-entry towards a particular (marine) site, or route the satellite to avoid collision with other spacecraft. Such maneuvers require precise information about the orbit. A geomagnetic storm can cause an orbit change over a few days that otherwise would occur over a year or more. The geomagnetic storm adds heat to the thermosphere, causing the thermosphere to expand and rise, increasing the drag on spacecraft. The 2009 satellite collision between the Iridium 33 and Cosmos 2251 demonstrated the importance of having precise knowledge of all objects in orbit. Iridium 33 had the capability to maneuver out of the path of Cosmos 2251 and could have evaded the crash, if a credible collision prediction had been available. Humans in space[ edit ] Main article: Effect of spaceflight on the human body The exposure of a human body to ionizing radiation has the same harmful effects whether the source of the radiation is a medical X-ray machine , a nuclear power plant , or radiation in space. The degree of the harmful effect depends on the length of exposure and the radiation's energy density . The ever-present radiation belts extend down to the altitude of crewed spacecraft such as the International Space Station (ISS) and the Space Shuttle , but the amount of exposure is within the acceptable lifetime exposure limit under normal conditions. During a major space weather event that includes an SEP burst, the flux can increase by orders of magnitude. Areas within ISS provide shielding that can keep the total dose within safe limits. [17] For the Space Shuttle , such an event would have required immediate mission termination. Spacecraft signals[ edit ] The ionosphere bends radio waves in the same manner that water in a pool bends visible light. When the medium through which such waves travel is disturbed, the light image or radio information is distorted and can become unrecognizable. The degree of distortion (scintillation) of a radio wave by the ionosphere depends on the signal frequency. Radio signals in the VHF band (30 to 300 MHz) can be distorted beyond recognition by a disturbed ionosphere. Radio signals in the UHF band (300 MHz to 3 GHz) transit a disturbed ionosphere, but a receiver may not be able to keep locked to the carrier frequency. GPS uses signals at 1575.42 MHz (L1) and 1227.6 MHz (L2) that can be distorted by a disturbed ionosphere. Space weather events that corrupt GPS signals can significantly impact society. For example, the Wide Area Augmentation System operated by the US Federal Aviation Administration (FAA) is used as a navigation tool for North American commercial aviation. It is disabled by every major space weather event. Outages can range from minutes to days. Major space weather events can push the disturbed polar ionosphere 10° to 30° of latitude toward the equator and can cause large ionospheric gradients (changes in density over distance of hundreds of km) at mid and low latitude. Both of these factors can distort GPS signals. Long-distance radio signals[ edit ] Radio waves in the HF band (3 to 30 MHz) (also known as the shortwave band) are reflected by the ionosphere. Since the ground also reflects HF waves, a signal can be transmitted around the curvature of the Earth beyond the line of sight. During the 20th century, HF communications was the only method for a ship or aircraft far from land or a base station to communicate. The advent of systems such as Iridium brought other methods of communications, but HF remains critical for vessels that do not carry the newer equipment and as a critical backup system for others. Space weather events can create irregularities in the ionosphere that scatter HF signals instead of reflecting them, preventing HF communications. At auroral and polar latitudes, small space weather events that occur frequently disrupt HF communications. At mid-latitudes, HF communications are disrupted by solar radio bursts, by X-rays from solar flares (which enhance and disturb the ionospheric D-layer) and by TEC enhancements and irregularities during major geomagnetic storms. Trans polar airline routes are particularly sensitive to space weather, in part because Federal Aviation Regulations require reliable communication over the entire flight. [18] Diverting such a flight is estimated to cost about $100,000. [19] All passengers in commercial aircraft flying above 26,000 feet (7,900 m) typically experience some exposure in this aviation radiation environment. Humans in commercial aviation[ edit ] The magnetosphere guides cosmic ray and solar energetic particles to polar latitudes, while high-energy charged particles enter the mesosphere, stratosphere, and troposphere. These energetic particles at the top of the atmosphere shatter atmospheric atoms and molecules, creating harmful lower-energy particles that penetrate deep into the atmosphere and create measurable radiation. All aircraft flying above 8 km (26,200 feet) altitude are exposed to these particles. The dose exposure is greater in polar regions than at midlatitude and equatorial regions. Many commercial aircraft fly over the polar region. When a space weather event causes radiation exposure to exceed the safe level set by aviation authorities, [20] the aircraft's flight path is diverted. Measurements of the radiation environment at commercial aircraft altitudes above 8 km (26,000 ft) have historically been done by instruments that record the data on board where the data are then processed later on the ground. However, a system of real-time radiation measurements on-board aircraft has been developed through the NASA Automated Radiation Measurements for Aerospace Safety (ARMAS) program. [21] ARMAS has flown hundreds of flights since 2013, mostly on research aircraft, and sent the data to the ground through Iridium satellite links. The eventual goal of these types of measurements is to data assimilate them into physics-based global radiation models, e.g., NASA's Nowcast of Atmospheric Ionizing Radiation System ( NAIRAS ), so as to provide the weather of the radiation environment rather than the climatology. Ground-induced electric fields[ edit ] Magnetic storm activity can induce geoelectric fields in the Earth's conducting lithosphere . [22] Corresponding voltage differentials can find their way into electric power grids through ground connections , driving uncontrolled electric currents that interfere with grid operation, damage transformers, trip protective relays, and sometimes cause blackouts. [23] This complicated chain of causes and effects was demonstrated during the magnetic storm of March 1989 , [24] which caused the complete collapse of the Hydro-Québec electric-power grid in Canada, temporarily leaving nine million people without electricity. The possible occurrence of an even more intense storm [25] led to operational standards intended to mitigate induction-hazard risks, while reinsurance companies commissioned revised risk assessments . [26] Geophysical exploration[ edit ] Air- and ship-borne magnetic surveys can be affected by rapid magnetic field variations during geomagnetic storms. Such storms cause data-interpretation problems because the space weather-related magnetic field changes are similar in magnitude to those of the subsurface crustal magnetic field in the survey area. Accurate geomagnetic storm warnings, including an assessment of storm magnitude and duration, allows for an economic use of survey equipment. Geophysics and hydrocarbon production[ edit ] For economic and other reasons, oil and gas production often involves horizontal drilling of well paths many kilometers from a single wellhead. Accuracy requirements are strict, due to target size – reservoirs may only be a few tens to hundreds of meters across – and safety, because of the proximity of other boreholes. The most accurate gyroscopic method is expensive, since it can stop drilling for hours. An alternative is to use a magnetic survey, which enables measurement while drilling (MWD) . Near real-time magnetic data can be used to correct drilling direction. [27] [28] Magnetic data and space weather forecasts can help to clarify unknown sources of drilling error. Terrestrial weather[ edit ] The amount of energy entering the troposphere and stratosphere from space weather phenomena is trivial compared to the solar insolation in the visible and infrared portions of the solar electromagnetic spectrum. Although some linkage between the 11-year sunspot cycle and the Earth's climate has been claimed., [29] this has never been verified. For example, the Maunder minimum , a 70-year period almost devoid of sunspots, has often been suggested to be correlated to a cooler climate, but these correlations have disappeared after deeper studies. The suggested link from changes in cosmic-ray flux causing changes in the amount of cloud formation [30] did not survive scientific tests. Another suggestion, that variations in the extreme ultraviolet (EUV) flux subtly influence existing drivers of the climate and tip the balance between El Niño / La Niña events [31] collapsed when new research showed this was not possible. As such, a linkage between space weather and the climate has not been demonstrated. In addition, a link has been suggested between high energy charged particles (such as SEPs and cosmic rays ) and cloud formation . This is because charged particles interact with the atmosphere to produce volatiles which then condense, creating cloud seeds . [32] This is a topic of ongoing research at CERN , where experiments test the effect of high-energy charged particles on atmosphere. [33] If proven, this may suggest a link between space weather (in the form of solar particle events ) and cloud formation. [34] Most recently, a statistical connection has been reported between the occurrence of heavy floods and the arrivals of high-speed solar wind streams (HSSs). The enhanced auroral energy deposition during HSSs is suggested as a mechanism for the generation of downward propagating atmospheric gravity waves (AGWs). As AGWs reach lower atmosphere , they may excite the conditional instability in the troposphere , thus leading to excessive rainfall. [35] Observation[ edit ] Observation of space weather is done both for scientific research and applications. Scientific observation has evolved with the state of knowledge, while application-related observation expanded with the ability to exploit such data. Ground-based[ edit ] Space weather is monitored at ground level by observing changes in the Earth's magnetic field over periods of seconds to days, by observing the surface of the Sun, and by observing radio noise created in the Sun's atmosphere. The Sunspot Number (SSN) is the number of sunspots on the Sun's photosphere in visible light on the side of the Sun visible to an Earth observer. The number and total area of sunspots are related to the brightness of the Sun in the EUV and X-ray portions of the solar spectrum and to solar activity such as solar flares and coronal mass ejections. The 10.7 cm radio flux (F10.7) is a measurement of RF emissions from the Sun and is roughly correlated with the solar EUV flux. Since this RF emission is easily obtained from the ground and EUV flux is not, this value has been measured and disseminated continuously since 1947. The world standard measurements are made by the Dominion Radio Astrophysical Observatory at Penticton, BC, Canada and reported once a day at local noon [36] in solar flux units (10−22W·m−2·Hz−1). F10.7 is archived by the National Geophysical Data Center. [37] Fundamental space weather monitoring data are provided by ground-based magnetometers and magnetic observatories. Magnetic storms were first discovered by ground-based measurement of occasional magnetic disturbance. Ground magnetometer data provide real-time situational awareness for postevent analysis. Magnetic observatories have been in continuous operations for decades to centuries, providing data to inform studies of long-term changes in space climatology. [38] [39] Disturbance storm time index (Dst index) is an estimate of the magnetic field change at the Earth's magnetic equator due to a ring of electric current at and just earthward of the geosynchronous orbit . [40] The index is based on data from four ground-based magnetic observatories between 21° and 33° magnetic latitude during a one-hour period. Stations closer to the magnetic equator are not used due to ionospheric effects. The Dst index is compiled and archived by the World Data Center for Geomagnetism, Kyoto. [41] Kp/ap index: 'a' is an index created from the geomagnetic disturbance at one midlatitude (40° to 50° latitude) geomagnetic observatory during a 3-hour period. 'K' is the quasilogarithmic counterpart of the 'a' index. Kp and ap are the average of K and an over 13 geomagnetic observatories to represent planetary-wide geomagnetic disturbances. The Kp/ap index [42] indicates both geomagnetic storms and substorms (auroral disturbance). Kp/ap data are available from 1932 onward. AE index is compiled from geomagnetic disturbances at 12 geomagnetic observatories in and near the auroral zones and is recorded at 1-minute intervals. [41] The public AE index is available with a lag of two to three days that limits its utility for space weather applications. The AE index indicates the intensity of geomagnetic substorms except during a major geomagnetic storm when the auroral zones expand equatorward from the observatories. Radio noise bursts are reported by the Radio Solar Telescope Network to the U.S. Air Force and to NOAA. The radio bursts are associated with solar flare plasma that interacts with the ambient solar atmosphere. The Sun's photosphere is observed continuously [43] for activity that can be the precursors to solar flares and CMEs. The Global Oscillation Network Group (GONG) [44] project monitors both the surface and the interior of the Sun by using helioseismology , the study of sound waves propagating through the Sun and observed as ripples on the solar surface. GONG can detect sunspot groups on the far side of the Sun. This ability has recently been verified by visual observations from the STEREO spacecraft. Neutron monitors on the ground indirectly monitor cosmic rays from the Sun and galactic sources. When cosmic rays interact with the atmosphere, atomic interactions occur that cause a shower of lower-energy particles to descend into the atmosphere and to ground level. The presence of cosmic rays in the near-Earth space environment can be detected by monitoring high-energy neutrons at ground level. Small fluxes of cosmic rays are present continuously. Large fluxes are produced by the Sun during events related to energetic solar flares. Total Electron Content (TEC) is a measure of the ionosphere over a given location. TEC is the number of electrons in a column one meter square from the base of the ionosphere (around 90 km altitude) to the top of the ionosphere (around 1000 km altitude). Many TEC measurements are made by monitoring the two frequencies transmitted by GPS spacecraft. Presently, GPS TEC is monitored and distributed in real time from more than 360 stations maintained by agencies in many countries. Geoeffectiveness is a measure of how strongly space weather magnetic fields, such as coronal mass ejections, couple with the Earth's magnetic field. This is determined by the direction of the magnetic field held within the plasma that originates from the Sun. New techniques measuring Faraday rotation in radio waves are in development to measure field direction. [45] [46] Satellite-based[ edit ] A host of research spacecraft have explored space weather. [47] [48] [49] [50] The Orbiting Geophysical Observatory series were among the first spacecraft with the mission of analyzing the space environment. Recent spacecraft include the NASA-ESA Solar-Terrestrial Relations Observatory (STEREO) pair of spacecraft launched in 2006 into solar orbit and the Van Allen Probes , launched in 2012 into a highly elliptical Earth orbit. The two STEREO spacecraft drift away from the Earth by about 22° per year, one leading and the other trailing the Earth in its orbit. Together they compile information about the solar surface and atmosphere in three dimensions. The Van Allen probes record detailed information about the radiation belts, geomagnetic storms, and the relationship between the two. Some spacecraft with other primary missions have carried auxiliary instruments for solar observation. Among the earliest such spacecraft were the Applications Technology Satellite [51] (ATS) series at GEO that were precursors to the modern Geostationary Operational Environmental Satellite (GOES) weather satellite and many communication satellites. The ATS spacecraft carried environmental particle sensors as auxiliary payloads and had their navigational magnetic field sensor used for sensing the environment. Many of the early instruments were research spacecraft that were re-purposed for space weather applications. One of the first of these was the IMP-8 (Interplanetary Monitoring Platform). [52] It orbited the Earth at 35 Earth radii and observed the solar wind for two-thirds of its 12-day orbits from 1973 to 2006. Since the solar wind carries disturbances that affect the magnetosphere and ionosphere, IMP-8 demonstrated the utility of continuous solar wind monitoring. IMP-8 was followed by ISEE-3 , which was placed near the L1 Sun-Earth Lagrangian point , 235 Earth radii above the surface (about 1.5 million km, or 924,000 miles) and continuously monitored the solar wind from 1978 to 1982. The next spacecraft to monitor the solar wind at the L1 point was WIND from 1994 to 1998. After April 1998, the WIND spacecraft orbit was changed to circle the Earth and occasionally pass the L1 point. The NASA Advanced Composition Explorer has monitored the solar wind at the L1 point from 1997 to present. In addition to monitoring the solar wind, monitoring the Sun is important to space weather. Because the solar EUV cannot be monitored from the ground, the joint NASA - ESA Solar and Heliospheric Observatory (SOHO) spacecraft was launched and has provided solar EUV images beginning in 1995. SOHO is a main source of near-real time solar data for both research and space weather prediction and inspired the STEREO mission. The Yohkoh spacecraft at LEO observed the Sun from 1991 to 2001 in the X-ray portion of the solar spectrum and was useful for both research and space weather prediction. Data from Yohkoh inspired the Solar X-ray Imager on GOES. GOES-7 monitors space weather conditions during the October 1989 solar activity resulted in a Forbush Decrease, ground level enhancements , and many satellite anomalies. [15] Spacecraft with instruments whose primary purpose is to provide data for space weather predictions and applications include the Geostationary Operational Environmental Satellite (GOES) series of spacecraft, the POES series, the DMSP series, and the Meteosat series. The GOES spacecraft have carried an X-ray sensor (XRS) which measures the flux from the whole solar disk in two bands – 0.05 to 0.4 nm and 0.1 to 0.8 nm – since 1974, an X-ray imager (SXI) since 2004, a magnetometer which measures the distortions of the Earth's magnetic field due to space weather, a whole disk EUV sensor since 2004, and particle sensors (EPS/HEPAD) which measure ions and electrons in the energy range of 50 keV to 500 MeV. Starting sometime after 2015, the GOES-R generation of GOES spacecraft will replace the SXI with a solar EUV image (SUVI) similar to the one on SOHO and STEREO and the particle sensor will be augmented with a component to extend the energy range down to 30 eV. The Deep Space Climate Observatory (DSCOVR) satellite is a NOAA Earth observation and space weather satellite that launched in February 2015. Among its features is advance warning of coronal mass ejections. [53] Models[ edit ] Space weather models are simulations of the space weather environment. Models use sets of mathematical equations to describe physical processes. These models take a limited data set and attempt to describe all or part of the space weather environment in or to predict how weather evolves over time. Early models were heuristic; i.e., they did not directly employ physics. These models take less resources than their more sophisticated descendants. Later models use physics to account for as many phenomena as possible. No model can yet reliably predict the environment from the surface of the Sun to the bottom of the Earth's ionosphere. Space weather models differ from meteorological models in that the amount of input is vastly smaller. A significant portion of space weather model research and development in the past two decades has been done as part of the Geospace Environmental Model (GEM) program of the National Science Foundation . The two major modeling centers are the Center for Space Environment Modeling (CSEM) [54] and the Center for Integrated Space weather Modeling (CISM). [55] The Community Coordinated Modeling Center [56] (CCMC) at the NASA Goddard Space Flight Center is a facility for coordinating the development and testing of research models, for improving and preparing models for use in space weather prediction and application. [57] Modeling techniques include (a) magnetohydrodynamics , in which the environment is treated as a fluid, (b) particle in cell, in which non-fluid interactions are handled within a cell and then cells are connected to describe the environment, (c) first principles, in which physical processes are in balance (or equilibrium) with one another, (d) semi-static modeling, in which a statistical or empirical relationship is described, or a combination of multiple methods. Commercial space weather development[ edit ] During the first decade of the 21st Century, a commercial sector emerged that engaged in space weather, serving agency, academia, commercial and consumer sectors. [58] Space weather providers are typically smaller companies, or small divisions within a larger company, that provide space weather data, models, derivative products and service distribution.[ citation needed ] The commercial sector includes scientific and engineering researchers as well as users. Activities are primarily directed toward the impacts of space weather upon technology. These include, for example: Atmospheric drag on LEO satellites caused by energy inputs into the thermosphere from solar UV, FUV, Lyman-alpha , EUV , XUV , X-ray, and gamma ray photons as well as by charged particle precipitation and Joule heating at high latitudes;[ citation needed ] Surface and internal charging from increased energetic particle fluxes, leading to effects such as discharges, single event upsets and latch-up, on LEO to GEO satellites;[ citation needed ] Disrupted GPS signals caused by ionospheric scintillation leading to increased uncertainty in navigation systems such as aviation's Wide Area Augmentation System (WAAS);[ citation needed ] Lost HF, UHF and L-band radio communications due to ionosphere scintillation, solar flares and geomagnetic storms; Increased radiation to human tissue and avionics from galactic cosmic rays SEP, especially during large solar flares, and possibly bremsstrahlung gamma-rays produced by precipitating radiation belt energetic electrons at altitudes above 8 km; [59] [60] Increased inaccuracy in surveying and oil/gas exploration that uses the Earth's main magnetic field when it is disturbed by geomagnetic storms; Loss of power transmission from GIC surges in the electrical power grid and transformer shutdowns during large geomagnetic storms. Many of these disturbances result in societal impacts that account for a significant part of the national GDP. [61] [62] The concept of incentivizing commercial space weather was first suggested by the idea of a Space Weather Economic Innovation Zone discussed by the American Commercial Space Weather Association (ACSWA) in 2015. The establishment of this economic innovation zone would encourage expanded economic activity developing applications to manage the risks space weather and would encourage broader research activities related to space weather by universities. It could encourage U.S. business investment in space weather services and products. It promoted the support of U.S. business innovation in space weather services and products by requiring U.S. government purchases of U.S. built commercial hardware, software, and associated products and services where no suitable government capability pre-exists. It also promoted U.S. built commercial hardware, software, and associated products and services sales to international partners. designate U.S. built commercial hardware, services, and products as “Space Weather Economic Innovation Zone” activities; Finally, it recommended that U.S. built commercial hardware, services, and products be tracked as Space Weather Economic Innovation Zone contributions within agency reports. In 2015 the U.S. Congress bill HR1561 provided groundwork where social and environmental impacts from a Space Weather Economic Innovation Zone could be far-reaching. In 2016, the Space Weather Research and Forecasting Act (S. 2817) was introduced to build on that legacy. Later, in 2017-2018 the HR3086 Bill took these concepts, included the breadth of material from parallel agency studies as part of the OSTP-sponsored Space Weather Action Program (SWAP), [63] and with bicameral and bipartisan support the 116th Congress (2019) is considering passage of the Space Weather Coordination Act (S141, 115th Congress).[ citation needed ] American Commercial Space Weather Association[ edit ] On April 29, 2010, the commercial space weather community created the American Commercial Space Weather Association ( ACSWA ) an industry association. ACSWA promotes space weather risk mitigation for national infrastructure, economic strength and national security. It seeks to: [64] provide quality space weather data and services to help mitigate risks to technology; provide advisory services to government agencies; provide guidance on the best task division between commercial providers and government agencies; represent the interests of commercial providers; represent commercial capabilities in the national and international arena; develop best-practices. A summary of the broad technical capabilities in space weather that are available from the association can be found on their web site http://www.acswa.us .
Toggle the table of contents Atmospheric science From Wikipedia, the free encyclopedia Study of the atmosphere, its processes, and its interactions with other systems e Atmospheric science is the study of the Earth's atmosphere and its various inner-working physical processes. Meteorology includes atmospheric chemistry and atmospheric physics with a major focus on weather forecasting . Climatology is the study of atmospheric changes (both long and short-term) that define average climates and their change over time climate variability . Aeronomy is the study of the upper layers of the atmosphere, where dissociation and ionization are important. Atmospheric science has been extended to the field of planetary science and the study of the atmospheres of the planets and natural satellites of the Solar System . Experimental instruments used in atmospheric science include satellites , rocketsondes , radiosondes , weather balloons , radars , and lasers . The term aerology (from Greek ἀήρ, aēr, " air "; and -λογία, -logia ) is sometimes used as an alternative term for the study of Earth's atmosphere; [1] in other definitions, aerology is restricted to the free atmosphere , the region above the planetary boundary layer . [2] Main article: Atmospheric chemistry Composition diagram showing the evolution/cycles of various elements in Earth's atmosphere. Atmospheric chemistry is a branch of atmospheric science in which the chemistry of the Earth's atmosphere and that of other planets is studied. It is a multidisciplinary field of research and draws on environmental chemistry, physics, meteorology, computer modeling, oceanography, geology and volcanology and other disciplines. Research is increasingly connected with other areas of study such as climatology. The composition and chemistry of the atmosphere is of importance for several reasons, but primarily because of the interactions between the atmosphere and living organisms. The composition of the Earth's atmosphere has been changed by human activity and some of these changes are harmful to human health, crops and ecosystems. Examples of problems which have been addressed by atmospheric chemistry include acid rain, photochemical smog and global warming. Atmospheric chemistry seeks to understand the causes of these problems, and by obtaining a theoretical understanding of them, allow possible solutions to be tested and the effects of changes in government policy evaluated. See also: Synoptic scale meteorology Atmospheric dynamics is the study of motion systems of meteorological importance, integrating observations at multiple locations and times and theories. Common topics studied include diverse phenomena such as thunderstorms , tornadoes , gravity waves , tropical cyclones , extratropical cyclones , jet streams , and global-scale circulations. The goal of dynamical studies is to explain the observed circulations on the basis of fundamental principles from physics . The objectives of such studies incorporate improving weather forecasting , developing methods for predicting seasonal and interannual climate fluctuations, and understanding the implications of human-induced perturbations (e.g., increased carbon dioxide concentrations or depletion of the ozone layer) on the global climate. [4] Main article: Atmospheric physics Atmospheric physics is the application of physics to the study of the atmosphere. Atmospheric physicists attempt to model Earth's atmosphere and the atmospheres of the other planets using fluid flow equations, chemical models, radiation balancing, and energy transfer processes in the atmosphere and underlying oceans and land. In order to model weather systems, atmospheric physicists employ elements of scattering theory, wave propagation models, cloud physics , statistical mechanics and spatial statistics , each of which incorporate high levels of mathematics and physics. Atmospheric physics has close links to meteorology and climatology and also covers the design and construction of instruments for studying the atmosphere and the interpretation of the data they provide, including remote sensing instruments. In the United Kingdom, atmospheric studies are underpinned by the Meteorological Office. Divisions of the U.S. National Oceanic and Atmospheric Administration (NOAA) oversee research projects and weather modeling involving atmospheric physics. The U.S. National Astronomy and Ionosphere Center also carries out studies of the high atmosphere.
Toggle the table of contents Earth observation From Wikipedia, the free encyclopedia Information about the Earth environment, remote or in situ Earth observation (EO) is the gathering of information about the physical, chemical, and biological systems of the planet Earth . [1] It can be performed via remote-sensing technologies ( Earth observation satellites ) or through direct-contact sensors in ground-based or airborne platforms (such as weather stations and weather balloons , for example). [2] [3] According to the Group on Earth Observations (GEO), the concept encompasses both " space-based or remotely-sensed data, as well as ground-based or in situ data ". [4] Earth observation is used to monitor and assess the status of and changes in natural and built environments . [1] Terminology[ edit ] In Europe, Earth observation has often been used to refer to satellite-based remote sensing, [1] but the term is also used to refer to any form of observations of the Earth system, including in situ and airborne observations, for example. The GEO, which has over 100 member countries and over 100 participating organizations, uses EO in this broader sense. [4] In the US, the term remote sensing was used since the 1960s [5] to refer to satellite-based remote sensing. Remote sensing has also been used more broadly for observations using any form of remote sensing technology, including airborne sensors and even ground-based sensors such as cameras. [5] Perhaps the least ambiguous term to use for satellite-based sensors is satellite remote sensing (SRS), an acronym which is gradually starting to appear in the literature. [5] [6]
Toggle the table of contents Remote sensing This is the latest accepted revision , reviewed on 8 April 2024. Acquisition of information at a significant distance from the subject Not to be confused with remote viewing . Synthetic aperture radar image of Death Valley colored using polarimetry Remote sensing is the acquisition of information about an object or phenomenon without making physical contact with the object, in contrast to in situ or on-site observation . The term is applied especially to acquiring information about Earth and other planets . Remote sensing is used in numerous fields, including geophysics , geography , land surveying and most Earth science disciplines (e.g. exploration geophysics , hydrology , ecology , meteorology , oceanography , glaciology , geology ). It also has military, intelligence, commercial, economic, planning, and humanitarian applications, among others. In current usage, the term remote sensing generally refers to the use of satellite - or aircraft-based sensor technologies to detect and classify objects on Earth. It includes the surface and the atmosphere and oceans , based on propagated signals (e.g. electromagnetic radiation ). It may be split into "active" remote sensing (when a signal is emitted by a satellite or aircraft to the object and its reflection is detected by the sensor) and "passive" remote sensing (when the reflection of sunlight is detected by the sensor). [1] [2] [3] [4] Overview[ edit ] This video is about how Landsat was used to identify areas of conservation in the Democratic Republic of the Congo , and how it was used to help map an area called MLW in the north. Remote sensing can be divided into two types of methods: Passive remote sensing and Active remote sensing. Passive sensors gather radiation that is emitted or reflected by the object or surrounding areas. Reflected sunlight is the most common source of radiation measured by passive sensors. Examples of passive remote sensors include film photography , infrared , charge-coupled devices , and radiometers . Active collection, on the other hand, emits energy in order to scan objects and areas whereupon a sensor then detects and measures the radiation that is reflected or backscattered from the target. RADAR and LiDAR are examples of active remote sensing where the time delay between emission and return is measured, establishing the location, speed and direction of an object. Illustration of remote sensing Remote sensing makes it possible to collect data of dangerous or inaccessible areas. Remote sensing applications include monitoring deforestation in areas such as the Amazon Basin , glacial features in Arctic and Antarctic regions, and depth sounding of coastal and ocean depths. Military collection during the Cold War made use of stand-off collection of data about dangerous border areas. Remote sensing also replaces costly and slow data collection on the ground, ensuring in the process that areas or objects are not disturbed. Orbital platforms collect and transmit data from different parts of the electromagnetic spectrum , which in conjunction with larger scale aerial or ground-based sensing and analysis, provides researchers with enough information to monitor trends such as El Niño and other natural long and short term phenomena. Other uses include different areas of the earth sciences such as natural resource management , agricultural fields such as land usage and conservation, [5] [6] greenhouse gas monitoring , [7] oil spill detection and monitoring, [8] and national security and overhead, ground-based and stand-off collection on border areas. [9] Types of data acquisition techniques[ edit ] The basis for multispectral collection and analysis is that of examined areas or objects that reflect or emit radiation that stand out from surrounding areas. For a summary of major remote sensing satellite systems see the overview table. Applications of remote sensing[ edit ] Radar image of Aswan Dam, Egypt taken by Umbra Conventional radar is mostly associated with aerial traffic control, early warning, and certain large-scale meteorological data. Doppler radar is used by local law enforcements' monitoring of speed limits and in enhanced meteorological collection such as wind speed and direction within weather systems in addition to precipitation location and intensity. Other types of active collection includes plasmas in the ionosphere . Interferometric synthetic aperture radar is used to produce precise digital elevation models of large scale terrain (See RADARSAT , TerraSAR-X , Magellan ). Laser and radar altimeters on satellites have provided a wide range of data. By measuring the bulges of water caused by gravity, they map features on the seafloor to a resolution of a mile or so. By measuring the height and wavelength of ocean waves, the altimeters measure wind speeds and direction, and surface ocean currents and directions. Ultrasound (acoustic) and radar tide gauges measure sea level, tides and wave direction in coastal and offshore tide gauges. Light detection and ranging (LIDAR) is well known in examples of weapon ranging, laser illuminated homing of projectiles. LIDAR is used to detect and measure the concentration of various chemicals in the atmosphere, while airborne LIDAR can be used to measure the heights of objects and features on the ground more accurately than with radar technology. Vegetation remote sensing is a principal application of LIDAR. [10] Radiometers and photometers are the most common instrument in use, collecting reflected and emitted radiation in a wide range of frequencies. The most common are visible and infrared sensors, followed by microwave, gamma-ray, and rarely, ultraviolet. They may also be used to detect the emission spectra of various chemicals, providing data on chemical concentrations in the atmosphere. Examples of remote sensing equipment deployed byor interfaced with oceanographic research vessels . [11] Radiometers are also used at night, because artificial light emissions are a key signature of human activity. [12] Applications include remote sensing of population, GDP, and damage to infrastructure from war or disasters. Radiometers and radar onboard of satellites can be used to monitor volcanic eruptions [13] [14] Spectropolarimetric Imaging has been reported to be useful for target tracking purposes by researchers at the U.S. Army Research Laboratory . They determined that manmade items possess polarimetric signatures that are not found in natural objects. These conclusions were drawn from the imaging of military trucks, like the Humvee , and trailers with their acousto-optic tunable filter dual hyperspectral and spectropolarimetric VNIR Spectropolarimetric Imager. [15] [16] Stereographic pairs of aerial photographs have often been used to make topographic maps by imagery and terrain analysts in trafficability and highway departments for potential routes, in addition to modelling terrestrial habitat features. [17] [18] [19] Simultaneous multi-spectral platforms such as Landsat have been in use since the 1970s. These thematic mappers take images in multiple wavelengths of electromagnetic radiation (multi-spectral) and are usually found on Earth observation satellites , including (for example) the Landsat program or the IKONOS satellite. Maps of land cover and land use from thematic mapping can be used to prospect for minerals, detect or monitor land usage, detect invasive vegetation, deforestation, and examine the health of indigenous plants and crops ( satellite crop monitoring ), including entire farming regions or forests. [20] Prominent scientists using remote sensing for this purpose include Janet Franklin and Ruth DeFries . Landsat images are used by regulatory agencies such as KYDOW to indicate water quality parameters including Secchi depth, chlorophyll density, and total phosphorus content. Weather satellites are used in meteorology and climatology. Hyperspectral imaging produces an image where each pixel has full spectral information with imaging narrow spectral bands over a contiguous spectral range. Hyperspectral imagers are used in various applications including mineralogy, biology, defence, and environmental measurements. Within the scope of the combat against desertification , remote sensing allows researchers to follow up and monitor risk areas in the long term, to determine desertification factors, to support decision-makers in defining relevant measures of environmental management, and to assess their impacts. [21] Remotely sensed multi- and hyperspectral images can be used for assessing biodiversity at different scales. Since the spectral properties of different plants species are unique, it is possible to get information about properties that relates to biodiversity such as habitat heterogeneity, spectral diversity and plant functional trait. [22] [23] [24] Remote sensing has been used to detect rare plants to aid in conservation efforts. Prediction, detection, and the ability to record biophysical conditions were possible from medium to very high resolutions. [25] Further information: Satellite geodesy Geodetic remote sensing can be gravimetric or geometric. Overhead gravity data collection was first used in aerial submarine detection. This data revealed minute perturbations in the Earth's gravitational field that may be used to determine changes in the mass distribution of the Earth, which in turn may be used for geophysical studies, as in GRACE . Geometric remote sensing includes position and deformation imaging using InSAR , LIDAR, etc. [27] Acoustic and near-acoustic[ edit ] Sonar : passive sonar, listening for the sound made by another object (a vessel, a whale etc.); active sonar, emitting pulses of sounds and listening for echoes, used for detecting, ranging and measurements of underwater objects and terrain. Seismograms taken at different locations can locate and measure earthquakes (after they occur) by comparing the relative intensity and precise timings. Ultrasound : Ultrasound sensors, that emit high-frequency pulses and listening for echoes, used for detecting water waves and water level, as in tide gauges or for towing tanks. To coordinate a series of large-scale observations, most sensing systems depend on the following: platform location and the orientation of the sensor. High-end instruments now often use positional information from satellite navigation systems . The rotation and orientation are often provided within a degree or two with electronic compasses. Compasses can measure not just azimuth (i. e. degrees to magnetic north), but also altitude (degrees above the horizon), since the magnetic field curves into the Earth at different angles at different latitudes. More exact orientations require gyroscopic-aided orientation , periodically realigned by different methods including navigation from stars or known benchmarks. Spectral resolution The wavelength of the different frequency bands recorded – usually, this is related to the number of frequency bands recorded by the platform. Current Landsat collection is that of seven bands, including several in the infrared spectrum, ranging from a spectral resolution of 0.7 to 2.1 μm. The Hyperion sensor on Earth Observing-1 resolves 220 bands from 0.4 to 2.5 μm, with a spectral resolution of 0.10 to 0.11 μm per band. Radiometric resolution The number of different intensities of radiation the sensor is able to distinguish. Typically, this ranges from 8 to 14 bits, corresponding to 256 levels of the gray scale and up to 16,384 intensities or "shades" of colour, in each band. It also depends on the instrument noise . Temporal resolution The frequency of flyovers by the satellite or plane, and is only relevant in time-series studies or those requiring an averaged or mosaic image as in deforesting monitoring. This was first used by the intelligence community where repeated coverage revealed changes in infrastructure, the deployment of units or the modification/introduction of equipment. Cloud cover over a given area or object makes it necessary to repeat the collection of said location. Data processing[ edit ] In order to create sensor-based maps, most remote sensing systems expect to extrapolate sensor data in relation to a reference point including distances between known points on the ground. This depends on the type of sensor used. For example, in conventional photographs, distances are accurate in the center of the image, with the distortion of measurements increasing the farther you get from the center. Another factor is that of the platen against which the film is pressed can cause severe errors when photographs are used to measure ground distances. The step in which this problem is resolved is called georeferencing and involves computer-aided matching of points in the image (typically 30 or more points per image) which is extrapolated with the use of an established benchmark, "warping" the image to produce accurate spatial data. As of the early 1990s, most satellite images are sold fully georeferenced. In addition, images may need to be radiometrically and atmospherically corrected. Radiometric correction Allows avoidance of radiometric errors and distortions. The illumination of objects on the Earth's surface is uneven because of different properties of the relief. This factor is taken into account in the method of radiometric distortion correction. [28] Radiometric correction gives a scale to the pixel values, e. g. the monochromatic scale of 0 to 255 will be converted to actual radiance values. Topographic correction (also called terrain correction) In rugged mountains, as a result of terrain, the effective illumination of pixels varies considerably. In a remote sensing image, the pixel on the shady slope receives weak illumination and has a low radiance value, in contrast, the pixel on the sunny slope receives strong illumination and has a high radiance value. For the same object, the pixel radiance value on the shady slope will be different from that on the sunny slope. Additionally, different objects may have similar radiance values. These ambiguities seriously affected remote sensing image information extraction accuracy in mountainous areas. It became the main obstacle to the further application of remote sensing images. The purpose of topographic correction is to eliminate this effect, recovering the true reflectivity or radiance of objects in horizontal conditions. It is the premise of quantitative remote sensing application. Atmospheric correction Elimination of atmospheric haze by rescaling each frequency band so that its minimum value (usually realised in water bodies) corresponds to a pixel value of 0. The digitizing of data also makes it possible to manipulate the data by changing gray-scale values. Interpretation is the critical process of making sense of the data. The first application was that of aerial photographic collection which used the following process; spatial measurement through the use of a light table in both conventional single or stereographic coverage, added skills such as the use of photogrammetry, the use of photomosaics, repeat coverage, Making use of objects' known dimensions in order to detect modifications. Image Analysis is the recently developed automated computer-aided application that is in increasing use. Object-Based Image Analysis (OBIA) is a sub-discipline of GIScience devoted to partitioning remote sensing (RS) imagery into meaningful image-objects, and assessing their characteristics through spatial, spectral and temporal scale. Old data from remote sensing is often valuable because it may provide the only long-term data for a large extent of geography. At the same time, the data is often complex to interpret, and bulky to store. Modern systems tend to store the data digitally, often with lossless compression . The difficulty with this approach is that the data is fragile, the format may be archaic, and the data may be easy to falsify. One of the best systems for archiving data series is as computer-generated machine-readable ultrafiche , usually in typefonts such as OCR-B , or as digitized half-tone images. Ultrafiches survive well in standard libraries, with lifetimes of several centuries. They can be created, copied, filed and retrieved by automated systems. They are about as compact as archival magnetic media, and yet can be read by human beings with minimal, standardized equipment. Generally speaking, remote sensing works on the principle of the inverse problem : while the object or phenomenon of interest (the state) may not be directly measured, there exists some other variable that can be detected and measured (the observation) which may be related to the object of interest through a calculation. The common analogy given to describe this is trying to determine the type of animal from its footprints. For example, while it is impossible to directly measure temperatures in the upper atmosphere, it is possible to measure the spectral emissions from a known chemical species (such as carbon dioxide) in that region. The frequency of the emissions may then be related via thermodynamics to the temperature in that region. Data processing levels[ edit ] To facilitate the discussion of data processing in practice, several processing "levels" were first defined in 1986 by NASA as part of its Earth Observing System [29] and steadily adopted since then, both internally at NASA (e. g., [30] ) and elsewhere (e. g., [31] ); these definitions are: Level Description 0 Reconstructed, unprocessed instrument and payload data at full resolution, with any and all communications artifacts (e. g., synchronization frames, communications headers, duplicate data) removed. 1a Reconstructed, unprocessed instrument data at full resolution, time-referenced, and annotated with ancillary information, including radiometric and geometric calibration coefficients and georeferencing parameters (e. g., platform ephemeris) computed and appended but not applied to the Level 0 data (or if applied, in a manner that level 0 is fully recoverable from level 1a data). 1b Level 1a data that have been processed to sensor units (e. g., radar backscatter cross section, brightness temperature, etc.); not all instruments have Level 1b data; level 0 data is not recoverable from level 1b data. 2 Derived geophysical variables (e. g., ocean wave height, soil moisture, ice concentration) at the same resolution and location as Level 1 source data. 3 Variables mapped on uniform spacetime grid scales, usually with some completeness and consistency (e. g., missing points interpolated, complete regions mosaicked together from multiple orbits, etc.). 4 Model output or results from analyses of lower level data (i. e., variables that were not measured by the instruments but instead are derived from these measurements). A Level 1 data record is the most fundamental (i. e., highest reversible level) data record that has significant scientific utility, and is the foundation upon which all subsequent data sets are produced. Level 2 is the first level that is directly usable for most scientific applications; its value is much greater than the lower levels. Level 2 data sets tend to be less voluminous than Level 1 data because they have been reduced temporally, spatially, or spectrally. Level 3 data sets are generally smaller than lower level data sets and thus can be dealt with without incurring a great deal of data handling overhead. These data tend to be generally more useful for many applications. The regular spatial and temporal organization of Level 3 datasets makes it feasible to readily combine data from different sources. While these processing levels are particularly suitable for typical satellite data processing pipelines, other data level vocabularies have been defined and may be appropriate for more heterogeneous workflows. Applications[ edit ] Satellite images provide very useful information to produce statistics on topics closely related to the territory, such as agriculture, forestry or land cover in general. The first large project to apply Landsata 1 images for statistics was LACIE (Large Area Crop Inventory Experiment), run by NASA, NOAA and the USDA in 1974–77. [32] [33] Many other application projects on crop area estimation have followed, including the Italian AGRIT project and the MARS project of the Joint Research Centre (JRC) of the European Commission. [34] Forest area and deforestation estimation have also been a frequent target of remote sensing projects [35] [36] , the same as land cover and land use [37] Groud truth or reference data to train and validate image classification require a field survey if we are targetting annual crops or individual forest species, but may be substituted by photointerpretation if we look at wider classes that can be reliably identified on aerial photos or satellite images. It is relevant to highlight that probabilistic sampling is not critical for the selection of training pixels for image classification, but it is necessary for accuracy assessment of the classified images and area estimation. [38] [39] [40] Additional care is recommended to ensure that training and validation datasets are not spatially correlated. [41] We suppose now that we have classified images or a land cover map produced by visual photo-interpretation, with a legend of mapped classes that suits our purpose, taking again the example of wheat. The straightforward approach is counting the number of pixels classified as wheat and multiplying by the area of each pixel. Many authors have noticed that estimator is that it is generally biased because commission and omission errors in a confusion matrix do not compensate each other [42] [43] [44] The main strength of classified satellite images or other indicators computed on satellite images is providing cheap information on the whole target area or most of it. This information usually has a good correlation with the target variable (ground truth) that is usually expensive to observe in an unbiased and accurate way. Therefore it can be observed on a probabilistic sample selected on an area sampling frame . Traditional survey methodology provides different methods to combine accurate information on a sample with less accurate, but exhaustive, data for a covariable or proxy that is cheaper to collect.  For agricultural statistics, field surveys are usually required, while photo-interpretation may better for land cover classes that can be reliably identified on aerial photographs or high resolution satellite images. Additional uncertainty can appear because of imperfect reference data (ground truth or similar). [45] [46]
Toggle the table of contents Earth observation satellite From Wikipedia, the free encyclopedia Satellite specifically designed to observe Earth from orbit Six Earth observation satellites comprising the A-train satellite constellation as of 2014. e An Earth observation satellite or Earth remote sensing satellite is a satellite used or designed for Earth observation (EO) from orbit , including spy satellites and similar ones intended for non-military uses such as environmental monitoring , meteorology , cartography and others. The most common type are Earth imaging satellites, that take satellite images , analogous to aerial photographs ; some EO satellites may perform remote sensing without forming pictures, such as in GNSS radio occultation . The first occurrence of satellite remote sensing can be dated to the launch of the first artificial satellite, Sputnik 1 , by the Soviet Union on October 4, 1957. [1] Sputnik 1 sent back radio signals, which scientists used to study the ionosphere . [2] The United States Army Ballistic Missile Agency launched the first American satellite, Explorer 1 , for NASA's Jet Propulsion Laboratory on January 31, 1958. The information sent back from its radiation detector led to the discovery of the Earth's Van Allen radiation belts . [3] The TIROS-1 spacecraft, launched on April 1, 1960, as part of NASA's Television Infrared Observation Satellite (TIROS) program, sent back the first television footage of weather patterns to be taken from space. [1] In 2008, more than 150 Earth observation satellites were in orbit, recording data with both passive and active sensors and acquiring more than 10 terabits of data daily. [1] By 2021, that total had grown to over 950, with the largest number of satellites operated by US-based company Planet Labs . [4] Most Earth observation satellites carry instruments that should be operated at a relatively low altitude. Most orbit at altitudes above 500 to 600 kilometers (310 to 370 mi). Lower orbits have significant air-drag , which makes frequent orbit reboost maneuvers necessary. The Earth observation satellites ERS-1, ERS-2 and Envisat of European Space Agency as well as the MetOp spacecraft of EUMETSAT are all operated at altitudes of about 800 km (500 mi). The Proba-1 , Proba-2 and SMOS spacecraft of European Space Agency are observing the Earth from an altitude of about 700 km (430 mi). The Earth observation satellites of UAE, DubaiSat-1 & DubaiSat-2 are also placed in Low Earth Orbits (LEO) orbits and providing satellite imagery of various parts of the Earth. [5] [6] To get global coverage with a low orbit, a polar orbit is used. A low orbit will have an orbital period of roughly 100 minutes and the Earth will rotate around its polar axis about 25° between successive orbits. The ground track moves towards the west 25° each orbit, allowing a different section of the globe to be scanned with each orbit. Most are in Sun-synchronous orbits . A geostationary orbit , at 36,000 km (22,000 mi), allows a satellite to hover over a constant spot on the earth since the orbital period at this altitude is 24 hours. This allows uninterrupted coverage of more than 1/3 of the Earth per satellite, so three satellites, spaced 120° apart, can cover the whole Earth. This type of orbit is mainly used for meteorological satellites . See also: Remote sensing § History Herman Potočnik explored the idea of using orbiting spacecraft for detailed peaceful and military observation of the ground in his 1928 book, The Problem of Space Travel. He described how the special conditions of space could be useful for scientific experiments. The book described geostationary satellites (first put forward by Konstantin Tsiolkovsky ) and discussed communication between them and the ground using radio, but fell short of the idea of using satellites for mass broadcasting and as telecommunications relays. [7]
Toggle the table of contents Landsat program From Wikipedia, the free encyclopedia American network of Earth-observing satellites for international research purposes Landsat 7 , launched in 1999, is the 7th of 9 satellites in the Landsat program. A false-color satellite image of Kolkata , India, from Landsat 7 in 2004, showing rivers, vegetated areas, and developed areas A land cover map of the big island of Hawaii using 1999–2001 data from Landsat 7 , showing black lava flows from Mauna Loa , grayish dormant Mauna Kea , a plume of smoke from active Kilauea , dark green tropical forests, and light green agricultural areas. The Landsat program is the longest-running enterprise for acquisition of satellite imagery of Earth . It is a joint NASA / USGS program. On 23 July 1972, the Earth Resources Technology Satellite was launched. This was eventually renamed to Landsat 1 in 1975. [1] The most recent, Landsat 9 , was launched on 27 September 2021. The instruments on the Landsat satellites have acquired millions of images. The images, archived in the United States and at Landsat receiving stations around the world, are a unique resource for global change research and applications in agriculture , cartography , geology , forestry , regional planning , surveillance and education , and can be viewed through the U.S. Geological Survey (USGS) "EarthExplorer" website. Landsat 7 data has eight spectral bands with spatial resolutions ranging from 15 to 60 m (49 to 197 ft); the temporal resolution is 16 days. [2] Landsat images are usually divided into scenes for easy downloading. Each Landsat scene is about 115 miles long and 115 miles wide (or 100 nautical miles long and 100 nautical miles wide, or 185 kilometers long and 185 kilometers wide). Virginia Norwood , "The Mother of Landsat", designed the multispectral scanner. Interview with Jim Irons – Landsat 8 Project Scientist – NASA Goddard Space Flight Center In 1965, William T. Pecora , the then director of the United States Geological Survey , proposed the idea of a remote sensing satellite program to gather facts about the natural resources of our planet. Pecora stated that the program was "conceived in 1966 largely as a direct result of the demonstrated utility of the Mercury and Gemini orbital photography to Earth resource studies." While weather satellites had been monitoring Earth's atmosphere since 1960 and were largely considered useful, there was no appreciation of terrain data from space until the mid-1960s. So, when Landsat 1 was proposed, it met with intense opposition from the Bureau of Budget and those who argued high-altitude aircraft would be the fiscally responsible choice for Earth remote sensing. Concurrently, the Department of Defense feared that a civilian program such as Landsat would compromise the secrecy of their reconnaissance missions. Additionally, there were geopolitical concerns about photographing foreign countries without permission. In 1965, NASA began methodical investigations of Earth remote sensing using instruments mounted on planes. In 1966, the USGS convinced the Secretary of the Interior , Stewart Udall , to announce that the Department of the Interior (DOI) was going to proceed with its own Earth-observing satellite program. This savvy political stunt coerced NASA to expedite the building of Landsat. But budgetary constraints and sensor disagreements between application agencies (notably the Department of Agriculture and DOI) again stymied the satellite construction process. Finally, by 1970 NASA had a green light to build a satellite. Remarkably, within only two years, Landsat 1 was launched, heralding a new age of remote sensing of land from space. [3] The Hughes Aircraft Company from Santa Barbara Research Center initiated, designed, and fabricated the first three Multispectral Scanners (MSS) in 1969. The first MSS prototype, designed by Virginia Norwood , was completed within nine months, in the fall of 1970. It was tested by scanning Half Dome at Yosemite National Park . For this design work Norwood was called "The Mother of Landsat". [4] Working at NASA's Goddard Space Flight Center, Valerie L. Thomas managed the development of early Landsat image processing software systems and became the resident expert on the Computer Compatible Tapes, or CCTs, that were used to store early Landsat imagery. Thomas was one of the image processing specialists who facilitated the ambitious Large Area Crop Inventory Experiment, known as LACIE — a project that showed for the first time that global crop monitoring could be done with Landsat satellite imagery. [5] The program was initially called the Earth Resources Technology Satellites Program, which was used from 1966 to 1975. In 1975, the name was changed to Landsat. In 1979, President of the United States Jimmy Carter 's Presidential Directive 54 [6] [7] transferred Landsat operations from NASA to National Oceanic and Atmospheric Administration (NOAA), recommended development of a long term operational system with four additional satellites beyond Landsat 3, and recommended transition to private sector operation of Landsat. This occurred in 1985 when the Earth Observation Satellite Company (EOSAT), a partnership of Hughes Aircraft Company and RCA , was selected by NOAA to operate the Landsat system with a ten-year contract. EOSAT operated Landsat 4 and Landsat 5, had exclusive rights to market Landsat data, and was to build Landsats 6 and 7. In 1989, this transition had not been fully completed when NOAA's funding for the Landsat program was due to run out (NOAA had not requested any funding, and U.S. Congress had appropriated only six months of funding for the fiscal year) [8] and NOAA directed that Landsat 4 and Landsat 5 be shut down. [9] The head of the newly formed National Space Council , Vice President Dan Quayle , noted the situation and arranged emergency funding that allowed the program to continue with the data archives intact. [8] [9] [10] [11] Again in 1990 and 1991, Congress provided only half of the year's funding to NOAA, requesting that agencies that used Landsat data provide the funding for the other six months of the upcoming year. [8] In 1992, various efforts were made to procure funding for follow on Landsats and continued operations, but by the end of the year EOSAT ceased processing Landsat data. Landsat 6 was finally launched on 5 October 1993, but was lost in a launch failure. Processing of Landsat 4 and 5 data was resumed by EOSAT in 1994. NASA finally launched Landsat 7 on 15 April 1999. The value of the Landsat program was recognized by Congress in October 1992 when it passed the Land Remote Sensing Policy Act (Public Law 102-555) authorizing the procurement of Landsat 7 and assuring the continued availability of Landsat digital data and images, at the lowest possible cost, to traditional and new users of the data. 6 January 1978 5 years, 6 months and 14 days Originally named Earth Resources Technology Satellite 1. Landsat 1 carried two vital instruments: a camera built by the Radio Corporation of America (RCA) known as the Return Beam Vidicon (RBV); and the Multi spectral Scanner (MSS) built by the Hughes Aircraft Company . 25 February 1982 7 years, 1 month and 3 days Nearly identical copy of Landsat 1. Payload consisting of a Return Beam Vidicon (RBV) and a Multi spectral Scanner (MSS). The specifications of these instruments were identical to Landsat 1. 31 March 1983 5 years and 26 days Nearly identical copy of Landsat 1 and Landsat 2. Payload consisting of a Return Beam Vidicon (RBV) as well as a Multi spectral Scanner (MSS). Included with the MSS was a short-lived thermal band. MSS data was considered more scientifically applicable than the RBV which was rarely used for engineering evaluation purposes. 14 December 1993 11 years, 4 months and 28 days Landsat 4 carried an updated Multi Spectral Scanner (MSS) used on previous Landsat missions, as well as a Thematic Mapper. 5 June 2013 [12] 29 years, 3 months and 4 days Nearly identical copy of Landsat 4. Longest Earth-observing satellite mission in history. Designed and built at the same time as Landsat 4, this satellite carried the same payload consisting of a Multi Spectral Scanner (MSS) as well as a Thematic Mapper. 5 October 1993 0 days Failed to reach orbit. Landsat 6 was an upgraded version of its predecessors. Carrying the same Multi spectral Scanner (MSS) but also carrying an Enhanced Thematic Mapper, which added a 15m resolution panchromatic band. 6 April 2022 24 years, 11 months and 22 days Operating with scan line corrector disabled since May 2003. [13] The main component on Landsat 7 was the Enhanced Thematic Mapper Plus (ETM+). Still consisting of the 15m-resolution panchromatic band, but also includes a full aperture calibration. This allows for 5% absolute radiometric calibration . [14] active 11 years, 1 month and 26 days Originally named Landsat Data Continuity Mission from launch until 30 May 2013, when NASA operations were turned over to United States Geological Survey (USGS). [15] Landsat 8 has two sensors with its payload, the Operational Land Imager (OLI) and the Thermal InfraRed Sensor (TIRS). [16] 2 years, 6 months and 10 days Landsat 9 is a rebuild of its predecessor Landsat 8. [17] [18] Timeline Spatial and spectral resolution[ edit ] Landsat 1 through 5 carried the Landsat Multispectral Scanner (MSS). Landsat 4 and 5 carried both the MSS and Thematic Mapper (TM) instruments. Landsat 7 uses the Enhanced Thematic Mapper Plus (ETM+) scanner. Landsat 8 uses two instruments, the Operational Land Imager (OLI) for optical bands and the Thermal Infrared Sensor (TIRS) for thermal bands. The band designations, bandpasses, and pixel sizes for the Landsat instruments are: [19] Landsat 1–5 Multispectral Scanner (MSS) Landsat 1–3 MSS Band 6 – Near Infrared (NIR) Band 3 – NIR 0.8 – 1.1 60* * Original MSS pixel size was 79 x 57 meters; production systems now resample the data to 60 meters. Landsat 4–5 Thematic Mapper (TM) Bands Band 5 – Shortwave Infrared (SWIR) 1 1.55 – 1.75 * TM Band 6 was acquired at 120-meter resolution, but products are resampled to 30-meter pixels. Landsat 7 Enhanced Thematic Mapper Plus (ETM+) Bands * ETM+ Band 6 is acquired at 60-meter resolution, but products are resampled to 30-meter pixels. The spectral band placement for each sensor of Landsat Landsat 8 Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) [20] Bands Band 1 - Ultra Blue (coastal/aerosol) 0.435 – 0.451 11.50 – 12.51 100* (30) * TIRS bands are acquired at 100 meter resolution, but are resampled to 30 meter in delivered data product. An advantage of Landsat imagery, and remote sensing in general, is that it provides data at a synoptic global level that is impossible to replicate with in situ measurements. However, there are tradeoffs between the local detail of the measurements (radiometric resolution, number of spectral bands) and the spatial scale of the area being measured. Landsat imagery is coarse in spatial resolution compared to using other remote sensing methods, such as imagery from airplanes. Compared to other satellites, Landsat's spatial resolution is relatively high, yet revisit time is relatively less frequent. MultiSpectral Scanner (MSS)[ edit ] The Landsat program incorporated the Multispectral Scanner (MSS) from its first mission up to its fifth. The MSS, gave the United States an advantage in satellite imaging, facilitating the launch of Landsat ahead of the French SPOT satellite. The MSS was unique in its design. Rather than a static camera, it employed a moving mirror, capturing Earth's images in four distinct spectral bands. This capability allowed the MSS to record variations in sunlight reflected from the Earth. Notably, Landsat 3's MSS was further advanced, with an added capability to detect heat radiation. [21] One of the prominent features of the MSS was its consistent imaging. Each captured frame represented an area on the Earth's surface approximately 83 meters in length and 68 meters in width. Additionally, the system was designed to ensure a continuous image sweep across a swath equivalent to 185 km on the Earth's surface. The MSS's design also emphasized precision; by precisely timing the mirror's movements, it ensured that consecutive images did not overlap. [21] However, by the 1980s, the cost dynamics shifted. Accessing Landsat's imagery became substantially more expensive, making the French SPOT satellite's images a more cost-effective alternative for many users. The rise in Landsat's prices can be attributed to U.S. policy shifts, initiated under President Carter's leadership and finalized during President Reagan's administration. [7] [22] Uses of Landsat imagery[ edit ] One year after launch, Landsat 8 imagery had over one million file downloads by data users. Landsat data provides information that allows scientists to predict the distribution of species, as well as detecting both naturally occurring and human-generated changes over a greater scale than traditional data from field work. The different spectral bands used on satellites in the Landsat program provide many applications, ranging from ecology to geopolitical matters. Land cover determination is a common use of Landsat imagery around the world. [23] Landsat imagery provides one of the longest uninterrupted time series available from any single remote sensing program, spanning from 1972 to present. [24] Looking to the future, the successful launch of Landsat-9 in 2021 shows that this time series will be continued forward. [25] A false-color image of irrigated fields near Garden City, Kansas , taken by the Landsat 7 satellite. In 2015, the Landsat Advisory Group of the National Geospatial Advisory Committee reported that the top 16 applications of Landsat imagery produced savings of approximately 350 million to over 436 million dollars each year for federal and state governments, NGO's, and the private sector. That estimate did not include further savings from other uses beyond the top sixteen categories. [26] The top 16 categories for Landsat imagery use, listed in order of estimated annual savings for users, are: U.S. Department of Agriculture risk management U.S. Government mapping World agriculture supply and demand estimates Vineyard management and water conservation Flood mitigation mapping Waterfowl habitat mapping and monitoring Coastal change analysis National Geospatial-Intelligence Agency global shoreline mapping Wildfire risk assessment [26] Further uses of Landsat imagery include, but are not limited to: fisheries, forestry, shrinking inland water bodies, fire damage, glacier retreat, urban development, and discovery of new species. A few specific examples are explained below. Natural resources management[ edit ] Landsat image of the Aral Sea in 2013. Landsat images of burned land in Yellowstone National Park in 1989 and 2011. Landsat-5 false color images of the Columbia Glacier, Alaska in 1986 and 2011. Landsat false color image highlighting developed areas in pink in Vancouver , British Columbia, Canada. Fisheries[ edit ] In 1975, one potential application for the new satellite-generated imagery was to find high yield fishery areas. Through the Landsat Menhaden and Thread Investigation, some satellite data of the eastern portion of the Mississippi sound and another area off the coast of the Louisiana coast data was run through classification algorithms to rate the areas as high and low probability fishing zones, these algorithms yielded a classification that was proven with in situ measurements – to be over 80% accurate and found that water color, as seen from space, and turbidity significantly correlate with the distribution of menhaden – while surface temperature and salinity do not appear to be significant factors. Water color – measured with the multispectral scanners four spectral bands, was used to infer Chlorophyll , turbidity , and possibly fish distribution. [27] Forestry[ edit ] An ecological study used 16 ortho-rectified Landsat images to generate a land cover map of Mozambique 's mangrove forest. The main objective was to measure the mangrove cover and above ground biomass on this zone that until now could only be estimated, the cover was found with 93% accuracy to be 2909 square kilometers (27% lower than previous estimates). Additionally, the study helped confirm that geological setting has a greater influence on biomass distribution than latitude alone - the mangrove area is spread across 16° of latitude but it the biomass volume of it was affected more strongly by geographic conditions. [28] Climate change and environmental disasters[ edit ] Shrinking of the Aral Sea[ edit ] The shrinking of the Aral Sea has been described as "One of the planet's worst environmental disasters". Landsat imagery has been used as a record to quantify the amount of water loss and the changes to the shoreline. Satellite visual images have a greater impact on people than just words, and this shows the importance of Landsat imagery and satellite images in general. [29] Fires in Yellowstone National Park[ edit ] The Yellowstone fires of 1988 were the worst in the recorded history of the national park. They lasted from 14 June to 11 September 1988, when rain and snow helped halt the spread of the fires. The area affected by the fire was estimated to be 3,213 square kilometers – 36% of the park. Landsat imagery was used for the area estimation, and it also helped determine the reasons why the fire spread so quickly. Historic drought and a significant number of lightning strikes were some of the factors that created conditions for the massive fire, but anthropogenic actions amplified the disaster. On images generated previous to the fire, there is an evident difference between lands that display preservation practices and the lands that display clear cut activities for timber production. These two type of lands reacted differently to the stress of fires, and it is believed that that was an important factor on the behavior of the wildfire. Landsat imagery, and satellite imagery in general, have contributed to understanding fire science; fire danger, wildfire behavior and the effects of wildfire on certain areas. It has helped understanding of how different features and vegetation fuel fires, change temperature, and affect the spreading speed. [30] [31] Glacier retreat[ edit ] The serial nature of Landsat missions and the fact that is the longest-running satellite program gives it a unique perspective to generate information of Earth. Glacier retreat in a big scale can be traced back to previous Landsat missions, and this information can be used to generate climate change knowledge. The Columbia glacier retreat for example, can be observed in false-composite images since Landsat 4 in 1986. [32] Urban development[ edit ] Landsat imagery gives a time-lapse like series of images of development. Human development specifically, can be measured by the size a city grows over time. Further than just population estimates and energy consumption, Landsat imagery gives an insight of the type of urban development, and study aspects of social and political change through visible change. In Beijing for example, a series of ring roads started to develop in 1980s following the economic reform of 1970, and the change in development rate and construction rate was accelerated in these time periods. [32] Ecology[ edit ] Discovery of new species[ edit ] In 2005, Landsat imagery assisted in the discovery of new species. Conservation scientist Julian Bayliss wanted to find areas that could potentially become conservation forests using Landsat generated satellite images. Bayliss saw a patch in Mozambique that until then had no detailed information. On a reconnaissance trip, he found great diversity of wildlife as well as three new species of butterflies and a new snake species. Following his discovery, he continued to study this forest and was able to map and determine the forest extent. [33] Recent and future Landsat satellites[ edit ] Landsat 8/9 and Landsat Next spectral band comparison Landsat 8 launched on 11 February 2013. It was launched on an Atlas V 401 from Vandenberg Air Force Base by the Launch Services Program . It will continue to obtain valuable data and imagery to be used in agriculture, education, business, science, and government. The new satellite was assembled in Arizona by Orbital Sciences Corporation . Landsat 9 launched on September 27, 2021. During FY2014 financial planning "appropriators chided NASA for unrealistic expectations that a Landsat 9 would cost US$1 billion, and capped spending at US$650 million" according to a report by the Congressional Research Service . United States Senate appropriators advised NASA to plan for a launch no later than 2020. [7] In April 2015, NASA and the USGS announced that work on Landsat 9 had commenced, with funding allocated for the satellite in the president's FY2016 budget, for a planned launch in 2023. [34] Funding for the development of a low-cost thermal infrared (TIR) free-flying satellite for launch in 2019 was also proposed, to ensure data continuity by flying in formation with Landsat 8. [34] In the future, there may also be more collaboration between Landsat satellites and other satellites with similar spatial and spectral resolution, such as the ESA 's Sentinel-2 constellation. [35] Landsat NeXt is planned to be launched in 2029. NeXt will measure 25 spectral bands; current Landsat's 8 and 9 can measure only 11. [36] Gallery[ edit ] Overview of the Thermal Infrared Sensor (TIRS), one of the instruments on Landsat 8. A timelapse of the Thermal Infrared Sensor (TIRS) instrument for Landsat 8 being cleaned, bagged, and packed to ship to Orbital Sciences Corp, where TIRS will be integrated with the spacecraft. Animation showing how different LDCM bands can be combined to obtain different information over the Florida Everglades . Screenshot capture from NASA TV showing the Atlas V during the launch of Landsat 8.
Toggle the table of contents Copernicus Programme From Wikipedia, the free encyclopedia Programme of the European Commission This article's use of external links may not follow Wikipedia's policies or guidelines. Please improve this article by removing excessive or inappropriate external links, and converting useful links where appropriate into footnote references . (December 2022) ( Cost €5,421 billion (2021-2027) Copernicus is the Earth observation component of the European Union Space Programme , managed by the European Commission and implemented in partnership with the EU Member States , the European Space Agency (ESA), the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT), the European Centre for Medium-Range Weather Forecasts (ECMWF), the Joint Research Centre (JRC), the European Environment Agency (EEA), the European Maritime Safety Agency (EMSA), Frontex , SatCen and Mercator Océan. [1] The programme aims at achieving a global, continuous, autonomous, high quality, wide range Earth observation capacity. Providing accurate, timely and easily accessible information to, among other things, improve the management of the environment, understand and mitigate the effects of climate change , and ensure civil security. Since 2021, Copernicus is a component of the EU Space Programme , which aims to bolster the EU Space policy in the fields of Earth Observation, Satellite Navigation, Connectivity, Space Research and Innovation and supports investments in critical infrastructure and disruptive technologies. Program definition[ edit ] The objective for Copernicus is to use vast amount of global data from satellites and from ground-based, airborne and seaborne measurement systems to produce timely and quality information, services and knowledge, and to provide autonomous and independent access to information in the domains of environment and security on a global level in order to help service providers, public authorities and other international organizations improve the quality of life for the citizens of Europe. In other words, it pulls together all the information obtained by the Copernicus environmental satellites , air and ground stations and sensors to provide a comprehensive picture of the "health" of Earth . [2] One of the benefits of the Copernicus programme is that the data and information produced in the framework of Copernicus are made available free-of-charge [3] to all its users and the public, thus allowing downstream services to be developed. The services offered by Copernicus cover six main interacting themes: atmosphere, marine, land, climate, emergency and security. [4] Copernicus builds upon three components: The space component (observation satellites and associated ground segment with missions observing land, atmospheric and oceanographic parameters). This comprises two types of satellite missions, ESA's six families of dedicated Sentinel (space missions) and missions from other space agencies, called Contributing Missions; [5] In-situ measurements (ground-based and airborne data-gathering networks providing information on oceans, continental surface and atmosphere); Services developed and managed by Copernicus and offered to its users and public in general. It was named after the scientist and observer Nicolaus Copernicus . Copernicus' theory of the heliocentric universe made a pioneering contribution to modern science. [6] Its costs during 1998 to 2020 are estimated at €6.7 billion with around €4.3 billion spent in the period 2014 to 2020 and shared between the EU (67%) and ESA (33%) with benefits of the data to the EU economy estimated at €30 billion through 2030. [7] ESA as a main partner has performed much of the design and oversees and co-funds the development of Sentinel missions 1, 2, 3, 4, 5 and 6 with each Sentinel mission consisting of at least 2 satellites and some, such as Sentinel 1, 2 and 3, consisting of 4 satellites. [8] They will also provide the instruments for Meteosat Third Generation and MetOp-SG weather satellites of EUMETSAT where ESA and EUMETSAT will also coordinate the delivery of data from upwards of 30 satellites that form the contributing satellite missions to Copernicus. [9] Italy and the Mediterranean, image captured by Copernicus Sentinel-3A on 28 September 2016. History[ edit ] The Copernicus programme was established by the Regulation (EU) No 377/2014 [3] in 2014, building on the previous EU's Earth monitoring initiative GMES (established by Regulation (EU) No 911/2010 [10] ). Over a few decades, European and national institutions have made substantial R&D efforts in the field of Earth observation. These efforts have resulted in tremendous achievements but the services and products developed during this period had limitations that were inherent to R&D activities (e.g. lack of service continuity on the long-term). The idea for a global and continuous European Earth observation system was developed under the name of Global Monitoring for Environment and Security (GMES) which was later re-branded into Copernicus after the EU became directly involved in financing and development. It follows and greatly expands on the work of the previous €2.3 billion European Envisat programme which operated from 2002 to 2012. [11] Copernicus moved from R&D to operational services following a phased approach. Pre-operational services (Fast Track Services and Pilot Services) were phased in between 2008 and 2010. Copernicus initial operations began in 2011. Copernicus became fully operational in 2014. [12] Chronology[ edit ] 19 May 1998: institutions involved in the development of space activities in Europe give birth to GMES through a declaration known as "The Baveno Manifesto". At that time, GMES stands for "Global Monitoring for Environmental Security". Year 1999: the name is changed to "Global Monitoring for Environment and Security" (GMES), thus illustrating that the management of the environment also has security implications. 2001: at the occasion of the Gothenburg Summit, the Heads of State and Government request that "the Community contribute to establishing by 2008 a European capacity for Global Monitoring for Environment and Security". October 2002: the nature and scope of the "Security" component of GMES are defined as addressing prevention of and response to crises related to natural and technological risk, humanitarian aid and international cooperation, monitoring of compliance with international treaties for conflict prevention, humanitarian and rescue tasks, peacekeeping tasks and surveillance of EU borders. February 2004: the Commission Communication "GMES: Establishing a GMES capacity by 2008" introduces an Action Plan aimed at establishing a working GMES capacity by 2008. In 2004, a Framework Agreement is also signed between EC and ESA, thus providing the basis for a space component of GMES. May 2005: the Commission Communication "GMES: From Concept to Reality" establishes priorities for the roll-out of GMES services in 2008, the initial focus being on land monitoring, marine monitoring and emergency response services, also known as Fast Track Services (FTS). Later services, also known as Pilot Services, are expected to address atmosphere monitoring, security and climate change. June 2006: the EC establishes the GMES Bureau, with the primary objective of ensuring the delivery of the priority services by 2008. Other objectives of the GMES Bureau are to address the issues of the GMES governance structure and the long-term financial sustainability of the system. May 2007: adoption of the European Space Policy Communication, recognising GMES as a major flagship of the Space Policy. September 2008: official launch of the three FTS services and two Pilot services in their pre-operational version at the occasion of the GMES Forum held in Lille , France . November 2008: the Commission Communication "GMES: We care for a Safer Planet" establishes a basis for further discussions on the financing, operational infrastructure and effective management of GMES. May 2009: the Commission Proposal for a Regulation on "the European Earth Observation Programme (GMES) and its initial operations (2011-2013)" proposes a legal basis for the GMES programme and EC funding of its initial operations. November 2010: the regulation on "the European Earth Observation Programme (GMES) and its initial operations (2011-2013)" entered into force. June 2011: the Commission presents its proposal for the next multiannual financial framework (MFF) corresponding to the period 2014-2020 (Communication "A Budget for Europe 2020"). In this document, the Commission proposes to foresee the funding of the GMES programme outside the multiannual financial framework after 2014. November 2011: The Commission Communication on the "European Earth monitoring programme (GMES) and its operations (from 2014 onwards)" presents the commission's proposals for the future funding, governance and operations of the GMES programme for the period 2014–2020. In particular, the Commission proposes to opt for the creation of a specific GMES fund, similar to the model chosen for the European Development Fund, with financial contributions from all Member States, based on their gross national income (GNI). April 2012: The Emergency Management Service – Mapping ("EMS-Mapping") is declared the first fully operational service within the GMES Initial Operations. [13] December 2012: the Commission announces the name change to Copernicus. October 2014: ESA and European Commission have established a budget for Copernicus Programme covering years 2014-2020 within Multiannual Financial Framework . Budget provided a total of €4.3 billion, including €3.15 billion for ESA to cover operations of the satellite network and construction of the remaining satellites. [14] [15] November 2020: launch of Sentinel-6 Michael Freilich to enable the provision of high-precision and timely observations of the topography of the global ocean January 2021: the regulation (EU) 2021/696 of the European Parliament and of the Council of 28 April 2021 establishing the Union Space Programme entered into force establishing a budget of €5,421 billion under the Multiannual Financial Framework (MFF) corresponding to the period 2021-2027. Earth Observation missions[ edit ] Sentinel missions[ edit ] ESA is currently developing seven missions under the Sentinel programme (Sentinel 1, 2, 3, 4, 5P, 5, 6). The Sentinel missions include radar and super-spectral imaging for land, ocean and atmospheric monitoring. Each Sentinel mission is based on a constellation of two satellites to fulfill and revisit the coverage requirements for each mission, providing robust datasets for all Copernicus services. The Sentinel missions have the following objectives: Sentinel-1 provides all-weather, day and night radar imaging for land and ocean services. [16] Sentinel-1A satellite was successfully launched on 3 April 2014, by a Soyuz , from the Centre Spatial Guyanais . [17] Sentinel-1B satellite was launched on 25 April 2016. Mission declared as ended 3 August 2022. Sentinel-2 provides high-resolution optical imaging for land services (e.g. imagery of vegetation, soil and water cover, inland waterways and coastal areas). [18] Sentinel-2 will also provide information for emergency services. Both satellites launched aboard Vega rockets from Centre Spatial Guyanais . Sentinel-2A , successfully launched on 23 June 2015. [19] Sentinel-2B , followed 7 March 2017. Sentinel-3 provides ocean and global land monitoring services. [20] Both satellites were launched by a Eurockot Rokot vehicle from the Plesetsk Cosmodrome in Russia . [21] [22] Sentinel-3A satellite was launched on 16 February 2016. Sentinel-3B satellite followed on 25 April 2018. Sentinel-4 will provide data for atmospheric composition monitoring as a payload upon a Meteosat Third Generation satellite. [23] It will be launched in 2024. [24] [25] Sentinel-5 Precursor , launched 13 October 2017. [26] The primary purpose of this mission is to reduce the data gap (especially SCIAMACHY atmospheric observations) between the loss of Envisat in 2012, and the launch of Sentinel-5 in 2021. [27] Sentinel-5 will also provide data for atmospheric composition monitoring. [28] It will be embarked on a EUMETSAT Polar System Second Generation ( EPS-SG ) spacecraft and launched in 2021. [25] Sentinel-6 is intended to provide continuity in high precision altimetry sea level measurements following the Jason-3 satellite. [29] Sentinel-6A , was launched in November 2020 by a SpaceX Falcon 9 vehicle from Vandenberg SLC-4E . [30] Sentinel-6B , is scheduled for launch in November 2025 by a SpaceX Falcon 9. [31] In preparation for the second-generation of Copernicus (Copernicus 2.0), six High Priority Candidate "expansion" missions are currently being studied by ESA to address EU Policy and gaps in Copernicus user needs, and to increase the current capabilities of the Copernicus Space Component: Sentinel-7 : Anthropogenic CO2 emissions monitoring (CO2M) [32] Sentinel-8 : High spatio-temporal resolution land surface temperature (LSTM) [33] Sentinel-9 : Copernicus Polar Ice and Snow Topography Altimeter (CRISTAL) [32] Sentinel-10 : Copernicus Hyperspectral Imaging Mission for the Environment (CHIME) [32] Sentinel-11 : Copernicus Imaging Microwave Radiometer (CIMR) [32] Sentinel-12 : Radar Observing System for Europe – L-band SAR (ROSE-L), scheduled for launch no earlier than 2028 [32] [34] Contributing missions[ edit ] Before the Sentinel missions provide data to Copernicus, numerous existing or planned space missions provide or will provide data useful to the provision of Copernicus services. These missions are often referred to as "Copernicus Contributing Missions (CCMs)": ERS : the European Remote Sensing Satellite ERS-1 (1991–2000) was ESA's first Earth observation satellite. ERS-2 (1995–2011) provided data related to ocean surface temperature, winds at sea and atmospheric ozone. Envisat (2002–2012): launched in 2002, ESA's Envisat was the largest civilian Earth Observation spacecraft ever built. It carried sophisticated optical and radar instruments among which the Advanced Synthetic Aperture Radar (ASAR) and the Medium Resolution Imaging Spectrometer (MERIS). Envisat provided continuous observation and monitoring of the Earth's land, atmosphere, oceans and ice caps. After losing contact with the satellite on 8 April 2012, ESA formally announced the end of Envisat's mission on 9 May 2012. [35] Earth Explorers : ESA's Earth Explorers are smaller research missions dedicated to specific aspects of our Earth environment. Earth Explorer missions focus on research of the atmosphere, biosphere, hydrosphere, cryosphere and the Earth's interior with the overall emphasis on learning more about the interactions between these components and the impact that human activity is having on natural Earth processes. The following two of the nine missions selected for implementation currently (as of 2020) contribute to Copernicus: SMOS (Soil Moisture and Ocean Salinity), launched on 2 November 2009. CryoSat-2 (the measurement of the thickness of floating ice), launched on 8 April 2010. MSG : the Meteosat Second Generation is a joint project between ESA and EUMETSAT. MetOp : MetOp is Europe's first polar-orbiting satellite dedicated to operational meteorology. MetOp is a series of three satellites launched sequentially over 12 years from October 2006 to November 2018. The series provides data for both operational meteorology and climate studies. French SPOT : SPOT (Satellite Pour l'Observation de la Terre) consists of a series of earth observation satellites providing high-resolution images of the Earth. SPOT-4 and SPOT-5 include sensors called VEGETATION able to monitor continental ecosystems. German TerraSAR-X : TerraSAR-X is an Earth observation satellite providing high quality topographic information. TerraSAR-X data has a wide range of applications (e.g. land use / land cover mapping, topographic mapping, forest monitoring, emergency response monitoring, and environmental monitoring ). Italian COSMO-SkyMed : the COnstellation of small Satellites for the Mediterranean basin Observation is an Earth observation satellite system that consists of (in the 1st generation) four satellites equipped with Synthetic-aperture radar (SAR) sensors. Applications include seismic hazard analysis, environmental disaster monitoring and agricultural mapping. As of 2020, a second-generation of COSMO-SkyMed satellites (called Cosmo-Skymed 2nd generation) is under development. UK and international DMC : the Disaster Monitoring Constellation (DMC) is a constellation of remote-sensing satellites. There have been eight satellites in the DMC-program; 3 are currently (as of 2020) active. The constellation provides emergency Earth imaging for disaster relief under the International Charter for Space and Major Disasters. French-American OSTM/Jason-2 (2008-2019): the OSTM/JASON-2 satellite provided precise measurements of ocean surface topography, surface wind speed, and wave height; as this type of measurement is a crucial requirement for the Copernicus Marine Services, the European Commission has included this type of mission in its latest communication on the future Copernicus Space Component as Sentinel-6. French Pléiades : the Pléiades constellation consists of two satellites providing very high-resolution images of the Earth. Planet Labs , a commercial satellite imagery provider whose goal is to image the entirety of the planet daily to monitor changes and pinpoint trends. OroraTech , a Germany-based commercial earth observation provider focussed on wildfire situational awareness, is delivering its FOREST-2 thermal-infrared data (MWIR, 2x LWIR). [36] Data provided by non-European satellite missions (e.g. Landsat , GOSAT , Radarsat-2 ) can also be used by Copernicus. DigitalGlobe , an American commercial vendor of space imagery and geospatial content, provides imagery from satellites with a true maximum resolution of up to 25 cm. The DigitalGlobe tasking constellation currently includes GeoEye-1 , WorldView-1 , WorldView-2 and WorldView-3 . Archive data is also available from Ikonos and QuickBird . LANDSAT program (8 satellites, 3 active). GOSAT program (2 satellites, 2 active). In-Situ Coordination[ edit ] GMES In-Situ Coordination (GISC) was a FP7 funded initiative, lasted for three years (January 2010 – December 2012) and was coordinated by the European Environment Agency (EEA). Since 2014 EEA has been responsible for Copernicus In-Situ coordination under the Contribution Agreement between the EU (represented by the European Commission) and the EEA, signed 1 December 2014. In situ data are all data from sources other than Earth observation satellites. Consequently, all ground-based, air-borne, and ship/buoy-based observations and measurements that are needed to implement and operate the Copernicus services are part of the in-situ component. In-situ data are indispensable; they are assimilated into forecasting models, provide calibration and validation of space-based information, and contribute to analysis or filling gaps not available from space sources. GISC was undertaken with reference to other initiatives, such as INSPIRE (Infrastructure for Spatial Information in the European Community) and SEIS (Shared Environmental Information System) as well as existing coordination and data exchange networks. The coordinated access to data retains the capacity to link directly data providers and the service providers because it is based on the principles of SEIS and INSPIRE. The implementation of INSPIRE is embedded in the synergies and meta-data standards that were used in GISC. Data and information aims to be managed as close as possible to its source in order to achieve a distributed system, by involving countries and existing capacities that maintain and operate the required observation infrastructure. Services component[ edit ] Copernicus services are dedicated to the monitoring and forecasting of the Earth's subsystems. They contribute directly to the monitoring of climate change. Copernicus services also address emergency management (e.g. in case of natural disaster, technological accidents or humanitarian crises) and security-related issues (e.g. maritime surveillance, border control). Copernicus services address six main thematic areas: Emergency Management Service (see video available on the Copernicus.eu website: Copernicus Emergency Management Service ). The service was declared operational on 1 April 2012. Land Monitoring (see video available on the Copernicus.eu website: Copernicus Land Monitoring Service ). The service was declared operational on 1 April 2012. Marine Environment Monitoring (see video available on the Copernicus.eu website: Copernicus Marine Environment Monitoring Service ). The service was declared operational on 1 May 2015. Atmosphere Monitoring (see video available on the Copernicus.eu website: Copernicus Atmosphere Monitoring Service ). The service was declared operational in July 2015. Climate Change (see video available on the Copernicus.eu website: Copernicus Climate Change Monitoring Service ) The development of the pre-operational version of the services has been realised by a series of projects launched by the European Commission and partly funded through the EU's 7th Framework Programme (FP7). These projects were geoland2 (land), MyOcean (marine), SAFER (emergency response), MACC and its successor MACC II (atmosphere) and G-MOSAIC (security). Most of these projects also contributed to the monitoring of Climate Change. geoland2 started on 1 September 2008. The project covered a wide range of domains such as land use, land cover change, soil sealing , water quality and availability, spatial planning, forest management , carbon storage and global food security . MyOcean started on 1 January 2009. It covered themes such as maritime security, oil spill prevention , marine resource management, climate change , seasonal forecast, coastal activities, ice survey and water pollution . SAFER started on 1 January 2009. The project addressed three main domains: civil protection, humanitarian aid and Security crises management. MACC started on 1 June 2009. The project continued and refined the products developed in the projects GEMS and PROMOTE . A second phase (MACC II) lasted until July 2014 allowing the now operational Copernicus atmospheric monitoring service (CAMS, see above). GMOSAIC started on 1 January 2009. Together with the LIMES project Wayback Machine (co-funded by the European Commission under FP6), GMOSAIC specifically dealt with the Security domain of Copernicus addressing topics such as Support to Intelligence and Early Warning and Support to Crisis Management Operations. Interaction[ edit ] "The information provided by the Copernicus services can be used by end-users for a wide range of applications in a variety of areas. These include urban area management, sustainable development and nature protection, regional and local planning, agriculture, forestry and fisheries, health, civil protection, infrastructure, transport and mobility, as well as tourism". [4] Copernicus is the European Union 's contribution to the Global Earth Observation System of Systems (GEOSS) thus delivering geospatial information globally. Some Copernicus services make use of OpenStreetMap data in their maps production. [37] Other relevant initiatives[ edit ] Other initiatives will also facilitate the development and functioning of Copernicus services: INSPIRE : this initiative aims at building a European spatial data infrastructure beyond national boundaries. Urban Atlas: Compiled from thousands of satellite photographs, the Urban Atlas provides detailed and cost-effective digital mapping, ensuring that city planners have the most up-to-date and accurate data available on land use and land cover. The Urban Atlas will enable urban planners to better assess risks and opportunities, ranging from the threat of flooding and the impact of climate change, to identifying new infrastructure and public transport needs. All cities in the EU will be covered by the Urban Atlas by 2011. SEIS : The Shared Environmental Information System (SEIS) is a collaborative initiative of the European Commission and the European Environment Agency (EEA) to establish together with the Member States an integrated and shared EU-wide environmental information system. Heterogeneous Missions Accessibility , the European Space Agency initiative for interoperability of Earth observation satellite payload data ground segments. Copernicus is one of three related initiatives that are the subject of the GIGAS ( GEOSS , INSPIRE and GMES an Action in Support) harmonization and analysis project [38] under the auspices of the EU 7th Framework Programme . [39] Third country participation[ edit ] In addition to the 27 Member States of the European Union, the Copernicus programme allows for the participation at various scope for third country participation. This participation is conducted through agreements with the European Union. One has to distinguish those countries that contribute to the budget and those that agree on exchanging data with the program. Many international partner countries get special access to Sentinel data in exchange for sharing in-situ data from their country. These states are: 2014–2020 budget contributing countries
Toggle the table of contents Moderate Resolution Imaging Spectroradiometer "MODIS" redirects here. For the singular, see Modi . For other uses, see Modis . Ash plumes on Kamchatka Peninsula , eastern Russia. Hurricane Katrina near Florida peninsula. California wildfires. Solar irradiance spectrum and MODIS bands. External view of the MODIS unit. Exploded view of the MODIS subsystems. This detailed, photo-like view of Earth is based largely on observations from MODIS. The Moderate Resolution Imaging Spectroradiometer (MODIS) is a satellite-based sensor used for earth and climate measurements. There are two MODIS sensors in Earth orbit : one on board the Terra ( EOS AM) satellite, launched by NASA in 1999; and one on board the Aqua (EOS PM) satellite, launched in 2002. MODIS has now been replaced by the VIIRS ,[ citation needed ] which first launched in 2011 aboard the Suomi NPP satellite. The MODIS instruments were built by Santa Barbara Remote Sensing. [1] They capture data in 36 spectral bands ranging in wavelength from 0.4 μm to 14.4 μm and at varying spatial resolutions (2 bands at 250 m, 5 bands at 500 m and 29 bands at 1 km). Together the instruments image the entire Earth every 1 to 2 days. They are designed to provide measurements in large-scale global dynamics including changes in Earth's cloud cover , radiation budget and processes occurring in the oceans, on land, and in the lower atmosphere . Support and calibration is provided by the MODIS characterization support team (MCST). [2] Applications[ edit ] This section needs expansion. You can help by adding to it . (September 2014) With its high temporal resolution although low spatial resolution, MODIS data are useful to track changes in the landscape over time. Examples of such applications are the monitoring of vegetation health by means of time-series analyses with vegetation indices, [3] long term land cover changes (e.g. to monitor deforestation rates), [4] [5] [6] [7] global snow cover trends, [8] [9] water inundation from pluvial, riverine, or sea level rise flooding in coastal areas, [10] change of water levels of major lakes such as the Aral Sea , [11] [12] and the detection and mapping of wildland fires in the United States. [13] The United States Forest Service 's Remote Sensing Applications Center analyzes MODIS imagery on a continuous basis to provide information for the management and suppression of wildfires. [14] Specifications Orbit 705 km, 10:30 a.m. descending node (Terra) or 1:30 p.m. ascending node (Aqua), Sun-synchronous, near-polar, circular Scan rate
Toggle the table of contents Weather satellite From Wikipedia, the free encyclopedia Type of satellite designed to record the state of the Earth's atmosphere Not to be confused with Atmospheric satellite . GOES-16, a United States weather satellite of the meteorological-satellite service A weather satellite or meteorological satellite is a type of Earth observation satellite that is primarily used to monitor the weather and climate of the Earth.  Satellites can be polar orbiting (covering the entire Earth asynchronously), or geostationary (hovering over the same spot on the equator ). [1] While primarily used to detect the development and movement of storm systems and other cloud patterns, meteorological satellites can also detect other phenomena such as city lights, fires, effects of pollution, auroras , sand and dust storms , snow cover, ice mapping, boundaries of ocean currents , and energy flows. Other types of environmental information are collected using weather satellites. Weather satellite images helped in monitoring the volcanic ash cloud from Mount St. Helens and activity from other volcanoes such as Mount Etna . [2] Smoke from fires in the western United States such as Colorado and Utah have also been monitored. El Niño and its effects on weather are monitored daily from satellite images.  The Antarctic ozone hole is mapped from weather satellite data.  Collectively, weather satellites flown by the U.S., Europe, India, China, Russia, and Japan provide nearly continuous observations for a global weather watch. Further information: First images of Earth from space The first television image of Earth from space from the TIROS-1 weather satellite in 1960 A mosaic of photographs of the United States from the ESSA-9 weather satellite, taken on June 26, 1969 As early as 1946, the idea of cameras in orbit to observe the weather was being developed.  This was due to sparse data observation coverage and the expense of using cloud cameras on rockets.  By 1958, the early prototypes for TIROS and Vanguard (developed by the Army Signal Corps) were created. [3] The first weather satellite, Vanguard 2 , was launched on February 17, 1959. [4] It was designed to measure cloud cover and resistance, but a poor axis of rotation and its elliptical orbit kept it from collecting a notable amount of useful data.  The Explorer VI and VII satellites also contained weather-related experiments. [3] The first weather satellite to be considered a success was TIROS-1 , launched by NASA on April 1, 1960. [5] TIROS operated for 78 days and proved to be much more successful than Vanguard 2.  TIROS paved the way for the Nimbus program , whose technology and findings are the heritage of most of the Earth-observing satellites NASA and NOAA have launched since then.  Beginning with the Nimbus 3 satellite in 1969, temperature information through the tropospheric column began to be retrieved by satellites from the eastern Atlantic and most of the Pacific Ocean, which led to significant improvements to weather forecasts . [6] The ESSA and NOAA polar orbiting satellites followed suit from the late 1960s onward.  Geostationary satellites followed, beginning with the ATS and SMS series in the late 1960s and early 1970s, then continuing with the GOES series from the 1970s onward.  Polar orbiting satellites such as QuikScat and TRMM began to relay wind information near the ocean's surface starting in the late 1970s, with microwave imagery which resembled radar displays, which significantly improved the diagnoses of tropical cyclone strength, intensification, and location during the 2000s and 2010s. The DSCOVR satellite, owned by NOAA, was launched in 2015 and became the first deep space satellite that can observe and predict space weather. It can detect potentially dangerous weather such as solar wind and geomagnetic storms . This is what has given humanity the capability to make accurate and preemptive space weather forecasts since the late 2010s. [7] In Europe, the first Meteosat geostationary operational meteorological satellite, Meteosat-1, was launched in 1977 on a Delta launch vehicle. The satellite was a spin-stabilised cylindrical design, 2.1m in diameter and 3.2m tall, rotating at approx. 100 rpm and carrying the Meteosat Visible and Infrared Imager (MVIRI) instrument. Successive Meteosat first generation satellites were launched, on European Ariane-4 launchers from Kourou in French Guyana, up to and including Meteosat-7 which acquired data from 1997 until 2017, operated initially by the European Space Agency and later, from 1995, by the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT). The Meteosat Second Generation (MSG) satellites - also spin stabilised although physically larger and twice the mass of the first generation - were developed by ESA with European industry and in cooperation with EUMETSAT who then operate the satellites from their headquarters in Darmstadt, Germany with this same approach followed for all subsequent European meteorological satellites. Meteosat-8, the first MSG satellite, was launched in 2002 on an Ariane-5 launcher, carrying the Spinning Enhanced Visible and Infrared Imager (SEVIRI) and Geostationary Earth Radiation Budget (GERB) instruments, along with payloads to support the COSPAS-SARSAT Search and Rescue (SAR) and ARGOS Data Collection Platform (DCP) missions. SEVIRI provided an increased number of spectral channels over MVIRI and imaged the full-Earth disc at double the rate. Meteosat-9 was launched to complement Meteosat-8 in 2005, with the second pair consisting of Meteosat-10 and Meteosat-11 launched in 2012 and 2015, respectively. The Meteosat Third Generation (MTG) programme launched its first satellite in 2022, and featured a number of changes over its predecessors in support of its mission to gather data for weather forecasting and climate monitoring. The MTG satellites are three-axis stabilised rather than spin stabilised, giving greater flexibility in satellite and instrument design. The MTG system features separate Imager and Sounder satellite models that share the same satellite bus, with a baseline of three satellites - two Imagers and one Sounder - forming the operational configuration. The imager satellites carry the Flexible Combined Imager (FCI), succeeding MVIRI and SEVIRI to give even greater resolution and spectral coverage, scanning the full Earth disc every ten minutes, as well as a new Lightning Imager (LI) payload. The sounder satellites carry the Infrared Sounder (IRS) and Ultra-violet Visible Near-infrared (UVN) instruments. UVN is part of the European Commission 's Copernicus programme and fulfils the Sentinel-4 mission to monitor air quality, trace gases and aerosols over Europe hourly at high spatial resolution. Two MTG satellites - one Imager and one Sounder - will operate in close proximity from the 0-deg geostationary location over western Africa to observe the eastern Atlantic Ocean, Europe, Africa and the Middle East, while a second imager satellite will operate from 9.5-deg East to perform a Rapid Scanning mission over Europe. MTG continues Meteosat support to the ARGOS and Search and Rescue missions. MTG-I1 launched in one of the last Ariane-5 launches, with the subsequent satellites planned to launch in Ariane-6 when it enters service. In 2006, the first European low-Earth orbit operational meteorological satellite, Metop -A was launched into a Sun-synchronous orbit at 817 km altitude by a Soyuz launcher from Baikonur, Kazakhstan. This operational satellite - which forms the space segment of the Eumetsat Polar System (EPS) - built on the heritage from ESA's ERS and Envisat experimental missions, and was followed at six-year intervals by Metop-B and Metop-C - the latter launched from French Guyana in a "Europeanised" Soyuz. Each carry thirteen different passive and active instruments ranging in design from imagers and sounders to a scatterometer and a radio-occultation instrument. The satellite service module is based on the SPOT-5 bus, while the payload suite is a combination of new and heritage instruments from both Europe and the US under the Initial Joint Polar System agreement between EUMETSAT and NOAA. A second generation of Metop satellites (Metop-SG) is in advanced development with launch of the first satellite foreseen in 2025. As with MTG, Metop-SG will launch on Ariane-6 and comprise two satellite models to be operated in pairs in replacement of the single first generation satellites to continue the EPS mission. Observation[ edit ] These meteorological-satellite service , however, see more than clouds and cloud systems Observation is typically made via different 'channels' of the electromagnetic spectrum , in particular, the visible and infrared portions. Some of these channels include: [8] [9] Visible and Near Infrared: 0.6–1.6 μm – for recording cloud cover during the day Infrared: 3.9–7.3 μm (water vapor), 8.7–13.4 μm (thermal imaging) Visible spectrum[ edit ] Visible-light images from weather satellites during local daylight hours are easy to interpret even by the average person, clouds, cloud systems such as fronts and tropical storms, lakes, forests, mountains, snow ice, fires, and pollution such as smoke, smog, dust and haze are readily apparent.  Even wind can be determined by cloud patterns, alignments and movement from successive photos. [10] Infrared spectrum[ edit ] The thermal or infrared images recorded by sensors called scanning radiometers enable a trained analyst to determine cloud heights and types, to calculate land and surface water temperatures, and to locate ocean surface features. Infrared satellite imagery can be used effectively for tropical cyclones with a visible eye pattern, using the Dvorak technique , where the difference between the temperature of the warm eye and the surrounding cold cloud tops can be used to determine its intensity (colder cloud tops generally indicate a more intense storm). [11] Infrared pictures depict ocean eddies or vortices and map currents such as the Gulf Stream which are valuable to the shipping industry. Fishermen and farmers are interested in knowing land and water temperatures to protect their crops against frost or increase their catch from the sea. Even El Niño phenomena can be spotted. Using color-digitized techniques, the gray shaded thermal images can be converted to color for easier identification of desired information. The geostationary Himawari 8 satellite's first true-colour composite PNG image The geostationary GOES-17 satellite's Level 1B Calibrated Radiances - True Colour Composite PNG image Each meteorological satellite is designed to use one of two different classes of orbit: geostationary and polar orbiting . Geostationary[ edit ] "geostationary meteorological satellite" redirects here. For the Japanese satellites called "Geostationary Meteorological Satellite", see Himawari (satellite) . Geostationary weather satellites orbit the Earth above the equator at altitudes of 35,880 km (22,300 miles).  Because of this orbit , they remain stationary with respect to the rotating Earth and thus can record or transmit images of the entire hemisphere below continuously with their visible-light and infrared sensors. The news media use the geostationary photos in their daily weather presentation as single images or made into movie loops. These are also available on the city forecast pages of www.noaa.gov (example Dallas, TX). [12] Several geostationary meteorological spacecraft are in operation. The United States' GOES series has three in operation: GOES-15 , GOES-16 and GOES-17 . GOES-16 and-17 remain stationary over the Atlantic and Pacific Oceans, respectively. [13] GOES-15 was retired in early July 2019. [14] The satellite GOES 13 that was previously owned by the National Oceanic and Atmospheric Association (NOAA) was transferred to the U.S. Space Force in 2019 and renamed the EWS-G1; becoming the first geostationary weather satellite to be owned and operated by the U.S. Department of Defense. [15] Russia 's new-generation weather satellite Elektro-L No.1 operates at 76°E over the Indian Ocean. The Japanese have the MTSAT -2 located over the mid Pacific at 145°E and the Himawari 8 at 140°E. The Europeans have four in operation, Meteosat -8 (3.5°W) and Meteosat-9 (0°) over the Atlantic Ocean and have Meteosat-6 (63°E) and Meteosat-7 (57.5°E) over the Indian Ocean. China currently has four Fengyun (风云) geostationary satellites (FY-2E at 86.5°E, FY-2F at 123.5°E, FY-2G at 105°E and FY-4A at 104.5 °E) operated. [16] India also operates geostationary satellites called INSAT which carry instruments for meteorological purposes. Polar orbiting[ edit ] Computer-controlled motorized parabolic dish antenna for tracking LEO weather satellites. Polar orbiting weather satellites circle the Earth at a typical altitude of 850 km (530 miles) in a north to south (or vice versa) path, passing over the poles in their continuous flight.  Polar orbiting weather satellites are in sun-synchronous orbits , which means they are able to observe any place on Earth and will view every location twice each day with the same general lighting conditions due to the near-constant local solar time . Polar orbiting weather satellites offer a much better resolution than their geostationary counterparts due their closeness to the Earth. The United States has the NOAA series of polar orbiting meteorological satellites, presently NOAA-15, NOAA-18 and NOAA-19 ( POES ) and NOAA-20 ( JPSS ). Europe has the Metop -A, Metop -B and Metop -C satellites operated by EUMETSAT . Russia has the Meteor and RESURS series of satellites. China has FY -3A, 3B and 3C. India has polar orbiting satellites as well. DMSP[ edit ] Turnstile antenna for reception of 137 MHz LEO weather satellite transmissions The United States Department of Defense 's Meteorological Satellite ( DMSP ) can "see" the best of all weather vehicles with its ability to detect objects almost as 'small' as a huge oil tanker .  In addition, of all the weather satellites in orbit, only DMSP can "see" at night in the visual.  Some of the most spectacular photos have been recorded by the night visual sensor; city lights, volcanoes , fires, lightning, meteors , oil field burn-offs, as well as the Aurora Borealis and Aurora Australis have been captured by this 720 kilometres (450 mi) high space vehicle's low moonlight sensor. At the same time, energy use and city growth can be monitored since both major and even minor cities, as well as highway lights, are conspicuous.  This informs astronomers of light pollution . The New York City Blackout of 1977 was captured by one of the night orbiter DMSP space vehicles. In addition to monitoring city lights, these photos are a life saving asset in the detection and monitoring of fires.  Not only do the satellites see the fires visually day and night, but the thermal and infrared scanners on board these weather satellites detect potential fire sources below the surface of the Earth where smoldering occurs.  Once the fire is detected, the same weather satellites provide vital information about wind that could fan or spread the fires.  These same cloud photos from space tell the firefighter when it will rain. Some of the most dramatic photos showed the 600 Kuwaiti oil fires that the fleeing Army of Iraq started on February 23, 1991.  The night photos showed huge flashes, far outstripping the glow of large populated areas.  The fires consumed huge quantities of oil; the last was doused on November 6, 1991. Uses[ edit ] Infrared image of storms over the central United States from the GOES-17 satellite Snowfield monitoring, especially in the Sierra Nevada , can be helpful to the hydrologist keeping track of available snowpack for runoff vital to the watersheds of the western United States.  This information is gleaned from existing satellites of all agencies of the U.S. government (in addition to local, on-the-ground measurements).  Ice floes, packs, and bergs can also be located and tracked from weather spacecraft. Even pollution whether it is nature-made or human-made can be pinpointed.  The visual and infrared photos show effects of pollution from their respective areas over the entire earth.  Aircraft and rocket pollution, as well as condensation trails , can also be spotted.  The ocean current and low level wind information gleaned from the space photos can help predict oceanic oil spill coverage and movement. Almost every summer, sand and dust from the Sahara Desert in Africa drifts across the equatorial regions of the Atlantic Ocean.  GOES-EAST photos enable meteorologists to observe, track and forecast this sand cloud.  In addition to reducing visibilities and causing respiratory problems, sand clouds suppress hurricane formation by modifying the solar radiation balance of the tropics. Other dust storms in Asia and mainland China are common and easy to spot and monitor, with recent examples of dust moving across the Pacific Ocean and reaching North America. In remote areas of the world with few local observers, fires could rage out of control for days or even weeks and consume huge areas before authorities are alerted.  Weather satellites can be a valuable asset in such situations. Nighttime photos also show the burn-off in gas and oil fields.  Atmospheric temperature and moisture profiles have been taken by weather satellites since 1969. [17]
Toggle the table of contents Imaging radar Application of radar which is used to create two-dimensional images Not to be confused with Radar display . A SAR radar image acquired by the SIR-C/X-SAR radar on board the Space Shuttle Endeavour shows the Teide volcano. The city of Santa Cruz de Tenerife is visible as the purple and white area on the lower right edge of the island. Lava flows at the summit crater appear in shades of green and brown, while vegetation zones appear as areas of purple, green and yellow on the volcano's flanks. Imaging radar is an application of radar which is used to create two-dimensional images , typically of landscapes. Imaging radar provides its light to illuminate an area on the ground and take a picture at radio wavelengths. It uses an antenna and digital computer storage to record its images. In a radar image, one can see only the energy that was reflected back towards the radar antenna. The radar moves along a flight path and the area illuminated by the radar, or footprint, is moved along the surface in a swath, building the image as it does so. [1] Digital radar images are composed of many dots.  Each pixel in the radar image represents the radar backscatter for that area on the ground ( terrain return ): brighter areas represent high backscatter, darker areas represents low backscatter. [1] The traditional application of radar is to display the position and motion of typically highly reflective objects (such as aircraft or ships ) by sending out a radiowave signal, and then detecting the direction and delay of the reflected signal. Imaging radar on the other hand attempts to form an image of one object (e.g. a landscape) by furthermore registering the intensity of the reflected signal to determine the amount of scattering . The registered electromagnetic scattering is then mapped onto a two-dimensional plane, with points with a higher reflectivity getting assigned usually a brighter color, thus creating an image. Several techniques have evolved to do this. Generally they take advantage of the Doppler effect caused by the rotation or other motion of the object and by the changing view of the object brought about by the relative motion between the object and the back-scatter that is perceived by the radar of the object (typically, a plane) flying over the earth. Through recent improvements of the techniques, radar imaging is getting more accurate. Imaging radar has been used to map the Earth, other planets, asteroids, other celestial objects and to categorize targets for military systems. Description[ edit ] An imaging radar is a kind of radar equipment which can be used for imaging. A typical radar technology includes emitting radio waves, receiving their reflection, and using this information to generate data. For an imaging radar, the returning waves are used to create an image. When the radio waves reflect off objects, this will make some changes in the radio waves and can provide data about the objects, including how far the waves traveled and what kind of objects they encountered. Using the acquired data, a computer can create a 3-D or 2-D image of the target. [2] Imaging radar has several advantages. [3] It can operate in the presence of obstacles that obscure the target, and can penetrate ground (sand), water, or walls. [4] [5] Applications[ edit ] Applications include: surface topography & coastal change; land use monitoring, agricultural monitoring, ice patrol, environmental monitoring ;weather radar- storm monitoring, wind shear warning;medical microwave tomography; [5] through wall radar imaging; [6] 3-D measurements, [7] etc. Through wall radar imaging[ edit ] Wall parameter estimation uses Ultra Wide-Band radar systems.  The handle M-sequence UWB radar with horn and circular antennas was used for data gathering and supporting the scanning method. [6] 3-D measurements[ edit ] 3-D measurements are supplied by amplitude-modulated laser radars—Erim sensor and Perceptron sensor.  In terms of speed and reliability for median-range operations, 3-D measurements have superior performance. [7] Techniques and methods[ edit ] Current radar imaging techniques rely mainly on synthetic aperture radar (SAR) and inverse synthetic aperture radar (ISAR) imaging.  Emerging technology utilizes monopulse radar 3-D imaging. Real aperture radar[ edit ] Real aperture radar (RAR) is a form of radar that transmits a narrow angle beam of pulse radio wave in the range direction at right angles to the flight direction and receives the backscattering from the targets which will be transformed to a radar image from the received signals. Usually the reflected pulse will be arranged in the order of return time from the targets, which corresponds to the range direction scanning. The resolution in the range direction depends on the pulse width.  The resolution in the azimuth direction is identical to the multiplication of beam width and the distance to a target. [8] AVTIS radar[ edit ] The AVTIS radar is a 94 GHz real aperture 3D imaging radar.  It uses Frequency-Modulated Continuous-Wave modulation and employs a mechanically scanned monostatic with sub-metre range resolution. [9] Main article: Lidar Laser radar is a remote sensing technology that measures distance by illuminating a target with a laser and analyzing the reflected light. [10] Laser radar is used for multi-dimensional imaging and information gathering.  In all information gathering modes, lasers that transmit in the eye-safe region are required as well as sensitive receivers at these wavelengths. [11] 3-D imaging requires the capacity to measure the range to the first scatter within every pixel. Hence, an array of range counters is needed.  A monolithic approach to an array of range counters is being developed.  This technology must be coupled with highly sensitive detectors of eye-safe wavelengths. [11] To measure Doppler information requires a different type of detection scheme than is used for spatial imaging. The returned laser energy must be mixed with a local oscillator in a heterodyne system to allow extraction of the Doppler shift. [11] Synthetic aperture radar (SAR)[ edit ] Main article: Synthetic aperture radar Synthetic-aperture radar (SAR) is a form of radar which moves a real aperture or antenna through a series of positions along the objects to provide distinctive long-term coherent-signal variations. This can be used to obtain higher resolution. SARs produce a two-dimensional (2-D) image. One dimension in the image is called range and is a measure of the "line-of-sight" distance from the radar to the object. Range is determined by measuring the time from transmission of a pulse to receiving the echo from a target. Also, range resolution is determined by the transmitted pulse width. The other dimension is called azimuth and is perpendicular to range. The ability of SAR to produce relatively fine azimuth resolution makes it different from other radars. To obtain fine azimuth resolution, a physically large antenna is needed to focus the transmitted and received energy into a sharp beam. The sharpness of the beam defines the azimuth resolution. An airborne radar could collect data while flying this distance and process the data as if it came from a physically long antenna. The distance the aircraft flies in synthesizing the antenna is known as the synthetic aperture. A narrow synthetic beam width results from the relatively long synthetic aperture, which gets finer resolution than a smaller physical antenna. [12] Inverse aperture radar (ISAR)[ edit ] Main article: Inverse synthetic aperture radar Inverse synthetic aperture radar (ISAR) is another kind of SAR system which can produce high-resolution on two- and three-dimensional images. An ISAR system consists of a stationary radar antenna and a target scene that is undergoing some motion. ISAR is theoretically equivalent to SAR in that high-azimuth resolution is achieved via relative motion between the sensor and object, yet the ISAR moving target scene is usually made up of non cooperative objects. Algorithms with more complex schemes for motion error correction are needed for ISAR imaging than those needed in SAR. ISAR technology uses the movement of the target rather than the emitter to make the synthetic aperture. ISAR radars are commonly used on vessels or aircraft and can provide a radar image of sufficient quality for target recognition. The ISAR image is often adequate to discriminate between various missiles, military aircraft, and civilian aircraft. [13] Disadvantages of ISAR[ edit ] The ISAR imaging cannot obtain the real azimuth of the target There sometimes exists a reverse image. For example, the image formed of a boat when it rolls forwards and backwards in the ocean.[ clarification needed ] The ISAR image is the 2-D projection image of the target on the Range-Doppler plane which is perpendicular to the rotating axis. When the Range-Doppler plane and the coordinate plane are different, the ISAR image can not reflect the real shape of the target. Thus, the ISAR imaging can not obtain the real shape information of the target in most situations. [13] Rolling is side to side.  Pitching is forward and backwards, yawing is turning left or right. Monopulse radar 3-D imaging technique[ edit ] Main article: Monopulse radar Monopulse radar 3-D imaging technique uses 1-D range image and monopulse angle measurement to get the real coordinates of each scatterer. Using this technique, the image doesn't vary with the change of the target's movement.  Monopulse radar 3-D imaging utilizes the ISAR techniques to separate scatterers in the Doppler domain and perform monopulse angle measurement. Monopulse radar 3-D imaging can obtain the 3 views of 3-D objects by using any two of the three parameters obtained from the azimuth difference beam, elevation difference beam and range measurement, which means the views of front, top and side can be azimuth-elevation, azimuth-range and elevation-range, respectively. Monopulse imaging generally adapts to near-range targets, and the image obtained by monopulse radar 3-D imaging is the physical image which is consistent with the real size of the object. [14] This article may have been created or edited in return for undisclosed payments, a violation of Wikipedia's terms of use . It may require cleanup to comply with Wikipedia's content policies , particularly neutral point of view . (May 2021) 4D imaging radar[ edit ] 4D imaging radar leverages a Multiple Input Multiple Output (MiMo) antenna array for high-resolution detection, mapping and tracking of multiple static and dynamic targets simultaneously. It combines 3D imaging with Doppler analysis to create the additional dimension – velocity. [15] A 60GHz 4D imaging radar sensor from Vayyar Imaging. A 4D imaging radar system measures the time of flight from each transmitting (Tx) antenna to a target and back to each receiving (Rx) antenna, processing data from the numerous ellipsoids formed. The point at which the ellipsoids intersect – known as a hot spot - reveals the exact position of a target at any given moment. Its versatility and reliability make 4D imaging radar ideal for smart home, automotive, retail, security, healthcare and many other environments. The technology is valued for combining all the benefits of camera, LIDAR, thermal imaging and ultrasonic technologies, with additional benefits: Resolution: the large MiMo antenna array enables accurate detection and tracking of multiple static and dynamic targets simultaneously. Cost efficiency: 4D imaging radar costs around the same as a 2D radar sensor, but with immense added value: richer data, higher accuracy and more functionality, while offering an optimal price-performance balance. Robustness and privacy: There are no optics involved, so this technology is robust in all lighting and weather conditions. 4D imaging radar does not require line of sight with targets, enabling its operation in darkness, smoke, steam, glare and inclement weather. It also ensures privacy [ dubious – discuss ] and discreet surveillance by design, an increasingly important concern across all industries.
