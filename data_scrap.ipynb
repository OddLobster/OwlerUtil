{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d9575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from resiliparse.parse.html import HTMLTree\n",
    "from urllib.request import Request, urlopen\n",
    "from resiliparse.parse.encoding import detect_encoding\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "\n",
    "# from chatnoir_api import Index\n",
    "# from chatnoir_api.v1 import search\n",
    "from ast import literal_eval\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import os.path\n",
    "import func_timeout\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e58f8-70b7-4b32-bf40-0b7a36ef88d5",
   "metadata": {},
   "source": [
    "## Function Workflow\n",
    "\n",
    "The function follows these steps:\n",
    "\n",
    "1. **Accessing the URL**:\n",
    "   - It attempts to access the web page at the provided `url` using the `urlopen` function from the `urllib` library.\n",
    "   - If there are any issues accessing the URL (e.g., network errors), it returns \"not_accessible.\"\n",
    "\n",
    "2. **Parsing HTML**:\n",
    "   - Once the web page is successfully accessed, the HTML content is parsed using the `HTMLTree` library, which allows for efficient HTML parsing.\n",
    "   - The encoding of the HTML content is detected using the `detect_encoding` function to ensure proper parsing.\n",
    "\n",
    "3. **Extracting Text Content**:\n",
    "   - The function extracts the main plain text content from the parsed HTML. Several parameters can be adjusted:\n",
    "     - `main_content`: Determines whether to extract the main content of the page.\n",
    "     - `alt_texts`: Specifies whether to include alternate texts (e.g., image descriptions).\n",
    "     - `preserve_formatting`: Indicates whether to preserve text formatting (e.g., line breaks).\n",
    "     - `noscript`: Controls the extraction of content within `<noscript>` tags, which may\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f8fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_uri(url):\n",
    "    try:\n",
    "        html = urlopen(url).read()\n",
    "        \n",
    "    except:\n",
    "        return \"not_accessible\"\n",
    "    \n",
    "    tree = HTMLTree.parse_from_bytes(html, detect_encoding(html))\n",
    "    text = extract_plain_text(tree,\n",
    "                             main_content=True,\n",
    "                             alt_texts=False,\n",
    "                             preserve_formatting=False,\n",
    "                             noscript=True)\n",
    "    return text \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df48e2-1dc9-41f4-9fbf-d025f3b4f8e0",
   "metadata": {},
   "source": [
    ").\n",
    "\n",
    "- **Input**:\n",
    "  - `url`: The URL of the web page from which text content is to be extracted.\n",
    "  - `max_wait`: The maximum time (in seconds) to wait for the text retrieval process (default is 5 seconds).\n",
    "  - `default_value`: The value to return if the retrieval process times out (default is \"timeout\").\n",
    "\n",
    "- **Output**:\n",
    "  - The function returns the retrieved text content from the URI if the retrieval is successful within the specified timeout. If the retrieval times out, it returns the `default_value`.\n",
    "\n",
    "## Function Workflow\n",
    "\n",
    "The function follows these steps:\n",
    "\n",
    "1. **Timeout Control**:\n",
    "   - It uses the `func_timeout` library to control the execution time of the `get_text_from_uri` function, which retrieves text content from the provided `url`.\n",
    "   - The `max_wait` parameter specifies the maximum waiting time for the retrieval process.\n",
    "\n",
    "2. **Handling Timeout**:\n",
    "   - If the retrieval process takes longer than the specified `max_wait` duration, the `func_timeout.FunctionTimedOut` exception is raised.\n",
    "   - In this case, the function catches the exception and returns the `default_value`.\n",
    "\n",
    "3. **Returning Text Content**:\n",
    "   - If the retrieval process completes successfully within the specified timeout, the function returns nism to prevent long delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfd8297-640d-45a0-a66e-69f57430d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctrl_timeout(url, max_wait = 5, default_value = \"timeout\"):\n",
    "    try:\n",
    "        return func_timeout.func_timeout(max_wait, get_text_from_uri, args=[url])\n",
    "    except func_timeout.FunctionTimedOut:\n",
    "        pass\n",
    "    return default_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e6e95-5fe3-47bb-9d66-2a013828948e",
   "metadata": {},
   "source": [
    "y.\n",
    "\n",
    "- **Input**:\n",
    "  - `path`: The path to an Excel file containing a list of URIs.\n",
    "  - `filepath`: The directory path where the retrieved content should be saved.\n",
    "\n",
    "- **Output**:\n",
    "  - The function retrieves and saves content from the URIs to JSON files in the specified directory.\n",
    "  - The Excel file with updated status information for each URI (e.g., \"not_accessible,\" \"timeout,\" \"no_text\") is saved.\n",
    "\n",
    "## Function Workflow\n",
    "\n",
    "The function follows these steps:\n",
    "\n",
    "1. **Loading URI List**:\n",
    "   - It reads the Excel file at the specified `path` to obtain a list of URIs.\n",
    "\n",
    "2. **Iterating Over URIs**:\n",
    "   - For each URI in the list, it performs the following steps:\n",
    "     - Checks if a JSON file with the same index exists in the `filepath` directory. If not, it proceeds with content retrieval.\n",
    "     - If the URI is not marked as \"not_accessible,\" \"timeout,\" or \"no_text\" in the Excel file, it attempts to retrieve the content using the `ctrl_timeout` function.\n",
    "     - Handles different outcomes:\n",
    "       - If the retrieval process returns \"not_accessible,\" it updates the Excel file to mark the URI as not accessible.\n",
    "       - If the retrieval process times out, it updates the Excel file to mark the URI as a timeout.\n",
    "       - If the retrieval process returns no text content, it updates the Excel file to mark the URI as having no text.\n",
    "       - If the retrieval is successful, it saves the retrieved text content to a JSON file in the `filepath` directory.\n",
    "\n",
    "3. **Periodic Saving**:\n",
    "   - The function periodically saves the updated Excel file (every 100 iterations) to ensure that progress is recorded even in the event of an interruption.\n",
    "\n",
    "4. **Final Saving**:\n",
    "   - After processing all URIs, the function saves the final Excel fiibility and content retrieval times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaac39c0-0210-4cdf-9785-e46cc7563aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uri_content(path, filepath):\n",
    "    \n",
    "    uri_list = pd.read_excel(path)\n",
    "    \n",
    "    for idx, row in tqdm(uri_list.iterrows()):\n",
    "        \n",
    "        if not os.path.isfile(filepath+\"/\"+str(row[\"index\"])+\".json\"):\n",
    "            if not (row[\"not_accessible\"] == 1 or row[\"timeout\"] == 1 or row[\"no_text\"] == 1):\n",
    "               \n",
    "                text = ctrl_timeout(row[\"uri\"])\n",
    "                \n",
    "                if text == \"not_accessible\":\n",
    "                    uri_list.at[idx,\"not_accessible\"] = 1\n",
    "                elif text == \"timeout\":\n",
    "                    uri_list.at[idx,\"timeout\"] = 1\n",
    "                elif not text:\n",
    "                    uri_list.at[idx,\"no_text\"] = 1 \n",
    "                    \n",
    "                else:\n",
    "                    with open(filepath+\"/\"+str(row[\"index\"])+\".json\", \"w\") as f:\n",
    "                        json.dump(text, f, indent=2)\n",
    "                if idx%100 == 0:\n",
    "                    uri_list.to_excel(\"./data_created/CommonCrawl17/refined_uri_list.xlsx\", index = None)\n",
    "        \n",
    "    uri_list.to_excel(\"./data_created/CommonCrawl17/refined_uri_list.xlsx\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a081f91-f420-4fd7-9a16-5a6645a176d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13930it [35:33,  6.53it/s]\n"
     ]
    }
   ],
   "source": [
    "get_uri_content(\"./data_created/ClueWeb22/combined_uri_list.xlsx\", \"./data_created/ClueWeb22/text_files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
